Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu
Requirement already satisfied: numpy~=1.26.4 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.46.3)
Requirement already satisfied: gguf>=0.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.14.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.5)
Requirement already satisfied: torch~=2.2.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2)
Requirement already satisfied: filelock in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.26.2)
Requirement already satisfied: packaging>=20.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)
Requirement already satisfied: requests in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)
Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.3)
Requirement already satisfied: safetensors>=0.4.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.0)
Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)
Requirement already satisfied: sympy in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.3)
Requirement already satisfied: networkx in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)
Requirement already satisfied: jinja2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)
Requirement already satisfied: fsspec in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.10.0)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from jinja2->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from sympy->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)
Obtaining file:///Users/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.14.0) (1.26.4)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.14.0) (6.0.2)
Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.14.0) (0.2.0)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.14.0) (4.67.0)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.14.0-py3-none-any.whl size=3404 sha256=10c1de38d4415d9eb981c30bef48c3aed1ca9b6d304b24472bad825a0e7c8214
  Stored in directory: /private/var/folders/3z/y7wjb5257l1dj_dxrng2zq0h0000gn/T/pip-ephem-wheel-cache-q6svs4o6/wheels/57/e8/ad/c90c2fcd445a1780e6969131e27698e4f239a16d34ccd95852
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.14.0
    Uninstalling gguf-0.14.0:
      Successfully uninstalled gguf-0.14.0
Successfully installed gguf-0.14.0
+ gg_run_ctest_debug
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-debug

real	0m3.115s
user	0m0.941s
sys	0m1.345s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_debug-make.log
++ nproc
+ make -j10
[  0%] Generating build details from Git
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Built target build_info
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Built target llava
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target test-c
[ 37%] Built target llama-simple
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_shared
[ 37%] Built target llava_static
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-sampling
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-log
[ 50%] Built target test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Linking CXX executable ../bin/test-gguf
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 63%] Linking CXX executable ../bin/test-barrier
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-gguf
[ 64%] Built target test-chat-template
[ 64%] Built target llama-batched-bench
[ 64%] Built target test-backend-ops
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-barrier
[ 64%] Built target test-rope
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-embedding
[ 73%] Built target llama-batched
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-bench
[ 74%] Built target llama-infill
[ 74%] Built target llama-lookahead
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-lookup-create
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-cli
[ 83%] Linking CXX executable ../../bin/llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-lookup-create
[ 84%] Built target llama-cli
[ 84%] Built target llama-quantize
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-passkey
[ 85%] Generating index.html.gz.hpp
[ 85%] Built target llama-retrieval
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-parallel
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 89%] Linking CXX executable ../../bin/llama-cvector-generator
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-tts
[ 93%] Built target llama-run
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-cvector-generator
[ 93%] Built target llama-speculative
[ 93%] Built target llama-speculative-simple
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.257s
user	0m7.472s
sys	0m11.009s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.82 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.24 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.70 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.46 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.00 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.12 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.20 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.90 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.25 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.90 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.78 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.38 sec*proc (28 tests)

Total Test time (real) = 221.39 sec

real	3m41.527s
user	7m39.950s
sys	0m6.806s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.531s
user	0m0.865s
sys	0m1.217s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_release-make.log
++ nproc
+ make -j10
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target build_info
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Built target llava
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llama-simple
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Built target test-rope
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-batched
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Built target llama-gbnf-validator
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-imatrix
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-infill
[ 76%] Built target llama-gritlm
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Built target llama-bench
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-merge
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-cli
[ 84%] Built target llama-parallel
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-passkey
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 87%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-run
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-speculative
[ 92%] Built target llama-gen-docs
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.984s
user	0m6.222s
sys	0m9.591s
+ '[' -z ']'
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.43 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.20 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.13 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.40 sec*proc (28 tests)

Total Test time (real) =  51.41 sec

real	0m51.424s
user	1m11.640s
sys	0m5.404s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/embd_bge_small.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:25 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:26 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json [711396/711396] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:27 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:27 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:28 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:29 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:29 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:30 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:30 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.538s
user	0m0.856s
sys	0m1.222s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/embd_bge_small-make.log
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Built target llava_static
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-sampling
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Built target test-log
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-chat-template
[ 62%] Built target test-gguf
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-backend-ops
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Built target llama-batched-bench
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Built target test-rope
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-imatrix
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Built target llama-infill
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Built target llama-bench
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-stats
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-create
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-perplexity
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Built target llama-parallel
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Built target llama-speculative
[ 92%] Built target llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.104s
user	0m6.041s
sys	0m9.464s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/bge-small --outfile ../models-mnt/bge-small/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: bge-small
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,               torch.float32 --> F16, shape = {384, 30522}
INFO:hf-to-gguf:position_embd.weight,            torch.float32 --> F32, shape = {384, 512}
INFO:hf-to-gguf:token_types.weight,              torch.float32 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.4.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.4.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.4.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.5.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.5.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.5.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.6.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.6.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.6.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.7.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.7.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.7.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.8.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.8.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.8.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.9.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.9.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.9.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.10.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.10.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.10.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.11.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.11.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.11.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 512
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Setting special token type unk to 100
INFO:gguf.vocab:Setting special token type sep to 102
INFO:gguf.vocab:Setting special token type pad to 0
INFO:gguf.vocab:Setting special token type cls to 101
INFO:gguf.vocab:Setting special token type mask to 103
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/bge-small/ggml-model-f16.gguf: n_tensors = 197, total_size = 66.9M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/66.9M [00:00<?, ?byte/s]Writing:  37%|      | 24.5M/66.9M [00:00<00:00, 242Mbyte/s]Writing:  73%|  | 49.1M/66.9M [00:00<00:00, 222Mbyte/s]Writing: 100%|| 66.9M/66.9M [00:00<00:00, 216Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/bge-small/ggml-model-f16.gguf
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/llama-quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Bge Small
llama_model_loader: - kv   3:                           general.basename str              = bge
llama_model_loader: - kv   4:                         general.size_label str              = small
llama_model_loader: - kv   5:                           bert.block_count u32              = 12
llama_model_loader: - kv   6:                        bert.context_length u32              = 512
llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  124 tensors
llama_model_loader: - type  f16:   73 tensors
[   1/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f32, size =    0.750 MB
[   2/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, converting to q8_0 .. size =    22.35 MiB ->    11.88 MiB
[   3/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   6/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   7/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   8/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   9/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  10/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  11/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  13/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  14/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  16/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  17/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  18/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  19/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  20/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  23/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  24/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  25/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  26/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  27/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  29/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  30/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  32/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  33/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  34/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  35/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  36/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  39/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  40/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  41/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  42/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  43/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  45/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  46/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  48/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  49/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  50/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  51/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  52/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  55/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  56/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  57/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  58/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  59/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  61/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  62/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  64/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  65/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  66/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  67/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  68/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  71/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  72/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  73/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  74/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  75/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  77/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  78/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  80/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  81/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  82/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  83/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  84/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  87/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  88/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  89/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  90/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  91/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  93/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  94/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  96/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  97/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  98/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  99/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 100/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 103/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 104/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 105/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 106/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 107/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 109/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 110/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 112/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 113/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 114/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 115/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 116/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 119/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 120/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 121/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 122/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 123/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 125/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 126/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 128/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 129/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 130/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 131/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 132/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 135/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 136/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 137/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 138/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 139/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 141/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 142/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 144/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 145/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 146/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 147/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 148/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 151/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 152/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 153/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 154/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 155/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 157/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 158/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 160/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 161/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 162/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 163/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 164/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 167/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 168/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 169/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 170/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 171/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 173/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 174/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 176/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 177/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 178/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 179/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 180/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 183/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 184/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 185/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 186/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 187/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 189/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 190/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 192/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 193/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 194/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 195/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 196/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_impl: model size  =    63.84 MB
llama_model_quantize_impl: quant size  =    34.38 MB

main: quantize time =    74.35 ms
main:    total time =    74.35 ms
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/embd_bge_small-tg-f16.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.082 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.133 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.975 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.981 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.984 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.985 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.986 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.986 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.988 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.988 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.989 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.989 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.993 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.996 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.996 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.997 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.999 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.000 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.000 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.001 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.670 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.672 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.672 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.673 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.673 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.025.674 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.674 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.025.675 I llama_model_loader: - type  f32:  124 tensors
0.00.025.675 I llama_model_loader: - type  f16:   73 tensors
0.00.029.702 I llm_load_vocab: special tokens cache size = 5
0.00.031.940 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.031.944 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.031.944 I llm_load_print_meta: arch             = bert
0.00.031.945 I llm_load_print_meta: vocab type       = WPM
0.00.031.945 I llm_load_print_meta: n_vocab          = 30522
0.00.031.945 I llm_load_print_meta: n_merges         = 0
0.00.031.946 I llm_load_print_meta: vocab_only       = 0
0.00.031.946 I llm_load_print_meta: n_ctx_train      = 512
0.00.031.946 I llm_load_print_meta: n_embd           = 384
0.00.031.946 I llm_load_print_meta: n_layer          = 12
0.00.031.951 I llm_load_print_meta: n_head           = 12
0.00.031.952 I llm_load_print_meta: n_head_kv        = 12
0.00.031.952 I llm_load_print_meta: n_rot            = 32
0.00.031.952 I llm_load_print_meta: n_swa            = 0
0.00.031.952 I llm_load_print_meta: n_embd_head_k    = 32
0.00.031.953 I llm_load_print_meta: n_embd_head_v    = 32
0.00.031.953 I llm_load_print_meta: n_gqa            = 1
0.00.031.955 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.031.955 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.031.956 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.031.957 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.031.957 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.031.957 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.031.958 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.031.959 I llm_load_print_meta: n_ff             = 1536
0.00.031.959 I llm_load_print_meta: n_expert         = 0
0.00.031.959 I llm_load_print_meta: n_expert_used    = 0
0.00.031.959 I llm_load_print_meta: causal attn      = 0
0.00.031.959 I llm_load_print_meta: pooling type     = 2
0.00.031.960 I llm_load_print_meta: rope type        = 2
0.00.031.960 I llm_load_print_meta: rope scaling     = linear
0.00.031.961 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.031.961 I llm_load_print_meta: freq_scale_train = 1
0.00.031.961 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.031.962 I llm_load_print_meta: rope_finetuned   = unknown
0.00.031.962 I llm_load_print_meta: ssm_d_conv       = 0
0.00.031.962 I llm_load_print_meta: ssm_d_inner      = 0
0.00.031.962 I llm_load_print_meta: ssm_d_state      = 0
0.00.031.964 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.031.965 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.031.965 I llm_load_print_meta: model type       = 33M
0.00.031.989 I llm_load_print_meta: model ftype      = F16
0.00.031.990 I llm_load_print_meta: model params     = 33.21 M
0.00.031.991 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.031.991 I llm_load_print_meta: general.name     = Bge Small
0.00.031.992 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.031.992 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.031.992 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.031.993 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.031.993 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.031.993 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.031.993 I llm_load_print_meta: max token length = 21
0.00.034.018 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.034.018 I llm_load_tensors: offloading output layer to GPU
0.00.034.020 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.034.048 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.050 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.034.312 I llama_new_context_with_model: n_seq_max     = 1
0.00.034.313 I llama_new_context_with_model: n_ctx         = 512
0.00.034.313 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.034.314 I llama_new_context_with_model: n_batch       = 2048
0.00.034.314 I llama_new_context_with_model: n_ubatch      = 2048
0.00.034.314 I llama_new_context_with_model: flash_attn    = 0
0.00.034.315 I llama_new_context_with_model: freq_base     = 10000.0
0.00.034.315 I llama_new_context_with_model: freq_scale    = 1
0.00.034.316 I ggml_metal_init: allocating
0.00.034.320 I ggml_metal_init: found device: Apple M4
0.00.034.323 I ggml_metal_init: picking default device: Apple M4
0.00.035.163 I ggml_metal_init: using embedded metal library
0.00.039.177 I ggml_metal_init: GPU name:   Apple M4
0.00.039.180 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.180 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.181 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.181 I ggml_metal_init: simdgroup reduction   = true
0.00.039.181 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.181 I ggml_metal_init: has bfloat            = true
0.00.039.181 I ggml_metal_init: use bfloat            = true
0.00.039.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.183 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.050.989 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.533 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.535 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.536 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.052.283 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.052.285 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.052.285 I llama_new_context_with_model: graph nodes  = 429
0.00.052.285 I llama_new_context_with_model: graph splits = 2
0.00.052.286 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.052.287 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.058.689 I 
0.00.058.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.340 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.052 I llama_perf_context_print:        load time =      43.55 ms
0.00.064.058 I llama_perf_context_print: prompt eval time =       4.58 ms /     9 tokens (    0.51 ms per token,  1963.35 tokens per second)
0.00.064.058 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.059 I llama_perf_context_print:       total time =       5.36 ms /    10 tokens
0.00.064.189 I ggml_metal_free: deallocating

real	0m0.242s
user	0m0.047s
sys	0m0.027s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/embd_bge_small-tg-q8_0.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.177 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.861 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.865 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.866 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.869 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.869 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.869 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.870 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.870 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.871 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.871 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.871 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.873 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.874 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.874 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.874 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.875 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.875 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.369 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.026 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.027 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.028 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.028 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.028 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.028 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.029 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.015.029 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.015.029 I llama_model_loader: - type  f32:  124 tensors
0.00.015.030 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.378 I llm_load_vocab: special tokens cache size = 5
0.00.018.684 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.687 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.687 I llm_load_print_meta: arch             = bert
0.00.018.687 I llm_load_print_meta: vocab type       = WPM
0.00.018.687 I llm_load_print_meta: n_vocab          = 30522
0.00.018.688 I llm_load_print_meta: n_merges         = 0
0.00.018.688 I llm_load_print_meta: vocab_only       = 0
0.00.018.688 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.688 I llm_load_print_meta: n_embd           = 384
0.00.018.688 I llm_load_print_meta: n_layer          = 12
0.00.018.691 I llm_load_print_meta: n_head           = 12
0.00.018.694 I llm_load_print_meta: n_head_kv        = 12
0.00.018.694 I llm_load_print_meta: n_rot            = 32
0.00.018.695 I llm_load_print_meta: n_swa            = 0
0.00.018.695 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.695 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.696 I llm_load_print_meta: n_gqa            = 1
0.00.018.696 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.701 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.702 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.702 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.702 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.702 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.702 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.706 I llm_load_print_meta: n_ff             = 1536
0.00.018.706 I llm_load_print_meta: n_expert         = 0
0.00.018.706 I llm_load_print_meta: n_expert_used    = 0
0.00.018.707 I llm_load_print_meta: causal attn      = 0
0.00.018.707 I llm_load_print_meta: pooling type     = 2
0.00.018.707 I llm_load_print_meta: rope type        = 2
0.00.018.707 I llm_load_print_meta: rope scaling     = linear
0.00.018.707 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.708 I llm_load_print_meta: freq_scale_train = 1
0.00.018.708 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.708 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.708 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.708 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.708 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.708 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.708 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.709 I llm_load_print_meta: model type       = 33M
0.00.018.716 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.716 I llm_load_print_meta: model params     = 33.21 M
0.00.018.717 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.717 I llm_load_print_meta: general.name     = Bge Small
0.00.018.717 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.718 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.720 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.721 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.721 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.721 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.739 I llm_load_print_meta: max token length = 21
0.00.020.024 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.024 I llm_load_tensors: offloading output layer to GPU
0.00.020.024 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.033 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.034 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.197 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.198 I llama_new_context_with_model: n_ctx         = 512
0.00.020.198 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.198 I llama_new_context_with_model: n_batch       = 2048
0.00.020.198 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.199 I llama_new_context_with_model: flash_attn    = 0
0.00.020.199 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.199 I llama_new_context_with_model: freq_scale    = 1
0.00.020.200 I ggml_metal_init: allocating
0.00.020.203 I ggml_metal_init: found device: Apple M4
0.00.020.205 I ggml_metal_init: picking default device: Apple M4
0.00.020.792 I ggml_metal_init: using embedded metal library
0.00.023.260 I ggml_metal_init: GPU name:   Apple M4
0.00.023.262 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.263 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.263 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.263 I ggml_metal_init: simdgroup reduction   = true
0.00.023.263 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.264 I ggml_metal_init: has bfloat            = true
0.00.023.264 I ggml_metal_init: use bfloat            = true
0.00.023.264 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.265 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.515 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.001 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.003 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.005 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.635 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.636 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.636 I llama_new_context_with_model: graph nodes  = 429
0.00.034.637 I llama_new_context_with_model: graph splits = 2
0.00.034.638 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.015 I 
0.00.039.034 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.552 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.766 I llama_perf_context_print:        load time =      29.83 ms
0.00.043.767 I llama_perf_context_print: prompt eval time =       4.09 ms /     9 tokens (    0.45 ms per token,  2202.10 tokens per second)
0.00.043.768 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.768 I llama_perf_context_print:       total time =       4.75 ms /    10 tokens
0.00.043.936 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.030s
sys	0m0.015s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_rerank_tiny
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:39 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:40 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json [2030772/2030772] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:40 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json [1215/1215] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:41 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json [280/280] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json:
2025-01-10 20:55:42 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt:
2025-01-10 20:55:42 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json:
2025-01-10 20:55:43 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:43 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/1_Pooling https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
+ local out=models-mnt/rerank-tiny/1_Pooling
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/1_Pooling
+ cd models-mnt/rerank-tiny/1_Pooling
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json:
2025-01-10 20:55:43 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/rerank-tiny
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.548s
user	0m0.880s
sys	0m1.242s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny-make.log
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 50%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-sampling
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-log
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-gguf
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-barrier
[ 64%] Built target test-model-load-cancel
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-autorelease
[ 64%] Built target test-quantize-fns
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Built target test-rope
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Built target llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Built target llama-batched
[ 72%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-imatrix
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-gritlm
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Built target llama-bench
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Built target llama-lookahead
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Built target llama-lookup
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Built target llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-cli
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-stats
[ 84%] Generating loading.html.hpp
[ 84%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Built target llama-parallel
[ 86%] Built target llama-passkey
[ 86%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-quantize
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-save-load-state
[ 89%] Built target llama-run
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative-simple
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Built target llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-gen-docs
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.042s
user	0m6.102s
sys	0m9.691s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/rerank-tiny --outfile ../models-mnt/rerank-tiny/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: rerank-tiny
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,              torch.bfloat16 --> F16, shape = {384, 61056}
INFO:hf-to-gguf:token_types.weight,             torch.bfloat16 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,           torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:cls.weight,                     torch.bfloat16 --> F16, shape = {384, 1}
INFO:hf-to-gguf:cls.bias,                       torch.bfloat16 --> F32, shape = {1}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 39382 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 2
INFO:gguf.vocab:Setting special token type unk to 3
INFO:gguf.vocab:Setting special token type sep to 2
INFO:gguf.vocab:Setting special token type pad to 1
INFO:gguf.vocab:Setting special token type cls to 0
INFO:gguf.vocab:Setting special token type mask to 4
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/rerank-tiny/ggml-model-f16.gguf: n_tensors = 70, total_size = 65.8M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/65.8M [00:00<?, ?byte/s]Writing:  80%|  | 52.8M/65.8M [00:00<00:00, 521Mbyte/s]Writing: 100%|| 65.8M/65.8M [00:00<00:00, 492Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ model_f16=../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny-rk-f16.log
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.253 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.728 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.481 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.485 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.488 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.488 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.489 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.490 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.491 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.492 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.493 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.493 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.494 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.495 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.498 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.498 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.499 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.499 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.500 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.642 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.346 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.348 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.348 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.349 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.349 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.349 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.350 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.350 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.350 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.351 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.351 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.351 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.352 I llama_model_loader: - type  f32:   40 tensors
0.00.048.352 I llama_model_loader: - type  f16:   30 tensors
0.00.065.117 W llm_load_vocab: empty token at index 5
0.00.069.298 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.070.512 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.541 I llm_load_vocab: special tokens cache size = 5
0.00.326.866 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.326.872 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.326.872 I llm_load_print_meta: arch             = jina-bert-v2
0.00.326.873 I llm_load_print_meta: vocab type       = BPE
0.00.326.873 I llm_load_print_meta: n_vocab          = 61056
0.00.326.873 I llm_load_print_meta: n_merges         = 39382
0.00.326.873 I llm_load_print_meta: vocab_only       = 0
0.00.326.874 I llm_load_print_meta: n_ctx_train      = 8192
0.00.326.874 I llm_load_print_meta: n_embd           = 384
0.00.326.874 I llm_load_print_meta: n_layer          = 4
0.00.326.879 I llm_load_print_meta: n_head           = 12
0.00.326.879 I llm_load_print_meta: n_head_kv        = 12
0.00.326.880 I llm_load_print_meta: n_rot            = 32
0.00.326.880 I llm_load_print_meta: n_swa            = 0
0.00.326.880 I llm_load_print_meta: n_embd_head_k    = 32
0.00.326.880 I llm_load_print_meta: n_embd_head_v    = 32
0.00.326.880 I llm_load_print_meta: n_gqa            = 1
0.00.326.881 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.326.882 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.326.883 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.326.883 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.326.883 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.326.884 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.326.884 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.326.884 I llm_load_print_meta: n_ff             = 1536
0.00.326.885 I llm_load_print_meta: n_expert         = 0
0.00.326.885 I llm_load_print_meta: n_expert_used    = 0
0.00.326.885 I llm_load_print_meta: causal attn      = 0
0.00.326.885 I llm_load_print_meta: pooling type     = -1
0.00.326.885 I llm_load_print_meta: rope type        = -1
0.00.326.886 I llm_load_print_meta: rope scaling     = linear
0.00.326.886 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.326.887 I llm_load_print_meta: freq_scale_train = 1
0.00.326.889 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.326.889 I llm_load_print_meta: rope_finetuned   = unknown
0.00.326.889 I llm_load_print_meta: ssm_d_conv       = 0
0.00.326.890 I llm_load_print_meta: ssm_d_inner      = 0
0.00.326.890 I llm_load_print_meta: ssm_d_state      = 0
0.00.326.890 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.326.890 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.326.891 I llm_load_print_meta: model type       = 33M
0.00.326.911 I llm_load_print_meta: model ftype      = F16
0.00.326.911 I llm_load_print_meta: model params     = 32.90 M
0.00.326.911 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.326.912 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.326.912 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.326.912 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.326.914 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.326.914 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.326.915 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.326.915 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.326.915 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.326.915 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.326.916 I llm_load_print_meta: max token length = 45
0.00.328.062 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.328.063 I llm_load_tensors: offloading output layer to GPU
0.00.328.063 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.328.089 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.328.090 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.328.409 I llama_new_context_with_model: n_seq_max     = 1
0.00.328.410 I llama_new_context_with_model: n_ctx         = 8192
0.00.328.410 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.328.411 I llama_new_context_with_model: n_batch       = 2048
0.00.328.411 I llama_new_context_with_model: n_ubatch      = 2048
0.00.328.411 I llama_new_context_with_model: flash_attn    = 0
0.00.328.412 I llama_new_context_with_model: freq_base     = 10000.0
0.00.328.412 I llama_new_context_with_model: freq_scale    = 1
0.00.328.413 I ggml_metal_init: allocating
0.00.328.420 I ggml_metal_init: found device: Apple M4
0.00.328.423 I ggml_metal_init: picking default device: Apple M4
0.00.329.384 I ggml_metal_init: using embedded metal library
0.00.332.223 I ggml_metal_init: GPU name:   Apple M4
0.00.332.225 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.332.225 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.332.226 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.332.226 I ggml_metal_init: simdgroup reduction   = true
0.00.332.226 I ggml_metal_init: simdgroup matrix mul. = true
0.00.332.226 I ggml_metal_init: has bfloat            = true
0.00.332.226 I ggml_metal_init: use bfloat            = true
0.00.332.227 I ggml_metal_init: hasUnifiedMemory      = true
0.00.332.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.341.669 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.344.027 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.344.028 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.344.030 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.344.571 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.344.572 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.344.572 I llama_new_context_with_model: graph nodes  = 154
0.00.344.572 I llama_new_context_with_model: graph splits = 2
0.00.344.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.344.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.354.195 I 
0.00.354.214 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.354.363 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.354.364 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.354.367 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.354.367 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.354.372 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.354.372 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.354.866 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.358.514 I llama_perf_context_print:        load time =     332.46 ms
0.00.358.515 I llama_perf_context_print: prompt eval time =       3.64 ms /    62 tokens (    0.06 ms per token, 17042.33 tokens per second)
0.00.358.516 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.358.516 I llama_perf_context_print:       total time =       4.32 ms /    63 tokens
0.00.358.773 I ggml_metal_free: deallocating

real	0m1.080s
user	0m0.335s
sys	0m0.041s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 0'
+ check_score 'rerank score 0' 'rerank score 0:    0.023' 0.00 0.05
+ qnt='rerank score 0'
++ echo 'rerank score 0:    0.023'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.023
++ echo '0.023 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.023 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 0' 0.023
+ return 0
  - rerank score 0 @ 0.023 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 1'
+ check_score 'rerank score 1' 'rerank score 1:    0.024' 0.00 0.05
+ qnt='rerank score 1'
++ echo 'rerank score 1:    0.024'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.024
++ echo '0.024 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.024 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 1' 0.024
+ return 0
  - rerank score 1 @ 0.024 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 2'
+ check_score 'rerank score 2' 'rerank score 2:    0.199' 0.10 0.30
+ qnt='rerank score 2'
++ echo 'rerank score 2:    0.199'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.199
++ echo '0.199 < 0.10'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.199 > 0.30'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 2' 0.199
+ return 0
  - rerank score 2 @ 0.199 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_pythia_1_4b
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:52 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json [570/570] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:53 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json [2113710/2113710] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:53 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json [396/396] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-01-10 20:55:54 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json [99/99] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /Users/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ head -n 60 models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ path_models=../models-mnt/pythia/1.4B
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.606s
user	0m0.892s
sys	0m1.243s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-make.log
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-sampling
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-arg-parser
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Built target test-gguf
[ 63%] Built target test-backend-ops
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-chat-template
[ 64%] Built target test-autorelease
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Built target test-rope
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Built target llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-batched
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-bench
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-lookup-create
[ 81%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-stats
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-passkey
[ 84%] Built target llama-parallel
[ 84%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-quantize
[ 85%] Built target llama-lookup
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-run
[ 87%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 92%] Built target llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 93%] Built target llama-gen-docs
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.118s
user	0m5.984s
sys	0m9.477s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/pythia/1.4B --outfile ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: 1.4B
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.0.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.0.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.0.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.0.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.1.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.1.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.1.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.1.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.2.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.2.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.2.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.2.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.3.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.3.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.3.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.3.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.4.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.4.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.4.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.4.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.5.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.5.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.5.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.5.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.6.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.6.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.6.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.6.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.7.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.7.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.7.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.7.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.8.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.8.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.8.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.8.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.9.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.9.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.9.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.9.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.10.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.10.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.10.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.10.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.11.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.11.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.11.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.11.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.12.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.12.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.12.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.12.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.13.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.13.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.13.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.13.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.14.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.14.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.14.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.14.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.15.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.15.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.15.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.15.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.16.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.16.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.16.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.16.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.17.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.17.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.17.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.17.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.18.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.18.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.18.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.18.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.19.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.19.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.19.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.19.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.20.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.20.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.20.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.20.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.21.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.21.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.21.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.21.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.22.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.22.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.22.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.22.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.23.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.23.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.23.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.23.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.bias,          torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output.weight,             torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 50009 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 0
INFO:gguf.vocab:Setting special token type unk to 0
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/pythia/1.4B/ggml-model-f16.gguf: n_tensors = 292, total_size = 2.8G
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/2.83G [00:00<?, ?byte/s]Writing:   7%|         | 206M/2.83G [00:01<00:21, 120Mbyte/s]Writing:   8%|         | 240M/2.83G [00:01<00:19, 130Mbyte/s]Writing:  10%|         | 273M/2.83G [00:02<00:19, 130Mbyte/s]Writing:  11%|         | 307M/2.83G [00:02<00:20, 126Mbyte/s]Writing:  13%|        | 374M/2.83G [00:02<00:15, 159Mbyte/s]Writing:  15%|        | 433M/2.83G [00:02<00:11, 207Mbyte/s]Writing:  18%|        | 508M/2.83G [00:02<00:08, 262Mbyte/s]Writing:  20%|        | 576M/2.83G [00:03<00:07, 316Mbyte/s]Writing:  23%|       | 643M/2.83G [00:03<00:06, 361Mbyte/s]Writing:  25%|       | 710M/2.83G [00:03<00:05, 389Mbyte/s]Writing:  27%|       | 777M/2.83G [00:03<00:04, 425Mbyte/s]Writing:  30%|       | 844M/2.83G [00:03<00:04, 459Mbyte/s]Writing:  32%|      | 911M/2.83G [00:03<00:04, 465Mbyte/s]Writing:  35%|      | 979M/2.83G [00:03<00:03, 487Mbyte/s]Writing:  37%|      | 1.05G/2.83G [00:03<00:03, 503Mbyte/s]Writing:  39%|      | 1.11G/2.83G [00:04<00:03, 481Mbyte/s]Writing:  42%|     | 1.18G/2.83G [00:04<00:03, 486Mbyte/s]Writing:  44%|     | 1.24G/2.83G [00:04<00:03, 503Mbyte/s]Writing:  46%|     | 1.31G/2.83G [00:04<00:03, 459Mbyte/s]Writing:  49%|     | 1.38G/2.83G [00:04<00:03, 470Mbyte/s]Writing:  51%|     | 1.44G/2.83G [00:04<00:02, 488Mbyte/s]Writing:  54%|    | 1.52G/2.83G [00:05<00:02, 459Mbyte/s]Writing:  56%|    | 1.58G/2.83G [00:05<00:02, 472Mbyte/s]Writing:  58%|    | 1.64G/2.83G [00:05<00:02, 484Mbyte/s]Writing:  61%|    | 1.72G/2.83G [00:05<00:02, 454Mbyte/s]Writing:  63%|   | 1.78G/2.83G [00:05<00:02, 459Mbyte/s]Writing:  65%|   | 1.84G/2.83G [00:05<00:02, 480Mbyte/s]Writing:  68%|   | 1.92G/2.83G [00:05<00:02, 454Mbyte/s]Writing:  70%|   | 1.99G/2.83G [00:06<00:01, 469Mbyte/s]Writing:  72%|  | 2.05G/2.83G [00:06<00:01, 477Mbyte/s]Writing:  75%|  | 2.12G/2.83G [00:06<00:01, 456Mbyte/s]Writing:  77%|  | 2.19G/2.83G [00:06<00:01, 465Mbyte/s]Writing:  79%|  | 2.25G/2.83G [00:06<00:01, 476Mbyte/s]Writing:  82%| | 2.32G/2.83G [00:06<00:01, 458Mbyte/s]Writing:  84%| | 2.39G/2.83G [00:06<00:00, 462Mbyte/s]Writing:  86%| | 2.45G/2.83G [00:06<00:00, 470Mbyte/s]Writing:  89%| | 2.52G/2.83G [00:07<00:00, 446Mbyte/s]Writing:  92%|| 2.59G/2.83G [00:07<00:00, 451Mbyte/s]Writing: 100%|| 2.83G/2.83G [00:07<00:00, 398Mbyte/s]Writing: 100%|| 2.83G/2.83G [00:08<00:00, 354Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_f16=../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf
+ wiki_test_60=../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf q8_0
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1435.23 MB

main: quantize time =  4019.23 ms
main:    total time =  4019.23 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf q4_0
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_0 .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   786.31 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf q4_1

main: quantize time =  2026.75 ms
main:    total time =  2026.75 ms
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_1 .. size =   196.50 MiB ->    61.41 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   864.46 MB

main: quantize time =  1909.82 ms
main:    total time =  1909.82 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf q5_0
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_0 .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   942.60 MB

main: quantize time =  2588.16 ms
main:    total time =  2588.16 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf q5_1
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_1 .. size =   196.50 MiB ->    73.69 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1020.74 MB

main: quantize time =  1427.09 ms
main:    total time =  1427.09 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf q2_k
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q2_K .. size =   196.50 MiB ->    32.24 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   542.04 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf q3_k

main: quantize time =  5238.77 ms
main:    total time =  5238.77 ms
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q3_K .. size =   196.50 MiB ->    42.22 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   724.27 MB

main: quantize time =  5582.61 ms
main:    total time =  5582.61 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf q4_k
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_K .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   871.81 MB

main: quantize time =  6621.32 ms
main:    total time =  6621.32 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf q5_k
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_K .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1006.35 MB

main: quantize time =  5864.16 ms
main:    total time =  5864.16 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf q6_k
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1108.64 MB

main: quantize time =  4778.65 ms
main:    total time =  4778.65 ms
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.162 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.283 I main: llama backend init
0.00.000.288 I main: load the model and apply lora adapter, if any
0.00.047.143 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.059.714 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.059.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.059.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.059.737 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.059.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.059.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.059.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.059.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.059.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.059.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.059.745 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.059.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.059.746 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.059.747 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.059.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.059.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.059.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.066.625 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.068.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.309 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.076.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.318 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.319 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.319 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.321 I llama_model_loader: - type  f32:  194 tensors
0.00.076.321 I llama_model_loader: - type  f16:   98 tensors
0.00.114.323 I llm_load_vocab: special tokens cache size = 25
0.00.121.721 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.121.725 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.121.725 I llm_load_print_meta: arch             = gptneox
0.00.121.725 I llm_load_print_meta: vocab type       = BPE
0.00.121.726 I llm_load_print_meta: n_vocab          = 50304
0.00.121.726 I llm_load_print_meta: n_merges         = 50009
0.00.121.726 I llm_load_print_meta: vocab_only       = 0
0.00.121.726 I llm_load_print_meta: n_ctx_train      = 2048
0.00.121.726 I llm_load_print_meta: n_embd           = 2048
0.00.121.726 I llm_load_print_meta: n_layer          = 24
0.00.121.730 I llm_load_print_meta: n_head           = 16
0.00.121.731 I llm_load_print_meta: n_head_kv        = 16
0.00.121.731 I llm_load_print_meta: n_rot            = 32
0.00.121.732 I llm_load_print_meta: n_swa            = 0
0.00.121.732 I llm_load_print_meta: n_embd_head_k    = 128
0.00.121.734 I llm_load_print_meta: n_embd_head_v    = 128
0.00.121.735 I llm_load_print_meta: n_gqa            = 1
0.00.121.736 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.121.736 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.121.737 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.121.737 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.121.737 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.121.737 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.121.738 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.121.738 I llm_load_print_meta: n_ff             = 8192
0.00.121.738 I llm_load_print_meta: n_expert         = 0
0.00.121.739 I llm_load_print_meta: n_expert_used    = 0
0.00.121.739 I llm_load_print_meta: causal attn      = 1
0.00.121.739 I llm_load_print_meta: pooling type     = 0
0.00.121.739 I llm_load_print_meta: rope type        = 2
0.00.121.739 I llm_load_print_meta: rope scaling     = linear
0.00.121.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.121.740 I llm_load_print_meta: freq_scale_train = 1
0.00.121.740 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.121.740 I llm_load_print_meta: rope_finetuned   = unknown
0.00.121.740 I llm_load_print_meta: ssm_d_conv       = 0
0.00.121.742 I llm_load_print_meta: ssm_d_inner      = 0
0.00.121.742 I llm_load_print_meta: ssm_d_state      = 0
0.00.121.742 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.121.743 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.121.743 I llm_load_print_meta: model type       = 1.4B
0.00.121.762 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.121.763 I llm_load_print_meta: model params     = 1.41 B
0.00.121.763 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.121.764 I llm_load_print_meta: general.name     = 1.4B
0.00.121.764 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.121.764 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.121.764 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.121.764 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.121.765 I llm_load_print_meta: LF token         = 128 ''
0.00.121.765 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.121.765 I llm_load_print_meta: max token length = 1024
0.00.124.440 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.124.440 I llm_load_tensors: offloading output layer to GPU
0.00.124.440 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.124.459 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.124.461 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.124.810 I llama_new_context_with_model: n_seq_max     = 1
0.00.124.810 I llama_new_context_with_model: n_ctx         = 2048
0.00.124.811 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.124.811 I llama_new_context_with_model: n_batch       = 2048
0.00.124.811 I llama_new_context_with_model: n_ubatch      = 512
0.00.124.811 I llama_new_context_with_model: flash_attn    = 0
0.00.124.812 I llama_new_context_with_model: freq_base     = 10000.0
0.00.124.812 I llama_new_context_with_model: freq_scale    = 1
0.00.124.812 I ggml_metal_init: allocating
0.00.124.816 I ggml_metal_init: found device: Apple M4
0.00.124.818 I ggml_metal_init: picking default device: Apple M4
0.00.125.536 I ggml_metal_init: using embedded metal library
0.00.137.319 I ggml_metal_init: GPU name:   Apple M4
0.00.137.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.137.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.137.321 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.137.322 I ggml_metal_init: simdgroup reduction   = true
0.00.137.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.137.322 I ggml_metal_init: has bfloat            = true
0.00.137.322 I ggml_metal_init: use bfloat            = true
0.00.137.323 I ggml_metal_init: hasUnifiedMemory      = true
0.00.137.323 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.161.765 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.183.862 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.183.871 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.183.891 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.184.853 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.184.855 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.184.856 I llama_new_context_with_model: graph nodes  = 967
0.00.184.856 I llama_new_context_with_model: graph splits = 2
0.00.184.859 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.184.995 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.184.996 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.267.841 I main: llama threadpool init, n_threads = 4
0.00.267.890 I 
0.00.267.918 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.267.919 I 
0.00.268.169 I sampler seed: 1234
0.00.268.175 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.268.201 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.268.203 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.268.203 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.102.684 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.02.102.685 I llama_perf_context_print:        load time =     220.68 ms
0.02.102.686 I llama_perf_context_print: prompt eval time =      44.09 ms /     7 tokens (    6.30 ms per token,   158.78 tokens per second)
0.02.102.686 I llama_perf_context_print:        eval time =    1787.45 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.102.687 I llama_perf_context_print:       total time =    1834.85 ms /    70 tokens
0.02.102.897 I ggml_metal_free: deallocating

real	0m2.410s
user	0m0.151s
sys	0m0.107s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.807 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.431 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.445 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.447 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.447 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.447 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.448 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.448 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.448 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.451 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.451 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.452 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.485 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.485 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.485 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.486 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.486 I llama_model_loader: - type  f32:  194 tensors
0.00.039.487 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.684 I llm_load_vocab: special tokens cache size = 25
0.00.068.968 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.972 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.973 I llm_load_print_meta: arch             = gptneox
0.00.068.973 I llm_load_print_meta: vocab type       = BPE
0.00.068.973 I llm_load_print_meta: n_vocab          = 50304
0.00.068.975 I llm_load_print_meta: n_merges         = 50009
0.00.068.975 I llm_load_print_meta: vocab_only       = 0
0.00.068.975 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.975 I llm_load_print_meta: n_embd           = 2048
0.00.068.975 I llm_load_print_meta: n_layer          = 24
0.00.068.981 I llm_load_print_meta: n_head           = 16
0.00.068.982 I llm_load_print_meta: n_head_kv        = 16
0.00.068.982 I llm_load_print_meta: n_rot            = 32
0.00.068.982 I llm_load_print_meta: n_swa            = 0
0.00.068.982 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.982 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.983 I llm_load_print_meta: n_gqa            = 1
0.00.068.984 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.984 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.985 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.985 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.986 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.986 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.988 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.988 I llm_load_print_meta: n_ff             = 8192
0.00.068.988 I llm_load_print_meta: n_expert         = 0
0.00.068.989 I llm_load_print_meta: n_expert_used    = 0
0.00.068.989 I llm_load_print_meta: causal attn      = 1
0.00.068.989 I llm_load_print_meta: pooling type     = 0
0.00.068.989 I llm_load_print_meta: rope type        = 2
0.00.068.989 I llm_load_print_meta: rope scaling     = linear
0.00.068.989 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.990 I llm_load_print_meta: freq_scale_train = 1
0.00.068.990 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.990 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.990 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.990 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.990 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.990 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.991 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.991 I llm_load_print_meta: model type       = 1.4B
0.00.069.004 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.004 I llm_load_print_meta: model params     = 1.41 B
0.00.069.005 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.005 I llm_load_print_meta: general.name     = 1.4B
0.00.069.005 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.005 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.006 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.006 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.007 I llm_load_print_meta: LF token         = 128 ''
0.00.069.007 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.007 I llm_load_print_meta: max token length = 1024
0.00.071.528 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.528 I llm_load_tensors: offloading output layer to GPU
0.00.071.528 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.539 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.541 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.922 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.923 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.923 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.923 I llama_new_context_with_model: n_batch       = 2048
0.00.071.923 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.923 I llama_new_context_with_model: flash_attn    = 0
0.00.071.924 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.924 I llama_new_context_with_model: freq_scale    = 1
0.00.071.925 I ggml_metal_init: allocating
0.00.071.929 I ggml_metal_init: found device: Apple M4
0.00.071.931 I ggml_metal_init: picking default device: Apple M4
0.00.072.715 I ggml_metal_init: using embedded metal library
0.00.075.344 I ggml_metal_init: GPU name:   Apple M4
0.00.075.345 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.346 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.346 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.347 I ggml_metal_init: simdgroup reduction   = true
0.00.075.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.347 I ggml_metal_init: has bfloat            = true
0.00.075.347 I ggml_metal_init: use bfloat            = true
0.00.075.348 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.348 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.262 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.112.130 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.138 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.163 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.396 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.398 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.399 I llama_new_context_with_model: graph nodes  = 967
0.00.113.399 I llama_new_context_with_model: graph splits = 2
0.00.113.404 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.113.532 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.533 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.317.726 I main: llama threadpool init, n_threads = 4
0.01.317.774 I 
0.01.317.811 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.317.812 I 
0.01.318.190 I sampler seed: 1234
0.01.318.198 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.318.225 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.318.227 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.318.227 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.414.297 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.02.414.298 I llama_perf_context_print:        load time =    1307.91 ms
0.02.414.299 I llama_perf_context_print: prompt eval time =      50.38 ms /     7 tokens (    7.20 ms per token,   138.94 tokens per second)
0.02.414.300 I llama_perf_context_print:        eval time =    1042.73 ms /    63 runs   (   16.55 ms per token,    60.42 tokens per second)
0.02.414.300 I llama_perf_context_print:       total time =    1096.57 ms /    70 tokens
0.02.414.556 I ggml_metal_free: deallocating

real	0m2.434s
user	0m0.125s
sys	0m0.250s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.013.026 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.175 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.176 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.176 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.176 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.178 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.181 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.186 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.186 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.186 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.152 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.265 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.406 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.407 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.407 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.407 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.408 I llama_model_loader: - type  f32:  194 tensors
0.00.043.408 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.408 I llama_model_loader: - type q6_K:    1 tensors
0.00.069.105 I llm_load_vocab: special tokens cache size = 25
0.00.077.585 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.589 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.590 I llm_load_print_meta: arch             = gptneox
0.00.077.590 I llm_load_print_meta: vocab type       = BPE
0.00.077.590 I llm_load_print_meta: n_vocab          = 50304
0.00.077.591 I llm_load_print_meta: n_merges         = 50009
0.00.077.591 I llm_load_print_meta: vocab_only       = 0
0.00.077.591 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.591 I llm_load_print_meta: n_embd           = 2048
0.00.077.592 I llm_load_print_meta: n_layer          = 24
0.00.077.597 I llm_load_print_meta: n_head           = 16
0.00.077.598 I llm_load_print_meta: n_head_kv        = 16
0.00.077.598 I llm_load_print_meta: n_rot            = 32
0.00.077.598 I llm_load_print_meta: n_swa            = 0
0.00.077.599 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.599 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.601 I llm_load_print_meta: n_gqa            = 1
0.00.077.602 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.603 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.604 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.604 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.604 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.606 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.606 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.607 I llm_load_print_meta: n_ff             = 8192
0.00.077.607 I llm_load_print_meta: n_expert         = 0
0.00.077.608 I llm_load_print_meta: n_expert_used    = 0
0.00.077.610 I llm_load_print_meta: causal attn      = 1
0.00.077.615 I llm_load_print_meta: pooling type     = 0
0.00.077.615 I llm_load_print_meta: rope type        = 2
0.00.077.615 I llm_load_print_meta: rope scaling     = linear
0.00.077.616 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.616 I llm_load_print_meta: freq_scale_train = 1
0.00.077.616 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.617 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.617 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.617 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.617 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.617 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.617 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.619 I llm_load_print_meta: model type       = 1.4B
0.00.077.633 I llm_load_print_meta: model ftype      = Q4_0
0.00.077.634 I llm_load_print_meta: model params     = 1.41 B
0.00.077.634 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.077.635 I llm_load_print_meta: general.name     = 1.4B
0.00.077.635 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.635 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.635 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.635 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.636 I llm_load_print_meta: LF token         = 128 ''
0.00.077.636 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.636 I llm_load_print_meta: max token length = 1024
0.00.080.340 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.340 I llm_load_tensors: offloading output layer to GPU
0.00.080.341 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.353 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.080.354 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.080.833 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.834 I llama_new_context_with_model: n_ctx         = 2048
0.00.080.834 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.080.834 I llama_new_context_with_model: n_batch       = 2048
0.00.080.835 I llama_new_context_with_model: n_ubatch      = 512
0.00.080.835 I llama_new_context_with_model: flash_attn    = 0
0.00.080.835 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.836 I llama_new_context_with_model: freq_scale    = 1
0.00.080.837 I ggml_metal_init: allocating
0.00.080.845 I ggml_metal_init: found device: Apple M4
0.00.080.848 I ggml_metal_init: picking default device: Apple M4
0.00.081.778 I ggml_metal_init: using embedded metal library
0.00.085.569 I ggml_metal_init: GPU name:   Apple M4
0.00.085.571 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.573 I ggml_metal_init: simdgroup reduction   = true
0.00.085.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.573 I ggml_metal_init: has bfloat            = true
0.00.085.575 I ggml_metal_init: use bfloat            = true
0.00.085.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.742 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.123.495 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.503 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.532 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.599 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.601 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.601 I llama_new_context_with_model: graph nodes  = 967
0.00.124.601 I llama_new_context_with_model: graph splits = 2
0.00.124.606 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.124.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.124.750 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.377 I main: llama threadpool init, n_threads = 4
0.00.740.418 I 
0.00.740.440 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.441 I 
0.00.740.671 I sampler seed: 1234
0.00.740.676 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.728 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.730 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.730 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.427.507 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.427.508 I llama_perf_context_print:        load time =     727.35 ms
0.01.427.509 I llama_perf_context_print: prompt eval time =      45.09 ms /     7 tokens (    6.44 ms per token,   155.23 tokens per second)
0.01.427.510 I llama_perf_context_print:        eval time =     638.60 ms /    63 runs   (   10.14 ms per token,    98.65 tokens per second)
0.01.427.510 I llama_perf_context_print:       total time =     687.13 ms /    70 tokens
0.01.427.739 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.124s
sys	0m0.156s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.100 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.894 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.898 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.900 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.900 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.904 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.905 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.905 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.905 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.906 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.906 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.909 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.913 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.605 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.606 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.606 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.606 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.606 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.607 I llama_model_loader: - type  f32:  194 tensors
0.00.034.607 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.608 I llama_model_loader: - type q6_K:    1 tensors
0.00.057.009 I llm_load_vocab: special tokens cache size = 25
0.00.063.191 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.193 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.194 I llm_load_print_meta: arch             = gptneox
0.00.063.194 I llm_load_print_meta: vocab type       = BPE
0.00.063.194 I llm_load_print_meta: n_vocab          = 50304
0.00.063.194 I llm_load_print_meta: n_merges         = 50009
0.00.063.195 I llm_load_print_meta: vocab_only       = 0
0.00.063.195 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.195 I llm_load_print_meta: n_embd           = 2048
0.00.063.195 I llm_load_print_meta: n_layer          = 24
0.00.063.198 I llm_load_print_meta: n_head           = 16
0.00.063.199 I llm_load_print_meta: n_head_kv        = 16
0.00.063.199 I llm_load_print_meta: n_rot            = 32
0.00.063.199 I llm_load_print_meta: n_swa            = 0
0.00.063.199 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.200 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.200 I llm_load_print_meta: n_gqa            = 1
0.00.063.201 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.202 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.203 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.203 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.203 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.203 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.203 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.204 I llm_load_print_meta: n_ff             = 8192
0.00.063.204 I llm_load_print_meta: n_expert         = 0
0.00.063.204 I llm_load_print_meta: n_expert_used    = 0
0.00.063.206 I llm_load_print_meta: causal attn      = 1
0.00.063.208 I llm_load_print_meta: pooling type     = 0
0.00.063.208 I llm_load_print_meta: rope type        = 2
0.00.063.208 I llm_load_print_meta: rope scaling     = linear
0.00.063.209 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.209 I llm_load_print_meta: freq_scale_train = 1
0.00.063.209 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.209 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.209 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.210 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.210 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.210 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.210 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.210 I llm_load_print_meta: model type       = 1.4B
0.00.063.221 I llm_load_print_meta: model ftype      = Q4_1
0.00.063.222 I llm_load_print_meta: model params     = 1.41 B
0.00.063.222 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.063.223 I llm_load_print_meta: general.name     = 1.4B
0.00.063.223 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.223 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.223 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.224 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.225 I llm_load_print_meta: LF token         = 128 ''
0.00.063.225 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.225 I llm_load_print_meta: max token length = 1024
0.00.065.150 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.150 I llm_load_tensors: offloading output layer to GPU
0.00.065.150 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.161 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.065.162 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.065.515 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.516 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.516 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.516 I llama_new_context_with_model: n_batch       = 2048
0.00.065.516 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.516 I llama_new_context_with_model: flash_attn    = 0
0.00.065.517 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.517 I llama_new_context_with_model: freq_scale    = 1
0.00.065.518 I ggml_metal_init: allocating
0.00.065.526 I ggml_metal_init: found device: Apple M4
0.00.065.529 I ggml_metal_init: picking default device: Apple M4
0.00.066.143 I ggml_metal_init: using embedded metal library
0.00.068.448 I ggml_metal_init: GPU name:   Apple M4
0.00.068.449 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.450 I ggml_metal_init: simdgroup reduction   = true
0.00.068.451 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.451 I ggml_metal_init: has bfloat            = true
0.00.068.452 I ggml_metal_init: use bfloat            = true
0.00.068.453 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.457 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.782 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.099.183 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.191 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.211 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.390 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.100.392 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.100.392 I llama_new_context_with_model: graph nodes  = 967
0.00.100.392 I llama_new_context_with_model: graph splits = 2
0.00.100.395 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.548 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.549 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.490 I main: llama threadpool init, n_threads = 4
0.00.805.532 I 
0.00.805.559 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.805.559 I 
0.00.805.792 I sampler seed: 1234
0.00.805.799 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.810 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.810 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.810 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.540.436 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.540.437 I llama_perf_context_print:        load time =     795.39 ms
0.01.540.438 I llama_perf_context_print: prompt eval time =      43.55 ms /     7 tokens (    6.22 ms per token,   160.75 tokens per second)
0.01.540.439 I llama_perf_context_print:        eval time =     688.56 ms /    63 runs   (   10.93 ms per token,    91.50 tokens per second)
0.01.540.439 I llama_perf_context_print:       total time =     734.95 ms /    70 tokens
0.01.540.682 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.113s
sys	0m0.146s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.199 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.433 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.435 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.438 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.438 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.442 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.046 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.047 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.047 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.048 I llama_model_loader: - type  f32:  194 tensors
0.00.027.048 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.048 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.661 I llm_load_vocab: special tokens cache size = 25
0.00.053.477 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.480 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.480 I llm_load_print_meta: arch             = gptneox
0.00.053.481 I llm_load_print_meta: vocab type       = BPE
0.00.053.481 I llm_load_print_meta: n_vocab          = 50304
0.00.053.481 I llm_load_print_meta: n_merges         = 50009
0.00.053.481 I llm_load_print_meta: vocab_only       = 0
0.00.053.482 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.482 I llm_load_print_meta: n_embd           = 2048
0.00.053.482 I llm_load_print_meta: n_layer          = 24
0.00.053.485 I llm_load_print_meta: n_head           = 16
0.00.053.486 I llm_load_print_meta: n_head_kv        = 16
0.00.053.486 I llm_load_print_meta: n_rot            = 32
0.00.053.487 I llm_load_print_meta: n_swa            = 0
0.00.053.487 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.490 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.490 I llm_load_print_meta: n_gqa            = 1
0.00.053.491 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.492 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.492 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.494 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.494 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.494 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.495 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.495 I llm_load_print_meta: n_ff             = 8192
0.00.053.495 I llm_load_print_meta: n_expert         = 0
0.00.053.496 I llm_load_print_meta: n_expert_used    = 0
0.00.053.497 I llm_load_print_meta: causal attn      = 1
0.00.053.497 I llm_load_print_meta: pooling type     = 0
0.00.053.497 I llm_load_print_meta: rope type        = 2
0.00.053.498 I llm_load_print_meta: rope scaling     = linear
0.00.053.498 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.498 I llm_load_print_meta: freq_scale_train = 1
0.00.053.502 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.502 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.503 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.503 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.503 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.503 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.506 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.506 I llm_load_print_meta: model type       = 1.4B
0.00.053.518 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.518 I llm_load_print_meta: model params     = 1.41 B
0.00.053.519 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.519 I llm_load_print_meta: general.name     = 1.4B
0.00.053.519 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.520 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.520 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.520 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.520 I llm_load_print_meta: LF token         = 128 ''
0.00.053.520 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.520 I llm_load_print_meta: max token length = 1024
0.00.055.514 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.515 I llm_load_tensors: offloading output layer to GPU
0.00.055.515 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.525 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.527 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.878 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.879 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.879 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.879 I llama_new_context_with_model: n_batch       = 2048
0.00.055.879 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.879 I llama_new_context_with_model: flash_attn    = 0
0.00.055.880 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.880 I llama_new_context_with_model: freq_scale    = 1
0.00.055.880 I ggml_metal_init: allocating
0.00.055.883 I ggml_metal_init: found device: Apple M4
0.00.055.885 I ggml_metal_init: picking default device: Apple M4
0.00.056.488 I ggml_metal_init: using embedded metal library
0.00.058.870 I ggml_metal_init: GPU name:   Apple M4
0.00.058.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.872 I ggml_metal_init: simdgroup reduction   = true
0.00.058.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.873 I ggml_metal_init: has bfloat            = true
0.00.058.873 I ggml_metal_init: use bfloat            = true
0.00.058.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.296 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.209 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.215 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.244 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.197 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.199 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.199 I llama_new_context_with_model: graph nodes  = 967
0.00.090.200 I llama_new_context_with_model: graph splits = 2
0.00.090.203 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.345 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.830.566 I main: llama threadpool init, n_threads = 4
0.00.830.601 I 
0.00.830.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.830.624 I 
0.00.830.854 I sampler seed: 1234
0.00.830.858 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.830.897 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.830.901 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.830.902 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.619.985 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.619.986 I llama_perf_context_print:        load time =     821.37 ms
0.01.619.987 I llama_perf_context_print: prompt eval time =      43.14 ms /     7 tokens (    6.16 ms per token,   162.27 tokens per second)
0.01.619.988 I llama_perf_context_print:        eval time =     742.94 ms /    63 runs   (   11.79 ms per token,    84.80 tokens per second)
0.01.619.989 I llama_perf_context_print:       total time =     789.42 ms /    70 tokens
0.01.620.246 I ggml_metal_free: deallocating

real	0m1.641s
user	0m0.110s
sys	0m0.158s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.030.540 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.249 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.044.255 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.262 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.264 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.265 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.265 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.266 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.269 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.269 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.031 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.833 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.834 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.834 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.834 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.835 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.054.835 I llama_model_loader: - type  f32:  194 tensors
0.00.054.836 I llama_model_loader: - type q5_1:   97 tensors
0.00.054.836 I llama_model_loader: - type q6_K:    1 tensors
0.00.077.789 I llm_load_vocab: special tokens cache size = 25
0.00.083.636 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.641 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.642 I llm_load_print_meta: arch             = gptneox
0.00.083.642 I llm_load_print_meta: vocab type       = BPE
0.00.083.642 I llm_load_print_meta: n_vocab          = 50304
0.00.083.642 I llm_load_print_meta: n_merges         = 50009
0.00.083.643 I llm_load_print_meta: vocab_only       = 0
0.00.083.643 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.643 I llm_load_print_meta: n_embd           = 2048
0.00.083.645 I llm_load_print_meta: n_layer          = 24
0.00.083.649 I llm_load_print_meta: n_head           = 16
0.00.083.652 I llm_load_print_meta: n_head_kv        = 16
0.00.083.653 I llm_load_print_meta: n_rot            = 32
0.00.083.653 I llm_load_print_meta: n_swa            = 0
0.00.083.653 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.653 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.654 I llm_load_print_meta: n_gqa            = 1
0.00.083.655 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.656 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.657 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.658 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.658 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.658 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.658 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.659 I llm_load_print_meta: n_ff             = 8192
0.00.083.659 I llm_load_print_meta: n_expert         = 0
0.00.083.659 I llm_load_print_meta: n_expert_used    = 0
0.00.083.659 I llm_load_print_meta: causal attn      = 1
0.00.083.660 I llm_load_print_meta: pooling type     = 0
0.00.083.660 I llm_load_print_meta: rope type        = 2
0.00.083.660 I llm_load_print_meta: rope scaling     = linear
0.00.083.660 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.661 I llm_load_print_meta: freq_scale_train = 1
0.00.083.661 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.661 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.661 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.663 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.663 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.663 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.663 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.663 I llm_load_print_meta: model type       = 1.4B
0.00.083.677 I llm_load_print_meta: model ftype      = Q5_1
0.00.083.677 I llm_load_print_meta: model params     = 1.41 B
0.00.083.678 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.083.678 I llm_load_print_meta: general.name     = 1.4B
0.00.083.678 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.678 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.678 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.679 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.679 I llm_load_print_meta: LF token         = 128 ''
0.00.083.679 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.680 I llm_load_print_meta: max token length = 1024
0.00.085.561 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.561 I llm_load_tensors: offloading output layer to GPU
0.00.085.561 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.572 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.085.574 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.085.920 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.921 I llama_new_context_with_model: n_ctx         = 2048
0.00.085.921 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.085.921 I llama_new_context_with_model: n_batch       = 2048
0.00.085.922 I llama_new_context_with_model: n_ubatch      = 512
0.00.085.922 I llama_new_context_with_model: flash_attn    = 0
0.00.085.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.923 I llama_new_context_with_model: freq_scale    = 1
0.00.085.923 I ggml_metal_init: allocating
0.00.085.927 I ggml_metal_init: found device: Apple M4
0.00.085.929 I ggml_metal_init: picking default device: Apple M4
0.00.086.563 I ggml_metal_init: using embedded metal library
0.00.089.019 I ggml_metal_init: GPU name:   Apple M4
0.00.089.021 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.022 I ggml_metal_init: simdgroup reduction   = true
0.00.089.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.022 I ggml_metal_init: has bfloat            = true
0.00.089.022 I ggml_metal_init: use bfloat            = true
0.00.089.023 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.068 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.117.229 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.237 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.259 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.191 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.192 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.193 I llama_new_context_with_model: graph nodes  = 967
0.00.118.193 I llama_new_context_with_model: graph splits = 2
0.00.118.195 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.118.345 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.118.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.908 I main: llama threadpool init, n_threads = 4
0.00.782.991 I 
0.00.783.033 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.783.038 I 
0.00.783.583 I sampler seed: 1234
0.00.783.604 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.637 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.639 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.639 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.614.984 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.614.984 I llama_perf_context_print:        load time =     752.36 ms
0.01.614.985 I llama_perf_context_print: prompt eval time =      43.07 ms /     7 tokens (    6.15 ms per token,   162.54 tokens per second)
0.01.614.986 I llama_perf_context_print:        eval time =     785.38 ms /    63 runs   (   12.47 ms per token,    80.22 tokens per second)
0.01.614.989 I llama_perf_context_print:       total time =     832.08 ms /    70 tokens
0.01.615.224 I ggml_metal_free: deallocating

real	0m1.644s
user	0m0.125s
sys	0m0.193s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.110 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.904 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.909 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.910 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.911 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.911 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.912 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.913 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.913 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.914 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.914 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.914 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.917 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.610 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.309 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.310 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.310 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.310 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.311 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.311 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.311 I llama_model_loader: - type  f32:  194 tensors
0.00.025.312 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.312 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.312 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.255 I llm_load_vocab: special tokens cache size = 25
0.00.051.186 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.188 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.189 I llm_load_print_meta: arch             = gptneox
0.00.051.189 I llm_load_print_meta: vocab type       = BPE
0.00.051.189 I llm_load_print_meta: n_vocab          = 50304
0.00.051.189 I llm_load_print_meta: n_merges         = 50009
0.00.051.190 I llm_load_print_meta: vocab_only       = 0
0.00.051.190 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.190 I llm_load_print_meta: n_embd           = 2048
0.00.051.190 I llm_load_print_meta: n_layer          = 24
0.00.051.192 I llm_load_print_meta: n_head           = 16
0.00.051.193 I llm_load_print_meta: n_head_kv        = 16
0.00.051.193 I llm_load_print_meta: n_rot            = 32
0.00.051.193 I llm_load_print_meta: n_swa            = 0
0.00.051.194 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.194 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.194 I llm_load_print_meta: n_gqa            = 1
0.00.051.195 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.198 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.198 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.199 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.199 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.199 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.199 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.200 I llm_load_print_meta: n_ff             = 8192
0.00.051.200 I llm_load_print_meta: n_expert         = 0
0.00.051.200 I llm_load_print_meta: n_expert_used    = 0
0.00.051.200 I llm_load_print_meta: causal attn      = 1
0.00.051.200 I llm_load_print_meta: pooling type     = 0
0.00.051.201 I llm_load_print_meta: rope type        = 2
0.00.051.201 I llm_load_print_meta: rope scaling     = linear
0.00.051.201 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.201 I llm_load_print_meta: freq_scale_train = 1
0.00.051.202 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.202 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.203 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.203 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.203 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.203 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.203 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.203 I llm_load_print_meta: model type       = 1.4B
0.00.051.215 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.216 I llm_load_print_meta: model params     = 1.41 B
0.00.051.216 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.217 I llm_load_print_meta: general.name     = 1.4B
0.00.051.217 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.217 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.217 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.218 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.218 I llm_load_print_meta: LF token         = 128 ''
0.00.051.218 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.218 I llm_load_print_meta: max token length = 1024
0.00.053.083 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.083 I llm_load_tensors: offloading output layer to GPU
0.00.053.083 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.094 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.095 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.448 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.449 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.449 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.449 I llama_new_context_with_model: n_batch       = 2048
0.00.053.449 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.449 I llama_new_context_with_model: flash_attn    = 0
0.00.053.450 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.450 I llama_new_context_with_model: freq_scale    = 1
0.00.053.450 I ggml_metal_init: allocating
0.00.053.453 I ggml_metal_init: found device: Apple M4
0.00.053.455 I ggml_metal_init: picking default device: Apple M4
0.00.054.027 I ggml_metal_init: using embedded metal library
0.00.056.342 I ggml_metal_init: GPU name:   Apple M4
0.00.056.343 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.344 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.344 I ggml_metal_init: simdgroup reduction   = true
0.00.056.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.345 I ggml_metal_init: has bfloat            = true
0.00.056.345 I ggml_metal_init: use bfloat            = true
0.00.056.345 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.346 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.858 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.421 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.431 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.451 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.495 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.497 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.497 I llama_new_context_with_model: graph nodes  = 967
0.00.086.497 I llama_new_context_with_model: graph splits = 2
0.00.086.500 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.449.063 I main: llama threadpool init, n_threads = 4
0.00.449.103 I 
0.00.449.128 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.449.128 I 
0.00.449.358 I sampler seed: 1234
0.00.449.364 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.449.375 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.449.375 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.449.375 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.130.637 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62776.30 tokens per second)
0.01.130.637 I llama_perf_context_print:        load time =     439.95 ms
0.01.130.638 I llama_perf_context_print: prompt eval time =      39.36 ms /     7 tokens (    5.62 ms per token,   177.84 tokens per second)
0.01.130.640 I llama_perf_context_print:        eval time =     639.03 ms /    63 runs   (   10.14 ms per token,    98.59 tokens per second)
0.01.130.640 I llama_perf_context_print:       total time =     681.58 ms /    70 tokens
0.01.130.829 I ggml_metal_free: deallocating

real	0m1.146s
user	0m0.109s
sys	0m0.107s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.481 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.050 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.055 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.056 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.058 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.058 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.059 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.059 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.061 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.062 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.893 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.676 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.676 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.677 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.678 I llama_model_loader: - type  f32:  194 tensors
0.00.025.678 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.678 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.679 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.679 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.445 I llm_load_vocab: special tokens cache size = 25
0.00.052.427 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.430 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.431 I llm_load_print_meta: arch             = gptneox
0.00.052.431 I llm_load_print_meta: vocab type       = BPE
0.00.052.431 I llm_load_print_meta: n_vocab          = 50304
0.00.052.431 I llm_load_print_meta: n_merges         = 50009
0.00.052.432 I llm_load_print_meta: vocab_only       = 0
0.00.052.432 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.432 I llm_load_print_meta: n_embd           = 2048
0.00.052.432 I llm_load_print_meta: n_layer          = 24
0.00.052.435 I llm_load_print_meta: n_head           = 16
0.00.052.435 I llm_load_print_meta: n_head_kv        = 16
0.00.052.435 I llm_load_print_meta: n_rot            = 32
0.00.052.436 I llm_load_print_meta: n_swa            = 0
0.00.052.436 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.436 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.437 I llm_load_print_meta: n_gqa            = 1
0.00.052.438 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.438 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.439 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.439 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.439 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.439 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.439 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.440 I llm_load_print_meta: n_ff             = 8192
0.00.052.440 I llm_load_print_meta: n_expert         = 0
0.00.052.440 I llm_load_print_meta: n_expert_used    = 0
0.00.052.441 I llm_load_print_meta: causal attn      = 1
0.00.052.441 I llm_load_print_meta: pooling type     = 0
0.00.052.441 I llm_load_print_meta: rope type        = 2
0.00.052.441 I llm_load_print_meta: rope scaling     = linear
0.00.052.442 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.442 I llm_load_print_meta: freq_scale_train = 1
0.00.052.442 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.442 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.443 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.444 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.445 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.446 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.446 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.446 I llm_load_print_meta: model type       = 1.4B
0.00.052.458 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.458 I llm_load_print_meta: model params     = 1.41 B
0.00.052.459 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.459 I llm_load_print_meta: general.name     = 1.4B
0.00.052.459 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.459 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.460 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.460 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.461 I llm_load_print_meta: LF token         = 128 ''
0.00.052.462 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.462 I llm_load_print_meta: max token length = 1024
0.00.054.414 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.414 I llm_load_tensors: offloading output layer to GPU
0.00.054.414 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.425 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.426 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.842 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.843 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.843 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.843 I llama_new_context_with_model: n_batch       = 2048
0.00.054.843 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.843 I llama_new_context_with_model: flash_attn    = 0
0.00.054.844 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.844 I llama_new_context_with_model: freq_scale    = 1
0.00.054.845 I ggml_metal_init: allocating
0.00.054.851 I ggml_metal_init: found device: Apple M4
0.00.054.853 I ggml_metal_init: picking default device: Apple M4
0.00.055.429 I ggml_metal_init: using embedded metal library
0.00.057.726 I ggml_metal_init: GPU name:   Apple M4
0.00.057.727 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.728 I ggml_metal_init: simdgroup reduction   = true
0.00.057.728 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.729 I ggml_metal_init: has bfloat            = true
0.00.057.729 I ggml_metal_init: use bfloat            = true
0.00.057.729 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.730 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.078 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.728 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.737 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.756 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.708 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.709 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.710 I llama_new_context_with_model: graph nodes  = 967
0.00.086.710 I llama_new_context_with_model: graph splits = 2
0.00.086.714 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.855 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.856 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.037 I main: llama threadpool init, n_threads = 4
0.00.543.080 I 
0.00.543.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.131 I 
0.00.543.354 I sampler seed: 1234
0.00.543.359 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.543.390 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.543.391 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.543.392 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.287.343 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.287.344 I llama_perf_context_print:        load time =     532.55 ms
0.01.287.345 I llama_perf_context_print: prompt eval time =      40.54 ms /     7 tokens (    5.79 ms per token,   172.66 tokens per second)
0.01.287.345 I llama_perf_context_print:        eval time =     700.25 ms /    63 runs   (   11.12 ms per token,    89.97 tokens per second)
0.01.287.346 I llama_perf_context_print:       total time =     744.31 ms /    70 tokens
0.01.287.623 I ggml_metal_free: deallocating

real	0m1.305s
user	0m0.110s
sys	0m0.121s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.013.546 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.925 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.936 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.936 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.937 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.939 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.772 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.526 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.527 I llama_model_loader: - type  f32:  194 tensors
0.00.028.527 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.527 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.527 I llama_model_loader: - type q6_K:   13 tensors
0.00.048.438 I llm_load_vocab: special tokens cache size = 25
0.00.054.193 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.196 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.197 I llm_load_print_meta: arch             = gptneox
0.00.054.197 I llm_load_print_meta: vocab type       = BPE
0.00.054.197 I llm_load_print_meta: n_vocab          = 50304
0.00.054.197 I llm_load_print_meta: n_merges         = 50009
0.00.054.198 I llm_load_print_meta: vocab_only       = 0
0.00.054.198 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.198 I llm_load_print_meta: n_embd           = 2048
0.00.054.198 I llm_load_print_meta: n_layer          = 24
0.00.054.201 I llm_load_print_meta: n_head           = 16
0.00.054.202 I llm_load_print_meta: n_head_kv        = 16
0.00.054.202 I llm_load_print_meta: n_rot            = 32
0.00.054.202 I llm_load_print_meta: n_swa            = 0
0.00.054.202 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.202 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.203 I llm_load_print_meta: n_gqa            = 1
0.00.054.204 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.205 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.205 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.205 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.208 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.208 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.208 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.209 I llm_load_print_meta: n_ff             = 8192
0.00.054.209 I llm_load_print_meta: n_expert         = 0
0.00.054.209 I llm_load_print_meta: n_expert_used    = 0
0.00.054.211 I llm_load_print_meta: causal attn      = 1
0.00.054.211 I llm_load_print_meta: pooling type     = 0
0.00.054.211 I llm_load_print_meta: rope type        = 2
0.00.054.211 I llm_load_print_meta: rope scaling     = linear
0.00.054.212 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.212 I llm_load_print_meta: freq_scale_train = 1
0.00.054.212 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.212 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.212 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.213 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.213 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.213 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.213 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.213 I llm_load_print_meta: model type       = 1.4B
0.00.054.220 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.054.220 I llm_load_print_meta: model params     = 1.41 B
0.00.054.221 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.054.222 I llm_load_print_meta: general.name     = 1.4B
0.00.054.222 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.222 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.222 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.222 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.223 I llm_load_print_meta: LF token         = 128 ''
0.00.054.223 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.223 I llm_load_print_meta: max token length = 1024
0.00.056.018 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.018 I llm_load_tensors: offloading output layer to GPU
0.00.056.019 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.024 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.056.025 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.425 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.426 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.426 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.426 I llama_new_context_with_model: n_batch       = 2048
0.00.056.426 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.426 I llama_new_context_with_model: flash_attn    = 0
0.00.056.427 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.427 I llama_new_context_with_model: freq_scale    = 1
0.00.056.427 I ggml_metal_init: allocating
0.00.056.430 I ggml_metal_init: found device: Apple M4
0.00.056.434 I ggml_metal_init: picking default device: Apple M4
0.00.057.041 I ggml_metal_init: using embedded metal library
0.00.059.349 I ggml_metal_init: GPU name:   Apple M4
0.00.059.350 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.350 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.351 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.351 I ggml_metal_init: simdgroup reduction   = true
0.00.059.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.351 I ggml_metal_init: has bfloat            = true
0.00.059.352 I ggml_metal_init: use bfloat            = true
0.00.059.352 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.353 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.830 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.220 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.227 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.244 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.290 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.292 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.292 I llama_new_context_with_model: graph nodes  = 967
0.00.089.292 I llama_new_context_with_model: graph splits = 2
0.00.089.295 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.250 I main: llama threadpool init, n_threads = 4
0.00.624.302 I 
0.00.624.330 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.624.330 I 
0.00.624.569 I sampler seed: 1234
0.00.624.573 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.624.618 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.624.619 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.624.619 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.388.462 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50969.13 tokens per second)
0.01.388.462 I llama_perf_context_print:        load time =     610.70 ms
0.01.388.464 I llama_perf_context_print: prompt eval time =      50.07 ms /     7 tokens (    7.15 ms per token,   139.81 tokens per second)
0.01.388.465 I llama_perf_context_print:        eval time =     710.67 ms /    63 runs   (   11.28 ms per token,    88.65 tokens per second)
0.01.388.465 I llama_perf_context_print:       total time =     764.22 ms /    70 tokens
0.01.388.696 I ggml_metal_free: deallocating

real	0m1.405s
user	0m0.109s
sys	0m0.134s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.754 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.250 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.251 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.251 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.251 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.252 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.253 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.253 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.253 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.254 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.254 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.254 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.255 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.257 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.257 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.257 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.063 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.904 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.905 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.905 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.906 I llama_model_loader: - type  f32:  194 tensors
0.00.023.906 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.906 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.138 I llm_load_vocab: special tokens cache size = 25
0.00.051.124 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.127 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.128 I llm_load_print_meta: arch             = gptneox
0.00.051.128 I llm_load_print_meta: vocab type       = BPE
0.00.051.128 I llm_load_print_meta: n_vocab          = 50304
0.00.051.129 I llm_load_print_meta: n_merges         = 50009
0.00.051.129 I llm_load_print_meta: vocab_only       = 0
0.00.051.129 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.129 I llm_load_print_meta: n_embd           = 2048
0.00.051.129 I llm_load_print_meta: n_layer          = 24
0.00.051.132 I llm_load_print_meta: n_head           = 16
0.00.051.133 I llm_load_print_meta: n_head_kv        = 16
0.00.051.133 I llm_load_print_meta: n_rot            = 32
0.00.051.134 I llm_load_print_meta: n_swa            = 0
0.00.051.134 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.134 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.135 I llm_load_print_meta: n_gqa            = 1
0.00.051.135 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.136 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.136 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.137 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.137 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.137 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.137 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.138 I llm_load_print_meta: n_ff             = 8192
0.00.051.138 I llm_load_print_meta: n_expert         = 0
0.00.051.138 I llm_load_print_meta: n_expert_used    = 0
0.00.051.138 I llm_load_print_meta: causal attn      = 1
0.00.051.138 I llm_load_print_meta: pooling type     = 0
0.00.051.139 I llm_load_print_meta: rope type        = 2
0.00.051.139 I llm_load_print_meta: rope scaling     = linear
0.00.051.139 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.139 I llm_load_print_meta: freq_scale_train = 1
0.00.051.139 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.142 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.142 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.142 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.142 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.143 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.143 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.143 I llm_load_print_meta: model type       = 1.4B
0.00.051.155 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.155 I llm_load_print_meta: model params     = 1.41 B
0.00.051.156 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.157 I llm_load_print_meta: general.name     = 1.4B
0.00.051.158 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.158 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.158 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.158 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.158 I llm_load_print_meta: LF token         = 128 ''
0.00.051.159 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.159 I llm_load_print_meta: max token length = 1024
0.00.053.109 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.110 I llm_load_tensors: offloading output layer to GPU
0.00.053.110 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.121 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.123 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.441 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.442 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.442 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.442 I llama_new_context_with_model: n_batch       = 2048
0.00.053.442 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.442 I llama_new_context_with_model: flash_attn    = 0
0.00.053.443 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.443 I llama_new_context_with_model: freq_scale    = 1
0.00.053.443 I ggml_metal_init: allocating
0.00.053.447 I ggml_metal_init: found device: Apple M4
0.00.053.449 I ggml_metal_init: picking default device: Apple M4
0.00.054.096 I ggml_metal_init: using embedded metal library
0.00.056.463 I ggml_metal_init: GPU name:   Apple M4
0.00.056.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.466 I ggml_metal_init: simdgroup reduction   = true
0.00.056.466 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.467 I ggml_metal_init: has bfloat            = true
0.00.056.467 I ggml_metal_init: use bfloat            = true
0.00.056.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.621 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.750 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.759 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.786 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.771 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.772 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.773 I llama_new_context_with_model: graph nodes  = 967
0.00.086.773 I llama_new_context_with_model: graph splits = 2
0.00.086.776 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.918 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.678 I main: llama threadpool init, n_threads = 4
0.00.713.743 I 
0.00.713.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.771 I 
0.00.714.005 I sampler seed: 1234
0.00.714.010 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.051 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.054 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.054 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.563.446 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49203.05 tokens per second)
0.01.563.446 I llama_perf_context_print:        load time =     704.91 ms
0.01.563.447 I llama_perf_context_print: prompt eval time =      55.53 ms /     7 tokens (    7.93 ms per token,   126.05 tokens per second)
0.01.563.448 I llama_perf_context_print:        eval time =     791.34 ms /    63 runs   (   12.56 ms per token,    79.61 tokens per second)
0.01.563.448 I llama_perf_context_print:       total time =     849.78 ms /    70 tokens
0.01.563.720 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.110s
sys	0m0.130s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.653 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.177 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.182 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.919 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.911 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.661 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.662 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.663 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.663 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.663 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.664 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.664 I llama_model_loader: - type  f32:  194 tensors
0.00.023.664 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.640 I llm_load_vocab: special tokens cache size = 25
0.00.049.572 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.574 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.575 I llm_load_print_meta: arch             = gptneox
0.00.049.575 I llm_load_print_meta: vocab type       = BPE
0.00.049.576 I llm_load_print_meta: n_vocab          = 50304
0.00.049.576 I llm_load_print_meta: n_merges         = 50009
0.00.049.576 I llm_load_print_meta: vocab_only       = 0
0.00.049.576 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.576 I llm_load_print_meta: n_embd           = 2048
0.00.049.576 I llm_load_print_meta: n_layer          = 24
0.00.049.579 I llm_load_print_meta: n_head           = 16
0.00.049.580 I llm_load_print_meta: n_head_kv        = 16
0.00.049.580 I llm_load_print_meta: n_rot            = 32
0.00.049.580 I llm_load_print_meta: n_swa            = 0
0.00.049.580 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.581 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.582 I llm_load_print_meta: n_gqa            = 1
0.00.049.583 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.584 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.584 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.585 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.585 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.585 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.585 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.586 I llm_load_print_meta: n_ff             = 8192
0.00.049.588 I llm_load_print_meta: n_expert         = 0
0.00.049.588 I llm_load_print_meta: n_expert_used    = 0
0.00.049.588 I llm_load_print_meta: causal attn      = 1
0.00.049.588 I llm_load_print_meta: pooling type     = 0
0.00.049.588 I llm_load_print_meta: rope type        = 2
0.00.049.588 I llm_load_print_meta: rope scaling     = linear
0.00.049.589 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.589 I llm_load_print_meta: freq_scale_train = 1
0.00.049.589 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.590 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.590 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.590 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.590 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.590 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.590 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.591 I llm_load_print_meta: model type       = 1.4B
0.00.049.602 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.602 I llm_load_print_meta: model params     = 1.41 B
0.00.049.603 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.603 I llm_load_print_meta: general.name     = 1.4B
0.00.049.603 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.603 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.603 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.604 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.605 I llm_load_print_meta: LF token         = 128 ''
0.00.049.605 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.605 I llm_load_print_meta: max token length = 1024
0.00.051.593 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.593 I llm_load_tensors: offloading output layer to GPU
0.00.051.593 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.604 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.605 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.932 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.932 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.932 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.932 I llama_new_context_with_model: n_batch       = 2048
0.00.051.933 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.933 I llama_new_context_with_model: flash_attn    = 0
0.00.051.933 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.933 I llama_new_context_with_model: freq_scale    = 1
0.00.051.934 I ggml_metal_init: allocating
0.00.051.936 I ggml_metal_init: found device: Apple M4
0.00.051.938 I ggml_metal_init: picking default device: Apple M4
0.00.052.501 I ggml_metal_init: using embedded metal library
0.00.054.822 I ggml_metal_init: GPU name:   Apple M4
0.00.054.824 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.824 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.825 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.825 I ggml_metal_init: simdgroup reduction   = true
0.00.054.825 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.825 I ggml_metal_init: has bfloat            = true
0.00.054.826 I ggml_metal_init: use bfloat            = true
0.00.054.826 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.364 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.213 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.230 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.253 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.317 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.318 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.318 I llama_new_context_with_model: graph nodes  = 967
0.00.085.318 I llama_new_context_with_model: graph splits = 2
0.00.085.321 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.414 I main: llama threadpool init, n_threads = 4
0.00.779.460 I 
0.00.779.486 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.488 I 
0.00.779.762 I sampler seed: 1234
0.00.779.768 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.808 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.810 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.810 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.655.187 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.01.655.187 I llama_perf_context_print:        load time =     770.75 ms
0.01.655.188 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.61 tokens per second)
0.01.655.190 I llama_perf_context_print:        eval time =     817.96 ms /    63 runs   (   12.98 ms per token,    77.02 tokens per second)
0.01.655.190 I llama_perf_context_print:       total time =     875.78 ms /    70 tokens
0.01.655.393 I ggml_metal_free: deallocating

real	0m1.673s
user	0m0.109s
sys	0m0.173s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.796 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.745 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.511 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.516 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.518 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.520 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.523 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.524 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.524 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.525 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.527 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.527 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.528 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.907 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.852 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.097 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.099 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.099 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.100 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.100 I llama_model_loader: - type  f32:  194 tensors
0.00.053.101 I llama_model_loader: - type  f16:   98 tensors
0.00.080.667 I llm_load_vocab: special tokens cache size = 25
0.00.087.044 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.047 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.047 I llm_load_print_meta: arch             = gptneox
0.00.087.048 I llm_load_print_meta: vocab type       = BPE
0.00.087.048 I llm_load_print_meta: n_vocab          = 50304
0.00.087.048 I llm_load_print_meta: n_merges         = 50009
0.00.087.048 I llm_load_print_meta: vocab_only       = 0
0.00.087.048 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.048 I llm_load_print_meta: n_embd           = 2048
0.00.087.048 I llm_load_print_meta: n_layer          = 24
0.00.087.052 I llm_load_print_meta: n_head           = 16
0.00.087.052 I llm_load_print_meta: n_head_kv        = 16
0.00.087.053 I llm_load_print_meta: n_rot            = 32
0.00.087.053 I llm_load_print_meta: n_swa            = 0
0.00.087.053 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.053 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.054 I llm_load_print_meta: n_gqa            = 1
0.00.087.054 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.055 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.055 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.056 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.056 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.056 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.056 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.057 I llm_load_print_meta: n_ff             = 8192
0.00.087.057 I llm_load_print_meta: n_expert         = 0
0.00.087.059 I llm_load_print_meta: n_expert_used    = 0
0.00.087.059 I llm_load_print_meta: causal attn      = 1
0.00.087.059 I llm_load_print_meta: pooling type     = 0
0.00.087.059 I llm_load_print_meta: rope type        = 2
0.00.087.060 I llm_load_print_meta: rope scaling     = linear
0.00.087.060 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.060 I llm_load_print_meta: freq_scale_train = 1
0.00.087.060 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.060 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.061 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.061 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.061 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.061 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.061 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.061 I llm_load_print_meta: model type       = 1.4B
0.00.087.073 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.074 I llm_load_print_meta: model params     = 1.41 B
0.00.087.074 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.074 I llm_load_print_meta: general.name     = 1.4B
0.00.087.075 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.075 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.075 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.075 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.075 I llm_load_print_meta: LF token         = 128 ''
0.00.087.075 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.076 I llm_load_print_meta: max token length = 1024
0.00.089.773 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.774 I llm_load_tensors: offloading output layer to GPU
0.00.089.774 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.785 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.786 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.111 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.111 I llama_new_context_with_model: n_ctx         = 128
0.00.090.111 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.112 I llama_new_context_with_model: n_batch       = 128
0.00.090.112 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.112 I llama_new_context_with_model: flash_attn    = 0
0.00.090.113 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.113 I llama_new_context_with_model: freq_scale    = 1
0.00.090.114 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.114 I ggml_metal_init: allocating
0.00.090.122 I ggml_metal_init: found device: Apple M4
0.00.090.125 I ggml_metal_init: picking default device: Apple M4
0.00.090.774 I ggml_metal_init: using embedded metal library
0.00.093.332 I ggml_metal_init: GPU name:   Apple M4
0.00.093.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.334 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.334 I ggml_metal_init: simdgroup reduction   = true
0.00.093.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.335 I ggml_metal_init: has bfloat            = true
0.00.093.335 I ggml_metal_init: use bfloat            = true
0.00.093.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.206 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.598 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.600 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.615 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.500 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.501 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.501 I llama_new_context_with_model: graph nodes  = 967
0.00.104.501 I llama_new_context_with_model: graph splits = 2
0.00.104.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.528.388 I 
0.01.528.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.528.442 I perplexity: tokenizing the input ..
0.01.541.235 I perplexity: tokenization took 12.789 ms
0.01.541.250 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.664.371 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.666.345 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.666.411 I llama_perf_context_print:        load time =    1506.63 ms
0.01.666.415 I llama_perf_context_print: prompt eval time =     122.14 ms /   128 tokens (    0.95 ms per token,  1047.96 tokens per second)
0.01.666.416 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.666.417 I llama_perf_context_print:       total time =     138.02 ms /   129 tokens
0.01.667.156 I ggml_metal_free: deallocating

real	0m1.860s
user	0m0.123s
sys	0m0.239s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.265 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.546 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.548 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.549 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.550 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.550 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.552 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.552 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.554 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.556 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.837 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.790 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.791 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.791 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.792 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.792 I llama_model_loader: - type  f32:  194 tensors
0.00.030.793 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.754 I llm_load_vocab: special tokens cache size = 25
0.00.059.808 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.811 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.811 I llm_load_print_meta: arch             = gptneox
0.00.059.811 I llm_load_print_meta: vocab type       = BPE
0.00.059.811 I llm_load_print_meta: n_vocab          = 50304
0.00.059.812 I llm_load_print_meta: n_merges         = 50009
0.00.059.812 I llm_load_print_meta: vocab_only       = 0
0.00.059.812 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.812 I llm_load_print_meta: n_embd           = 2048
0.00.059.812 I llm_load_print_meta: n_layer          = 24
0.00.059.815 I llm_load_print_meta: n_head           = 16
0.00.059.816 I llm_load_print_meta: n_head_kv        = 16
0.00.059.816 I llm_load_print_meta: n_rot            = 32
0.00.059.816 I llm_load_print_meta: n_swa            = 0
0.00.059.817 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.817 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.817 I llm_load_print_meta: n_gqa            = 1
0.00.059.818 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.819 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.819 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.820 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.820 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.821 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.821 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.823 I llm_load_print_meta: n_ff             = 8192
0.00.059.823 I llm_load_print_meta: n_expert         = 0
0.00.059.823 I llm_load_print_meta: n_expert_used    = 0
0.00.059.824 I llm_load_print_meta: causal attn      = 1
0.00.059.824 I llm_load_print_meta: pooling type     = 0
0.00.059.824 I llm_load_print_meta: rope type        = 2
0.00.059.824 I llm_load_print_meta: rope scaling     = linear
0.00.059.824 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.825 I llm_load_print_meta: freq_scale_train = 1
0.00.059.825 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.825 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.825 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.825 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.825 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.826 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.826 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.826 I llm_load_print_meta: model type       = 1.4B
0.00.059.838 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.839 I llm_load_print_meta: model params     = 1.41 B
0.00.059.840 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.840 I llm_load_print_meta: general.name     = 1.4B
0.00.059.841 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.841 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.841 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.841 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.841 I llm_load_print_meta: LF token         = 128 ''
0.00.059.841 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.842 I llm_load_print_meta: max token length = 1024
0.00.062.101 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.102 I llm_load_tensors: offloading output layer to GPU
0.00.062.102 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.113 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.114 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.525 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.526 I llama_new_context_with_model: n_ctx         = 128
0.00.062.526 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.526 I llama_new_context_with_model: n_batch       = 128
0.00.062.526 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.527 I llama_new_context_with_model: flash_attn    = 0
0.00.062.527 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.528 I llama_new_context_with_model: freq_scale    = 1
0.00.062.528 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.529 I ggml_metal_init: allocating
0.00.062.534 I ggml_metal_init: found device: Apple M4
0.00.062.536 I ggml_metal_init: picking default device: Apple M4
0.00.063.158 I ggml_metal_init: using embedded metal library
0.00.065.503 I ggml_metal_init: GPU name:   Apple M4
0.00.065.504 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.505 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.505 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.505 I ggml_metal_init: simdgroup reduction   = true
0.00.065.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.506 I ggml_metal_init: has bfloat            = true
0.00.065.506 I ggml_metal_init: use bfloat            = true
0.00.065.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.348 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.076.739 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.744 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.761 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.077.666 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.077.667 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.077.668 I llama_new_context_with_model: graph nodes  = 967
0.00.077.668 I llama_new_context_with_model: graph splits = 2
0.00.077.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.077.669 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.948.154 I 
0.00.948.175 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.948.186 I perplexity: tokenizing the input ..
0.00.956.395 I perplexity: tokenization took 8.208 ms
0.00.956.399 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.080.860 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.082.003 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.082.035 I llama_perf_context_print:        load time =     937.88 ms
0.01.082.036 I llama_perf_context_print: prompt eval time =     124.20 ms /   128 tokens (    0.97 ms per token,  1030.56 tokens per second)
0.01.082.037 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.082.037 I llama_perf_context_print:       total time =     133.88 ms /   129 tokens
0.01.082.429 I ggml_metal_free: deallocating

real	0m1.101s
user	0m0.089s
sys	0m0.141s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.859 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.026 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.030 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.032 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.033 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.033 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.033 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.033 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.035 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.035 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.035 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.036 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.036 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.036 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.038 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.041 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.041 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.041 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.836 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.638 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.639 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.639 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.639 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.640 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.640 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.640 I llama_model_loader: - type  f32:  194 tensors
0.00.026.641 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.641 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.010 I llm_load_vocab: special tokens cache size = 25
0.00.052.977 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.979 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.980 I llm_load_print_meta: arch             = gptneox
0.00.052.980 I llm_load_print_meta: vocab type       = BPE
0.00.052.980 I llm_load_print_meta: n_vocab          = 50304
0.00.052.980 I llm_load_print_meta: n_merges         = 50009
0.00.052.981 I llm_load_print_meta: vocab_only       = 0
0.00.052.981 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.981 I llm_load_print_meta: n_embd           = 2048
0.00.052.981 I llm_load_print_meta: n_layer          = 24
0.00.052.984 I llm_load_print_meta: n_head           = 16
0.00.052.984 I llm_load_print_meta: n_head_kv        = 16
0.00.052.985 I llm_load_print_meta: n_rot            = 32
0.00.052.985 I llm_load_print_meta: n_swa            = 0
0.00.052.985 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.988 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.988 I llm_load_print_meta: n_gqa            = 1
0.00.052.989 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.990 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.991 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.992 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.992 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.992 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.993 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.993 I llm_load_print_meta: n_ff             = 8192
0.00.052.993 I llm_load_print_meta: n_expert         = 0
0.00.052.994 I llm_load_print_meta: n_expert_used    = 0
0.00.052.994 I llm_load_print_meta: causal attn      = 1
0.00.052.994 I llm_load_print_meta: pooling type     = 0
0.00.052.994 I llm_load_print_meta: rope type        = 2
0.00.052.994 I llm_load_print_meta: rope scaling     = linear
0.00.052.994 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.995 I llm_load_print_meta: freq_scale_train = 1
0.00.052.995 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.995 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.995 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.995 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.996 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.998 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.998 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.998 I llm_load_print_meta: model type       = 1.4B
0.00.053.010 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.010 I llm_load_print_meta: model params     = 1.41 B
0.00.053.011 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.011 I llm_load_print_meta: general.name     = 1.4B
0.00.053.011 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.012 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.012 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.012 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.013 I llm_load_print_meta: LF token         = 128 ''
0.00.053.013 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.013 I llm_load_print_meta: max token length = 1024
0.00.055.014 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.014 I llm_load_tensors: offloading output layer to GPU
0.00.055.014 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.025 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.026 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.340 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.340 I llama_new_context_with_model: n_ctx         = 128
0.00.055.340 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.341 I llama_new_context_with_model: n_batch       = 128
0.00.055.341 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.341 I llama_new_context_with_model: flash_attn    = 0
0.00.055.341 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.341 I llama_new_context_with_model: freq_scale    = 1
0.00.055.342 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.342 I ggml_metal_init: allocating
0.00.055.345 I ggml_metal_init: found device: Apple M4
0.00.055.347 I ggml_metal_init: picking default device: Apple M4
0.00.055.906 I ggml_metal_init: using embedded metal library
0.00.058.218 I ggml_metal_init: GPU name:   Apple M4
0.00.058.220 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.221 I ggml_metal_init: simdgroup reduction   = true
0.00.058.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.221 I ggml_metal_init: has bfloat            = true
0.00.058.221 I ggml_metal_init: use bfloat            = true
0.00.058.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.222 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.804 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.087 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.089 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.103 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.063 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.064 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.064 I llama_new_context_with_model: graph nodes  = 967
0.00.070.065 I llama_new_context_with_model: graph splits = 2
0.00.070.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.923 I 
0.00.689.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.967 I perplexity: tokenizing the input ..
0.00.698.209 I perplexity: tokenization took 8.241 ms
0.00.698.219 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.142 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.821.319 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.821.344 I llama_perf_context_print:        load time =     679.06 ms
0.00.821.347 I llama_perf_context_print: prompt eval time =     121.70 ms /   128 tokens (    0.95 ms per token,  1051.78 tokens per second)
0.00.821.348 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.349 I llama_perf_context_print:       total time =     131.42 ms /   129 tokens
0.00.821.706 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.079s
sys	0m0.106s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.180 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.297 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.303 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.304 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.304 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.305 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.308 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.308 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.308 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.309 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.309 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.314 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.047 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.763 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.764 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.765 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.765 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.765 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.766 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.766 I llama_model_loader: - type  f32:  194 tensors
0.00.024.766 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.767 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.378 I llm_load_vocab: special tokens cache size = 25
0.00.050.413 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.416 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.416 I llm_load_print_meta: arch             = gptneox
0.00.050.417 I llm_load_print_meta: vocab type       = BPE
0.00.050.417 I llm_load_print_meta: n_vocab          = 50304
0.00.050.417 I llm_load_print_meta: n_merges         = 50009
0.00.050.417 I llm_load_print_meta: vocab_only       = 0
0.00.050.418 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.418 I llm_load_print_meta: n_embd           = 2048
0.00.050.418 I llm_load_print_meta: n_layer          = 24
0.00.050.421 I llm_load_print_meta: n_head           = 16
0.00.050.421 I llm_load_print_meta: n_head_kv        = 16
0.00.050.421 I llm_load_print_meta: n_rot            = 32
0.00.050.422 I llm_load_print_meta: n_swa            = 0
0.00.050.422 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.422 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.423 I llm_load_print_meta: n_gqa            = 1
0.00.050.423 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.424 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.425 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.425 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.425 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.425 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.425 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.426 I llm_load_print_meta: n_ff             = 8192
0.00.050.426 I llm_load_print_meta: n_expert         = 0
0.00.050.426 I llm_load_print_meta: n_expert_used    = 0
0.00.050.427 I llm_load_print_meta: causal attn      = 1
0.00.050.427 I llm_load_print_meta: pooling type     = 0
0.00.050.427 I llm_load_print_meta: rope type        = 2
0.00.050.427 I llm_load_print_meta: rope scaling     = linear
0.00.050.427 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.428 I llm_load_print_meta: freq_scale_train = 1
0.00.050.428 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.428 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.428 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.429 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.429 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.429 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.429 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.429 I llm_load_print_meta: model type       = 1.4B
0.00.050.441 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.443 I llm_load_print_meta: model params     = 1.41 B
0.00.050.443 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.444 I llm_load_print_meta: general.name     = 1.4B
0.00.050.444 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: LF token         = 128 ''
0.00.050.446 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.446 I llm_load_print_meta: max token length = 1024
0.00.052.425 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.425 I llm_load_tensors: offloading output layer to GPU
0.00.052.425 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.436 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.437 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.771 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.771 I llama_new_context_with_model: n_ctx         = 128
0.00.052.771 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.772 I llama_new_context_with_model: n_batch       = 128
0.00.052.772 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.772 I llama_new_context_with_model: flash_attn    = 0
0.00.052.772 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.773 I llama_new_context_with_model: freq_scale    = 1
0.00.052.773 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.774 I ggml_metal_init: allocating
0.00.052.780 I ggml_metal_init: found device: Apple M4
0.00.052.782 I ggml_metal_init: picking default device: Apple M4
0.00.053.350 I ggml_metal_init: using embedded metal library
0.00.055.692 I ggml_metal_init: GPU name:   Apple M4
0.00.055.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.694 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.694 I ggml_metal_init: simdgroup reduction   = true
0.00.055.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.695 I ggml_metal_init: has bfloat            = true
0.00.055.695 I ggml_metal_init: use bfloat            = true
0.00.055.695 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.696 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.015 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.268 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.271 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.292 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.151 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.152 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.153 I llama_new_context_with_model: graph nodes  = 967
0.00.067.153 I llama_new_context_with_model: graph splits = 2
0.00.067.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.154 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.580.530 I 
0.00.580.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.580.596 I perplexity: tokenizing the input ..
0.00.588.633 I perplexity: tokenization took 8.036 ms
0.00.588.637 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.580 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.712.756 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.712.785 I llama_perf_context_print:        load time =     571.34 ms
0.00.712.786 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1043.05 tokens per second)
0.00.712.786 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.712.787 I llama_perf_context_print:       total time =     132.26 ms /   129 tokens
0.00.713.346 I ggml_metal_free: deallocating

real	0m0.727s
user	0m0.078s
sys	0m0.096s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.952 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.979 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.983 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.986 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.990 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.990 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.990 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.991 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.991 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.993 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.478 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.480 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.480 I llama_model_loader: - type  f32:  194 tensors
0.00.024.480 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.481 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.164 I llm_load_vocab: special tokens cache size = 25
0.00.050.080 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.083 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.083 I llm_load_print_meta: arch             = gptneox
0.00.050.083 I llm_load_print_meta: vocab type       = BPE
0.00.050.084 I llm_load_print_meta: n_vocab          = 50304
0.00.050.084 I llm_load_print_meta: n_merges         = 50009
0.00.050.084 I llm_load_print_meta: vocab_only       = 0
0.00.050.084 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.084 I llm_load_print_meta: n_embd           = 2048
0.00.050.084 I llm_load_print_meta: n_layer          = 24
0.00.050.087 I llm_load_print_meta: n_head           = 16
0.00.050.088 I llm_load_print_meta: n_head_kv        = 16
0.00.050.088 I llm_load_print_meta: n_rot            = 32
0.00.050.088 I llm_load_print_meta: n_swa            = 0
0.00.050.088 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.088 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.089 I llm_load_print_meta: n_gqa            = 1
0.00.050.090 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.090 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.091 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.091 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.092 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.092 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.092 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.093 I llm_load_print_meta: n_ff             = 8192
0.00.050.095 I llm_load_print_meta: n_expert         = 0
0.00.050.095 I llm_load_print_meta: n_expert_used    = 0
0.00.050.095 I llm_load_print_meta: causal attn      = 1
0.00.050.095 I llm_load_print_meta: pooling type     = 0
0.00.050.095 I llm_load_print_meta: rope type        = 2
0.00.050.096 I llm_load_print_meta: rope scaling     = linear
0.00.050.096 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.096 I llm_load_print_meta: freq_scale_train = 1
0.00.050.097 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.097 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.097 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.097 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.098 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.098 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.098 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.098 I llm_load_print_meta: model type       = 1.4B
0.00.050.110 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.110 I llm_load_print_meta: model params     = 1.41 B
0.00.050.111 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.111 I llm_load_print_meta: general.name     = 1.4B
0.00.050.111 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.111 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.111 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.111 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.112 I llm_load_print_meta: LF token         = 128 ''
0.00.050.112 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.112 I llm_load_print_meta: max token length = 1024
0.00.052.074 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.074 I llm_load_tensors: offloading output layer to GPU
0.00.052.074 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.085 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.086 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.421 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.422 I llama_new_context_with_model: n_ctx         = 128
0.00.052.422 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.422 I llama_new_context_with_model: n_batch       = 128
0.00.052.423 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.423 I llama_new_context_with_model: flash_attn    = 0
0.00.052.423 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.423 I llama_new_context_with_model: freq_scale    = 1
0.00.052.424 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.424 I ggml_metal_init: allocating
0.00.052.427 I ggml_metal_init: found device: Apple M4
0.00.052.429 I ggml_metal_init: picking default device: Apple M4
0.00.052.982 I ggml_metal_init: using embedded metal library
0.00.055.497 I ggml_metal_init: GPU name:   Apple M4
0.00.055.498 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.499 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.499 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.499 I ggml_metal_init: simdgroup reduction   = true
0.00.055.500 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.500 I ggml_metal_init: has bfloat            = true
0.00.055.500 I ggml_metal_init: use bfloat            = true
0.00.055.500 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.501 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.894 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.205 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.207 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.222 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.136 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.137 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.138 I llama_new_context_with_model: graph nodes  = 967
0.00.067.138 I llama_new_context_with_model: graph splits = 2
0.00.067.139 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.139 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.788 I 
0.00.788.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.846 I perplexity: tokenizing the input ..
0.00.796.791 I perplexity: tokenization took 7.943 ms
0.00.796.794 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.931.970 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.933.114 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.933.148 I llama_perf_context_print:        load time =     779.83 ms
0.00.933.149 I llama_perf_context_print: prompt eval time =     134.95 ms /   128 tokens (    1.05 ms per token,   948.52 tokens per second)
0.00.933.150 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.933.151 I llama_perf_context_print:       total time =     144.36 ms /   129 tokens
0.00.933.620 I ggml_metal_free: deallocating

real	0m0.947s
user	0m0.078s
sys	0m0.128s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.968 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.311 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.312 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.318 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.319 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.320 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.320 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.056 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.060 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.739 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.741 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.741 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.742 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.742 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.742 I llama_model_loader: - type  f32:  194 tensors
0.00.024.743 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.743 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.997 I llm_load_vocab: special tokens cache size = 25
0.00.050.951 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.954 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.954 I llm_load_print_meta: arch             = gptneox
0.00.050.954 I llm_load_print_meta: vocab type       = BPE
0.00.050.955 I llm_load_print_meta: n_vocab          = 50304
0.00.050.955 I llm_load_print_meta: n_merges         = 50009
0.00.050.955 I llm_load_print_meta: vocab_only       = 0
0.00.050.955 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.955 I llm_load_print_meta: n_embd           = 2048
0.00.050.955 I llm_load_print_meta: n_layer          = 24
0.00.050.958 I llm_load_print_meta: n_head           = 16
0.00.050.959 I llm_load_print_meta: n_head_kv        = 16
0.00.050.959 I llm_load_print_meta: n_rot            = 32
0.00.050.960 I llm_load_print_meta: n_swa            = 0
0.00.050.960 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.960 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.961 I llm_load_print_meta: n_gqa            = 1
0.00.050.962 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.963 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.963 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.964 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.964 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.964 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.964 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.965 I llm_load_print_meta: n_ff             = 8192
0.00.050.965 I llm_load_print_meta: n_expert         = 0
0.00.050.965 I llm_load_print_meta: n_expert_used    = 0
0.00.050.965 I llm_load_print_meta: causal attn      = 1
0.00.050.966 I llm_load_print_meta: pooling type     = 0
0.00.050.966 I llm_load_print_meta: rope type        = 2
0.00.050.966 I llm_load_print_meta: rope scaling     = linear
0.00.050.966 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.967 I llm_load_print_meta: freq_scale_train = 1
0.00.050.967 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.967 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.967 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.967 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.969 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.969 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.969 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.969 I llm_load_print_meta: model type       = 1.4B
0.00.050.981 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.982 I llm_load_print_meta: model params     = 1.41 B
0.00.050.982 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.982 I llm_load_print_meta: general.name     = 1.4B
0.00.050.983 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.983 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.983 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.983 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.983 I llm_load_print_meta: LF token         = 128 ''
0.00.050.985 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.985 I llm_load_print_meta: max token length = 1024
0.00.053.207 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.207 I llm_load_tensors: offloading output layer to GPU
0.00.053.207 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.218 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.219 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.561 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.562 I llama_new_context_with_model: n_ctx         = 128
0.00.053.562 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.562 I llama_new_context_with_model: n_batch       = 128
0.00.053.562 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.563 I llama_new_context_with_model: flash_attn    = 0
0.00.053.563 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.563 I llama_new_context_with_model: freq_scale    = 1
0.00.053.564 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.564 I ggml_metal_init: allocating
0.00.053.570 I ggml_metal_init: found device: Apple M4
0.00.053.573 I ggml_metal_init: picking default device: Apple M4
0.00.054.134 I ggml_metal_init: using embedded metal library
0.00.056.436 I ggml_metal_init: GPU name:   Apple M4
0.00.056.437 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.438 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.438 I ggml_metal_init: simdgroup reduction   = true
0.00.056.438 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.439 I ggml_metal_init: has bfloat            = true
0.00.056.439 I ggml_metal_init: use bfloat            = true
0.00.056.439 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.440 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.925 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.140 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.142 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.156 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.103 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.104 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.105 I llama_new_context_with_model: graph nodes  = 967
0.00.068.105 I llama_new_context_with_model: graph splits = 2
0.00.068.106 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.106 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.016 I 
0.00.615.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.062 I perplexity: tokenizing the input ..
0.00.623.182 I perplexity: tokenization took 8.117 ms
0.00.623.185 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.758.272 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.759.531 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.759.560 I llama_perf_context_print:        load time =     605.04 ms
0.00.759.561 I llama_perf_context_print: prompt eval time =     134.86 ms /   128 tokens (    1.05 ms per token,   949.14 tokens per second)
0.00.759.562 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.759.562 I llama_perf_context_print:       total time =     144.55 ms /   129 tokens
0.00.760.061 I ggml_metal_free: deallocating

real	0m0.774s
user	0m0.078s
sys	0m0.105s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.697 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.632 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.634 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.635 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.635 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.636 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.637 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.637 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.638 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.403 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.203 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.204 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.205 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.205 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.206 I llama_model_loader: - type  f32:  194 tensors
0.00.024.206 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.206 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.207 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.825 I llm_load_vocab: special tokens cache size = 25
0.00.049.672 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.675 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.675 I llm_load_print_meta: arch             = gptneox
0.00.049.675 I llm_load_print_meta: vocab type       = BPE
0.00.049.676 I llm_load_print_meta: n_vocab          = 50304
0.00.049.676 I llm_load_print_meta: n_merges         = 50009
0.00.049.676 I llm_load_print_meta: vocab_only       = 0
0.00.049.676 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.676 I llm_load_print_meta: n_embd           = 2048
0.00.049.676 I llm_load_print_meta: n_layer          = 24
0.00.049.679 I llm_load_print_meta: n_head           = 16
0.00.049.680 I llm_load_print_meta: n_head_kv        = 16
0.00.049.680 I llm_load_print_meta: n_rot            = 32
0.00.049.680 I llm_load_print_meta: n_swa            = 0
0.00.049.680 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.680 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.681 I llm_load_print_meta: n_gqa            = 1
0.00.049.682 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.683 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.683 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.686 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.686 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.686 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.686 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.687 I llm_load_print_meta: n_ff             = 8192
0.00.049.687 I llm_load_print_meta: n_expert         = 0
0.00.049.688 I llm_load_print_meta: n_expert_used    = 0
0.00.049.689 I llm_load_print_meta: causal attn      = 1
0.00.049.689 I llm_load_print_meta: pooling type     = 0
0.00.049.689 I llm_load_print_meta: rope type        = 2
0.00.049.691 I llm_load_print_meta: rope scaling     = linear
0.00.049.695 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.700 I llm_load_print_meta: freq_scale_train = 1
0.00.049.702 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.702 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.702 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.702 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.702 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.703 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.703 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.703 I llm_load_print_meta: model type       = 1.4B
0.00.049.715 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.717 I llm_load_print_meta: model params     = 1.41 B
0.00.049.717 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.718 I llm_load_print_meta: general.name     = 1.4B
0.00.049.718 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.718 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.718 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.718 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.719 I llm_load_print_meta: LF token         = 128 ''
0.00.049.720 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.722 I llm_load_print_meta: max token length = 1024
0.00.051.728 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.728 I llm_load_tensors: offloading output layer to GPU
0.00.051.728 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.739 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.741 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.173 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.174 I llama_new_context_with_model: n_ctx         = 128
0.00.052.174 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.174 I llama_new_context_with_model: n_batch       = 128
0.00.052.174 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.175 I llama_new_context_with_model: flash_attn    = 0
0.00.052.175 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.175 I llama_new_context_with_model: freq_scale    = 1
0.00.052.176 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.176 I ggml_metal_init: allocating
0.00.052.179 I ggml_metal_init: found device: Apple M4
0.00.052.181 I ggml_metal_init: picking default device: Apple M4
0.00.052.780 I ggml_metal_init: using embedded metal library
0.00.055.146 I ggml_metal_init: GPU name:   Apple M4
0.00.055.147 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.147 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.148 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.148 I ggml_metal_init: simdgroup reduction   = true
0.00.055.148 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.148 I ggml_metal_init: has bfloat            = true
0.00.055.148 I ggml_metal_init: use bfloat            = true
0.00.055.149 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.151 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.608 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.021 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.028 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.044 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.918 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.919 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.920 I llama_new_context_with_model: graph nodes  = 967
0.00.066.920 I llama_new_context_with_model: graph splits = 2
0.00.066.921 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.384.264 I 
0.00.384.289 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.384.299 I perplexity: tokenizing the input ..
0.00.391.898 I perplexity: tokenization took 7.597 ms
0.00.391.907 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.523.495 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.524.917 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.524.936 I llama_perf_context_print:        load time =     375.56 ms
0.00.524.937 I llama_perf_context_print: prompt eval time =     131.34 ms /   128 tokens (    1.03 ms per token,   974.55 tokens per second)
0.00.524.937 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.524.938 I llama_perf_context_print:       total time =     140.67 ms /   129 tokens
0.00.525.256 I ggml_metal_free: deallocating

real	0m0.540s
user	0m0.077s
sys	0m0.069s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.690 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.620 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.628 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.629 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.629 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.631 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.631 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.632 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.633 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.634 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.635 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.432 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.280 I llama_model_loader: - type  f32:  194 tensors
0.00.024.280 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.280 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.280 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.281 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.642 I llm_load_vocab: special tokens cache size = 25
0.00.050.569 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.573 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.574 I llm_load_print_meta: arch             = gptneox
0.00.050.574 I llm_load_print_meta: vocab type       = BPE
0.00.050.574 I llm_load_print_meta: n_vocab          = 50304
0.00.050.575 I llm_load_print_meta: n_merges         = 50009
0.00.050.576 I llm_load_print_meta: vocab_only       = 0
0.00.050.576 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.578 I llm_load_print_meta: n_embd           = 2048
0.00.050.579 I llm_load_print_meta: n_layer          = 24
0.00.050.582 I llm_load_print_meta: n_head           = 16
0.00.050.583 I llm_load_print_meta: n_head_kv        = 16
0.00.050.583 I llm_load_print_meta: n_rot            = 32
0.00.050.584 I llm_load_print_meta: n_swa            = 0
0.00.050.584 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.584 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.584 I llm_load_print_meta: n_gqa            = 1
0.00.050.585 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.585 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.586 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.586 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.586 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.587 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.587 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.587 I llm_load_print_meta: n_ff             = 8192
0.00.050.587 I llm_load_print_meta: n_expert         = 0
0.00.050.588 I llm_load_print_meta: n_expert_used    = 0
0.00.050.588 I llm_load_print_meta: causal attn      = 1
0.00.050.588 I llm_load_print_meta: pooling type     = 0
0.00.050.590 I llm_load_print_meta: rope type        = 2
0.00.050.591 I llm_load_print_meta: rope scaling     = linear
0.00.050.591 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.591 I llm_load_print_meta: freq_scale_train = 1
0.00.050.591 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.592 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.593 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.593 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.593 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.593 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.593 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.594 I llm_load_print_meta: model type       = 1.4B
0.00.050.607 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.607 I llm_load_print_meta: model params     = 1.41 B
0.00.050.608 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.608 I llm_load_print_meta: general.name     = 1.4B
0.00.050.608 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.608 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.608 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.609 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.609 I llm_load_print_meta: LF token         = 128 ''
0.00.050.609 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.609 I llm_load_print_meta: max token length = 1024
0.00.052.600 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.601 I llm_load_tensors: offloading output layer to GPU
0.00.052.601 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.612 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.613 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.942 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.943 I llama_new_context_with_model: n_ctx         = 128
0.00.052.943 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.943 I llama_new_context_with_model: n_batch       = 128
0.00.052.943 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.943 I llama_new_context_with_model: flash_attn    = 0
0.00.052.944 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.944 I llama_new_context_with_model: freq_scale    = 1
0.00.052.944 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.945 I ggml_metal_init: allocating
0.00.052.948 I ggml_metal_init: found device: Apple M4
0.00.052.950 I ggml_metal_init: picking default device: Apple M4
0.00.053.589 I ggml_metal_init: using embedded metal library
0.00.055.954 I ggml_metal_init: GPU name:   Apple M4
0.00.055.955 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.956 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.956 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.958 I ggml_metal_init: simdgroup reduction   = true
0.00.055.958 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.958 I ggml_metal_init: has bfloat            = true
0.00.055.958 I ggml_metal_init: use bfloat            = true
0.00.055.959 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.960 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.098 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.381 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.384 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.400 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.335 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.336 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.336 I llama_new_context_with_model: graph nodes  = 967
0.00.068.336 I llama_new_context_with_model: graph splits = 2
0.00.068.338 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.338 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.480.021 I 
0.00.480.044 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.480.053 I perplexity: tokenizing the input ..
0.00.487.233 I perplexity: tokenization took 7.178 ms
0.00.487.236 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.727 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.621.121 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.621.153 I llama_perf_context_print:        load time =     471.33 ms
0.00.621.154 I llama_perf_context_print: prompt eval time =     132.27 ms /   128 tokens (    1.03 ms per token,   967.71 tokens per second)
0.00.621.154 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.155 I llama_perf_context_print:       total time =     141.13 ms /   129 tokens
0.00.621.697 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.078s
sys	0m0.082s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.623 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.249 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.250 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.252 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.253 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.255 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.256 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.986 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.733 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.734 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.734 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.734 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.735 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.735 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.735 I llama_model_loader: - type  f32:  194 tensors
0.00.025.736 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.736 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.736 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.385 I llm_load_vocab: special tokens cache size = 25
0.00.051.418 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.421 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.422 I llm_load_print_meta: arch             = gptneox
0.00.051.422 I llm_load_print_meta: vocab type       = BPE
0.00.051.422 I llm_load_print_meta: n_vocab          = 50304
0.00.051.422 I llm_load_print_meta: n_merges         = 50009
0.00.051.422 I llm_load_print_meta: vocab_only       = 0
0.00.051.423 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.423 I llm_load_print_meta: n_embd           = 2048
0.00.051.423 I llm_load_print_meta: n_layer          = 24
0.00.051.425 I llm_load_print_meta: n_head           = 16
0.00.051.426 I llm_load_print_meta: n_head_kv        = 16
0.00.051.426 I llm_load_print_meta: n_rot            = 32
0.00.051.426 I llm_load_print_meta: n_swa            = 0
0.00.051.427 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.427 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.427 I llm_load_print_meta: n_gqa            = 1
0.00.051.428 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.429 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.429 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.430 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.432 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.432 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.432 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.433 I llm_load_print_meta: n_ff             = 8192
0.00.051.433 I llm_load_print_meta: n_expert         = 0
0.00.051.433 I llm_load_print_meta: n_expert_used    = 0
0.00.051.433 I llm_load_print_meta: causal attn      = 1
0.00.051.433 I llm_load_print_meta: pooling type     = 0
0.00.051.433 I llm_load_print_meta: rope type        = 2
0.00.051.434 I llm_load_print_meta: rope scaling     = linear
0.00.051.436 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.436 I llm_load_print_meta: freq_scale_train = 1
0.00.051.437 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.437 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.437 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.437 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.437 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.437 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.438 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.438 I llm_load_print_meta: model type       = 1.4B
0.00.051.450 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.450 I llm_load_print_meta: model params     = 1.41 B
0.00.051.451 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.451 I llm_load_print_meta: general.name     = 1.4B
0.00.051.451 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.451 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.451 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.451 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.452 I llm_load_print_meta: LF token         = 128 ''
0.00.051.452 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.452 I llm_load_print_meta: max token length = 1024
0.00.053.482 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.483 I llm_load_tensors: offloading output layer to GPU
0.00.053.483 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.493 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.495 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.815 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.815 I llama_new_context_with_model: n_ctx         = 128
0.00.053.816 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.816 I llama_new_context_with_model: n_batch       = 128
0.00.053.816 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.816 I llama_new_context_with_model: flash_attn    = 0
0.00.053.816 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.817 I llama_new_context_with_model: freq_scale    = 1
0.00.053.817 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.818 I ggml_metal_init: allocating
0.00.053.820 I ggml_metal_init: found device: Apple M4
0.00.053.822 I ggml_metal_init: picking default device: Apple M4
0.00.054.401 I ggml_metal_init: using embedded metal library
0.00.056.743 I ggml_metal_init: GPU name:   Apple M4
0.00.056.744 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.745 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.745 I ggml_metal_init: simdgroup reduction   = true
0.00.056.746 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.746 I ggml_metal_init: has bfloat            = true
0.00.056.746 I ggml_metal_init: use bfloat            = true
0.00.056.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.747 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.105 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.409 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.412 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.426 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.366 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.367 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.367 I llama_new_context_with_model: graph nodes  = 967
0.00.068.367 I llama_new_context_with_model: graph splits = 2
0.00.068.368 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.556.001 I 
0.00.556.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.556.041 I perplexity: tokenizing the input ..
0.00.564.050 I perplexity: tokenization took 8.007 ms
0.00.564.059 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.698.357 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.699.503 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.699.529 I llama_perf_context_print:        load time =     545.37 ms
0.00.699.532 I llama_perf_context_print: prompt eval time =     134.07 ms /   128 tokens (    1.05 ms per token,   954.71 tokens per second)
0.00.699.534 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.699.534 I llama_perf_context_print:       total time =     143.53 ms /   129 tokens
0.00.699.976 I ggml_metal_free: deallocating

real	0m0.714s
user	0m0.078s
sys	0m0.094s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.134 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.835 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.835 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.836 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.836 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.836 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.837 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.838 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.838 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.840 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.841 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.841 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.842 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.844 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.845 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.845 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.570 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.333 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.334 I llama_model_loader: - type  f32:  194 tensors
0.00.024.334 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.334 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.781 I llm_load_vocab: special tokens cache size = 25
0.00.050.813 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.815 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.815 I llm_load_print_meta: arch             = gptneox
0.00.050.816 I llm_load_print_meta: vocab type       = BPE
0.00.050.816 I llm_load_print_meta: n_vocab          = 50304
0.00.050.816 I llm_load_print_meta: n_merges         = 50009
0.00.050.816 I llm_load_print_meta: vocab_only       = 0
0.00.050.817 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.817 I llm_load_print_meta: n_embd           = 2048
0.00.050.817 I llm_load_print_meta: n_layer          = 24
0.00.050.820 I llm_load_print_meta: n_head           = 16
0.00.050.820 I llm_load_print_meta: n_head_kv        = 16
0.00.050.821 I llm_load_print_meta: n_rot            = 32
0.00.050.821 I llm_load_print_meta: n_swa            = 0
0.00.050.821 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.821 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.822 I llm_load_print_meta: n_gqa            = 1
0.00.050.823 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.823 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.824 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.824 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.824 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.825 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.825 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.827 I llm_load_print_meta: n_ff             = 8192
0.00.050.827 I llm_load_print_meta: n_expert         = 0
0.00.050.827 I llm_load_print_meta: n_expert_used    = 0
0.00.050.828 I llm_load_print_meta: causal attn      = 1
0.00.050.828 I llm_load_print_meta: pooling type     = 0
0.00.050.829 I llm_load_print_meta: rope type        = 2
0.00.050.829 I llm_load_print_meta: rope scaling     = linear
0.00.050.830 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.830 I llm_load_print_meta: freq_scale_train = 1
0.00.050.830 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.831 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.831 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.831 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.831 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.831 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.831 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.832 I llm_load_print_meta: model type       = 1.4B
0.00.050.843 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.844 I llm_load_print_meta: model params     = 1.41 B
0.00.050.844 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.845 I llm_load_print_meta: general.name     = 1.4B
0.00.050.845 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.845 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.845 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.846 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.846 I llm_load_print_meta: LF token         = 128 ''
0.00.050.847 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.847 I llm_load_print_meta: max token length = 1024
0.00.052.881 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.881 I llm_load_tensors: offloading output layer to GPU
0.00.052.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.892 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.893 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.220 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.220 I llama_new_context_with_model: n_ctx         = 128
0.00.053.221 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.221 I llama_new_context_with_model: n_batch       = 128
0.00.053.221 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.221 I llama_new_context_with_model: flash_attn    = 0
0.00.053.221 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.222 I llama_new_context_with_model: freq_scale    = 1
0.00.053.222 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.222 I ggml_metal_init: allocating
0.00.053.225 I ggml_metal_init: found device: Apple M4
0.00.053.227 I ggml_metal_init: picking default device: Apple M4
0.00.053.792 I ggml_metal_init: using embedded metal library
0.00.056.200 I ggml_metal_init: GPU name:   Apple M4
0.00.056.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.202 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.202 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.203 I ggml_metal_init: simdgroup reduction   = true
0.00.056.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.203 I ggml_metal_init: has bfloat            = true
0.00.056.203 I ggml_metal_init: use bfloat            = true
0.00.056.203 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.204 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.727 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.970 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.974 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.990 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.925 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.927 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.927 I llama_new_context_with_model: graph nodes  = 967
0.00.067.927 I llama_new_context_with_model: graph splits = 2
0.00.067.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.929 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.255 I 
0.00.615.285 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.300 I perplexity: tokenizing the input ..
0.00.623.495 I perplexity: tokenization took 8.194 ms
0.00.623.498 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.669 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.765.856 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.765.874 I llama_perf_context_print:        load time =     606.11 ms
0.00.765.875 I llama_perf_context_print: prompt eval time =     140.91 ms /   128 tokens (    1.10 ms per token,   908.38 tokens per second)
0.00.765.875 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.876 I llama_perf_context_print:       total time =     150.62 ms /   129 tokens
0.00.766.208 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.079s
sys	0m0.100s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.279 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.756 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.757 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.758 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.758 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.758 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.759 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.759 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.760 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.760 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.761 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.761 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.761 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.763 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.763 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.764 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.427 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.121 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.122 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.123 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.123 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.123 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.123 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.124 I llama_model_loader: - type  f32:  194 tensors
0.00.024.124 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.652 I llm_load_vocab: special tokens cache size = 25
0.00.049.682 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.684 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.684 I llm_load_print_meta: arch             = gptneox
0.00.049.685 I llm_load_print_meta: vocab type       = BPE
0.00.049.685 I llm_load_print_meta: n_vocab          = 50304
0.00.049.685 I llm_load_print_meta: n_merges         = 50009
0.00.049.685 I llm_load_print_meta: vocab_only       = 0
0.00.049.686 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.686 I llm_load_print_meta: n_embd           = 2048
0.00.049.686 I llm_load_print_meta: n_layer          = 24
0.00.049.689 I llm_load_print_meta: n_head           = 16
0.00.049.689 I llm_load_print_meta: n_head_kv        = 16
0.00.049.690 I llm_load_print_meta: n_rot            = 32
0.00.049.690 I llm_load_print_meta: n_swa            = 0
0.00.049.690 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.690 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.691 I llm_load_print_meta: n_gqa            = 1
0.00.049.692 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.692 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.694 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.695 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.695 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.695 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.697 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.698 I llm_load_print_meta: n_ff             = 8192
0.00.049.698 I llm_load_print_meta: n_expert         = 0
0.00.049.698 I llm_load_print_meta: n_expert_used    = 0
0.00.049.698 I llm_load_print_meta: causal attn      = 1
0.00.049.698 I llm_load_print_meta: pooling type     = 0
0.00.049.698 I llm_load_print_meta: rope type        = 2
0.00.049.700 I llm_load_print_meta: rope scaling     = linear
0.00.049.700 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.701 I llm_load_print_meta: freq_scale_train = 1
0.00.049.701 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.701 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.701 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.701 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.701 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.702 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.702 I llm_load_print_meta: model type       = 1.4B
0.00.049.709 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.710 I llm_load_print_meta: model params     = 1.41 B
0.00.049.711 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.711 I llm_load_print_meta: general.name     = 1.4B
0.00.049.711 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.711 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.712 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.712 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.712 I llm_load_print_meta: LF token         = 128 ''
0.00.049.712 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.712 I llm_load_print_meta: max token length = 1024
0.00.051.508 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.508 I llm_load_tensors: offloading output layer to GPU
0.00.051.509 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.514 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.515 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.863 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.864 I llama_new_context_with_model: n_ctx         = 128
0.00.051.864 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.864 I llama_new_context_with_model: n_batch       = 128
0.00.051.865 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.865 I llama_new_context_with_model: flash_attn    = 0
0.00.051.865 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.865 I llama_new_context_with_model: freq_scale    = 1
0.00.051.866 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.866 I ggml_metal_init: allocating
0.00.051.869 I ggml_metal_init: found device: Apple M4
0.00.051.871 I ggml_metal_init: picking default device: Apple M4
0.00.052.429 I ggml_metal_init: using embedded metal library
0.00.054.723 I ggml_metal_init: GPU name:   Apple M4
0.00.054.725 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.725 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.726 I ggml_metal_init: simdgroup reduction   = true
0.00.054.726 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.726 I ggml_metal_init: has bfloat            = true
0.00.054.726 I ggml_metal_init: use bfloat            = true
0.00.054.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.727 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.007 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.306 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.311 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.326 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.175 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.177 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.177 I llama_new_context_with_model: graph nodes  = 967
0.00.066.177 I llama_new_context_with_model: graph splits = 2
0.00.066.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.095 I 
0.00.648.127 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.165 I perplexity: tokenizing the input ..
0.00.655.987 I perplexity: tokenization took 7.819 ms
0.00.655.991 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.858 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.797.036 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.797.063 I llama_perf_context_print:        load time =     638.81 ms
0.00.797.064 I llama_perf_context_print: prompt eval time =     139.64 ms /   128 tokens (    1.09 ms per token,   916.62 tokens per second)
0.00.797.065 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.066 I llama_perf_context_print:       total time =     148.98 ms /   129 tokens
0.00.797.562 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.077s
sys	0m0.115s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.304 I build: 4461 (2739a71e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.732 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.018 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.025 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.026 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.032 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.032 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.033 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.036 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.036 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.037 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.041 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.042 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.043 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.046 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.465 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.470 I llama_model_loader: - type  f32:  194 tensors
0.00.055.470 I llama_model_loader: - type  f16:   98 tensors
0.00.084.789 I llm_load_vocab: special tokens cache size = 25
0.00.091.536 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.538 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.539 I llm_load_print_meta: arch             = gptneox
0.00.091.539 I llm_load_print_meta: vocab type       = BPE
0.00.091.539 I llm_load_print_meta: n_vocab          = 50304
0.00.091.539 I llm_load_print_meta: n_merges         = 50009
0.00.091.539 I llm_load_print_meta: vocab_only       = 0
0.00.091.540 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.540 I llm_load_print_meta: n_embd           = 2048
0.00.091.540 I llm_load_print_meta: n_layer          = 24
0.00.091.543 I llm_load_print_meta: n_head           = 16
0.00.091.545 I llm_load_print_meta: n_head_kv        = 16
0.00.091.545 I llm_load_print_meta: n_rot            = 32
0.00.091.545 I llm_load_print_meta: n_swa            = 0
0.00.091.545 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.546 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.546 I llm_load_print_meta: n_gqa            = 1
0.00.091.551 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.552 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.552 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.552 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.553 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.555 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.556 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.556 I llm_load_print_meta: n_ff             = 8192
0.00.091.556 I llm_load_print_meta: n_expert         = 0
0.00.091.557 I llm_load_print_meta: n_expert_used    = 0
0.00.091.557 I llm_load_print_meta: causal attn      = 1
0.00.091.557 I llm_load_print_meta: pooling type     = 0
0.00.091.557 I llm_load_print_meta: rope type        = 2
0.00.091.557 I llm_load_print_meta: rope scaling     = linear
0.00.091.558 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.558 I llm_load_print_meta: freq_scale_train = 1
0.00.091.558 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.558 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.558 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.559 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.559 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.559 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.559 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.559 I llm_load_print_meta: model type       = 1.4B
0.00.091.571 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.571 I llm_load_print_meta: model params     = 1.41 B
0.00.091.572 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.572 I llm_load_print_meta: general.name     = 1.4B
0.00.091.572 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.573 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.573 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.573 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.573 I llm_load_print_meta: LF token         = 128 ''
0.00.091.574 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.574 I llm_load_print_meta: max token length = 1024
0.00.094.108 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.109 I llm_load_tensors: offloading output layer to GPU
0.00.094.109 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.120 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.121 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.430 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.430 I llama_new_context_with_model: n_ctx         = 128
0.00.094.431 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.431 I llama_new_context_with_model: n_batch       = 128
0.00.094.431 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.431 I llama_new_context_with_model: flash_attn    = 0
0.00.094.431 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.432 I llama_new_context_with_model: freq_scale    = 1
0.00.094.432 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.432 I ggml_metal_init: allocating
0.00.094.436 I ggml_metal_init: found device: Apple M4
0.00.094.437 I ggml_metal_init: picking default device: Apple M4
0.00.095.051 I ggml_metal_init: using embedded metal library
0.00.097.554 I ggml_metal_init: GPU name:   Apple M4
0.00.097.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.557 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.557 I ggml_metal_init: simdgroup reduction   = true
0.00.097.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.557 I ggml_metal_init: has bfloat            = true
0.00.097.557 I ggml_metal_init: use bfloat            = true
0.00.097.558 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.558 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.792 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.049 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.051 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.065 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.954 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.955 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.955 I llama_new_context_with_model: graph nodes  = 967
0.00.108.955 I llama_new_context_with_model: graph splits = 2
0.00.108.957 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.957 I 
0.00.108.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.108.984 I compute_imatrix: tokenizing the input ..
0.00.115.479 I compute_imatrix: tokenization took 6.494 ms
0.00.115.480 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.692.857 I compute_imatrix: 1.58 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.696.979 I llama_perf_context_print:        load time =    1669.12 ms
0.01.696.980 I llama_perf_context_print: prompt eval time =    1576.90 ms /   128 tokens (   12.32 ms per token,    81.17 tokens per second)
0.01.696.981 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.696.982 I llama_perf_context_print:       total time =    1673.23 ms /   129 tokens
0.01.697.573 I ggml_metal_free: deallocating

real	0m1.881s
user	0m0.174s
sys	0m0.252s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149c0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149c0b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149c0bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149c0c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149c0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149c0cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149c0d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149c0d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149c0dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149c0e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149c0e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149c0ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149c0f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149c0ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149c10770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149c10e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149c115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149c11cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149c123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149c12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149c132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149c13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149c14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149c149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149c150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149c153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149c159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149c16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149c16b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149c16e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149c172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149c17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149c17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149c18350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149c18610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149c18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149c18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149c193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149c19890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149c19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149c1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149c1a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149c1ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149c1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149c1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149c1b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149c1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149c1c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149c1cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149c1d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149c1d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149c1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149c1e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149c1ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149c1f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149c1f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149c1fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149c20000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149c20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149c20e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149c210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149c21560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149c21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149c21ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149c22340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149c227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149c22c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149c23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149c235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149c23a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149c23f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149c243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149c24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149c24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149c252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149c25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149c25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149c262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149c26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149c26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149c272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149c27810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149c27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149c282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149c28800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149c28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149c292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149c297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149c29d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149c2a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149c2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149c2ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149c2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149c2b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149c2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149c2c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149c2c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149c1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149c2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149c2d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149c2d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149c2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149c2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149c2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149c2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149c2f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149c2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149c2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149c303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149c30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149c30e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149c313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149c318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149c31d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149c32230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149c326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149c32b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149c33010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149c334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149c33950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149c33df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149c34290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149c34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149c34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149c35070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149c35510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149c359b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149c35e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149c362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149c36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149c36c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149c370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149c37570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149c37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149c37eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149c38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149c387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149c38c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149c39130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149c395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149c39a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149c39f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149c3a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149c3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149c3acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149c3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149c3b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149c3bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149c3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149c3c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149c3c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149c3cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149c3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149c3d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149c3db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149c3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149c3e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149c3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149c3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149c3f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149c3f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149c3fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149c40030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149c404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149c40970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149c40e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149c412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149c41750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149c41bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149c42090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149c42530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149c429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149c42e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149c43310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149c437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149c43c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149c440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149c44590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149c44a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149c44ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149c45370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149c45810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149c45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149c46150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149c465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149c46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149c46f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149c473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149c47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149c47d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149c481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149c48650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149c48af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149c49040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149c49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149c49ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149c4a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149c4a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149c4a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149c4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149c4b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149c4bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149c4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149c4c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149c4ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149c4d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149c4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149c4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149c4e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149c4e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149c4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149c4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149c4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149c4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149c50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149c508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149c50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149c51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149c51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149c51de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149c52330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149c52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149c52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149c53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149c53870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149c53dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149c54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149c54860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149c54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149c55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149c55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149c55da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149c562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149c56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149c56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149c572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149c57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149c57d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149c582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149c58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149c58d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149c592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149c59810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149c59d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149c5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149c5a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149c5ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149c5b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149c5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149c5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149c5c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149c5c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149c5cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149c5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149c5d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149c5dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149c5e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149c5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149c5ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149c5f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149c5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149c5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149c60250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149c607a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149c60cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149c61240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149c61790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149c61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149c620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149c62570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149c62a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149c62eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149c63350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149c637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149c63c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149c64130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149c645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149c64a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149c64f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149c653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149c65850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149c65cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149c66240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149c66960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149c67080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149c677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149c67ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149c68180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149c68970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149c68c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149c69240 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.143.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.207 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149d05ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149d06360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149d067d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149d06c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149d070b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149d07520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149d07990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149d07e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149d08270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149d086e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149d08b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149d09240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149d09d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149d0a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149d0ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149d0b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149d0bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149d0c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149d0c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149d0d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149d0d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149d0df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149d0e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149d0ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149d0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149d0f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149d0f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149d0fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149d102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149d10740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149d10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149d110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149d11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149d11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149d11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149d120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149d12560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149d129d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149d12e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149d132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149d13720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149d13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149d14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149d14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149d148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149d14d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149d151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149d15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149d15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149d15f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149d16380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149d167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149d16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149d170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149d17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149d179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149d17f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149d18420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149d18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149c4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149c4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149c68ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149c4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149c4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149c1e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149c1dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149c202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149c4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149c15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149c1c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149c1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149c1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149c1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149c1b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149c1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149c1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149c14660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149c0ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149c0a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149c1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149c208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149c2cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149c68440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149c17840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149c17b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149c4d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149c4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149c15c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149c15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149c161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149c696a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149c69960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149c69c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149c69ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149c6a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149c6a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149c6a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149c6a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149c6aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149c6af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149c6b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149c6b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149c6b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149c6ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149c6bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149c6bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149c6c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149c6c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149c6c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149c6cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149c6cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149c6d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149c6d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149c6d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149c6d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149c6db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149c6de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149c6e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149c6e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149c6e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149c6e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149c6ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149c6eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149c6f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149c6f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149c6f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149c6f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149c6fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149c6ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149c701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149c704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149c70760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149c70a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149c70ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149c70fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149c71260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149c71520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149c717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149c71aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149c71d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149c72020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149c722e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149c725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149c72860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149c72b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149c72de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149c730a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149c73360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149c73620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149c738e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149c73ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149c73e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149c74120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149c743e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149c746a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149c74960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149c74c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149c74ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149c751a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149c75460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149c75720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149c759e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149c75ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149c75f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149c76220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149c764e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149c767a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149c76a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149c76d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149c76fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149c772a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149c77560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149c77820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149c77ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149c77da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149c78060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149c78320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149c785e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149c788a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149c78b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149c78e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149c790e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149c793a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149c79660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149c79920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149c79be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149c79ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149c7a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149c7a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149c7a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149c7a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149c7ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149c7af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149c7b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149c7b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149c7b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149c7ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149c7bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149c7bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149c7c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149c7c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149c7c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149c7caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149c7cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149c7d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149c7d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149c7d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149c7d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149c7db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149c7dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149c7e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149c7e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149c7e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149c7ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149c7eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149c7f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149c7f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149c7f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149c7f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149c7fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149c7ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149c801f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149c804b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149c80770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149c80a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149c80cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149c80fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149c81270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149c81530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149c817f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149c81ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149c81d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149c82030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149c822f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149c825b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149c82870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149c82b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149c82df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149c830b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149c83370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149c83630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149c838f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149c83bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149c83e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149c84130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149c843f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149c846b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149c84970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149c84c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149c84ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149c851b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149c85470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149c85730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149c859f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149c85cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149c85f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149c86230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149c864f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149c867b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149c86a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149c86d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149c86ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149c872b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149c87570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149c87830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149c87af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149c87db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149c88070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149c88330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149c885f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149c888b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149c88b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149c88e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149c890f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149c893b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149c89670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149c89930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149c89bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149c89eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149c8a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149c8a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149c8a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149c8a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149c8ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149c8af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149c8b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149c8b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149c8b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149c8ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149c8bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149c8bfb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149c8c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149c8c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149c8cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149c8cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149c8d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149c8d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149c8d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149c8d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149c8dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149c8df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149c8e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149c8e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149c8eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149c8f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149c8f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149c8f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149c8fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149c8fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149c901b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149c90470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149c90730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149c909f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149c90cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149c90f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149c91230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149c914f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149c917b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149c91a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149c91d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149c91ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149c922b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149c92570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149c92830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149c92af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149c92db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149c93070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149c93330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149c935f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149c938b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149c93b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149c93e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149c940f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149c943b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149c94670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149c94930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149c94bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149c94eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149c95170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149c95430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149c956f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149c959b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149c95c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149c95f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149c961f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149c964b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149c96770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149c96a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149c96cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149c96fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149c97270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149c97530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149c977f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149c97ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149c97d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149c98030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149c982f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149c985b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149c98870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149c98b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149c98df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149c990b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149c99370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149c99630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149c998f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149c99bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149c99e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149c9a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149c9a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149c9a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149c9a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149c9ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149c9aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149c9b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149c9b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149c9b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149c9b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149c9bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149c9bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149c9c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149c9c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149c9c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149c9ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149c9cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149c9cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149c9d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149c9d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149c9d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149c9daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149c9ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149c9e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149c9e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149c9e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149c9e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149c9eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149c9ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149c9f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149c9f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149c9f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149c9f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149c9fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149c9feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149ca0170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149ca0430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149ca06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149ca09b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149ca0c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149ca0f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149ca11f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149ca14b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149ca1770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149ca1a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149ca1cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149ca1fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149ca2270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149ca2530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149ca27f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149ca2ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149ca2d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149ca3030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149ca32f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149ca35b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149ca3870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149ca3b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149ca3df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149ca40b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149ca4370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149ca4630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149ca48f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149ca4bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149ca4e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149ca5130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149ca53f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149ca56b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149ca5970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149ca5c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149ca5ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149ca61b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149ca6470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149ca6730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149ca69f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149ca6cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149ca6f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149ca7230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149ca74f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149ca77b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149ca7a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149ca7d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149ca7ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149ca82b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149ca8570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149ca8830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149ca8af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149ca8db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149ca9070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149ca9330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149ca95f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149ca98b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149ca9b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149ca9e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149caa0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149caa3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149caa670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149caa930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149caabf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149caaeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149cab170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149cab430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149cab6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149cab9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149cabc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149cabf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149cac1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149cac4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149cac770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149caca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149caccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149cacfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149cad270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149cad530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149cad7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149cadab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149cadd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149cae030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149cae2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149cae5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149cae870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149caeb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149caedf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149caf0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149caf370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149caf630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149caf8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149cafbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149cafe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149cb0130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149cb03f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149cb06b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149cb0970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149cb0c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149cb0ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149cb14c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149cb1780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149cb1a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149cb1d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149cb1fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149cb2280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149cb2540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149cb2800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149cb2ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149cb2d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149cb3040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149cb3300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149cb35c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149cb3880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149cb3b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149cb3e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149cb40c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149cb4380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149cb4640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149cb4900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149cb4bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149cb4e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149cb5140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149cb5400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149cb56c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149cb5980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149cb5c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149cb5f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149cb61c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149cb6480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149cb6740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149cb6a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149cb6f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149cb74a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149cb79f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149cb7f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149cb8490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149cb89e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149cb8f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149cb9480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149cb99d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149cb9f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149cba470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149cba9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149cbaf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149cbb460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149cbb9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149cbbf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149cbc450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149cbc9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149cbcef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149cbd440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149cbd990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149cbdee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149cbe430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149cbe980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149cbeed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149cbf190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149cbf450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149cbf950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149cbfe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149cc0350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149cc0850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149cc0d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149cc1250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149cc1750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149cc1c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149cc2150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149cc2650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149cc2b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149cc3050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149cc3550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149cc3a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149cc4460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149cc4b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149cc52a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149cc59c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149cc5c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149cc6470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149cc6730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149cc6d40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.848s
user	0m0.298s
sys	0m0.316s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4461 (2739a71e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12fe07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12fe07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12fe08010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12fe085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12fe08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12fe09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12fe096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12fe09c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12fe0a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12fe0a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12fe0ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12fe0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12fe0bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12fe0c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12fe0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12fe0d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12fe0da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12fe0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12fe0e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12fe0f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12fe0f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12fe0fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12fe105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12fe10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12fe11580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12fe11840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12fe11e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12fe12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12fe13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12fe132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12fe13760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12fe13a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12fe142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12fe147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12fe14ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12fe14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12fe153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12fe15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12fe15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12fe161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12fe16670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12fe16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12fe16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12fe17450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12fe17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12fe17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12fe18330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12fe18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12fe19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12fe19870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12fe19e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12fe1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12fe1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12fe1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12fe1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12fe1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12fe1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12fe1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12fe1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12fe1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12fe1d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12fe1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12fe1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12fe1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12fe1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12fe1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12fe1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12fe1f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12fe1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12fe1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12fe203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12fe20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12fe20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12fe21230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12fe21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12fe21cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12fe22220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12fe22770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12fe22cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12fe23210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12fe23760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12fe23cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12fe24200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12fe24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12fe24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12fe251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12fe25740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12fe25c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12fe261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12fe26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12fe26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12fe271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12fe27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12fe27c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12fe281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12fe28710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12fe28c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12fe18940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12fe290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12fe29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12fe29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12fe2a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12fe2a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12fe2adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12fe2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12fe2b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12fe2bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12fe2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12fe2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12fe2cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12fe2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12fe2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12fe2dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12fe2e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12fe2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12fe2eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12fe2f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12fe2f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12fe2f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12fe2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12fe30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12fe30730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12fe30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12fe31070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12fe31510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12fe319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12fe31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12fe322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12fe32790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12fe32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12fe330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12fe33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12fe33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12fe33eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12fe34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12fe347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12fe34c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12fe35130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12fe355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12fe35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12fe35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12fe363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12fe36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12fe36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12fe37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12fe37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12fe37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12fe37f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12fe38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12fe388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12fe38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12fe391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12fe39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12fe39b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12fe39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12fe3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12fe3a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12fe3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12fe3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12fe3b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12fe3bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12fe3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12fe3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12fe3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12fe3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12fe3d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12fe3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12fe3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12fe3e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12fe3e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12fe3e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12fe3ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12fe3f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12fe3f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12fe3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12fe400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12fe40590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12fe40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12fe40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12fe41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12fe41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12fe41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12fe42150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12fe425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12fe42a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12fe42f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12fe433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12fe43870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12fe43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12fe441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12fe44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12fe44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12fe44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12fe454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12fe45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12fe45f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12fe464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12fe46790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12fe46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12fe473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12fe479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12fe481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12fe48650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12fe48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12fe48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12fe49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12fe49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12fe4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12fe4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12fe4ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12fe4b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12fe4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12fe4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12fe4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12fe4c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12fe4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12fe4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12fe4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12fe4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12fe4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12fe4e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12fe4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12fe4f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12fe4f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12fe4fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12fe50260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12fe507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12fe50d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12fe51250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12fe517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12fe51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12fe52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12fe52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12fe52ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12fe53230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12fe53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12fe53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12fe54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12fe54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12fe54cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12fe55210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12fe55760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12fe55cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12fe56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12fe56750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12fe56ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12fe571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12fe57740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12fe57c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12fe581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12fe58730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12fe58c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12fe591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12fe59720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12fe59c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12fe5a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12fe5a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12fe5ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12fe5b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12fe5b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12fe5bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12fe5c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12fe5c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12fe5cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12fe5d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12fe5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12fe5dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12fe5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12fe5e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12fe5ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12fe5eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12fe5f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12fe5f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12fe5fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12fe60130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12fe605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12fe60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12fe60f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12fe613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12fe61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12fe61cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12fe62190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12fe626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12fe62e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12fe63520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12fe63c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12fe64360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12fe64620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12fe64e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12fe650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12fe656e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.096.038 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff09730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff09ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff0a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff0a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff0a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff0ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff0b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff0caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff0eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff0f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff11f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff12d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff13030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff13760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff13bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ff14040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff14ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff15180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff15a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff15fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ff169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff16ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ff173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff17dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff18c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff19510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ff19980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ff1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff1ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ff1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ff1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff1c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff1c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ff1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ff1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ff1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ff1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ff1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ff1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ff1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ff1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ff20430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ff208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ff20d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ff212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ff21810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ff21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ff222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ff22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ff22d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ff232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ff237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ff23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ff24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ff247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ff24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ff25280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ff257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ff25d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ff26270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ff267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ff26d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ff27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ff277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ff27d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ff28250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ff287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ff28cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ff29240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ff29790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ff29ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ff2a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ff2a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ff2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ff2b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ff2b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ff2bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ff2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ff2c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ff2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ff2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ff2d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ff2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ff2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ff2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ff2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ff2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ff2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ff2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ff2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ff30250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ff306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ff30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ff31030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ff314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ff31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ff31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ff322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ff32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ff32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ff33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ff33530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ff339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ff33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ff34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ff347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ff34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ff350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ff35590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ff35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ff35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ff36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ff36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ff36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ff37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ff375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ff37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ff37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ff383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ff38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ff38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ff391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ff39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ff39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ff39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ff3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ff3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ff3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ff3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ff3b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ff3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ff3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ff3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ff3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ff3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ff3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ff3d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ff3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ff3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ff3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ff3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ff3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ff3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ff3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ff3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ff400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ff40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ff409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ff40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ff41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ff417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ff41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ff42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ff425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ff42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ff42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ff43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ff43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ff43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ff44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ff44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ff44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ff44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ff453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ff45940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ff45e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ff463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ff46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ff46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ff47200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ff47810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ff47e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ff48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ff48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ff48d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ff49380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ff49990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ff4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ff4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ff4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ff4af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ff4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ff4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ff4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ff4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ff4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ff4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ff4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ff4dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ff4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ff4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ff4ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ff4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ff4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ff4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ff50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ff506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ff50c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ff51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ff516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ff51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ff52150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ff526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ff52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ff53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ff53690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ff53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ff54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ff54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ff54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ff55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ff55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ff55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ff56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ff56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ff56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ff57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ff57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ff57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ff580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ff58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ff58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ff590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ff59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ff59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ff5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ff5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ff5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ff5b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ff5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ff5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ff5c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ff5c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ff5cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ff5d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ff5d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ff5db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ff5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ff5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ff5e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ff5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ff5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ff5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ff5fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ff600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ff60590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ff60a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ff60ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ff61370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ff61810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ff61cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ff62150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ff625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ff62b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ff63260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ff63980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ff640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ff647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ff64a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ff65270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ff65530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ff65b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12fe65390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12fe48bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12fe46a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12fe47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12fe1a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12fe1a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12fe1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12fe491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12fe11b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12fe185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12fe18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12fe19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12fe179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12fe19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12fe10b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12fe1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12fe29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12fe648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12fe13ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12fe13fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12fe497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12fe47c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12fe12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12fe123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12fe12690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12fe65b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12fe65e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12fe660c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12fe66380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12fe66640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12fe66900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12fe66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12fe66e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12fe67140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12fe67400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12fe676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12fe67980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12fe67c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12fe67f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12fe681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12fe68480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12fe68740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12fe68a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12fe68cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12fe68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12fe69240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12fe69500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12fe697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12fe69a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12fe69d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12fe6a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12fe6a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12fe6a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12fe6a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12fe6ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12fe6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12fe6b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12fe6b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12fe6b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12fe6b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12fe6bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12fe6be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12fe6c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12fe6c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12fe6c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12fe6c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12fe6cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12fe6cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12fe6d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12fe6d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12fe6d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12fe6d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12fe6dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12fe6df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12fe6e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12fe6e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12fe6e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12fe6ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12fe6ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12fe6efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12fe6f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12fe6f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12fe6f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12fe6fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12fe6fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12fe70040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12fe70300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12fe705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12fe70880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12fe70b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12fe70e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12fe710c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12fe71380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12fe71640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12fe71900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12fe71bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12fe71e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12fe72140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12fe72400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12fe726c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12fe72980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12fe72c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12fe72f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12fe731c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12fe73480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12fe73740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12fe73a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12fe73cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12fe73f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12fe74240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12fe74500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12fe747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12fe74a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12fe74d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12fe75000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12fe752c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12fe75580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12fe75840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12fe75b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12fe75dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12fe76080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12fe76340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12fe76600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12fe768c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12fe76b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12fe76e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12fe77100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12fe773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12fe77680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12fe77940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12fe77c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12fe77ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12fe78180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12fe78440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12fe78700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12fe789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12fe78c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12fe78f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12fe79200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12fe794c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12fe79780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12fe79a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12fe79d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12fe79fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12fe7a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12fe7a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12fe7a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12fe7aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12fe7ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12fe7b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12fe7b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12fe7b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12fe7b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12fe7bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12fe7be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12fe7c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12fe7c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12fe7c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12fe7c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12fe7cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12fe7ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12fe7d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12fe7d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12fe7d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12fe7d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12fe7dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12fe7df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12fe7e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12fe7e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12fe7e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12fe7ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12fe7ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12fe7ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12fe7f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12fe7f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12fe7f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12fe7fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12fe7fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12fe80000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12fe802c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12fe80580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12fe80840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12fe80b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12fe80dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12fe81080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12fe81340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12fe81600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12fe818c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12fe81b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12fe81e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12fe82100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12fe823c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12fe82680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12fe82940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12fe82c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12fe82ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12fe83180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12fe83440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12fe83700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12fe839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12fe83c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12fe83f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12fe84200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12fe844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12fe84780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12fe84a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12fe84d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12fe84fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12fe85280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12fe85540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12fe85b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12fe85dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12fe86320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12fe86870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12fe86dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12fe87310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12fe87860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12fe87db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12fe88300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12fe88850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12fe88da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12fe892f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12fe89840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12fe89d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12fe8a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12fe8a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12fe8ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12fe8b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12fe8b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12fe8bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12fe8c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12fe8c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12fe8cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12fe8d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12fe8d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12fe8dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12fe8e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12fe8e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12fe8ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12fe8f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12fe8f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12fe8fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12fe90280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12fe907d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12fe90d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12fe91270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12fe917c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12fe91d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12fe92260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12fe927b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12fe92d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12fe93250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12fe937a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12fe93cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12fe94240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12fe94790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12fe94ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12fe95230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12fe95780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12fe95cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12fe96220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12fe96770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12fe96cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12fe97210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12fe97760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12fe97cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12fe98200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12fe984c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12fe98780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12fe98a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12fe98eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12fe99320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12fe99790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12fe99c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12fe9a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12fe9a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12fe9a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12fe9adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12fe9b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12fe9b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12fe9bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12fe9bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12fe9c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12fe9c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12fe9d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12fe9dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12fe9e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12fe9e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12fe9eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12fe9f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12fe9f6d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.921s
user	0m0.244s
sys	0m0.133s
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]10.1498,'
+ qnt=f16
++ echo '[1]10.1498,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1498
++ echo '10.1498 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 10.1498
+ return 0
  - f16 @ 10.1498 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]10.1362,'
+ qnt=q8_0
++ echo '[1]10.1362,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1362
++ echo '10.1362 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 10.1362
+ return 0
  - q8_0 @ 10.1362 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]11.1740,'
+ qnt=q4_0
++ echo '[1]11.1740,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=11.1740
++ echo '11.1740 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 11.1740
+ return 0
  - q4_0 @ 11.1740 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]10.5507,'
+ qnt=q4_1
++ echo '[1]10.5507,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.5507
++ echo '10.5507 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 10.5507
+ return 0
  - q4_1 @ 10.5507 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]10.0972,'
+ qnt=q5_0
++ echo '[1]10.0972,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.0972
++ echo '10.0972 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 10.0972
+ return 0
  - q5_0 @ 10.0972 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]10.1971,'
+ qnt=q5_1
++ echo '[1]10.1971,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1971
++ echo '10.1971 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 10.1971
+ return 0
  - q5_1 @ 10.1971 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]12.0517,'
+ qnt=q3_k
++ echo '[1]12.0517,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=12.0517
++ echo '12.0517 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 12.0517
+ return 0
  - q3_k @ 12.0517 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]10.1031,'
+ qnt=q4_k
++ echo '[1]10.1031,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1031
++ echo '10.1031 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 10.1031
+ return 0
  - q4_k @ 10.1031 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]10.2433,'
+ qnt=q5_k
++ echo '[1]10.2433,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.2433
++ echo '10.2433 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 10.2433
+ return 0
  - q5_k @ 10.2433 OK
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]10.3179,'
+ qnt=q6_k
++ echo '[1]10.3179,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.3179
++ echo '10.3179 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 10.3179
+ return 0
  - q6_k @ 10.3179 OK
+ cat /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ grep Final
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_with_model_debug.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.74 user         0.05 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_with_model_release.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/27/39a71e4b88474833b64aa974ca4515574fd3c4/ggml-100-mac-m4/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.15 user         0.04 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
