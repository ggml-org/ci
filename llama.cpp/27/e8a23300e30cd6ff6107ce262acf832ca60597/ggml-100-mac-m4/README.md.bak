### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.11 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.18 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.45 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.29 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.68 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.19 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.50 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.28 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.94 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.18 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  194.14 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.83 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.09 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 255.51 sec*proc (29 tests)

Total Test time (real) = 255.52 sec

real	4m15.548s
user	8m43.840s
sys	0m7.258s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.16 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.16 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.83 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.20 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.29 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.17 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.46 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.48 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.73 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.11 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.78 sec*proc (29 tests)

Total Test time (real) =  54.79 sec

real	0m54.801s
user	1m17.314s
sys	0m6.281s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.092 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.279 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.018.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.287 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.018.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.288 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.018.288 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.018.289 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.018.290 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.018.291 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.018.291 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.018.292 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.018.292 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.018.295 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.018.295 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.018.296 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.018.296 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.018.297 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.018.297 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.018.298 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.021.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.022.765 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.767 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.022.768 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.022.768 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.022.769 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.022.769 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.022.770 I llama_model_loader: - type  f32:  124 tensors
0.00.022.770 I llama_model_loader: - type  f16:   73 tensors
0.00.022.771 I print_info: file format = GGUF V3 (latest)
0.00.022.771 I print_info: file type   = F16
0.00.022.772 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.026.499 I load: special tokens cache size = 5
0.00.028.359 I load: token to piece cache size = 0.2032 MB
0.00.028.363 I print_info: arch             = bert
0.00.028.364 I print_info: vocab_only       = 0
0.00.028.364 I print_info: n_ctx_train      = 512
0.00.028.364 I print_info: n_embd           = 384
0.00.028.364 I print_info: n_layer          = 12
0.00.028.367 I print_info: n_head           = 12
0.00.028.368 I print_info: n_head_kv        = 12
0.00.028.368 I print_info: n_rot            = 32
0.00.028.368 I print_info: n_swa            = 0
0.00.028.369 I print_info: n_embd_head_k    = 32
0.00.028.369 I print_info: n_embd_head_v    = 32
0.00.028.370 I print_info: n_gqa            = 1
0.00.028.371 I print_info: n_embd_k_gqa     = 384
0.00.028.371 I print_info: n_embd_v_gqa     = 384
0.00.028.372 I print_info: f_norm_eps       = 1.0e-12
0.00.028.373 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.028.373 I print_info: f_clamp_kqv      = 0.0e+00
0.00.028.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.028.373 I print_info: f_logit_scale    = 0.0e+00
0.00.028.374 I print_info: n_ff             = 1536
0.00.028.374 I print_info: n_expert         = 0
0.00.028.374 I print_info: n_expert_used    = 0
0.00.028.375 I print_info: causal attn      = 0
0.00.028.375 I print_info: pooling type     = 2
0.00.028.375 I print_info: rope type        = 2
0.00.028.375 I print_info: rope scaling     = linear
0.00.028.376 I print_info: freq_base_train  = 10000.0
0.00.028.376 I print_info: freq_scale_train = 1
0.00.028.376 I print_info: n_ctx_orig_yarn  = 512
0.00.028.377 I print_info: rope_finetuned   = unknown
0.00.028.377 I print_info: ssm_d_conv       = 0
0.00.028.377 I print_info: ssm_d_inner      = 0
0.00.028.377 I print_info: ssm_d_state      = 0
0.00.028.378 I print_info: ssm_dt_rank      = 0
0.00.028.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.028.378 I print_info: model type       = 33M
0.00.028.378 I print_info: model params     = 33.21 M
0.00.028.379 I print_info: general.name     = Bge Small
0.00.028.379 I print_info: vocab type       = WPM
0.00.028.380 I print_info: n_vocab          = 30522
0.00.028.380 I print_info: n_merges         = 0
0.00.028.383 I print_info: BOS token        = 101 '[CLS]'
0.00.028.383 I print_info: UNK token        = 100 '[UNK]'
0.00.028.383 I print_info: SEP token        = 102 '[SEP]'
0.00.028.384 I print_info: PAD token        = 0 '[PAD]'
0.00.028.384 I print_info: MASK token       = 103 '[MASK]'
0.00.028.384 I print_info: LF token         = 0 '[PAD]'
0.00.028.385 I print_info: max token length = 21
0.00.028.385 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.031.131 I load_tensors: offloading 12 repeating layers to GPU
0.00.031.132 I load_tensors: offloading output layer to GPU
0.00.031.133 I load_tensors: offloaded 13/13 layers to GPU
0.00.031.154 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.031.156 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.031.480 I llama_init_from_model: n_seq_max     = 1
0.00.031.481 I llama_init_from_model: n_ctx         = 512
0.00.031.482 I llama_init_from_model: n_ctx_per_seq = 512
0.00.031.482 I llama_init_from_model: n_batch       = 2048
0.00.031.482 I llama_init_from_model: n_ubatch      = 2048
0.00.031.482 I llama_init_from_model: flash_attn    = 0
0.00.031.483 I llama_init_from_model: freq_base     = 10000.0
0.00.031.483 I llama_init_from_model: freq_scale    = 1
0.00.031.484 I ggml_metal_init: allocating
0.00.031.496 I ggml_metal_init: found device: Apple M4
0.00.031.503 I ggml_metal_init: picking default device: Apple M4
0.00.032.163 I ggml_metal_init: using embedded metal library
0.00.035.842 I ggml_metal_init: GPU name:   Apple M4
0.00.035.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.035.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.035.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.035.846 I ggml_metal_init: simdgroup reduction   = true
0.00.035.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.035.846 I ggml_metal_init: has residency sets    = true
0.00.035.846 I ggml_metal_init: has bfloat            = true
0.00.035.846 I ggml_metal_init: use bfloat            = true
0.00.035.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.035.848 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.047.135 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.047.816 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.047.818 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.047.839 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.049.036 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.049.037 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.049.037 I llama_init_from_model: graph nodes  = 429
0.00.049.038 I llama_init_from_model: graph splits = 2
0.00.049.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.049.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.054.751 I 
0.00.054.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.055.409 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.060.532 I llama_perf_context_print:        load time =      40.87 ms
0.00.060.533 I llama_perf_context_print: prompt eval time =       4.98 ms /     9 tokens (    0.55 ms per token,  1806.14 tokens per second)
0.00.060.534 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.060.534 I llama_perf_context_print:       total time =       5.78 ms /    10 tokens
0.00.060.664 I ggml_metal_free: deallocating

real	0m0.232s
user	0m0.043s
sys	0m0.026s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.042 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.532 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.141 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.147 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.148 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.150 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.150 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.151 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.151 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.152 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.152 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.152 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.157 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.157 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.157 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.158 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.158 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.158 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.484 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.100 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.101 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.101 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.102 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.102 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.102 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.103 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.103 I llama_model_loader: - type  f32:  124 tensors
0.00.015.103 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.104 I print_info: file format = GGUF V3 (latest)
0.00.015.104 I print_info: file type   = Q8_0
0.00.015.105 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.588 I load: special tokens cache size = 5
0.00.018.896 I load: token to piece cache size = 0.2032 MB
0.00.018.900 I print_info: arch             = bert
0.00.018.900 I print_info: vocab_only       = 0
0.00.018.900 I print_info: n_ctx_train      = 512
0.00.018.900 I print_info: n_embd           = 384
0.00.018.901 I print_info: n_layer          = 12
0.00.018.904 I print_info: n_head           = 12
0.00.018.905 I print_info: n_head_kv        = 12
0.00.018.905 I print_info: n_rot            = 32
0.00.018.905 I print_info: n_swa            = 0
0.00.018.905 I print_info: n_embd_head_k    = 32
0.00.018.906 I print_info: n_embd_head_v    = 32
0.00.018.906 I print_info: n_gqa            = 1
0.00.018.907 I print_info: n_embd_k_gqa     = 384
0.00.018.908 I print_info: n_embd_v_gqa     = 384
0.00.018.908 I print_info: f_norm_eps       = 1.0e-12
0.00.018.909 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.909 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.909 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.909 I print_info: f_logit_scale    = 0.0e+00
0.00.018.910 I print_info: n_ff             = 1536
0.00.018.910 I print_info: n_expert         = 0
0.00.018.910 I print_info: n_expert_used    = 0
0.00.018.910 I print_info: causal attn      = 0
0.00.018.911 I print_info: pooling type     = 2
0.00.018.912 I print_info: rope type        = 2
0.00.018.913 I print_info: rope scaling     = linear
0.00.018.913 I print_info: freq_base_train  = 10000.0
0.00.018.913 I print_info: freq_scale_train = 1
0.00.018.913 I print_info: n_ctx_orig_yarn  = 512
0.00.018.914 I print_info: rope_finetuned   = unknown
0.00.018.914 I print_info: ssm_d_conv       = 0
0.00.018.914 I print_info: ssm_d_inner      = 0
0.00.018.914 I print_info: ssm_d_state      = 0
0.00.018.914 I print_info: ssm_dt_rank      = 0
0.00.018.914 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.914 I print_info: model type       = 33M
0.00.018.915 I print_info: model params     = 33.21 M
0.00.018.915 I print_info: general.name     = Bge Small
0.00.018.916 I print_info: vocab type       = WPM
0.00.018.916 I print_info: n_vocab          = 30522
0.00.018.916 I print_info: n_merges         = 0
0.00.018.916 I print_info: BOS token        = 101 '[CLS]'
0.00.018.916 I print_info: UNK token        = 100 '[UNK]'
0.00.018.917 I print_info: SEP token        = 102 '[SEP]'
0.00.018.917 I print_info: PAD token        = 0 '[PAD]'
0.00.018.917 I print_info: MASK token       = 103 '[MASK]'
0.00.018.917 I print_info: LF token         = 0 '[PAD]'
0.00.018.917 I print_info: max token length = 21
0.00.018.918 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.807 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.808 I load_tensors: offloading output layer to GPU
0.00.020.809 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.817 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.818 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.073 I llama_init_from_model: n_seq_max     = 1
0.00.021.074 I llama_init_from_model: n_ctx         = 512
0.00.021.074 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.074 I llama_init_from_model: n_batch       = 2048
0.00.021.074 I llama_init_from_model: n_ubatch      = 2048
0.00.021.074 I llama_init_from_model: flash_attn    = 0
0.00.021.075 I llama_init_from_model: freq_base     = 10000.0
0.00.021.075 I llama_init_from_model: freq_scale    = 1
0.00.021.076 I ggml_metal_init: allocating
0.00.021.092 I ggml_metal_init: found device: Apple M4
0.00.021.099 I ggml_metal_init: picking default device: Apple M4
0.00.021.622 I ggml_metal_init: using embedded metal library
0.00.023.972 I ggml_metal_init: GPU name:   Apple M4
0.00.023.974 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.974 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.975 I ggml_metal_init: simdgroup reduction   = true
0.00.023.975 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.975 I ggml_metal_init: has residency sets    = true
0.00.023.976 I ggml_metal_init: has bfloat            = true
0.00.023.976 I ggml_metal_init: use bfloat            = true
0.00.023.976 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.978 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.157 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.773 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.775 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.788 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.866 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.867 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.867 I llama_init_from_model: graph nodes  = 429
0.00.035.867 I llama_init_from_model: graph splits = 2
0.00.035.869 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.965 I 
0.00.039.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.515 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.986 I llama_perf_context_print:        load time =      30.42 ms
0.00.044.988 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2073.73 tokens per second)
0.00.044.989 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.990 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.045.198 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.030s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.285 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.818 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.745 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.752 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.753 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.754 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.754 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.755 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.756 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.757 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.758 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.758 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.759 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.762 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.763 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.763 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.764 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.765 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.429 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.760 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.376 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.379 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.379 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.380 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.380 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.380 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.381 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.381 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.381 I llama_model_loader: - type  f32:   40 tensors
0.00.048.382 I llama_model_loader: - type  f16:   30 tensors
0.00.048.388 I print_info: file format = GGUF V3 (latest)
0.00.048.389 I print_info: file type   = F16
0.00.048.391 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.699 W load: empty token at index 5
0.00.057.836 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.370 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.407 I load: special tokens cache size = 5
0.00.317.777 I load: token to piece cache size = 1.5060 MB
0.00.317.783 I print_info: arch             = jina-bert-v2
0.00.317.784 I print_info: vocab_only       = 0
0.00.317.784 I print_info: n_ctx_train      = 8192
0.00.317.784 I print_info: n_embd           = 384
0.00.317.784 I print_info: n_layer          = 4
0.00.317.798 I print_info: n_head           = 12
0.00.317.799 I print_info: n_head_kv        = 12
0.00.317.799 I print_info: n_rot            = 32
0.00.317.799 I print_info: n_swa            = 0
0.00.317.804 I print_info: n_embd_head_k    = 32
0.00.317.804 I print_info: n_embd_head_v    = 32
0.00.317.805 I print_info: n_gqa            = 1
0.00.317.806 I print_info: n_embd_k_gqa     = 384
0.00.317.806 I print_info: n_embd_v_gqa     = 384
0.00.317.807 I print_info: f_norm_eps       = 1.0e-12
0.00.317.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.317.808 I print_info: f_clamp_kqv      = 0.0e+00
0.00.317.808 I print_info: f_max_alibi_bias = 8.0e+00
0.00.317.808 I print_info: f_logit_scale    = 0.0e+00
0.00.317.811 I print_info: n_ff             = 1536
0.00.317.811 I print_info: n_expert         = 0
0.00.317.811 I print_info: n_expert_used    = 0
0.00.317.811 I print_info: causal attn      = 0
0.00.317.811 I print_info: pooling type     = -1
0.00.317.811 I print_info: rope type        = -1
0.00.317.812 I print_info: rope scaling     = linear
0.00.317.812 I print_info: freq_base_train  = 10000.0
0.00.317.812 I print_info: freq_scale_train = 1
0.00.317.812 I print_info: n_ctx_orig_yarn  = 8192
0.00.317.813 I print_info: rope_finetuned   = unknown
0.00.317.813 I print_info: ssm_d_conv       = 0
0.00.317.813 I print_info: ssm_d_inner      = 0
0.00.317.813 I print_info: ssm_d_state      = 0
0.00.317.813 I print_info: ssm_dt_rank      = 0
0.00.317.815 I print_info: ssm_dt_b_c_rms   = 0
0.00.317.815 I print_info: model type       = 33M
0.00.317.815 I print_info: model params     = 32.90 M
0.00.317.816 I print_info: general.name     = Jina Bert Implementation
0.00.317.817 I print_info: vocab type       = BPE
0.00.317.817 I print_info: n_vocab          = 61056
0.00.317.818 I print_info: n_merges         = 39382
0.00.317.819 I print_info: BOS token        = 0 '<s>'
0.00.317.819 I print_info: EOS token        = 2 '</s>'
0.00.317.819 I print_info: UNK token        = 3 '<unk>'
0.00.317.819 I print_info: SEP token        = 2 '</s>'
0.00.317.819 I print_info: PAD token        = 1 '<pad>'
0.00.317.819 I print_info: MASK token       = 4 '<mask>'
0.00.317.820 I print_info: EOG token        = 2 '</s>'
0.00.317.820 I print_info: max token length = 45
0.00.317.820 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.320.226 I load_tensors: offloading 4 repeating layers to GPU
0.00.320.227 I load_tensors: offloading output layer to GPU
0.00.320.227 I load_tensors: offloaded 5/5 layers to GPU
0.00.320.252 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.320.253 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.320.801 I llama_init_from_model: n_seq_max     = 1
0.00.320.802 I llama_init_from_model: n_ctx         = 8192
0.00.320.802 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.320.802 I llama_init_from_model: n_batch       = 2048
0.00.320.803 I llama_init_from_model: n_ubatch      = 2048
0.00.320.803 I llama_init_from_model: flash_attn    = 0
0.00.320.803 I llama_init_from_model: freq_base     = 10000.0
0.00.320.804 I llama_init_from_model: freq_scale    = 1
0.00.320.804 I ggml_metal_init: allocating
0.00.320.818 I ggml_metal_init: found device: Apple M4
0.00.320.823 I ggml_metal_init: picking default device: Apple M4
0.00.321.774 I ggml_metal_init: using embedded metal library
0.00.324.665 I ggml_metal_init: GPU name:   Apple M4
0.00.324.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.324.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.324.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.324.667 I ggml_metal_init: simdgroup reduction   = true
0.00.324.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.324.668 I ggml_metal_init: has residency sets    = true
0.00.324.668 I ggml_metal_init: has bfloat            = true
0.00.324.668 I ggml_metal_init: use bfloat            = true
0.00.324.668 I ggml_metal_init: hasUnifiedMemory      = true
0.00.324.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.334.048 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.337.045 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.337.046 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.337.067 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.343.554 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.343.556 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.343.556 I llama_init_from_model: graph nodes  = 154
0.00.343.556 I llama_init_from_model: graph splits = 2
0.00.343.558 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.343.558 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.351.006 I 
0.00.351.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.351.436 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.351.437 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.351.450 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.351.450 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.351.457 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.351.457 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.351.975 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.355.468 I llama_perf_context_print:        load time =     330.16 ms
0.00.355.470 I llama_perf_context_print: prompt eval time =       3.48 ms /    62 tokens (    0.06 ms per token, 17795.64 tokens per second)
0.00.355.470 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.355.471 I llama_perf_context_print:       total time =       4.46 ms /    63 tokens
0.00.355.682 I ggml_metal_free: deallocating

real	0m1.074s
user	0m0.326s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.200 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.357 I main: llama backend init
0.00.000.363 I main: load the model and apply lora adapter, if any
0.00.049.076 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.062.079 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.062.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.062.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.062.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.062.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.062.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.062.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.062.107 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.062.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.062.108 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.062.109 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.062.109 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.062.110 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.062.111 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.062.116 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.062.117 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.062.118 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.072.306 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.074.587 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.082.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.082.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.082.391 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.082.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.082.392 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.082.393 I llama_model_loader: - type  f32:  194 tensors
0.00.082.393 I llama_model_loader: - type  f16:   98 tensors
0.00.082.395 I print_info: file format = GGUF V3 (latest)
0.00.082.396 I print_info: file type   = all F32 (guessed)
0.00.082.398 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.083 I load: special tokens cache size = 25
0.00.104.796 I load: token to piece cache size = 0.2984 MB
0.00.104.799 I print_info: arch             = gptneox
0.00.104.800 I print_info: vocab_only       = 0
0.00.104.800 I print_info: n_ctx_train      = 2048
0.00.104.800 I print_info: n_embd           = 2048
0.00.104.800 I print_info: n_layer          = 24
0.00.104.803 I print_info: n_head           = 16
0.00.104.804 I print_info: n_head_kv        = 16
0.00.104.805 I print_info: n_rot            = 32
0.00.104.805 I print_info: n_swa            = 0
0.00.104.805 I print_info: n_embd_head_k    = 128
0.00.104.805 I print_info: n_embd_head_v    = 128
0.00.104.806 I print_info: n_gqa            = 1
0.00.104.807 I print_info: n_embd_k_gqa     = 2048
0.00.104.808 I print_info: n_embd_v_gqa     = 2048
0.00.104.808 I print_info: f_norm_eps       = 1.0e-05
0.00.104.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.104.809 I print_info: f_clamp_kqv      = 0.0e+00
0.00.104.809 I print_info: f_max_alibi_bias = 0.0e+00
0.00.104.809 I print_info: f_logit_scale    = 0.0e+00
0.00.104.810 I print_info: n_ff             = 8192
0.00.104.810 I print_info: n_expert         = 0
0.00.104.810 I print_info: n_expert_used    = 0
0.00.104.810 I print_info: causal attn      = 1
0.00.104.810 I print_info: pooling type     = 0
0.00.104.810 I print_info: rope type        = 2
0.00.104.811 I print_info: rope scaling     = linear
0.00.104.811 I print_info: freq_base_train  = 10000.0
0.00.104.812 I print_info: freq_scale_train = 1
0.00.104.812 I print_info: n_ctx_orig_yarn  = 2048
0.00.104.812 I print_info: rope_finetuned   = unknown
0.00.104.812 I print_info: ssm_d_conv       = 0
0.00.104.812 I print_info: ssm_d_inner      = 0
0.00.104.812 I print_info: ssm_d_state      = 0
0.00.104.813 I print_info: ssm_dt_rank      = 0
0.00.104.814 I print_info: ssm_dt_b_c_rms   = 0
0.00.104.814 I print_info: model type       = 1.4B
0.00.104.816 I print_info: model params     = 1.41 B
0.00.104.816 I print_info: general.name     = 1.4B
0.00.104.817 I print_info: vocab type       = BPE
0.00.104.817 I print_info: n_vocab          = 50304
0.00.104.817 I print_info: n_merges         = 50009
0.00.104.818 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.104.818 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.104.818 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.104.818 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.104.819 I print_info: LF token         = 187 ''
0.00.104.819 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.104.823 I print_info: max token length = 1024
0.00.104.823 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.149.487 I load_tensors: offloading 24 repeating layers to GPU
0.00.149.493 I load_tensors: offloading output layer to GPU
0.00.149.493 I load_tensors: offloaded 25/25 layers to GPU
0.00.149.519 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.149.520 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.149.916 I llama_init_from_model: n_seq_max     = 1
0.00.149.917 I llama_init_from_model: n_ctx         = 2048
0.00.149.917 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.149.917 I llama_init_from_model: n_batch       = 2048
0.00.149.918 I llama_init_from_model: n_ubatch      = 512
0.00.149.918 I llama_init_from_model: flash_attn    = 0
0.00.149.919 I llama_init_from_model: freq_base     = 10000.0
0.00.149.919 I llama_init_from_model: freq_scale    = 1
0.00.149.920 I ggml_metal_init: allocating
0.00.149.955 I ggml_metal_init: found device: Apple M4
0.00.149.961 I ggml_metal_init: picking default device: Apple M4
0.00.150.632 I ggml_metal_init: using embedded metal library
0.00.159.936 I ggml_metal_init: GPU name:   Apple M4
0.00.159.938 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.159.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.159.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.159.939 I ggml_metal_init: simdgroup reduction   = true
0.00.159.939 I ggml_metal_init: simdgroup matrix mul. = true
0.00.159.940 I ggml_metal_init: has residency sets    = true
0.00.159.940 I ggml_metal_init: has bfloat            = true
0.00.159.940 I ggml_metal_init: use bfloat            = true
0.00.159.940 I ggml_metal_init: hasUnifiedMemory      = true
0.00.159.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.184.467 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.212.485 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.212.492 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.212.537 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.217.239 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.217.242 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.217.242 I llama_init_from_model: graph nodes  = 967
0.00.217.243 I llama_init_from_model: graph splits = 2
0.00.217.246 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.217.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.217.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.282.810 I main: llama threadpool init, n_threads = 4
0.00.282.849 I 
0.00.282.865 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.282.866 I 
0.00.282.914 I sampler seed: 1234
0.00.282.919 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.282.944 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.282.946 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.282.946 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.116.665 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.02.116.666 I llama_perf_context_print:        load time =     232.84 ms
0.02.116.667 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.11 tokens per second)
0.02.116.668 I llama_perf_context_print:        eval time =    1787.12 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.116.669 I llama_perf_context_print:       total time =    1834.72 ms /    70 tokens
0.02.116.864 I ggml_metal_free: deallocating

real	0m2.425s
user	0m0.131s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.735 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.917 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.001 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.009 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.021 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.021 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.023 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.026 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.027 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.027 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.028 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.029 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.032 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.036 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.036 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.242 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.243 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.243 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.244 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.244 I llama_model_loader: - type  f32:  194 tensors
0.00.055.245 I llama_model_loader: - type  f16:   98 tensors
0.00.055.246 I print_info: file format = GGUF V3 (latest)
0.00.055.246 I print_info: file type   = all F32 (guessed)
0.00.055.248 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.544 I load: special tokens cache size = 25
0.00.075.449 I load: token to piece cache size = 0.2984 MB
0.00.075.452 I print_info: arch             = gptneox
0.00.075.452 I print_info: vocab_only       = 0
0.00.075.452 I print_info: n_ctx_train      = 2048
0.00.075.452 I print_info: n_embd           = 2048
0.00.075.453 I print_info: n_layer          = 24
0.00.075.456 I print_info: n_head           = 16
0.00.075.457 I print_info: n_head_kv        = 16
0.00.075.457 I print_info: n_rot            = 32
0.00.075.457 I print_info: n_swa            = 0
0.00.075.457 I print_info: n_embd_head_k    = 128
0.00.075.459 I print_info: n_embd_head_v    = 128
0.00.075.460 I print_info: n_gqa            = 1
0.00.075.461 I print_info: n_embd_k_gqa     = 2048
0.00.075.462 I print_info: n_embd_v_gqa     = 2048
0.00.075.462 I print_info: f_norm_eps       = 1.0e-05
0.00.075.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.463 I print_info: f_logit_scale    = 0.0e+00
0.00.075.464 I print_info: n_ff             = 8192
0.00.075.464 I print_info: n_expert         = 0
0.00.075.464 I print_info: n_expert_used    = 0
0.00.075.464 I print_info: causal attn      = 1
0.00.075.464 I print_info: pooling type     = 0
0.00.075.465 I print_info: rope type        = 2
0.00.075.465 I print_info: rope scaling     = linear
0.00.075.466 I print_info: freq_base_train  = 10000.0
0.00.075.467 I print_info: freq_scale_train = 1
0.00.075.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.467 I print_info: rope_finetuned   = unknown
0.00.075.467 I print_info: ssm_d_conv       = 0
0.00.075.467 I print_info: ssm_d_inner      = 0
0.00.075.467 I print_info: ssm_d_state      = 0
0.00.075.467 I print_info: ssm_dt_rank      = 0
0.00.075.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.468 I print_info: model type       = 1.4B
0.00.075.468 I print_info: model params     = 1.41 B
0.00.075.468 I print_info: general.name     = 1.4B
0.00.075.469 I print_info: vocab type       = BPE
0.00.075.469 I print_info: n_vocab          = 50304
0.00.075.471 I print_info: n_merges         = 50009
0.00.075.471 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.471 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.472 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.472 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.472 I print_info: LF token         = 187 ''
0.00.075.473 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.473 I print_info: max token length = 1024
0.00.075.474 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.080.449 I load_tensors: offloading 24 repeating layers to GPU
0.01.080.453 I load_tensors: offloading output layer to GPU
0.01.080.453 I load_tensors: offloaded 25/25 layers to GPU
0.01.080.479 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.080.481 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.081.125 I llama_init_from_model: n_seq_max     = 1
0.01.081.127 I llama_init_from_model: n_ctx         = 128
0.01.081.127 I llama_init_from_model: n_ctx_per_seq = 128
0.01.081.127 I llama_init_from_model: n_batch       = 128
0.01.081.128 I llama_init_from_model: n_ubatch      = 128
0.01.081.128 I llama_init_from_model: flash_attn    = 0
0.01.081.128 I llama_init_from_model: freq_base     = 10000.0
0.01.081.129 I llama_init_from_model: freq_scale    = 1
0.01.081.129 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.081.130 I ggml_metal_init: allocating
0.01.081.200 I ggml_metal_init: found device: Apple M4
0.01.081.211 I ggml_metal_init: picking default device: Apple M4
0.01.082.260 I ggml_metal_init: using embedded metal library
0.01.086.058 I ggml_metal_init: GPU name:   Apple M4
0.01.086.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.086.060 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.086.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.086.061 I ggml_metal_init: simdgroup reduction   = true
0.01.086.062 I ggml_metal_init: simdgroup matrix mul. = true
0.01.086.062 I ggml_metal_init: has residency sets    = true
0.01.086.062 I ggml_metal_init: has bfloat            = true
0.01.086.062 I ggml_metal_init: use bfloat            = true
0.01.086.062 I ggml_metal_init: hasUnifiedMemory      = true
0.01.086.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.096.774 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.098.413 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.098.415 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.098.448 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.100.002 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.100.003 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.100.004 I llama_init_from_model: graph nodes  = 967
0.01.100.004 I llama_init_from_model: graph splits = 2
0.01.100.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.100.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.134.112 I 
0.01.134.135 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.134.158 I perplexity: tokenizing the input ..
0.01.138.975 I perplexity: tokenization took 4.815 ms
0.01.138.995 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.257.382 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.258.723 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.258.735 I llama_perf_context_print:        load time =    1110.17 ms
0.01.258.736 I llama_perf_context_print: prompt eval time =     118.12 ms /   128 tokens (    0.92 ms per token,  1083.60 tokens per second)
0.01.258.737 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.258.738 I llama_perf_context_print:       total time =     124.62 ms /   129 tokens
0.01.259.093 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.096s
sys	0m0.215s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.652 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.657 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.662 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.663 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.541 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.542 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.543 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.543 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.544 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.544 I llama_model_loader: - type  f32:  194 tensors
0.00.031.545 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.546 I print_info: file format = GGUF V3 (latest)
0.00.031.546 I print_info: file type   = Q8_0
0.00.031.547 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.039.661 I load: special tokens cache size = 25
0.00.045.919 I load: token to piece cache size = 0.2984 MB
0.00.045.923 I print_info: arch             = gptneox
0.00.045.924 I print_info: vocab_only       = 0
0.00.045.924 I print_info: n_ctx_train      = 2048
0.00.045.924 I print_info: n_embd           = 2048
0.00.045.924 I print_info: n_layer          = 24
0.00.045.935 I print_info: n_head           = 16
0.00.045.936 I print_info: n_head_kv        = 16
0.00.045.936 I print_info: n_rot            = 32
0.00.045.937 I print_info: n_swa            = 0
0.00.045.937 I print_info: n_embd_head_k    = 128
0.00.045.937 I print_info: n_embd_head_v    = 128
0.00.045.938 I print_info: n_gqa            = 1
0.00.045.938 I print_info: n_embd_k_gqa     = 2048
0.00.045.939 I print_info: n_embd_v_gqa     = 2048
0.00.045.940 I print_info: f_norm_eps       = 1.0e-05
0.00.045.941 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.942 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.942 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.942 I print_info: f_logit_scale    = 0.0e+00
0.00.045.943 I print_info: n_ff             = 8192
0.00.045.943 I print_info: n_expert         = 0
0.00.045.943 I print_info: n_expert_used    = 0
0.00.045.943 I print_info: causal attn      = 1
0.00.045.944 I print_info: pooling type     = 0
0.00.045.944 I print_info: rope type        = 2
0.00.045.944 I print_info: rope scaling     = linear
0.00.045.944 I print_info: freq_base_train  = 10000.0
0.00.045.945 I print_info: freq_scale_train = 1
0.00.045.945 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.945 I print_info: rope_finetuned   = unknown
0.00.045.946 I print_info: ssm_d_conv       = 0
0.00.045.946 I print_info: ssm_d_inner      = 0
0.00.045.946 I print_info: ssm_d_state      = 0
0.00.045.946 I print_info: ssm_dt_rank      = 0
0.00.045.946 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.946 I print_info: model type       = 1.4B
0.00.045.947 I print_info: model params     = 1.41 B
0.00.045.947 I print_info: general.name     = 1.4B
0.00.045.948 I print_info: vocab type       = BPE
0.00.045.948 I print_info: n_vocab          = 50304
0.00.045.948 I print_info: n_merges         = 50009
0.00.045.948 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.948 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.948 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.948 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.949 I print_info: LF token         = 187 ''
0.00.045.949 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.949 I print_info: max token length = 1024
0.00.045.950 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.200.379 I load_tensors: offloading 24 repeating layers to GPU
0.01.200.385 I load_tensors: offloading output layer to GPU
0.01.200.387 I load_tensors: offloaded 25/25 layers to GPU
0.01.200.409 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.200.412 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.201.323 I llama_init_from_model: n_seq_max     = 1
0.01.201.325 I llama_init_from_model: n_ctx         = 2048
0.01.201.325 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.201.325 I llama_init_from_model: n_batch       = 2048
0.01.201.326 I llama_init_from_model: n_ubatch      = 512
0.01.201.326 I llama_init_from_model: flash_attn    = 0
0.01.201.327 I llama_init_from_model: freq_base     = 10000.0
0.01.201.327 I llama_init_from_model: freq_scale    = 1
0.01.201.329 I ggml_metal_init: allocating
0.01.201.340 I ggml_metal_init: found device: Apple M4
0.01.201.346 I ggml_metal_init: picking default device: Apple M4
0.01.202.548 I ggml_metal_init: using embedded metal library
0.01.207.775 I ggml_metal_init: GPU name:   Apple M4
0.01.207.778 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.207.779 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.207.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.207.780 I ggml_metal_init: simdgroup reduction   = true
0.01.207.780 I ggml_metal_init: simdgroup matrix mul. = true
0.01.207.780 I ggml_metal_init: has residency sets    = true
0.01.207.781 I ggml_metal_init: has bfloat            = true
0.01.207.781 I ggml_metal_init: use bfloat            = true
0.01.207.781 I ggml_metal_init: hasUnifiedMemory      = true
0.01.207.785 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.223.862 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.275.577 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.275.583 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.275.616 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.280.038 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.280.040 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.280.040 I llama_init_from_model: graph nodes  = 967
0.01.280.040 I llama_init_from_model: graph splits = 2
0.01.280.045 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.280.162 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.280.162 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.334.711 I main: llama threadpool init, n_threads = 4
0.01.334.756 I 
0.01.334.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.334.773 I 
0.01.334.950 I sampler seed: 1234
0.01.334.955 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.334.990 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.334.993 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.334.993 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.420.845 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.02.420.845 I llama_perf_context_print:        load time =    1324.12 ms
0.02.420.847 I llama_perf_context_print: prompt eval time =      49.19 ms /     7 tokens (    7.03 ms per token,   142.30 tokens per second)
0.02.420.848 I llama_perf_context_print:        eval time =    1033.84 ms /    63 runs   (   16.41 ms per token,    60.94 tokens per second)
0.02.420.848 I llama_perf_context_print:       total time =    1086.84 ms /    70 tokens
0.02.421.096 I ggml_metal_free: deallocating

real	0m2.440s
user	0m0.107s
sys	0m0.276s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.102 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.394 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.401 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.406 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.409 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.242 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.270 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.066 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.066 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.067 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.067 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.067 I llama_model_loader: - type  f32:  194 tensors
0.00.025.068 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.068 I print_info: file format = GGUF V3 (latest)
0.00.025.069 I print_info: file type   = Q8_0
0.00.025.074 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.092 I load: special tokens cache size = 25
0.00.039.259 I load: token to piece cache size = 0.2984 MB
0.00.039.263 I print_info: arch             = gptneox
0.00.039.263 I print_info: vocab_only       = 0
0.00.039.263 I print_info: n_ctx_train      = 2048
0.00.039.264 I print_info: n_embd           = 2048
0.00.039.264 I print_info: n_layer          = 24
0.00.039.268 I print_info: n_head           = 16
0.00.039.269 I print_info: n_head_kv        = 16
0.00.039.269 I print_info: n_rot            = 32
0.00.039.269 I print_info: n_swa            = 0
0.00.039.269 I print_info: n_embd_head_k    = 128
0.00.039.269 I print_info: n_embd_head_v    = 128
0.00.039.270 I print_info: n_gqa            = 1
0.00.039.271 I print_info: n_embd_k_gqa     = 2048
0.00.039.274 I print_info: n_embd_v_gqa     = 2048
0.00.039.275 I print_info: f_norm_eps       = 1.0e-05
0.00.039.275 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.276 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.276 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.277 I print_info: f_logit_scale    = 0.0e+00
0.00.039.277 I print_info: n_ff             = 8192
0.00.039.277 I print_info: n_expert         = 0
0.00.039.278 I print_info: n_expert_used    = 0
0.00.039.278 I print_info: causal attn      = 1
0.00.039.278 I print_info: pooling type     = 0
0.00.039.278 I print_info: rope type        = 2
0.00.039.278 I print_info: rope scaling     = linear
0.00.039.279 I print_info: freq_base_train  = 10000.0
0.00.039.280 I print_info: freq_scale_train = 1
0.00.039.280 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.280 I print_info: rope_finetuned   = unknown
0.00.039.280 I print_info: ssm_d_conv       = 0
0.00.039.280 I print_info: ssm_d_inner      = 0
0.00.039.280 I print_info: ssm_d_state      = 0
0.00.039.280 I print_info: ssm_dt_rank      = 0
0.00.039.281 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.281 I print_info: model type       = 1.4B
0.00.039.281 I print_info: model params     = 1.41 B
0.00.039.281 I print_info: general.name     = 1.4B
0.00.039.282 I print_info: vocab type       = BPE
0.00.039.282 I print_info: n_vocab          = 50304
0.00.039.283 I print_info: n_merges         = 50009
0.00.039.284 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.284 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.284 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.284 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.284 I print_info: LF token         = 187 ''
0.00.039.284 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: max token length = 1024
0.00.039.285 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.906.531 I load_tensors: offloading 24 repeating layers to GPU
0.00.906.537 I load_tensors: offloading output layer to GPU
0.00.906.538 I load_tensors: offloaded 25/25 layers to GPU
0.00.906.563 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.906.567 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.907.946 I llama_init_from_model: n_seq_max     = 1
0.00.907.948 I llama_init_from_model: n_ctx         = 128
0.00.907.949 I llama_init_from_model: n_ctx_per_seq = 128
0.00.907.949 I llama_init_from_model: n_batch       = 128
0.00.907.950 I llama_init_from_model: n_ubatch      = 128
0.00.907.950 I llama_init_from_model: flash_attn    = 0
0.00.907.951 I llama_init_from_model: freq_base     = 10000.0
0.00.907.952 I llama_init_from_model: freq_scale    = 1
0.00.907.952 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.907.954 I ggml_metal_init: allocating
0.00.907.999 I ggml_metal_init: found device: Apple M4
0.00.908.010 I ggml_metal_init: picking default device: Apple M4
0.00.909.286 I ggml_metal_init: using embedded metal library
0.00.915.074 I ggml_metal_init: GPU name:   Apple M4
0.00.915.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.915.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.915.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.915.079 I ggml_metal_init: simdgroup reduction   = true
0.00.915.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.915.079 I ggml_metal_init: has residency sets    = true
0.00.915.079 I ggml_metal_init: has bfloat            = true
0.00.915.080 I ggml_metal_init: use bfloat            = true
0.00.915.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.915.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.931.151 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.934.553 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.934.557 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.934.618 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.937.766 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.937.768 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.937.768 I llama_init_from_model: graph nodes  = 967
0.00.937.769 I llama_init_from_model: graph splits = 2
0.00.937.772 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.937.772 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.966.720 I 
0.00.966.794 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.966.823 I perplexity: tokenizing the input ..
0.00.974.320 I perplexity: tokenization took 7.494 ms
0.00.974.339 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.113.132 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.114.469 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.114.485 I llama_perf_context_print:        load time =     957.60 ms
0.01.114.487 I llama_perf_context_print: prompt eval time =     137.88 ms /   128 tokens (    1.08 ms per token,   928.31 tokens per second)
0.01.114.490 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.114.491 I llama_perf_context_print:       total time =     147.77 ms /   129 tokens
0.01.114.908 I ggml_metal_free: deallocating

real	0m1.130s
user	0m0.078s
sys	0m0.179s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.015.996 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.708 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.708 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.708 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.709 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.710 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.710 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.710 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.711 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.711 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.712 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.712 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.716 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.255 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.989 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.991 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.991 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.992 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.992 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.993 I llama_model_loader: - type  f32:  194 tensors
0.00.043.993 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.993 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.994 I print_info: file format = GGUF V3 (latest)
0.00.043.995 I print_info: file type   = Q4_0
0.00.043.996 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.054.357 I load: special tokens cache size = 25
0.00.063.520 I load: token to piece cache size = 0.2984 MB
0.00.063.524 I print_info: arch             = gptneox
0.00.063.524 I print_info: vocab_only       = 0
0.00.063.524 I print_info: n_ctx_train      = 2048
0.00.063.524 I print_info: n_embd           = 2048
0.00.063.525 I print_info: n_layer          = 24
0.00.063.529 I print_info: n_head           = 16
0.00.063.530 I print_info: n_head_kv        = 16
0.00.063.531 I print_info: n_rot            = 32
0.00.063.531 I print_info: n_swa            = 0
0.00.063.531 I print_info: n_embd_head_k    = 128
0.00.063.531 I print_info: n_embd_head_v    = 128
0.00.063.532 I print_info: n_gqa            = 1
0.00.063.533 I print_info: n_embd_k_gqa     = 2048
0.00.063.534 I print_info: n_embd_v_gqa     = 2048
0.00.063.535 I print_info: f_norm_eps       = 1.0e-05
0.00.063.535 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.538 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.538 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.539 I print_info: f_logit_scale    = 0.0e+00
0.00.063.539 I print_info: n_ff             = 8192
0.00.063.540 I print_info: n_expert         = 0
0.00.063.541 I print_info: n_expert_used    = 0
0.00.063.542 I print_info: causal attn      = 1
0.00.063.542 I print_info: pooling type     = 0
0.00.063.542 I print_info: rope type        = 2
0.00.063.542 I print_info: rope scaling     = linear
0.00.063.543 I print_info: freq_base_train  = 10000.0
0.00.063.543 I print_info: freq_scale_train = 1
0.00.063.543 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.543 I print_info: rope_finetuned   = unknown
0.00.063.544 I print_info: ssm_d_conv       = 0
0.00.063.544 I print_info: ssm_d_inner      = 0
0.00.063.544 I print_info: ssm_d_state      = 0
0.00.063.544 I print_info: ssm_dt_rank      = 0
0.00.063.544 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.544 I print_info: model type       = 1.4B
0.00.063.545 I print_info: model params     = 1.41 B
0.00.063.545 I print_info: general.name     = 1.4B
0.00.063.546 I print_info: vocab type       = BPE
0.00.063.546 I print_info: n_vocab          = 50304
0.00.063.546 I print_info: n_merges         = 50009
0.00.063.547 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.547 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.547 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.547 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.548 I print_info: LF token         = 187 ''
0.00.063.548 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.548 I print_info: max token length = 1024
0.00.063.549 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.627.830 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.844 I load_tensors: offloading output layer to GPU
0.00.627.845 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.881 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.627.883 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.629.372 I llama_init_from_model: n_seq_max     = 1
0.00.629.376 I llama_init_from_model: n_ctx         = 2048
0.00.629.376 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.629.377 I llama_init_from_model: n_batch       = 2048
0.00.629.378 I llama_init_from_model: n_ubatch      = 512
0.00.629.378 I llama_init_from_model: flash_attn    = 0
0.00.629.381 I llama_init_from_model: freq_base     = 10000.0
0.00.629.381 I llama_init_from_model: freq_scale    = 1
0.00.629.383 I ggml_metal_init: allocating
0.00.629.483 I ggml_metal_init: found device: Apple M4
0.00.629.496 I ggml_metal_init: picking default device: Apple M4
0.00.631.379 I ggml_metal_init: using embedded metal library
0.00.639.126 I ggml_metal_init: GPU name:   Apple M4
0.00.639.145 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.146 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.146 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.147 I ggml_metal_init: simdgroup reduction   = true
0.00.639.147 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.147 I ggml_metal_init: has residency sets    = true
0.00.639.148 I ggml_metal_init: has bfloat            = true
0.00.639.148 I ggml_metal_init: use bfloat            = true
0.00.639.152 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.157 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.397 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.895 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.717.902 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.717.936 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.206 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.208 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.208 I llama_init_from_model: graph nodes  = 967
0.00.722.209 I llama_init_from_model: graph splits = 2
0.00.722.219 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.347 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.348 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.681 I main: llama threadpool init, n_threads = 4
0.00.779.722 I 
0.00.779.753 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.754 I 
0.00.779.981 I sampler seed: 1234
0.00.779.990 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.022 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.025 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.025 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.457.337 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51005.75 tokens per second)
0.01.457.338 I llama_perf_context_print:        load time =     762.96 ms
0.01.457.340 I llama_perf_context_print: prompt eval time =      46.31 ms /     7 tokens (    6.62 ms per token,   151.16 tokens per second)
0.01.457.340 I llama_perf_context_print:        eval time =     628.15 ms /    63 runs   (    9.97 ms per token,   100.29 tokens per second)
0.01.457.342 I llama_perf_context_print:       total time =     678.37 ms /    70 tokens
0.01.457.603 I ggml_metal_free: deallocating

real	0m1.483s
user	0m0.120s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.049 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.247 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.247 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.248 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.248 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.248 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.250 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.250 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.251 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.251 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.253 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.254 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.185 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.100 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.101 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.102 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.102 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.102 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.103 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.103 I llama_model_loader: - type  f32:  194 tensors
0.00.026.103 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.104 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.105 I print_info: file format = GGUF V3 (latest)
0.00.026.105 I print_info: file type   = Q4_0
0.00.026.106 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.196 I load: special tokens cache size = 25
0.00.040.482 I load: token to piece cache size = 0.2984 MB
0.00.040.486 I print_info: arch             = gptneox
0.00.040.487 I print_info: vocab_only       = 0
0.00.040.487 I print_info: n_ctx_train      = 2048
0.00.040.487 I print_info: n_embd           = 2048
0.00.040.487 I print_info: n_layer          = 24
0.00.040.491 I print_info: n_head           = 16
0.00.040.492 I print_info: n_head_kv        = 16
0.00.040.493 I print_info: n_rot            = 32
0.00.040.494 I print_info: n_swa            = 0
0.00.040.495 I print_info: n_embd_head_k    = 128
0.00.040.495 I print_info: n_embd_head_v    = 128
0.00.040.496 I print_info: n_gqa            = 1
0.00.040.497 I print_info: n_embd_k_gqa     = 2048
0.00.040.497 I print_info: n_embd_v_gqa     = 2048
0.00.040.498 I print_info: f_norm_eps       = 1.0e-05
0.00.040.499 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.499 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.499 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.499 I print_info: f_logit_scale    = 0.0e+00
0.00.040.500 I print_info: n_ff             = 8192
0.00.040.500 I print_info: n_expert         = 0
0.00.040.500 I print_info: n_expert_used    = 0
0.00.040.501 I print_info: causal attn      = 1
0.00.040.501 I print_info: pooling type     = 0
0.00.040.501 I print_info: rope type        = 2
0.00.040.501 I print_info: rope scaling     = linear
0.00.040.502 I print_info: freq_base_train  = 10000.0
0.00.040.502 I print_info: freq_scale_train = 1
0.00.040.502 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.502 I print_info: rope_finetuned   = unknown
0.00.040.503 I print_info: ssm_d_conv       = 0
0.00.040.503 I print_info: ssm_d_inner      = 0
0.00.040.504 I print_info: ssm_d_state      = 0
0.00.040.506 I print_info: ssm_dt_rank      = 0
0.00.040.506 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.506 I print_info: model type       = 1.4B
0.00.040.506 I print_info: model params     = 1.41 B
0.00.040.506 I print_info: general.name     = 1.4B
0.00.040.507 I print_info: vocab type       = BPE
0.00.040.507 I print_info: n_vocab          = 50304
0.00.040.507 I print_info: n_merges         = 50009
0.00.040.508 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.508 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.508 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.508 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.508 I print_info: LF token         = 187 ''
0.00.040.509 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.509 I print_info: max token length = 1024
0.00.040.509 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.577.147 I load_tensors: offloading 24 repeating layers to GPU
0.00.577.160 I load_tensors: offloading output layer to GPU
0.00.577.161 I load_tensors: offloaded 25/25 layers to GPU
0.00.577.194 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.577.196 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.578.967 I llama_init_from_model: n_seq_max     = 1
0.00.578.970 I llama_init_from_model: n_ctx         = 128
0.00.578.971 I llama_init_from_model: n_ctx_per_seq = 128
0.00.578.971 I llama_init_from_model: n_batch       = 128
0.00.578.972 I llama_init_from_model: n_ubatch      = 128
0.00.578.972 I llama_init_from_model: flash_attn    = 0
0.00.578.975 I llama_init_from_model: freq_base     = 10000.0
0.00.578.975 I llama_init_from_model: freq_scale    = 1
0.00.578.976 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.578.978 I ggml_metal_init: allocating
0.00.579.055 I ggml_metal_init: found device: Apple M4
0.00.579.068 I ggml_metal_init: picking default device: Apple M4
0.00.580.853 I ggml_metal_init: using embedded metal library
0.00.587.702 I ggml_metal_init: GPU name:   Apple M4
0.00.587.707 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.587.708 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.587.709 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.587.710 I ggml_metal_init: simdgroup reduction   = true
0.00.587.710 I ggml_metal_init: simdgroup matrix mul. = true
0.00.587.710 I ggml_metal_init: has residency sets    = true
0.00.587.710 I ggml_metal_init: has bfloat            = true
0.00.587.711 I ggml_metal_init: use bfloat            = true
0.00.587.711 I ggml_metal_init: hasUnifiedMemory      = true
0.00.587.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.605.550 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.034 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.609.042 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.609.092 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.594 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.612.596 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.612.597 I llama_init_from_model: graph nodes  = 967
0.00.612.597 I llama_init_from_model: graph splits = 2
0.00.612.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.612.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.709 I 
0.00.636.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.796 I perplexity: tokenizing the input ..
0.00.643.832 I perplexity: tokenization took 7.034 ms
0.00.643.850 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.077 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.779.421 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.779.438 I llama_perf_context_print:        load time =     626.65 ms
0.00.779.439 I llama_perf_context_print: prompt eval time =     133.93 ms /   128 tokens (    1.05 ms per token,   955.73 tokens per second)
0.00.779.439 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.440 I llama_perf_context_print:       total time =     142.73 ms /   129 tokens
0.00.779.823 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.079s
sys	0m0.119s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.949 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.686 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.690 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.692 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.692 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.693 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.697 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.699 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.701 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.702 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.702 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.554 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.549 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.340 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.341 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.341 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.342 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.342 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.342 I llama_model_loader: - type  f32:  194 tensors
0.00.027.343 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.343 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.344 I print_info: file format = GGUF V3 (latest)
0.00.027.344 I print_info: file type   = Q4_1
0.00.027.345 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.204 I load: special tokens cache size = 25
0.00.041.272 I load: token to piece cache size = 0.2984 MB
0.00.041.276 I print_info: arch             = gptneox
0.00.041.276 I print_info: vocab_only       = 0
0.00.041.276 I print_info: n_ctx_train      = 2048
0.00.041.276 I print_info: n_embd           = 2048
0.00.041.276 I print_info: n_layer          = 24
0.00.041.279 I print_info: n_head           = 16
0.00.041.280 I print_info: n_head_kv        = 16
0.00.041.280 I print_info: n_rot            = 32
0.00.041.281 I print_info: n_swa            = 0
0.00.041.281 I print_info: n_embd_head_k    = 128
0.00.041.281 I print_info: n_embd_head_v    = 128
0.00.041.282 I print_info: n_gqa            = 1
0.00.041.283 I print_info: n_embd_k_gqa     = 2048
0.00.041.283 I print_info: n_embd_v_gqa     = 2048
0.00.041.284 I print_info: f_norm_eps       = 1.0e-05
0.00.041.287 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.287 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.287 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.287 I print_info: f_logit_scale    = 0.0e+00
0.00.041.288 I print_info: n_ff             = 8192
0.00.041.288 I print_info: n_expert         = 0
0.00.041.288 I print_info: n_expert_used    = 0
0.00.041.288 I print_info: causal attn      = 1
0.00.041.289 I print_info: pooling type     = 0
0.00.041.290 I print_info: rope type        = 2
0.00.041.291 I print_info: rope scaling     = linear
0.00.041.291 I print_info: freq_base_train  = 10000.0
0.00.041.292 I print_info: freq_scale_train = 1
0.00.041.292 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.292 I print_info: rope_finetuned   = unknown
0.00.041.292 I print_info: ssm_d_conv       = 0
0.00.041.292 I print_info: ssm_d_inner      = 0
0.00.041.292 I print_info: ssm_d_state      = 0
0.00.041.293 I print_info: ssm_dt_rank      = 0
0.00.041.293 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.293 I print_info: model type       = 1.4B
0.00.041.293 I print_info: model params     = 1.41 B
0.00.041.293 I print_info: general.name     = 1.4B
0.00.041.298 I print_info: vocab type       = BPE
0.00.041.299 I print_info: n_vocab          = 50304
0.00.041.300 I print_info: n_merges         = 50009
0.00.041.300 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.300 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.300 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.300 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.301 I print_info: LF token         = 187 ''
0.00.041.301 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.301 I print_info: max token length = 1024
0.00.041.301 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.655.592 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.606 I load_tensors: offloading output layer to GPU
0.00.655.607 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.637 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.655.638 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.657.137 I llama_init_from_model: n_seq_max     = 1
0.00.657.143 I llama_init_from_model: n_ctx         = 2048
0.00.657.144 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.657.144 I llama_init_from_model: n_batch       = 2048
0.00.657.145 I llama_init_from_model: n_ubatch      = 512
0.00.657.146 I llama_init_from_model: flash_attn    = 0
0.00.657.146 I llama_init_from_model: freq_base     = 10000.0
0.00.657.147 I llama_init_from_model: freq_scale    = 1
0.00.657.150 I ggml_metal_init: allocating
0.00.657.197 I ggml_metal_init: found device: Apple M4
0.00.657.214 I ggml_metal_init: picking default device: Apple M4
0.00.659.018 I ggml_metal_init: using embedded metal library
0.00.665.167 I ggml_metal_init: GPU name:   Apple M4
0.00.665.172 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.665.173 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.665.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.665.175 I ggml_metal_init: simdgroup reduction   = true
0.00.665.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.665.176 I ggml_metal_init: has residency sets    = true
0.00.665.176 I ggml_metal_init: has bfloat            = true
0.00.665.176 I ggml_metal_init: use bfloat            = true
0.00.665.177 I ggml_metal_init: hasUnifiedMemory      = true
0.00.665.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.684.002 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.156 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.742.161 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.742.196 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.746.879 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.746.881 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.746.881 I llama_init_from_model: graph nodes  = 967
0.00.746.881 I llama_init_from_model: graph splits = 2
0.00.746.888 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.747.020 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.021 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.145 I main: llama threadpool init, n_threads = 4
0.00.804.189 I 
0.00.804.204 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.204 I 
0.00.804.355 I sampler seed: 1234
0.00.804.360 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.371 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.371 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.371 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.527.321 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55949.57 tokens per second)
0.01.527.322 I llama_perf_context_print:        load time =     793.49 ms
0.01.527.323 I llama_perf_context_print: prompt eval time =      48.09 ms /     7 tokens (    6.87 ms per token,   145.56 tokens per second)
0.01.527.324 I llama_perf_context_print:        eval time =     672.14 ms /    63 runs   (   10.67 ms per token,    93.73 tokens per second)
0.01.527.324 I llama_perf_context_print:       total time =     723.87 ms /    70 tokens
0.01.527.596 I ggml_metal_free: deallocating

real	0m1.547s
user	0m0.111s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.299 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.304 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.312 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.312 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.187 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.252 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.144 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.145 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.146 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.147 I llama_model_loader: - type  f32:  194 tensors
0.00.025.147 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.148 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.149 I print_info: file format = GGUF V3 (latest)
0.00.025.149 I print_info: file type   = Q4_1
0.00.025.150 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.351 I load: special tokens cache size = 25
0.00.039.584 I load: token to piece cache size = 0.2984 MB
0.00.039.587 I print_info: arch             = gptneox
0.00.039.588 I print_info: vocab_only       = 0
0.00.039.588 I print_info: n_ctx_train      = 2048
0.00.039.588 I print_info: n_embd           = 2048
0.00.039.588 I print_info: n_layer          = 24
0.00.039.593 I print_info: n_head           = 16
0.00.039.594 I print_info: n_head_kv        = 16
0.00.039.594 I print_info: n_rot            = 32
0.00.039.594 I print_info: n_swa            = 0
0.00.039.594 I print_info: n_embd_head_k    = 128
0.00.039.594 I print_info: n_embd_head_v    = 128
0.00.039.595 I print_info: n_gqa            = 1
0.00.039.596 I print_info: n_embd_k_gqa     = 2048
0.00.039.597 I print_info: n_embd_v_gqa     = 2048
0.00.039.597 I print_info: f_norm_eps       = 1.0e-05
0.00.039.598 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.598 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.598 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.598 I print_info: f_logit_scale    = 0.0e+00
0.00.039.600 I print_info: n_ff             = 8192
0.00.039.600 I print_info: n_expert         = 0
0.00.039.600 I print_info: n_expert_used    = 0
0.00.039.601 I print_info: causal attn      = 1
0.00.039.601 I print_info: pooling type     = 0
0.00.039.601 I print_info: rope type        = 2
0.00.039.601 I print_info: rope scaling     = linear
0.00.039.601 I print_info: freq_base_train  = 10000.0
0.00.039.602 I print_info: freq_scale_train = 1
0.00.039.602 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.602 I print_info: rope_finetuned   = unknown
0.00.039.602 I print_info: ssm_d_conv       = 0
0.00.039.602 I print_info: ssm_d_inner      = 0
0.00.039.603 I print_info: ssm_d_state      = 0
0.00.039.603 I print_info: ssm_dt_rank      = 0
0.00.039.603 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.603 I print_info: model type       = 1.4B
0.00.039.603 I print_info: model params     = 1.41 B
0.00.039.604 I print_info: general.name     = 1.4B
0.00.039.604 I print_info: vocab type       = BPE
0.00.039.604 I print_info: n_vocab          = 50304
0.00.039.605 I print_info: n_merges         = 50009
0.00.039.605 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.605 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.605 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.607 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.607 I print_info: LF token         = 187 ''
0.00.039.607 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.608 I print_info: max token length = 1024
0.00.039.608 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.321 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.334 I load_tensors: offloading output layer to GPU
0.00.637.335 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.365 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.637.367 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.638.961 I llama_init_from_model: n_seq_max     = 1
0.00.638.965 I llama_init_from_model: n_ctx         = 128
0.00.638.966 I llama_init_from_model: n_ctx_per_seq = 128
0.00.638.966 I llama_init_from_model: n_batch       = 128
0.00.638.967 I llama_init_from_model: n_ubatch      = 128
0.00.638.967 I llama_init_from_model: flash_attn    = 0
0.00.638.969 I llama_init_from_model: freq_base     = 10000.0
0.00.638.969 I llama_init_from_model: freq_scale    = 1
0.00.638.970 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.638.972 I ggml_metal_init: allocating
0.00.639.044 I ggml_metal_init: found device: Apple M4
0.00.639.058 I ggml_metal_init: picking default device: Apple M4
0.00.640.756 I ggml_metal_init: using embedded metal library
0.00.647.365 I ggml_metal_init: GPU name:   Apple M4
0.00.647.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.372 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.373 I ggml_metal_init: simdgroup reduction   = true
0.00.647.374 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.374 I ggml_metal_init: has residency sets    = true
0.00.647.374 I ggml_metal_init: has bfloat            = true
0.00.647.374 I ggml_metal_init: use bfloat            = true
0.00.647.375 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.378 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.371 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.668.768 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.668.772 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.668.814 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.135 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.672.137 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.672.137 I llama_init_from_model: graph nodes  = 967
0.00.672.138 I llama_init_from_model: graph splits = 2
0.00.672.141 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.672.143 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.647 I 
0.00.700.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.736 I perplexity: tokenizing the input ..
0.00.706.203 I perplexity: tokenization took 5.466 ms
0.00.706.219 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.026 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.833.365 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.833.381 I llama_perf_context_print:        load time =     691.73 ms
0.00.833.382 I llama_perf_context_print: prompt eval time =     125.58 ms /   128 tokens (    0.98 ms per token,  1019.28 tokens per second)
0.00.833.382 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.383 I llama_perf_context_print:       total time =     132.74 ms /   129 tokens
0.00.833.756 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.077s
sys	0m0.120s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.859 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.263 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.269 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.269 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.275 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.275 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.277 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.278 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.278 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.279 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.279 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.279 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.280 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.280 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.282 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.282 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.282 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.205 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.209 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.050 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.051 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.051 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.051 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.052 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.052 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.053 I llama_model_loader: - type  f32:  194 tensors
0.00.026.053 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.053 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.054 I print_info: file format = GGUF V3 (latest)
0.00.026.054 I print_info: file type   = Q5_0
0.00.026.055 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.244 I load: special tokens cache size = 25
0.00.040.222 I load: token to piece cache size = 0.2984 MB
0.00.040.225 I print_info: arch             = gptneox
0.00.040.225 I print_info: vocab_only       = 0
0.00.040.225 I print_info: n_ctx_train      = 2048
0.00.040.225 I print_info: n_embd           = 2048
0.00.040.225 I print_info: n_layer          = 24
0.00.040.228 I print_info: n_head           = 16
0.00.040.229 I print_info: n_head_kv        = 16
0.00.040.229 I print_info: n_rot            = 32
0.00.040.229 I print_info: n_swa            = 0
0.00.040.229 I print_info: n_embd_head_k    = 128
0.00.040.229 I print_info: n_embd_head_v    = 128
0.00.040.230 I print_info: n_gqa            = 1
0.00.040.231 I print_info: n_embd_k_gqa     = 2048
0.00.040.232 I print_info: n_embd_v_gqa     = 2048
0.00.040.233 I print_info: f_norm_eps       = 1.0e-05
0.00.040.233 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.233 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.234 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.234 I print_info: f_logit_scale    = 0.0e+00
0.00.040.234 I print_info: n_ff             = 8192
0.00.040.235 I print_info: n_expert         = 0
0.00.040.235 I print_info: n_expert_used    = 0
0.00.040.235 I print_info: causal attn      = 1
0.00.040.235 I print_info: pooling type     = 0
0.00.040.235 I print_info: rope type        = 2
0.00.040.235 I print_info: rope scaling     = linear
0.00.040.236 I print_info: freq_base_train  = 10000.0
0.00.040.236 I print_info: freq_scale_train = 1
0.00.040.238 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.238 I print_info: rope_finetuned   = unknown
0.00.040.238 I print_info: ssm_d_conv       = 0
0.00.040.238 I print_info: ssm_d_inner      = 0
0.00.040.239 I print_info: ssm_d_state      = 0
0.00.040.239 I print_info: ssm_dt_rank      = 0
0.00.040.239 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.239 I print_info: model type       = 1.4B
0.00.040.240 I print_info: model params     = 1.41 B
0.00.040.240 I print_info: general.name     = 1.4B
0.00.040.240 I print_info: vocab type       = BPE
0.00.040.241 I print_info: n_vocab          = 50304
0.00.040.241 I print_info: n_merges         = 50009
0.00.040.241 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.241 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.241 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.242 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.243 I print_info: LF token         = 187 ''
0.00.040.243 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.243 I print_info: max token length = 1024
0.00.040.244 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.703.839 I load_tensors: offloading 24 repeating layers to GPU
0.00.703.850 I load_tensors: offloading output layer to GPU
0.00.703.851 I load_tensors: offloaded 25/25 layers to GPU
0.00.703.885 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.703.886 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.705.298 I llama_init_from_model: n_seq_max     = 1
0.00.705.301 I llama_init_from_model: n_ctx         = 2048
0.00.705.301 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.705.302 I llama_init_from_model: n_batch       = 2048
0.00.705.302 I llama_init_from_model: n_ubatch      = 512
0.00.705.302 I llama_init_from_model: flash_attn    = 0
0.00.705.305 I llama_init_from_model: freq_base     = 10000.0
0.00.705.305 I llama_init_from_model: freq_scale    = 1
0.00.705.308 I ggml_metal_init: allocating
0.00.705.385 I ggml_metal_init: found device: Apple M4
0.00.705.399 I ggml_metal_init: picking default device: Apple M4
0.00.707.232 I ggml_metal_init: using embedded metal library
0.00.713.865 I ggml_metal_init: GPU name:   Apple M4
0.00.713.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.713.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.713.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.713.871 I ggml_metal_init: simdgroup reduction   = true
0.00.713.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.713.871 I ggml_metal_init: has residency sets    = true
0.00.713.872 I ggml_metal_init: has bfloat            = true
0.00.713.872 I ggml_metal_init: use bfloat            = true
0.00.713.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.713.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.731.467 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.788.117 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.788.124 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.788.162 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.792.872 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.792.875 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.792.876 I llama_init_from_model: graph nodes  = 967
0.00.792.876 I llama_init_from_model: graph splits = 2
0.00.792.883 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.793.012 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.793.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.840.474 I main: llama threadpool init, n_threads = 4
0.00.840.517 I 
0.00.840.532 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.840.532 I 
0.00.840.631 I sampler seed: 1234
0.00.840.636 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.840.647 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.840.647 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.840.647 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.633.070 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48233.70 tokens per second)
0.01.633.071 I llama_perf_context_print:        load time =     830.88 ms
0.01.633.072 I llama_perf_context_print: prompt eval time =      42.83 ms /     7 tokens (    6.12 ms per token,   163.43 tokens per second)
0.01.633.076 I llama_perf_context_print:        eval time =     746.98 ms /    63 runs   (   11.86 ms per token,    84.34 tokens per second)
0.01.633.077 I llama_perf_context_print:       total time =     793.32 ms /    70 tokens
0.01.633.334 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.110s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.775 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.134 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.141 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.148 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.149 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.150 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.152 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.153 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.155 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.156 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.982 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.980 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.798 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.799 I llama_model_loader: - type  f32:  194 tensors
0.00.025.799 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.799 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.800 I print_info: file format = GGUF V3 (latest)
0.00.025.800 I print_info: file type   = Q5_0
0.00.025.802 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.188 I load: special tokens cache size = 25
0.00.040.173 I load: token to piece cache size = 0.2984 MB
0.00.040.178 I print_info: arch             = gptneox
0.00.040.178 I print_info: vocab_only       = 0
0.00.040.179 I print_info: n_ctx_train      = 2048
0.00.040.179 I print_info: n_embd           = 2048
0.00.040.179 I print_info: n_layer          = 24
0.00.040.183 I print_info: n_head           = 16
0.00.040.184 I print_info: n_head_kv        = 16
0.00.040.187 I print_info: n_rot            = 32
0.00.040.187 I print_info: n_swa            = 0
0.00.040.187 I print_info: n_embd_head_k    = 128
0.00.040.187 I print_info: n_embd_head_v    = 128
0.00.040.188 I print_info: n_gqa            = 1
0.00.040.189 I print_info: n_embd_k_gqa     = 2048
0.00.040.189 I print_info: n_embd_v_gqa     = 2048
0.00.040.190 I print_info: f_norm_eps       = 1.0e-05
0.00.040.190 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.190 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.191 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.192 I print_info: f_logit_scale    = 0.0e+00
0.00.040.193 I print_info: n_ff             = 8192
0.00.040.193 I print_info: n_expert         = 0
0.00.040.193 I print_info: n_expert_used    = 0
0.00.040.193 I print_info: causal attn      = 1
0.00.040.194 I print_info: pooling type     = 0
0.00.040.194 I print_info: rope type        = 2
0.00.040.194 I print_info: rope scaling     = linear
0.00.040.194 I print_info: freq_base_train  = 10000.0
0.00.040.195 I print_info: freq_scale_train = 1
0.00.040.195 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.195 I print_info: rope_finetuned   = unknown
0.00.040.195 I print_info: ssm_d_conv       = 0
0.00.040.195 I print_info: ssm_d_inner      = 0
0.00.040.196 I print_info: ssm_d_state      = 0
0.00.040.196 I print_info: ssm_dt_rank      = 0
0.00.040.196 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.197 I print_info: model type       = 1.4B
0.00.040.197 I print_info: model params     = 1.41 B
0.00.040.198 I print_info: general.name     = 1.4B
0.00.040.198 I print_info: vocab type       = BPE
0.00.040.198 I print_info: n_vocab          = 50304
0.00.040.198 I print_info: n_merges         = 50009
0.00.040.199 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.199 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.199 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.199 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.199 I print_info: LF token         = 187 ''
0.00.040.200 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.200 I print_info: max token length = 1024
0.00.040.200 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.728.997 I load_tensors: offloading 24 repeating layers to GPU
0.00.729.013 I load_tensors: offloading output layer to GPU
0.00.729.014 I load_tensors: offloaded 25/25 layers to GPU
0.00.729.046 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.729.048 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.730.733 I llama_init_from_model: n_seq_max     = 1
0.00.730.739 I llama_init_from_model: n_ctx         = 128
0.00.730.739 I llama_init_from_model: n_ctx_per_seq = 128
0.00.730.740 I llama_init_from_model: n_batch       = 128
0.00.730.740 I llama_init_from_model: n_ubatch      = 128
0.00.730.741 I llama_init_from_model: flash_attn    = 0
0.00.730.743 I llama_init_from_model: freq_base     = 10000.0
0.00.730.743 I llama_init_from_model: freq_scale    = 1
0.00.730.744 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.730.749 I ggml_metal_init: allocating
0.00.730.833 I ggml_metal_init: found device: Apple M4
0.00.730.848 I ggml_metal_init: picking default device: Apple M4
0.00.732.879 I ggml_metal_init: using embedded metal library
0.00.739.195 I ggml_metal_init: GPU name:   Apple M4
0.00.739.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.739.210 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.739.211 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.739.211 I ggml_metal_init: simdgroup reduction   = true
0.00.739.225 I ggml_metal_init: simdgroup matrix mul. = true
0.00.739.225 I ggml_metal_init: has residency sets    = true
0.00.739.225 I ggml_metal_init: has bfloat            = true
0.00.739.226 I ggml_metal_init: use bfloat            = true
0.00.739.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.739.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.759.513 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.763.089 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.763.098 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.763.173 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.766.507 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.766.509 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.766.509 I llama_init_from_model: graph nodes  = 967
0.00.766.510 I llama_init_from_model: graph splits = 2
0.00.766.513 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.766.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.575 I 
0.00.794.632 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.659 I perplexity: tokenizing the input ..
0.00.801.617 I perplexity: tokenization took 6.955 ms
0.00.801.634 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.937.746 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.939.172 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.939.189 I llama_perf_context_print:        load time =     784.78 ms
0.00.939.190 I llama_perf_context_print: prompt eval time =     135.15 ms /   128 tokens (    1.06 ms per token,   947.12 tokens per second)
0.00.939.194 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.939.194 I llama_perf_context_print:       total time =     144.62 ms /   129 tokens
0.00.939.533 I ggml_metal_free: deallocating

real	0m0.955s
user	0m0.080s
sys	0m0.158s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.188 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.295 I main: llama backend init
0.00.000.302 I main: load the model and apply lora adapter, if any
0.00.078.021 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.126.901 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.126.913 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.126.916 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.126.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.126.918 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.126.918 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.126.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.126.931 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.126.932 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.126.932 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.126.933 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.126.934 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.126.934 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.126.937 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.126.940 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.126.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.126.941 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.133.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.136.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.143.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.143.152 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.143.153 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.143.153 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.143.154 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.143.155 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.143.156 I llama_model_loader: - type  f32:  194 tensors
0.00.143.156 I llama_model_loader: - type q5_1:   97 tensors
0.00.143.157 I llama_model_loader: - type q6_K:    1 tensors
0.00.143.158 I print_info: file format = GGUF V3 (latest)
0.00.143.159 I print_info: file type   = Q5_1
0.00.143.161 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.159.030 I load: special tokens cache size = 25
0.00.165.123 I load: token to piece cache size = 0.2984 MB
0.00.165.131 I print_info: arch             = gptneox
0.00.165.131 I print_info: vocab_only       = 0
0.00.165.131 I print_info: n_ctx_train      = 2048
0.00.165.132 I print_info: n_embd           = 2048
0.00.165.132 I print_info: n_layer          = 24
0.00.165.136 I print_info: n_head           = 16
0.00.165.137 I print_info: n_head_kv        = 16
0.00.165.137 I print_info: n_rot            = 32
0.00.165.137 I print_info: n_swa            = 0
0.00.165.137 I print_info: n_embd_head_k    = 128
0.00.165.138 I print_info: n_embd_head_v    = 128
0.00.165.140 I print_info: n_gqa            = 1
0.00.165.140 I print_info: n_embd_k_gqa     = 2048
0.00.165.141 I print_info: n_embd_v_gqa     = 2048
0.00.165.142 I print_info: f_norm_eps       = 1.0e-05
0.00.165.142 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.165.142 I print_info: f_clamp_kqv      = 0.0e+00
0.00.165.142 I print_info: f_max_alibi_bias = 0.0e+00
0.00.165.142 I print_info: f_logit_scale    = 0.0e+00
0.00.165.143 I print_info: n_ff             = 8192
0.00.165.143 I print_info: n_expert         = 0
0.00.165.143 I print_info: n_expert_used    = 0
0.00.165.143 I print_info: causal attn      = 1
0.00.165.143 I print_info: pooling type     = 0
0.00.165.144 I print_info: rope type        = 2
0.00.165.145 I print_info: rope scaling     = linear
0.00.165.147 I print_info: freq_base_train  = 10000.0
0.00.165.148 I print_info: freq_scale_train = 1
0.00.165.148 I print_info: n_ctx_orig_yarn  = 2048
0.00.165.148 I print_info: rope_finetuned   = unknown
0.00.165.148 I print_info: ssm_d_conv       = 0
0.00.165.148 I print_info: ssm_d_inner      = 0
0.00.165.148 I print_info: ssm_d_state      = 0
0.00.165.148 I print_info: ssm_dt_rank      = 0
0.00.165.149 I print_info: ssm_dt_b_c_rms   = 0
0.00.165.149 I print_info: model type       = 1.4B
0.00.165.149 I print_info: model params     = 1.41 B
0.00.165.149 I print_info: general.name     = 1.4B
0.00.165.151 I print_info: vocab type       = BPE
0.00.165.151 I print_info: n_vocab          = 50304
0.00.165.151 I print_info: n_merges         = 50009
0.00.165.151 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.165.152 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.165.152 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.165.152 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.165.152 I print_info: LF token         = 187 ''
0.00.165.153 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.165.153 I print_info: max token length = 1024
0.00.165.153 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.736.569 I load_tensors: offloading 24 repeating layers to GPU
0.00.736.576 I load_tensors: offloading output layer to GPU
0.00.736.577 I load_tensors: offloaded 25/25 layers to GPU
0.00.736.599 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.736.600 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.737.499 I llama_init_from_model: n_seq_max     = 1
0.00.737.502 I llama_init_from_model: n_ctx         = 2048
0.00.737.503 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.737.503 I llama_init_from_model: n_batch       = 2048
0.00.737.504 I llama_init_from_model: n_ubatch      = 512
0.00.737.504 I llama_init_from_model: flash_attn    = 0
0.00.737.505 I llama_init_from_model: freq_base     = 10000.0
0.00.737.505 I llama_init_from_model: freq_scale    = 1
0.00.737.506 I ggml_metal_init: allocating
0.00.737.545 I ggml_metal_init: found device: Apple M4
0.00.737.556 I ggml_metal_init: picking default device: Apple M4
0.00.738.603 I ggml_metal_init: using embedded metal library
0.00.742.756 I ggml_metal_init: GPU name:   Apple M4
0.00.742.764 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.742.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.742.765 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.742.766 I ggml_metal_init: simdgroup reduction   = true
0.00.742.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.742.766 I ggml_metal_init: has residency sets    = true
0.00.742.766 I ggml_metal_init: has bfloat            = true
0.00.742.767 I ggml_metal_init: use bfloat            = true
0.00.742.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.742.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.755.610 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.787.591 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.787.599 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.787.637 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.791.965 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.791.967 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.791.967 I llama_init_from_model: graph nodes  = 967
0.00.791.967 I llama_init_from_model: graph splits = 2
0.00.791.972 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.792.101 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.792.101 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.850.632 I main: llama threadpool init, n_threads = 4
0.00.850.677 I 
0.00.850.713 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.850.717 I 
0.00.850.923 I sampler seed: 1234
0.00.850.930 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.850.946 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.850.948 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.850.948 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.704.002 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48233.70 tokens per second)
0.01.704.004 I llama_perf_context_print:        load time =     771.88 ms
0.01.704.005 I llama_perf_context_print: prompt eval time =      52.15 ms /     7 tokens (    7.45 ms per token,   134.24 tokens per second)
0.01.704.006 I llama_perf_context_print:        eval time =     798.26 ms /    63 runs   (   12.67 ms per token,    78.92 tokens per second)
0.01.704.007 I llama_perf_context_print:       total time =     854.08 ms /    70 tokens
0.01.704.234 I ggml_metal_free: deallocating

real	0m1.835s
user	0m0.127s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.581 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.690 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.694 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.696 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.697 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.700 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.700 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.702 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.703 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.704 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.704 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.657 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.659 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.660 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.661 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.661 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.661 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.662 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.662 I llama_model_loader: - type  f32:  194 tensors
0.00.027.662 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.663 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.663 I print_info: file format = GGUF V3 (latest)
0.00.027.664 I print_info: file type   = Q5_1
0.00.027.665 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.961 I load: special tokens cache size = 25
0.00.042.136 I load: token to piece cache size = 0.2984 MB
0.00.042.141 I print_info: arch             = gptneox
0.00.042.141 I print_info: vocab_only       = 0
0.00.042.141 I print_info: n_ctx_train      = 2048
0.00.042.141 I print_info: n_embd           = 2048
0.00.042.141 I print_info: n_layer          = 24
0.00.042.146 I print_info: n_head           = 16
0.00.042.147 I print_info: n_head_kv        = 16
0.00.042.147 I print_info: n_rot            = 32
0.00.042.147 I print_info: n_swa            = 0
0.00.042.147 I print_info: n_embd_head_k    = 128
0.00.042.147 I print_info: n_embd_head_v    = 128
0.00.042.148 I print_info: n_gqa            = 1
0.00.042.149 I print_info: n_embd_k_gqa     = 2048
0.00.042.149 I print_info: n_embd_v_gqa     = 2048
0.00.042.150 I print_info: f_norm_eps       = 1.0e-05
0.00.042.151 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.151 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.151 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.151 I print_info: f_logit_scale    = 0.0e+00
0.00.042.151 I print_info: n_ff             = 8192
0.00.042.152 I print_info: n_expert         = 0
0.00.042.155 I print_info: n_expert_used    = 0
0.00.042.155 I print_info: causal attn      = 1
0.00.042.155 I print_info: pooling type     = 0
0.00.042.155 I print_info: rope type        = 2
0.00.042.155 I print_info: rope scaling     = linear
0.00.042.156 I print_info: freq_base_train  = 10000.0
0.00.042.156 I print_info: freq_scale_train = 1
0.00.042.156 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.156 I print_info: rope_finetuned   = unknown
0.00.042.156 I print_info: ssm_d_conv       = 0
0.00.042.157 I print_info: ssm_d_inner      = 0
0.00.042.157 I print_info: ssm_d_state      = 0
0.00.042.157 I print_info: ssm_dt_rank      = 0
0.00.042.157 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.157 I print_info: model type       = 1.4B
0.00.042.157 I print_info: model params     = 1.41 B
0.00.042.158 I print_info: general.name     = 1.4B
0.00.042.158 I print_info: vocab type       = BPE
0.00.042.158 I print_info: n_vocab          = 50304
0.00.042.159 I print_info: n_merges         = 50009
0.00.042.159 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.159 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.159 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.159 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.160 I print_info: LF token         = 187 ''
0.00.042.160 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.160 I print_info: max token length = 1024
0.00.042.160 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.840 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.848 I load_tensors: offloading output layer to GPU
0.00.604.849 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.880 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.884 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.606.227 I llama_init_from_model: n_seq_max     = 1
0.00.606.229 I llama_init_from_model: n_ctx         = 128
0.00.606.230 I llama_init_from_model: n_ctx_per_seq = 128
0.00.606.230 I llama_init_from_model: n_batch       = 128
0.00.606.231 I llama_init_from_model: n_ubatch      = 128
0.00.606.231 I llama_init_from_model: flash_attn    = 0
0.00.606.232 I llama_init_from_model: freq_base     = 10000.0
0.00.606.233 I llama_init_from_model: freq_scale    = 1
0.00.606.234 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.606.235 I ggml_metal_init: allocating
0.00.606.307 I ggml_metal_init: found device: Apple M4
0.00.606.320 I ggml_metal_init: picking default device: Apple M4
0.00.607.886 I ggml_metal_init: using embedded metal library
0.00.613.937 I ggml_metal_init: GPU name:   Apple M4
0.00.613.941 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.942 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.943 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.943 I ggml_metal_init: simdgroup reduction   = true
0.00.613.944 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.944 I ggml_metal_init: has residency sets    = true
0.00.613.944 I ggml_metal_init: has bfloat            = true
0.00.613.944 I ggml_metal_init: use bfloat            = true
0.00.613.945 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.947 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.748 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.634.215 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.634.218 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.634.261 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.637.465 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.637.467 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.637.467 I llama_init_from_model: graph nodes  = 967
0.00.637.468 I llama_init_from_model: graph splits = 2
0.00.637.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.178 I 
0.00.668.246 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.277 I perplexity: tokenizing the input ..
0.00.675.759 I perplexity: tokenization took 7.479 ms
0.00.675.780 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.634 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.825.973 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.825.985 I llama_perf_context_print:        load time =     659.58 ms
0.00.825.986 I llama_perf_context_print: prompt eval time =     147.93 ms /   128 tokens (    1.16 ms per token,   865.28 tokens per second)
0.00.825.987 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.987 I llama_perf_context_print:       total time =     157.81 ms /   129 tokens
0.00.826.379 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.080s
sys	0m0.125s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.058 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.746 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.747 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.749 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.750 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.750 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.750 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.751 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.753 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.753 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.753 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.716 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.821 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.738 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.739 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.740 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.740 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.740 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.741 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.741 I llama_model_loader: - type  f32:  194 tensors
0.00.025.742 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.742 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.742 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.743 I print_info: file format = GGUF V3 (latest)
0.00.025.743 I print_info: file type   = Q2_K - Medium
0.00.025.745 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.061 I load: special tokens cache size = 25
0.00.040.452 I load: token to piece cache size = 0.2984 MB
0.00.040.455 I print_info: arch             = gptneox
0.00.040.456 I print_info: vocab_only       = 0
0.00.040.456 I print_info: n_ctx_train      = 2048
0.00.040.456 I print_info: n_embd           = 2048
0.00.040.456 I print_info: n_layer          = 24
0.00.040.459 I print_info: n_head           = 16
0.00.040.460 I print_info: n_head_kv        = 16
0.00.040.460 I print_info: n_rot            = 32
0.00.040.460 I print_info: n_swa            = 0
0.00.040.461 I print_info: n_embd_head_k    = 128
0.00.040.461 I print_info: n_embd_head_v    = 128
0.00.040.462 I print_info: n_gqa            = 1
0.00.040.462 I print_info: n_embd_k_gqa     = 2048
0.00.040.463 I print_info: n_embd_v_gqa     = 2048
0.00.040.464 I print_info: f_norm_eps       = 1.0e-05
0.00.040.464 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.464 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.464 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.465 I print_info: f_logit_scale    = 0.0e+00
0.00.040.465 I print_info: n_ff             = 8192
0.00.040.465 I print_info: n_expert         = 0
0.00.040.465 I print_info: n_expert_used    = 0
0.00.040.466 I print_info: causal attn      = 1
0.00.040.466 I print_info: pooling type     = 0
0.00.040.468 I print_info: rope type        = 2
0.00.040.469 I print_info: rope scaling     = linear
0.00.040.470 I print_info: freq_base_train  = 10000.0
0.00.040.470 I print_info: freq_scale_train = 1
0.00.040.470 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.470 I print_info: rope_finetuned   = unknown
0.00.040.470 I print_info: ssm_d_conv       = 0
0.00.040.470 I print_info: ssm_d_inner      = 0
0.00.040.471 I print_info: ssm_d_state      = 0
0.00.040.471 I print_info: ssm_dt_rank      = 0
0.00.040.471 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.471 I print_info: model type       = 1.4B
0.00.040.471 I print_info: model params     = 1.41 B
0.00.040.471 I print_info: general.name     = 1.4B
0.00.040.472 I print_info: vocab type       = BPE
0.00.040.472 I print_info: n_vocab          = 50304
0.00.040.472 I print_info: n_merges         = 50009
0.00.040.472 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.472 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.473 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.473 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.473 I print_info: LF token         = 187 ''
0.00.040.473 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.473 I print_info: max token length = 1024
0.00.040.475 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.351.371 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.379 I load_tensors: offloading output layer to GPU
0.00.351.379 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.397 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.398 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.352.210 I llama_init_from_model: n_seq_max     = 1
0.00.352.215 I llama_init_from_model: n_ctx         = 2048
0.00.352.216 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.352.216 I llama_init_from_model: n_batch       = 2048
0.00.352.216 I llama_init_from_model: n_ubatch      = 512
0.00.352.217 I llama_init_from_model: flash_attn    = 0
0.00.352.218 I llama_init_from_model: freq_base     = 10000.0
0.00.352.218 I llama_init_from_model: freq_scale    = 1
0.00.352.219 I ggml_metal_init: allocating
0.00.352.260 I ggml_metal_init: found device: Apple M4
0.00.352.271 I ggml_metal_init: picking default device: Apple M4
0.00.353.367 I ggml_metal_init: using embedded metal library
0.00.357.649 I ggml_metal_init: GPU name:   Apple M4
0.00.357.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.357.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.357.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.357.656 I ggml_metal_init: simdgroup reduction   = true
0.00.357.656 I ggml_metal_init: simdgroup matrix mul. = true
0.00.357.657 I ggml_metal_init: has residency sets    = true
0.00.357.657 I ggml_metal_init: has bfloat            = true
0.00.357.657 I ggml_metal_init: use bfloat            = true
0.00.357.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.357.663 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.490 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.410.857 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.410.864 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.410.901 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.415.124 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.415.126 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.415.126 I llama_init_from_model: graph nodes  = 967
0.00.415.127 I llama_init_from_model: graph splits = 2
0.00.415.132 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.415.247 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.415.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.475.508 I main: llama threadpool init, n_threads = 4
0.00.475.544 I 
0.00.475.560 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.475.561 I 
0.00.475.735 I sampler seed: 1234
0.00.475.740 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.475.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.475.770 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.475.770 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.149.050 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52090.98 tokens per second)
0.01.149.051 I llama_perf_context_print:        load time =     464.74 ms
0.01.149.051 I llama_perf_context_print: prompt eval time =      44.21 ms /     7 tokens (    6.32 ms per token,   158.32 tokens per second)
0.01.149.052 I llama_perf_context_print:        eval time =     626.79 ms /    63 runs   (    9.95 ms per token,   100.51 tokens per second)
0.01.149.053 I llama_perf_context_print:       total time =     674.24 ms /    70 tokens
0.01.149.293 I ggml_metal_free: deallocating

real	0m1.168s
user	0m0.107s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.934 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.402 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.403 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.405 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.405 I llama_model_loader: - type  f32:  194 tensors
0.00.025.406 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.406 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.406 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.407 I print_info: file format = GGUF V3 (latest)
0.00.025.407 I print_info: file type   = Q2_K - Medium
0.00.025.408 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.206 I load: special tokens cache size = 25
0.00.039.260 I load: token to piece cache size = 0.2984 MB
0.00.039.262 I print_info: arch             = gptneox
0.00.039.262 I print_info: vocab_only       = 0
0.00.039.263 I print_info: n_ctx_train      = 2048
0.00.039.263 I print_info: n_embd           = 2048
0.00.039.263 I print_info: n_layer          = 24
0.00.039.267 I print_info: n_head           = 16
0.00.039.267 I print_info: n_head_kv        = 16
0.00.039.268 I print_info: n_rot            = 32
0.00.039.268 I print_info: n_swa            = 0
0.00.039.269 I print_info: n_embd_head_k    = 128
0.00.039.269 I print_info: n_embd_head_v    = 128
0.00.039.270 I print_info: n_gqa            = 1
0.00.039.270 I print_info: n_embd_k_gqa     = 2048
0.00.039.271 I print_info: n_embd_v_gqa     = 2048
0.00.039.272 I print_info: f_norm_eps       = 1.0e-05
0.00.039.272 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.274 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.274 I print_info: f_logit_scale    = 0.0e+00
0.00.039.275 I print_info: n_ff             = 8192
0.00.039.275 I print_info: n_expert         = 0
0.00.039.275 I print_info: n_expert_used    = 0
0.00.039.277 I print_info: causal attn      = 1
0.00.039.277 I print_info: pooling type     = 0
0.00.039.277 I print_info: rope type        = 2
0.00.039.277 I print_info: rope scaling     = linear
0.00.039.278 I print_info: freq_base_train  = 10000.0
0.00.039.278 I print_info: freq_scale_train = 1
0.00.039.278 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.278 I print_info: rope_finetuned   = unknown
0.00.039.279 I print_info: ssm_d_conv       = 0
0.00.039.279 I print_info: ssm_d_inner      = 0
0.00.039.279 I print_info: ssm_d_state      = 0
0.00.039.279 I print_info: ssm_dt_rank      = 0
0.00.039.279 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.279 I print_info: model type       = 1.4B
0.00.039.280 I print_info: model params     = 1.41 B
0.00.039.284 I print_info: general.name     = 1.4B
0.00.039.284 I print_info: vocab type       = BPE
0.00.039.285 I print_info: n_vocab          = 50304
0.00.039.285 I print_info: n_merges         = 50009
0.00.039.285 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.286 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.286 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.286 I print_info: LF token         = 187 ''
0.00.039.286 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.286 I print_info: max token length = 1024
0.00.039.287 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.357.426 I load_tensors: offloading 24 repeating layers to GPU
0.00.357.438 I load_tensors: offloading output layer to GPU
0.00.357.439 I load_tensors: offloaded 25/25 layers to GPU
0.00.357.470 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.357.471 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.359.078 I llama_init_from_model: n_seq_max     = 1
0.00.359.084 I llama_init_from_model: n_ctx         = 128
0.00.359.085 I llama_init_from_model: n_ctx_per_seq = 128
0.00.359.086 I llama_init_from_model: n_batch       = 128
0.00.359.086 I llama_init_from_model: n_ubatch      = 128
0.00.359.086 I llama_init_from_model: flash_attn    = 0
0.00.359.088 I llama_init_from_model: freq_base     = 10000.0
0.00.359.088 I llama_init_from_model: freq_scale    = 1
0.00.359.089 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.359.094 I ggml_metal_init: allocating
0.00.359.181 I ggml_metal_init: found device: Apple M4
0.00.359.196 I ggml_metal_init: picking default device: Apple M4
0.00.360.995 I ggml_metal_init: using embedded metal library
0.00.366.754 I ggml_metal_init: GPU name:   Apple M4
0.00.366.767 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.366.768 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.366.769 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.366.770 I ggml_metal_init: simdgroup reduction   = true
0.00.366.770 I ggml_metal_init: simdgroup matrix mul. = true
0.00.366.770 I ggml_metal_init: has residency sets    = true
0.00.366.771 I ggml_metal_init: has bfloat            = true
0.00.366.771 I ggml_metal_init: use bfloat            = true
0.00.366.773 I ggml_metal_init: hasUnifiedMemory      = true
0.00.366.776 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.388.459 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.392.086 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.392.091 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.392.134 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.395.737 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.395.739 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.395.739 I llama_init_from_model: graph nodes  = 967
0.00.395.740 I llama_init_from_model: graph splits = 2
0.00.395.742 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.395.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.426.905 I 
0.00.426.965 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.426.993 I perplexity: tokenizing the input ..
0.00.433.956 I perplexity: tokenization took 6.961 ms
0.00.433.976 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.133 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.578.478 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.578.495 I llama_perf_context_print:        load time =     416.96 ms
0.00.578.497 I llama_perf_context_print: prompt eval time =     142.22 ms /   128 tokens (    1.11 ms per token,   899.98 tokens per second)
0.00.578.497 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.578.498 I llama_perf_context_print:       total time =     151.59 ms /   129 tokens
0.00.578.904 I ggml_metal_free: deallocating

real	0m0.594s
user	0m0.081s
sys	0m0.106s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.687 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.266 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.277 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.278 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.278 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.279 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.279 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.282 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.282 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.283 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.283 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.283 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.284 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.284 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.287 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.288 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.288 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.215 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.216 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.035 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.036 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.036 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.037 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.038 I llama_model_loader: - type  f32:  194 tensors
0.00.025.038 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.038 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.038 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.038 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.039 I print_info: file format = GGUF V3 (latest)
0.00.025.039 I print_info: file type   = Q3_K - Medium
0.00.025.040 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.931 I load: special tokens cache size = 25
0.00.039.002 I load: token to piece cache size = 0.2984 MB
0.00.039.005 I print_info: arch             = gptneox
0.00.039.005 I print_info: vocab_only       = 0
0.00.039.005 I print_info: n_ctx_train      = 2048
0.00.039.005 I print_info: n_embd           = 2048
0.00.039.006 I print_info: n_layer          = 24
0.00.039.008 I print_info: n_head           = 16
0.00.039.009 I print_info: n_head_kv        = 16
0.00.039.010 I print_info: n_rot            = 32
0.00.039.010 I print_info: n_swa            = 0
0.00.039.012 I print_info: n_embd_head_k    = 128
0.00.039.012 I print_info: n_embd_head_v    = 128
0.00.039.012 I print_info: n_gqa            = 1
0.00.039.013 I print_info: n_embd_k_gqa     = 2048
0.00.039.014 I print_info: n_embd_v_gqa     = 2048
0.00.039.014 I print_info: f_norm_eps       = 1.0e-05
0.00.039.015 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.015 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.015 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.015 I print_info: f_logit_scale    = 0.0e+00
0.00.039.016 I print_info: n_ff             = 8192
0.00.039.016 I print_info: n_expert         = 0
0.00.039.016 I print_info: n_expert_used    = 0
0.00.039.016 I print_info: causal attn      = 1
0.00.039.016 I print_info: pooling type     = 0
0.00.039.017 I print_info: rope type        = 2
0.00.039.017 I print_info: rope scaling     = linear
0.00.039.021 I print_info: freq_base_train  = 10000.0
0.00.039.021 I print_info: freq_scale_train = 1
0.00.039.021 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.021 I print_info: rope_finetuned   = unknown
0.00.039.022 I print_info: ssm_d_conv       = 0
0.00.039.022 I print_info: ssm_d_inner      = 0
0.00.039.022 I print_info: ssm_d_state      = 0
0.00.039.022 I print_info: ssm_dt_rank      = 0
0.00.039.022 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.022 I print_info: model type       = 1.4B
0.00.039.023 I print_info: model params     = 1.41 B
0.00.039.023 I print_info: general.name     = 1.4B
0.00.039.024 I print_info: vocab type       = BPE
0.00.039.024 I print_info: n_vocab          = 50304
0.00.039.024 I print_info: n_merges         = 50009
0.00.039.024 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.024 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.027 I print_info: LF token         = 187 ''
0.00.039.027 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.027 I print_info: max token length = 1024
0.00.039.027 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.439.285 I load_tensors: offloading 24 repeating layers to GPU
0.00.439.301 I load_tensors: offloading output layer to GPU
0.00.439.302 I load_tensors: offloaded 25/25 layers to GPU
0.00.439.335 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.439.337 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.440.668 I llama_init_from_model: n_seq_max     = 1
0.00.440.672 I llama_init_from_model: n_ctx         = 2048
0.00.440.673 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.440.674 I llama_init_from_model: n_batch       = 2048
0.00.440.674 I llama_init_from_model: n_ubatch      = 512
0.00.440.675 I llama_init_from_model: flash_attn    = 0
0.00.440.676 I llama_init_from_model: freq_base     = 10000.0
0.00.440.677 I llama_init_from_model: freq_scale    = 1
0.00.440.679 I ggml_metal_init: allocating
0.00.440.752 I ggml_metal_init: found device: Apple M4
0.00.440.766 I ggml_metal_init: picking default device: Apple M4
0.00.442.543 I ggml_metal_init: using embedded metal library
0.00.448.280 I ggml_metal_init: GPU name:   Apple M4
0.00.448.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.448.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.448.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.448.287 I ggml_metal_init: simdgroup reduction   = true
0.00.448.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.448.288 I ggml_metal_init: has residency sets    = true
0.00.448.288 I ggml_metal_init: has bfloat            = true
0.00.448.288 I ggml_metal_init: use bfloat            = true
0.00.448.289 I ggml_metal_init: hasUnifiedMemory      = true
0.00.448.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.467.887 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.524.231 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.524.237 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.524.272 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.528.900 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.528.903 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.528.903 I llama_init_from_model: graph nodes  = 967
0.00.528.903 I llama_init_from_model: graph splits = 2
0.00.528.908 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.529.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.529.037 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.966 I main: llama threadpool init, n_threads = 4
0.00.587.017 I 
0.00.587.032 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.032 I 
0.00.587.187 I sampler seed: 1234
0.00.587.192 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.587.203 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.587.204 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.587.204 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.339.309 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.01.339.310 I llama_perf_context_print:        load time =     577.57 ms
0.01.339.311 I llama_perf_context_print: prompt eval time =      49.82 ms /     7 tokens (    7.12 ms per token,   140.51 tokens per second)
0.01.339.311 I llama_perf_context_print:        eval time =     699.33 ms /    63 runs   (   11.10 ms per token,    90.09 tokens per second)
0.01.339.316 I llama_perf_context_print:       total time =     753.04 ms /    70 tokens
0.01.339.566 I ggml_metal_free: deallocating

real	0m1.356s
user	0m0.110s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.880 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.006 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.007 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.007 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.007 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.008 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.009 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.010 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.010 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.013 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.013 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.785 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.838 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.775 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.776 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.777 I llama_model_loader: - type  f32:  194 tensors
0.00.024.778 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.778 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.778 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.779 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.786 I print_info: file format = GGUF V3 (latest)
0.00.024.786 I print_info: file type   = Q3_K - Medium
0.00.024.788 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.160 I load: special tokens cache size = 25
0.00.039.224 I load: token to piece cache size = 0.2984 MB
0.00.039.227 I print_info: arch             = gptneox
0.00.039.227 I print_info: vocab_only       = 0
0.00.039.228 I print_info: n_ctx_train      = 2048
0.00.039.228 I print_info: n_embd           = 2048
0.00.039.228 I print_info: n_layer          = 24
0.00.039.232 I print_info: n_head           = 16
0.00.039.233 I print_info: n_head_kv        = 16
0.00.039.233 I print_info: n_rot            = 32
0.00.039.233 I print_info: n_swa            = 0
0.00.039.233 I print_info: n_embd_head_k    = 128
0.00.039.233 I print_info: n_embd_head_v    = 128
0.00.039.234 I print_info: n_gqa            = 1
0.00.039.237 I print_info: n_embd_k_gqa     = 2048
0.00.039.238 I print_info: n_embd_v_gqa     = 2048
0.00.039.239 I print_info: f_norm_eps       = 1.0e-05
0.00.039.239 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.239 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.242 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.242 I print_info: f_logit_scale    = 0.0e+00
0.00.039.242 I print_info: n_ff             = 8192
0.00.039.243 I print_info: n_expert         = 0
0.00.039.243 I print_info: n_expert_used    = 0
0.00.039.245 I print_info: causal attn      = 1
0.00.039.245 I print_info: pooling type     = 0
0.00.039.245 I print_info: rope type        = 2
0.00.039.246 I print_info: rope scaling     = linear
0.00.039.246 I print_info: freq_base_train  = 10000.0
0.00.039.247 I print_info: freq_scale_train = 1
0.00.039.247 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.247 I print_info: rope_finetuned   = unknown
0.00.039.247 I print_info: ssm_d_conv       = 0
0.00.039.247 I print_info: ssm_d_inner      = 0
0.00.039.247 I print_info: ssm_d_state      = 0
0.00.039.247 I print_info: ssm_dt_rank      = 0
0.00.039.248 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.248 I print_info: model type       = 1.4B
0.00.039.248 I print_info: model params     = 1.41 B
0.00.039.248 I print_info: general.name     = 1.4B
0.00.039.250 I print_info: vocab type       = BPE
0.00.039.250 I print_info: n_vocab          = 50304
0.00.039.250 I print_info: n_merges         = 50009
0.00.039.250 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.250 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.250 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.250 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.251 I print_info: LF token         = 187 ''
0.00.039.251 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.251 I print_info: max token length = 1024
0.00.039.252 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.429.325 I load_tensors: offloading 24 repeating layers to GPU
0.00.429.341 I load_tensors: offloading output layer to GPU
0.00.429.342 I load_tensors: offloaded 25/25 layers to GPU
0.00.429.380 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.429.381 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.430.522 I llama_init_from_model: n_seq_max     = 1
0.00.430.528 I llama_init_from_model: n_ctx         = 128
0.00.430.528 I llama_init_from_model: n_ctx_per_seq = 128
0.00.430.529 I llama_init_from_model: n_batch       = 128
0.00.430.529 I llama_init_from_model: n_ubatch      = 128
0.00.430.530 I llama_init_from_model: flash_attn    = 0
0.00.430.532 I llama_init_from_model: freq_base     = 10000.0
0.00.430.533 I llama_init_from_model: freq_scale    = 1
0.00.430.533 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.430.536 I ggml_metal_init: allocating
0.00.430.632 I ggml_metal_init: found device: Apple M4
0.00.430.645 I ggml_metal_init: picking default device: Apple M4
0.00.432.419 I ggml_metal_init: using embedded metal library
0.00.436.867 I ggml_metal_init: GPU name:   Apple M4
0.00.436.874 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.436.875 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.436.876 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.436.876 I ggml_metal_init: simdgroup reduction   = true
0.00.436.876 I ggml_metal_init: simdgroup matrix mul. = true
0.00.436.877 I ggml_metal_init: has residency sets    = true
0.00.436.877 I ggml_metal_init: has bfloat            = true
0.00.436.877 I ggml_metal_init: use bfloat            = true
0.00.436.878 I ggml_metal_init: hasUnifiedMemory      = true
0.00.436.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.452.597 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.454.255 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.454.258 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.454.300 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.455.848 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.455.849 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.455.849 I llama_init_from_model: graph nodes  = 967
0.00.455.849 I llama_init_from_model: graph splits = 2
0.00.455.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.455.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.480.710 I 
0.00.480.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.480.751 I perplexity: tokenizing the input ..
0.00.484.730 I perplexity: tokenization took 3.977 ms
0.00.484.743 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.626.128 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.627.465 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.627.480 I llama_perf_context_print:        load time =     471.82 ms
0.00.627.481 I llama_perf_context_print: prompt eval time =     141.16 ms /   128 tokens (    1.10 ms per token,   906.79 tokens per second)
0.00.627.482 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.627.482 I llama_perf_context_print:       total time =     146.77 ms /   129 tokens
0.00.627.855 I ggml_metal_free: deallocating

real	0m0.641s
user	0m0.071s
sys	0m0.089s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.239 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.660 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.665 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.667 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.667 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.672 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.672 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.672 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.673 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.675 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.676 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.216 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.217 I llama_model_loader: - type  f32:  194 tensors
0.00.025.218 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.218 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.218 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.219 I print_info: file format = GGUF V3 (latest)
0.00.025.219 I print_info: file type   = Q4_K - Medium
0.00.025.224 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.155 I load: special tokens cache size = 25
0.00.039.259 I load: token to piece cache size = 0.2984 MB
0.00.039.262 I print_info: arch             = gptneox
0.00.039.262 I print_info: vocab_only       = 0
0.00.039.262 I print_info: n_ctx_train      = 2048
0.00.039.263 I print_info: n_embd           = 2048
0.00.039.263 I print_info: n_layer          = 24
0.00.039.266 I print_info: n_head           = 16
0.00.039.266 I print_info: n_head_kv        = 16
0.00.039.267 I print_info: n_rot            = 32
0.00.039.267 I print_info: n_swa            = 0
0.00.039.267 I print_info: n_embd_head_k    = 128
0.00.039.267 I print_info: n_embd_head_v    = 128
0.00.039.268 I print_info: n_gqa            = 1
0.00.039.269 I print_info: n_embd_k_gqa     = 2048
0.00.039.269 I print_info: n_embd_v_gqa     = 2048
0.00.039.270 I print_info: f_norm_eps       = 1.0e-05
0.00.039.270 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.270 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.270 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.271 I print_info: f_logit_scale    = 0.0e+00
0.00.039.274 I print_info: n_ff             = 8192
0.00.039.274 I print_info: n_expert         = 0
0.00.039.274 I print_info: n_expert_used    = 0
0.00.039.276 I print_info: causal attn      = 1
0.00.039.277 I print_info: pooling type     = 0
0.00.039.277 I print_info: rope type        = 2
0.00.039.278 I print_info: rope scaling     = linear
0.00.039.278 I print_info: freq_base_train  = 10000.0
0.00.039.278 I print_info: freq_scale_train = 1
0.00.039.279 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.279 I print_info: rope_finetuned   = unknown
0.00.039.279 I print_info: ssm_d_conv       = 0
0.00.039.279 I print_info: ssm_d_inner      = 0
0.00.039.279 I print_info: ssm_d_state      = 0
0.00.039.279 I print_info: ssm_dt_rank      = 0
0.00.039.279 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.280 I print_info: model type       = 1.4B
0.00.039.281 I print_info: model params     = 1.41 B
0.00.039.281 I print_info: general.name     = 1.4B
0.00.039.282 I print_info: vocab type       = BPE
0.00.039.282 I print_info: n_vocab          = 50304
0.00.039.282 I print_info: n_merges         = 50009
0.00.039.283 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.283 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.283 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.283 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.283 I print_info: LF token         = 187 ''
0.00.039.283 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.284 I print_info: max token length = 1024
0.00.039.284 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.510.362 I load_tensors: offloading 24 repeating layers to GPU
0.00.510.374 I load_tensors: offloading output layer to GPU
0.00.510.375 I load_tensors: offloaded 25/25 layers to GPU
0.00.510.409 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.510.410 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.511.993 I llama_init_from_model: n_seq_max     = 1
0.00.511.996 I llama_init_from_model: n_ctx         = 2048
0.00.511.996 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.511.997 I llama_init_from_model: n_batch       = 2048
0.00.511.997 I llama_init_from_model: n_ubatch      = 512
0.00.511.997 I llama_init_from_model: flash_attn    = 0
0.00.511.999 I llama_init_from_model: freq_base     = 10000.0
0.00.512.000 I llama_init_from_model: freq_scale    = 1
0.00.512.003 I ggml_metal_init: allocating
0.00.512.083 I ggml_metal_init: found device: Apple M4
0.00.512.096 I ggml_metal_init: picking default device: Apple M4
0.00.513.944 I ggml_metal_init: using embedded metal library
0.00.520.623 I ggml_metal_init: GPU name:   Apple M4
0.00.520.627 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.520.628 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.520.628 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.520.629 I ggml_metal_init: simdgroup reduction   = true
0.00.520.629 I ggml_metal_init: simdgroup matrix mul. = true
0.00.520.630 I ggml_metal_init: has residency sets    = true
0.00.520.630 I ggml_metal_init: has bfloat            = true
0.00.520.630 I ggml_metal_init: use bfloat            = true
0.00.520.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.520.633 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.538.849 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.594.966 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.594.972 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.595.007 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.599.320 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.599.322 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.599.322 I llama_init_from_model: graph nodes  = 967
0.00.599.322 I llama_init_from_model: graph splits = 2
0.00.599.327 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.599.440 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.599.440 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.134 I main: llama threadpool init, n_threads = 4
0.00.654.180 I 
0.00.654.195 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.196 I 
0.00.654.346 I sampler seed: 1234
0.00.654.351 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.654.372 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.654.372 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.654.372 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.415.563 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49203.05 tokens per second)
0.01.415.564 I llama_perf_context_print:        load time =     644.20 ms
0.01.415.565 I llama_perf_context_print: prompt eval time =      46.75 ms /     7 tokens (    6.68 ms per token,   149.73 tokens per second)
0.01.415.565 I llama_perf_context_print:        eval time =     711.44 ms /    63 runs   (   11.29 ms per token,    88.55 tokens per second)
0.01.415.567 I llama_perf_context_print:       total time =     762.12 ms /    70 tokens
0.01.415.848 I ggml_metal_free: deallocating

real	0m1.433s
user	0m0.110s
sys	0m0.191s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.053 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.154 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.161 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.162 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.165 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.167 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.169 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.169 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.170 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.041 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.045 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.906 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.908 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.909 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.909 I llama_model_loader: - type  f32:  194 tensors
0.00.025.910 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.910 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.910 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.911 I print_info: file format = GGUF V3 (latest)
0.00.025.911 I print_info: file type   = Q4_K - Medium
0.00.025.912 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.112 I load: special tokens cache size = 25
0.00.040.191 I load: token to piece cache size = 0.2984 MB
0.00.040.193 I print_info: arch             = gptneox
0.00.040.194 I print_info: vocab_only       = 0
0.00.040.194 I print_info: n_ctx_train      = 2048
0.00.040.194 I print_info: n_embd           = 2048
0.00.040.194 I print_info: n_layer          = 24
0.00.040.197 I print_info: n_head           = 16
0.00.040.197 I print_info: n_head_kv        = 16
0.00.040.197 I print_info: n_rot            = 32
0.00.040.198 I print_info: n_swa            = 0
0.00.040.200 I print_info: n_embd_head_k    = 128
0.00.040.200 I print_info: n_embd_head_v    = 128
0.00.040.201 I print_info: n_gqa            = 1
0.00.040.201 I print_info: n_embd_k_gqa     = 2048
0.00.040.202 I print_info: n_embd_v_gqa     = 2048
0.00.040.203 I print_info: f_norm_eps       = 1.0e-05
0.00.040.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.203 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.203 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.203 I print_info: f_logit_scale    = 0.0e+00
0.00.040.204 I print_info: n_ff             = 8192
0.00.040.204 I print_info: n_expert         = 0
0.00.040.204 I print_info: n_expert_used    = 0
0.00.040.205 I print_info: causal attn      = 1
0.00.040.205 I print_info: pooling type     = 0
0.00.040.205 I print_info: rope type        = 2
0.00.040.205 I print_info: rope scaling     = linear
0.00.040.206 I print_info: freq_base_train  = 10000.0
0.00.040.207 I print_info: freq_scale_train = 1
0.00.040.207 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.207 I print_info: rope_finetuned   = unknown
0.00.040.207 I print_info: ssm_d_conv       = 0
0.00.040.207 I print_info: ssm_d_inner      = 0
0.00.040.208 I print_info: ssm_d_state      = 0
0.00.040.208 I print_info: ssm_dt_rank      = 0
0.00.040.208 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.208 I print_info: model type       = 1.4B
0.00.040.209 I print_info: model params     = 1.41 B
0.00.040.209 I print_info: general.name     = 1.4B
0.00.040.209 I print_info: vocab type       = BPE
0.00.040.209 I print_info: n_vocab          = 50304
0.00.040.211 I print_info: n_merges         = 50009
0.00.040.211 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.211 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.211 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.211 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: LF token         = 187 ''
0.00.040.212 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: max token length = 1024
0.00.040.213 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.511.874 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.880 I load_tensors: offloading output layer to GPU
0.00.511.881 I load_tensors: offloaded 25/25 layers to GPU
0.00.511.918 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.511.919 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.513.615 I llama_init_from_model: n_seq_max     = 1
0.00.513.617 I llama_init_from_model: n_ctx         = 128
0.00.513.617 I llama_init_from_model: n_ctx_per_seq = 128
0.00.513.618 I llama_init_from_model: n_batch       = 128
0.00.513.619 I llama_init_from_model: n_ubatch      = 128
0.00.513.619 I llama_init_from_model: flash_attn    = 0
0.00.513.622 I llama_init_from_model: freq_base     = 10000.0
0.00.513.623 I llama_init_from_model: freq_scale    = 1
0.00.513.623 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.513.625 I ggml_metal_init: allocating
0.00.513.695 I ggml_metal_init: found device: Apple M4
0.00.513.709 I ggml_metal_init: picking default device: Apple M4
0.00.515.506 I ggml_metal_init: using embedded metal library
0.00.522.286 I ggml_metal_init: GPU name:   Apple M4
0.00.522.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.522.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.522.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.522.295 I ggml_metal_init: simdgroup reduction   = true
0.00.522.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.522.296 I ggml_metal_init: has residency sets    = true
0.00.522.296 I ggml_metal_init: has bfloat            = true
0.00.522.296 I ggml_metal_init: use bfloat            = true
0.00.522.299 I ggml_metal_init: hasUnifiedMemory      = true
0.00.522.301 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.539.981 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.425 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.543.429 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.543.472 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.546.727 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.546.729 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.546.730 I llama_init_from_model: graph nodes  = 967
0.00.546.730 I llama_init_from_model: graph splits = 2
0.00.546.733 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.546.733 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.573.661 I 
0.00.573.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.573.752 I perplexity: tokenizing the input ..
0.00.580.321 I perplexity: tokenization took 6.567 ms
0.00.580.339 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.715.037 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.716.373 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.716.387 I llama_perf_context_print:        load time =     563.59 ms
0.00.716.388 I llama_perf_context_print: prompt eval time =     134.15 ms /   128 tokens (    1.05 ms per token,   954.14 tokens per second)
0.00.716.388 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.716.389 I llama_perf_context_print:       total time =     142.73 ms /   129 tokens
0.00.716.771 I ggml_metal_free: deallocating

real	0m0.731s
user	0m0.079s
sys	0m0.119s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.718 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.415 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.420 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.425 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.425 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.427 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.430 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.430 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.430 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.338 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.281 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.282 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.282 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.283 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.283 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.283 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.284 I llama_model_loader: - type  f32:  194 tensors
0.00.025.284 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.284 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.285 I print_info: file format = GGUF V3 (latest)
0.00.025.285 I print_info: file type   = Q5_K - Medium
0.00.025.286 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.109 I load: special tokens cache size = 25
0.00.039.001 I load: token to piece cache size = 0.2984 MB
0.00.039.004 I print_info: arch             = gptneox
0.00.039.004 I print_info: vocab_only       = 0
0.00.039.005 I print_info: n_ctx_train      = 2048
0.00.039.005 I print_info: n_embd           = 2048
0.00.039.005 I print_info: n_layer          = 24
0.00.039.008 I print_info: n_head           = 16
0.00.039.009 I print_info: n_head_kv        = 16
0.00.039.009 I print_info: n_rot            = 32
0.00.039.009 I print_info: n_swa            = 0
0.00.039.011 I print_info: n_embd_head_k    = 128
0.00.039.011 I print_info: n_embd_head_v    = 128
0.00.039.012 I print_info: n_gqa            = 1
0.00.039.013 I print_info: n_embd_k_gqa     = 2048
0.00.039.013 I print_info: n_embd_v_gqa     = 2048
0.00.039.014 I print_info: f_norm_eps       = 1.0e-05
0.00.039.014 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.014 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.015 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.015 I print_info: f_logit_scale    = 0.0e+00
0.00.039.015 I print_info: n_ff             = 8192
0.00.039.016 I print_info: n_expert         = 0
0.00.039.016 I print_info: n_expert_used    = 0
0.00.039.016 I print_info: causal attn      = 1
0.00.039.016 I print_info: pooling type     = 0
0.00.039.018 I print_info: rope type        = 2
0.00.039.019 I print_info: rope scaling     = linear
0.00.039.019 I print_info: freq_base_train  = 10000.0
0.00.039.020 I print_info: freq_scale_train = 1
0.00.039.020 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.020 I print_info: rope_finetuned   = unknown
0.00.039.021 I print_info: ssm_d_conv       = 0
0.00.039.021 I print_info: ssm_d_inner      = 0
0.00.039.022 I print_info: ssm_d_state      = 0
0.00.039.022 I print_info: ssm_dt_rank      = 0
0.00.039.022 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.022 I print_info: model type       = 1.4B
0.00.039.023 I print_info: model params     = 1.41 B
0.00.039.023 I print_info: general.name     = 1.4B
0.00.039.023 I print_info: vocab type       = BPE
0.00.039.024 I print_info: n_vocab          = 50304
0.00.039.024 I print_info: n_merges         = 50009
0.00.039.024 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.025 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.025 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: LF token         = 187 ''
0.00.039.026 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: max token length = 1024
0.00.039.027 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.598.245 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.262 I load_tensors: offloading output layer to GPU
0.00.598.263 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.297 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.598.298 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.873 I llama_init_from_model: n_seq_max     = 1
0.00.599.876 I llama_init_from_model: n_ctx         = 2048
0.00.599.877 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.599.877 I llama_init_from_model: n_batch       = 2048
0.00.599.878 I llama_init_from_model: n_ubatch      = 512
0.00.599.878 I llama_init_from_model: flash_attn    = 0
0.00.599.881 I llama_init_from_model: freq_base     = 10000.0
0.00.599.881 I llama_init_from_model: freq_scale    = 1
0.00.599.883 I ggml_metal_init: allocating
0.00.599.957 I ggml_metal_init: found device: Apple M4
0.00.599.970 I ggml_metal_init: picking default device: Apple M4
0.00.601.589 I ggml_metal_init: using embedded metal library
0.00.608.181 I ggml_metal_init: GPU name:   Apple M4
0.00.608.185 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.186 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.187 I ggml_metal_init: simdgroup reduction   = true
0.00.608.188 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.188 I ggml_metal_init: has residency sets    = true
0.00.608.188 I ggml_metal_init: has bfloat            = true
0.00.608.188 I ggml_metal_init: use bfloat            = true
0.00.608.189 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.191 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.069 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.777 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.783 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.821 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.400 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.402 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.402 I llama_init_from_model: graph nodes  = 967
0.00.685.402 I llama_init_from_model: graph splits = 2
0.00.685.412 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.539 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.540 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.579 I main: llama threadpool init, n_threads = 4
0.00.751.624 I 
0.00.751.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.639 I 
0.00.751.803 I sampler seed: 1234
0.00.751.808 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.830 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.833 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.833 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.601.851 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.01.601.852 I llama_perf_context_print:        load time =     742.15 ms
0.01.601.852 I llama_perf_context_print: prompt eval time =      51.50 ms /     7 tokens (    7.36 ms per token,   135.93 tokens per second)
0.01.601.853 I llama_perf_context_print:        eval time =     795.62 ms /    63 runs   (   12.63 ms per token,    79.18 tokens per second)
0.01.601.854 I llama_perf_context_print:       total time =     850.98 ms /    70 tokens
0.01.602.100 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.109s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.632 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.763 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.769 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.773 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.774 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.775 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.775 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.776 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.776 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.778 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.779 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.615 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.638 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.393 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.394 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.395 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.396 I llama_model_loader: - type  f32:  194 tensors
0.00.024.396 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.397 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.397 I print_info: file format = GGUF V3 (latest)
0.00.024.398 I print_info: file type   = Q5_K - Medium
0.00.024.399 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.419 I load: special tokens cache size = 25
0.00.038.887 I load: token to piece cache size = 0.2984 MB
0.00.038.892 I print_info: arch             = gptneox
0.00.038.892 I print_info: vocab_only       = 0
0.00.038.892 I print_info: n_ctx_train      = 2048
0.00.038.892 I print_info: n_embd           = 2048
0.00.038.892 I print_info: n_layer          = 24
0.00.038.897 I print_info: n_head           = 16
0.00.038.898 I print_info: n_head_kv        = 16
0.00.038.898 I print_info: n_rot            = 32
0.00.038.898 I print_info: n_swa            = 0
0.00.038.899 I print_info: n_embd_head_k    = 128
0.00.038.899 I print_info: n_embd_head_v    = 128
0.00.038.899 I print_info: n_gqa            = 1
0.00.038.900 I print_info: n_embd_k_gqa     = 2048
0.00.038.901 I print_info: n_embd_v_gqa     = 2048
0.00.038.901 I print_info: f_norm_eps       = 1.0e-05
0.00.038.902 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.902 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.905 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.905 I print_info: f_logit_scale    = 0.0e+00
0.00.038.905 I print_info: n_ff             = 8192
0.00.038.906 I print_info: n_expert         = 0
0.00.038.906 I print_info: n_expert_used    = 0
0.00.038.906 I print_info: causal attn      = 1
0.00.038.906 I print_info: pooling type     = 0
0.00.038.907 I print_info: rope type        = 2
0.00.038.907 I print_info: rope scaling     = linear
0.00.038.907 I print_info: freq_base_train  = 10000.0
0.00.038.908 I print_info: freq_scale_train = 1
0.00.038.908 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.908 I print_info: rope_finetuned   = unknown
0.00.038.908 I print_info: ssm_d_conv       = 0
0.00.038.908 I print_info: ssm_d_inner      = 0
0.00.038.910 I print_info: ssm_d_state      = 0
0.00.038.910 I print_info: ssm_dt_rank      = 0
0.00.038.910 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.911 I print_info: model type       = 1.4B
0.00.038.911 I print_info: model params     = 1.41 B
0.00.038.911 I print_info: general.name     = 1.4B
0.00.038.912 I print_info: vocab type       = BPE
0.00.038.912 I print_info: n_vocab          = 50304
0.00.038.912 I print_info: n_merges         = 50009
0.00.038.912 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.912 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.912 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.913 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.913 I print_info: LF token         = 187 ''
0.00.038.913 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.913 I print_info: max token length = 1024
0.00.038.914 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.627.512 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.524 I load_tensors: offloading output layer to GPU
0.00.627.524 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.558 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.627.562 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.629.253 I llama_init_from_model: n_seq_max     = 1
0.00.629.256 I llama_init_from_model: n_ctx         = 128
0.00.629.257 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.257 I llama_init_from_model: n_batch       = 128
0.00.629.258 I llama_init_from_model: n_ubatch      = 128
0.00.629.258 I llama_init_from_model: flash_attn    = 0
0.00.629.261 I llama_init_from_model: freq_base     = 10000.0
0.00.629.261 I llama_init_from_model: freq_scale    = 1
0.00.629.262 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.264 I ggml_metal_init: allocating
0.00.629.330 I ggml_metal_init: found device: Apple M4
0.00.629.343 I ggml_metal_init: picking default device: Apple M4
0.00.631.038 I ggml_metal_init: using embedded metal library
0.00.637.534 I ggml_metal_init: GPU name:   Apple M4
0.00.637.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.541 I ggml_metal_init: simdgroup reduction   = true
0.00.637.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.541 I ggml_metal_init: has residency sets    = true
0.00.637.541 I ggml_metal_init: has bfloat            = true
0.00.637.542 I ggml_metal_init: use bfloat            = true
0.00.637.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.321 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.658.851 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.658.859 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.658.910 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.662.179 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.662.181 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.662.181 I llama_init_from_model: graph nodes  = 967
0.00.662.182 I llama_init_from_model: graph splits = 2
0.00.662.184 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.662.185 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.246 I 
0.00.696.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.331 I perplexity: tokenizing the input ..
0.00.703.363 I perplexity: tokenization took 7.028 ms
0.00.703.385 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.288 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.846.623 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.846.641 I llama_perf_context_print:        load time =     687.60 ms
0.00.846.642 I llama_perf_context_print: prompt eval time =     140.96 ms /   128 tokens (    1.10 ms per token,   908.05 tokens per second)
0.00.846.643 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.643 I llama_perf_context_print:       total time =     150.40 ms /   129 tokens
0.00.847.059 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.080s
sys	0m0.163s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.864 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.652 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.662 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.664 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.664 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.665 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.665 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.665 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.668 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.668 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.668 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.669 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.669 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.670 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.671 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.671 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.565 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.336 I llama_model_loader: - type  f32:  194 tensors
0.00.026.336 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.337 I print_info: file format = GGUF V3 (latest)
0.00.026.338 I print_info: file type   = Q6_K
0.00.026.338 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.580 I load: special tokens cache size = 25
0.00.040.589 I load: token to piece cache size = 0.2984 MB
0.00.040.591 I print_info: arch             = gptneox
0.00.040.592 I print_info: vocab_only       = 0
0.00.040.592 I print_info: n_ctx_train      = 2048
0.00.040.592 I print_info: n_embd           = 2048
0.00.040.592 I print_info: n_layer          = 24
0.00.040.595 I print_info: n_head           = 16
0.00.040.596 I print_info: n_head_kv        = 16
0.00.040.596 I print_info: n_rot            = 32
0.00.040.596 I print_info: n_swa            = 0
0.00.040.596 I print_info: n_embd_head_k    = 128
0.00.040.596 I print_info: n_embd_head_v    = 128
0.00.040.597 I print_info: n_gqa            = 1
0.00.040.598 I print_info: n_embd_k_gqa     = 2048
0.00.040.600 I print_info: n_embd_v_gqa     = 2048
0.00.040.601 I print_info: f_norm_eps       = 1.0e-05
0.00.040.601 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.601 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.601 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.601 I print_info: f_logit_scale    = 0.0e+00
0.00.040.602 I print_info: n_ff             = 8192
0.00.040.602 I print_info: n_expert         = 0
0.00.040.602 I print_info: n_expert_used    = 0
0.00.040.602 I print_info: causal attn      = 1
0.00.040.608 I print_info: pooling type     = 0
0.00.040.608 I print_info: rope type        = 2
0.00.040.608 I print_info: rope scaling     = linear
0.00.040.609 I print_info: freq_base_train  = 10000.0
0.00.040.609 I print_info: freq_scale_train = 1
0.00.040.609 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.609 I print_info: rope_finetuned   = unknown
0.00.040.610 I print_info: ssm_d_conv       = 0
0.00.040.610 I print_info: ssm_d_inner      = 0
0.00.040.610 I print_info: ssm_d_state      = 0
0.00.040.610 I print_info: ssm_dt_rank      = 0
0.00.040.610 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.610 I print_info: model type       = 1.4B
0.00.040.611 I print_info: model params     = 1.41 B
0.00.040.611 I print_info: general.name     = 1.4B
0.00.040.611 I print_info: vocab type       = BPE
0.00.040.612 I print_info: n_vocab          = 50304
0.00.040.612 I print_info: n_merges         = 50009
0.00.040.612 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.612 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.612 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.612 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.613 I print_info: LF token         = 187 ''
0.00.040.613 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.613 I print_info: max token length = 1024
0.00.040.613 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.835 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.840 I load_tensors: offloading output layer to GPU
0.00.638.841 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.866 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.638.871 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.640.176 I llama_init_from_model: n_seq_max     = 1
0.00.640.178 I llama_init_from_model: n_ctx         = 2048
0.00.640.179 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.640.179 I llama_init_from_model: n_batch       = 2048
0.00.640.179 I llama_init_from_model: n_ubatch      = 512
0.00.640.180 I llama_init_from_model: flash_attn    = 0
0.00.640.181 I llama_init_from_model: freq_base     = 10000.0
0.00.640.181 I llama_init_from_model: freq_scale    = 1
0.00.640.182 I ggml_metal_init: allocating
0.00.640.193 I ggml_metal_init: found device: Apple M4
0.00.640.200 I ggml_metal_init: picking default device: Apple M4
0.00.641.731 I ggml_metal_init: using embedded metal library
0.00.647.867 I ggml_metal_init: GPU name:   Apple M4
0.00.647.871 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.871 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.872 I ggml_metal_init: simdgroup reduction   = true
0.00.647.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.873 I ggml_metal_init: has residency sets    = true
0.00.647.873 I ggml_metal_init: has bfloat            = true
0.00.647.873 I ggml_metal_init: use bfloat            = true
0.00.647.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.423 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.303 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.721.309 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.721.344 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.726.472 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.726.473 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.726.474 I llama_init_from_model: graph nodes  = 967
0.00.726.474 I llama_init_from_model: graph splits = 2
0.00.726.480 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.726.609 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.610 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.753 I main: llama threadpool init, n_threads = 4
0.00.789.799 I 
0.00.789.815 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.815 I 
0.00.789.969 I sampler seed: 1234
0.00.789.974 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.995 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.995 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.995 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.668.783 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51226.55 tokens per second)
0.01.668.784 I llama_perf_context_print:        load time =     779.18 ms
0.01.668.785 I llama_perf_context_print: prompt eval time =      54.11 ms /     7 tokens (    7.73 ms per token,   129.36 tokens per second)
0.01.668.785 I llama_perf_context_print:        eval time =     821.71 ms /    63 runs   (   13.04 ms per token,    76.67 tokens per second)
0.01.668.786 I llama_perf_context_print:       total time =     879.73 ms /    70 tokens
0.01.669.048 I ggml_metal_free: deallocating

real	0m1.687s
user	0m0.109s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4705 (27e8a233) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.890 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.802 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.807 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.813 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.813 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.814 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.814 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.814 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.816 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.817 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.817 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.819 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.820 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.820 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.487 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.489 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.489 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.490 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.490 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.491 I llama_model_loader: - type  f32:  194 tensors
0.00.025.491 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.492 I print_info: file format = GGUF V3 (latest)
0.00.025.492 I print_info: file type   = Q6_K
0.00.025.493 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.350 I load: special tokens cache size = 25
0.00.039.505 I load: token to piece cache size = 0.2984 MB
0.00.039.509 I print_info: arch             = gptneox
0.00.039.509 I print_info: vocab_only       = 0
0.00.039.509 I print_info: n_ctx_train      = 2048
0.00.039.509 I print_info: n_embd           = 2048
0.00.039.510 I print_info: n_layer          = 24
0.00.039.513 I print_info: n_head           = 16
0.00.039.514 I print_info: n_head_kv        = 16
0.00.039.516 I print_info: n_rot            = 32
0.00.039.516 I print_info: n_swa            = 0
0.00.039.516 I print_info: n_embd_head_k    = 128
0.00.039.516 I print_info: n_embd_head_v    = 128
0.00.039.517 I print_info: n_gqa            = 1
0.00.039.518 I print_info: n_embd_k_gqa     = 2048
0.00.039.518 I print_info: n_embd_v_gqa     = 2048
0.00.039.519 I print_info: f_norm_eps       = 1.0e-05
0.00.039.521 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.521 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.521 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.521 I print_info: f_logit_scale    = 0.0e+00
0.00.039.522 I print_info: n_ff             = 8192
0.00.039.522 I print_info: n_expert         = 0
0.00.039.522 I print_info: n_expert_used    = 0
0.00.039.523 I print_info: causal attn      = 1
0.00.039.523 I print_info: pooling type     = 0
0.00.039.523 I print_info: rope type        = 2
0.00.039.523 I print_info: rope scaling     = linear
0.00.039.523 I print_info: freq_base_train  = 10000.0
0.00.039.524 I print_info: freq_scale_train = 1
0.00.039.524 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.524 I print_info: rope_finetuned   = unknown
0.00.039.524 I print_info: ssm_d_conv       = 0
0.00.039.524 I print_info: ssm_d_inner      = 0
0.00.039.525 I print_info: ssm_d_state      = 0
0.00.039.525 I print_info: ssm_dt_rank      = 0
0.00.039.525 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.525 I print_info: model type       = 1.4B
0.00.039.529 I print_info: model params     = 1.41 B
0.00.039.529 I print_info: general.name     = 1.4B
0.00.039.530 I print_info: vocab type       = BPE
0.00.039.530 I print_info: n_vocab          = 50304
0.00.039.531 I print_info: n_merges         = 50009
0.00.039.532 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.532 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.532 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.532 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.532 I print_info: LF token         = 187 ''
0.00.039.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.533 I print_info: max token length = 1024
0.00.039.533 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.307.590 I load_tensors: offloading 24 repeating layers to GPU
0.00.307.596 I load_tensors: offloading output layer to GPU
0.00.307.597 I load_tensors: offloaded 25/25 layers to GPU
0.00.307.623 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.307.624 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.309.069 I llama_init_from_model: n_seq_max     = 1
0.00.309.071 I llama_init_from_model: n_ctx         = 128
0.00.309.071 I llama_init_from_model: n_ctx_per_seq = 128
0.00.309.072 I llama_init_from_model: n_batch       = 128
0.00.309.072 I llama_init_from_model: n_ubatch      = 128
0.00.309.072 I llama_init_from_model: flash_attn    = 0
0.00.309.073 I llama_init_from_model: freq_base     = 10000.0
0.00.309.074 I llama_init_from_model: freq_scale    = 1
0.00.309.074 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.309.076 I ggml_metal_init: allocating
0.00.309.115 I ggml_metal_init: found device: Apple M4
0.00.309.126 I ggml_metal_init: picking default device: Apple M4
0.00.310.404 I ggml_metal_init: using embedded metal library
0.00.316.344 I ggml_metal_init: GPU name:   Apple M4
0.00.316.348 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.316.349 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.316.350 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.316.351 I ggml_metal_init: simdgroup reduction   = true
0.00.316.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.316.351 I ggml_metal_init: has residency sets    = true
0.00.316.352 I ggml_metal_init: has bfloat            = true
0.00.316.352 I ggml_metal_init: use bfloat            = true
0.00.316.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.316.354 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.332.349 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.335.840 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.335.846 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.335.893 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.339.141 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.339.142 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.339.143 I llama_init_from_model: graph nodes  = 967
0.00.339.143 I llama_init_from_model: graph splits = 2
0.00.339.147 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.339.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.369.466 I 
0.00.369.542 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.369.571 I perplexity: tokenizing the input ..
0.00.377.066 I perplexity: tokenization took 7.491 ms
0.00.377.118 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.518.489 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.519.821 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.519.838 I llama_perf_context_print:        load time =     359.56 ms
0.00.519.839 I llama_perf_context_print: prompt eval time =     140.41 ms /   128 tokens (    1.10 ms per token,   911.64 tokens per second)
0.00.519.840 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.519.840 I llama_perf_context_print:       total time =     150.38 ms /   129 tokens
0.00.520.228 I ggml_metal_free: deallocating

real	0m0.535s
user	0m0.078s
sys	0m0.098s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4705 (27e8a233)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x114607ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1146085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x114608ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x114609150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x114609700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x114609cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11460a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11460a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11460adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11460b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11460b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11460bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11460c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11460cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11460d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11460dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11460e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11460ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11460f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11460fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x114610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x114610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x114611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1146119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x114612110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1146123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1146129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x114613650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x114613b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x114613e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1146142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1146145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x114614e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x114615380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x114615640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x114615ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x114615f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x114616420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1146168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x114616d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x114617200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1146176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x114617b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x114617fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1146182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1146188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x114618ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1146197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x114619df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11461a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11461aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11461b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11461b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11461bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11461c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11461c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11461cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11461d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11461d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11461de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11461e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11461e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11461ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11461eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11461f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11461f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11461fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x114620150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1146205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x114620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x114620f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1146213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x114621870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x114621dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x114622310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x114622860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x114622db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x114623300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x114623850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x114623da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1146242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x114624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x114624d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1146252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x114625830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x114625d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1146262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x114626820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x114626d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1146272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x114627810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114627d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1146282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x114628800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114628d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1146292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1146297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1146194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x114629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11462a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11462a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11462aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11462b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11462b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11462bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11462c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11462c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11462ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11462d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11462d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11462de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11462e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11462e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11462edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11462f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11462f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11462fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x114630040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1146304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x114630980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x114630e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1146312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x114631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x114631c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1146320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x114632540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1146329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x114632e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x114633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1146337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x114633c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x114634100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1146345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x114634a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x114634ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x114635380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x114635820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x114635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x114636160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x114636600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x114636aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x114636f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1146373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x114637880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x114637d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1146381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x114638660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x114638b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x114638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x114639440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1146398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x114639d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11463a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11463a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11463ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11463b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11463b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11463b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11463bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11463c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11463c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11463cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11463d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11463d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11463d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11463de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11463e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11463e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11463ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11463f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11463f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11463fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11463fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x114640340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1146407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x114640c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x114641120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1146415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x114641a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x114641f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1146423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x114642840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x114642ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x114643180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x114643620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x114643ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x114643f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x114644400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1146448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x114644d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1146451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x114645680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x114645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x114646070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1146465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x114646b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x114647060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x114647320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x114647930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x114647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x114648550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x114648d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1146491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1146494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x114649ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11464a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11464a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11464ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11464b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11464b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11464be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11464c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11464c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11464ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11464d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11464d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11464de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11464e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11464e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11464ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11464f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11464f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11464fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x114650350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1146508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x114650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x114651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x114651890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x114651de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x114652330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x114652880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x114652dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x114653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x114653870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x114653dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x114654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x114654860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x114654db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x114655300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114655850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114655da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1146562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x114656840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x114656d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1146572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x114657830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x114657d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1146582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x114658820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x114658d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1146592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x114659810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x114659d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11465a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11465a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11465ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11465b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11465b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11465bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11465c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11465c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11465cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11465d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11465d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11465dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11465e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11465e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11465ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11465f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11465f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11465fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11465fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114660380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x114660820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114660cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114661160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x114661600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114661aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x114661f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1146623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x114662880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x114662d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114663270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x114663990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1146640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1146647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x114664ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1146651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1146659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114665c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114666270 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.714.898 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x114665f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x114649760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1146475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x114648200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11461b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11461acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11461d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x114649d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x114612690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x114619180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x114619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11461a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x114618560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11461a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x114611690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11461d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x114629f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x114665470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x114614870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x114614b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11464a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x114648810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x114612ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x114612f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x114613220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1146666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x114666990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x114666c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x114666f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1146671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x114667490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x114667750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x114667a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x114667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x114667f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x114668250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x114668510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1146687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x114668a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x114668d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x114669010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1146692d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x114669590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x114669850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x114669b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x114669dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11466a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11466a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11466a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11466a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11466ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11466ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11466b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11466b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11466b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11466b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11466bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11466bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11466c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11466c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11466c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11466c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11466cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11466cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11466d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11466d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11466d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11466da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11466dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11466dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11466e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11466e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11466e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11466ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11466ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11466f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11466f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11466f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11466f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11466fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11466fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1146700d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x114670390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x114670650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x114670910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x114670bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x114670e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x114671150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x114671410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1146716d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x114671990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114671c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x114671f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1146721d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114672490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x114672750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x114672a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x114672cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x114672f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x114673250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x114673510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1146737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x114673a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x114673d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x114674010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1146742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x114674590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x114674850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x114674b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x114674dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x114675090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x114675350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x114675610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1146758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x114675b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x114675e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x114676110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1146763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x114676690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x114676950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x114676c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x114676ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x114677190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x114677450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x114677710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1146779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x114677c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x114677f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x114678210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1146784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x114678790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x114678a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x114678d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x114678fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x114679290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x114679550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x114679810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x114679ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x114679d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11467a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11467a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11467a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11467a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11467ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11467ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11467b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11467b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11467b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11467b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11467bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11467be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11467c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11467c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11467c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11467c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11467cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11467cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11467d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11467d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11467d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11467da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11467dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11467df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11467e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11467e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11467e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11467ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11467ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11467f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11467f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11467f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11467f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11467fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11467fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x114680090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x114680350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x114680610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1146808d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x114680b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x114680e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x114681110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1146813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x114681690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x114681950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x114681c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x114681ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x114682190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x114682450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x114682710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1146829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x114682c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x114682f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x114683210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1146834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x114683790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x114683a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x114683d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x114683fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x114684290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x114684550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x114684810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x114684ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x114684d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x114685050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x114685310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x114685850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x114685b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x114685fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x114686450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1146868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1146870a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x114687360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x114687620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x114687a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114687f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114688370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1146887e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114688c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1146890c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x114689530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1146899a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x114689e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11468a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11468a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11468ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11468afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11468b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11468b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11468bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11468c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11468c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11468ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11468cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11468d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11468d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11468dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11468e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11468e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11468e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11468edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11468f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11468f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11468fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11468ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x114690420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x114690890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x114690d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x114691170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1146915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x114691a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x114691ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x114692330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1146927a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x114692c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x114693080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1146934f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x114693960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x114693dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x114694240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1146946b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x114694b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x114694f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x114695400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114695870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x114695ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x114696150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1146965c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x114696a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x114696ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x114697310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x114697780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x114697bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114698060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1146984d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114698940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114698db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x114699220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114699690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x114699b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x114699f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11469a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11469a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11469acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11469b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11469be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11469c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11469cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11469cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11469d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11469da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11469e010 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1251044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125104950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125104dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125105230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1251056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125105b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125105f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1251063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125106860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125106d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1251071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125107860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125108380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125108b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125109340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125109a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12510a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12510a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12510afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12510b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12510beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12510c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12510ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12510d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12510db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12510ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12510e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12510e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12510e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12510ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12510f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12510f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12510fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12510fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125110340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1251107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125110c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125111090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125111500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125111970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125111de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125112250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1251126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125112b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125112fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125113410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125113880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125113cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125114160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1251145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125114a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125114eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125115320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125115790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125115c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125116070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1251165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125116ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125116f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1251173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125117830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125117ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125118110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125118580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1251189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125118e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1251192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125119740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125119bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12511a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12511a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12511a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12511ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12511b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12511b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12511bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12511bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12511c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12511c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12511cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12511d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12511d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12511d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12511de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12511e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12511e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12511eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12511f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12511f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12511f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12511fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1251201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125120630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125120aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125120f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125121380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1251217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125121c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1251220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125122540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1251229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125122e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125123290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125123b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125123de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125124250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1251246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125124b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125124fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125125410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125125880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125125cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125126160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1251265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125126a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125126eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125127320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125127790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125127c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125128070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1251284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125128950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125128dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125129230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1251296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125129b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125129f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12512a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12512a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12512acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12512b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12512b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12512ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12512be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12512c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12512c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12512cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12512d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12512d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12512d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12512dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12512e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12512e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12512eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12512ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12512f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12512f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12512fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125130120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125130590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125130a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125130e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1251312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125131750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125131bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125132030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1251324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125132910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125132d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1251331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125133660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125133ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125133f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1251343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125134820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125134c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125135100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125135570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1251359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125135e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1251362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125136730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125136ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125137010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125137480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1251378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125137d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1251381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125138640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125138ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125138f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125139390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125139800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125139c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12513a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12513a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12513a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12513ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12513b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12513b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12513bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12513bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12513c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12513c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12513cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12513d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12513d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12513da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12513df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12513e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12513e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12513ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12513f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12513f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12513f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12513fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125140280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1251406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125140b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125140fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125141b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125141e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1251420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125142540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1251429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125142e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125143290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125143700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125143b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125143fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125144450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1251448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125144d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1251451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125145610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125145a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125145ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125146360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1251467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125146c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1251470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125147520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125147990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125147e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125148270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1251486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125148b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125148fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125149430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1251498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125149d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12514a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12514a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12514aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12514aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12514b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12514b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12514bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12514c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12514c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12514c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12514cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12514d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12514d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12514db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12514dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12514e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12514e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12514ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12514f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12514f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12514fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12514feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125150320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125150790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125150c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125151070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1251514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125151950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125151dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125152230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1251526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125152b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125152f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1251533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125153860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125153cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125154140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1251545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125154a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125154e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125155300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125155770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1251561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125156900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125157020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125157740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125157a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125157e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125158470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125158a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.755s
user	0m0.279s
sys	0m0.291s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4705 (27e8a233)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12160e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12160edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12160f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12160f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12160fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121610470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121610a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121610fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121611580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121611a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121611f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121612480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121612fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121613750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121613f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121614680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121614da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1216154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121615be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1216163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121616ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1216171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121617910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1216181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1216188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121618b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1216191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121619e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12161a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12161a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12161aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12161ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12161b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12161bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12161be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12161c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12161c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12161cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12161d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12161d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12161d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12161de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12161e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12161e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12161ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12161f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12161f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12161ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1216205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121620bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1216211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1216217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121621df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121622400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121622bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121623090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121623530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1216237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121623e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1216245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1216248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121624d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1216251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121625690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121625b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121625fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121626470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121626910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121626db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121627250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1216276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121627b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121628030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121628580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121628ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121629020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121629570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121629ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12162a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12162a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12162aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12162b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12162b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12162baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12162bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12162c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12162ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12162cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12162d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12162da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12162dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12162e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12162ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12162efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12162f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12162fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12162ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12161fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121630420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121630bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121631120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121631670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121631bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121632110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121632660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121632bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121633100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121633650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121633ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1216340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121634640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121634b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1216350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121635580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121635a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121635ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121636360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121636800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121636ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1216375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121637a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121637f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1216383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121638860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121638d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1216391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121639640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121639ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121639f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12163a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12163a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12163ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12163b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12163b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12163bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12163bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12163c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12163c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12163cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12163d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12163d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12163dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12163e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12163e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12163e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12163ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12163f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12163f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12163fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1216400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121640540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1216409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121640e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121641320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1216417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121641c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1216425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121642a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121642ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121643380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121643820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121643cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121644160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121644600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121644aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121644f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1216453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121645880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121645d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1216461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121646660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121646b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121646fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121647440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1216478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121647d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121648220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1216486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121648b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121649000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1216494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121649940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121649de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12164a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12164a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12164abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12164b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12164b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12164b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12164be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12164c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12164c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12164cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12164d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12164d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12164dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12164e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12164e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12164ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12164f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12164f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12164fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121650880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121651070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121651510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1216519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121651e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121652600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121652b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1216530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1216535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121653b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121654090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1216545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121654b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121655080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1216555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121655b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121656070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1216565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121656b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121657060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1216575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121657b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121658050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1216585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121658af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121659040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121659590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121659ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12165a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12165a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12165aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12165b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12165b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12165bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12165c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12165c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12165cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12165d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12165d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12165daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12165dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12165e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12165ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12165efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12165f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12165fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12165ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121660520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121660a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121660fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121661510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121661a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121661fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121662500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121662a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121662fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1216634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121663a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121663f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1216644e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121664a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121664f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121665420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1216658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121665d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121666200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1216666a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121666b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121666fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121667480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121667920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121667dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121668260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121668700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121668ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121669040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1216694e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121669a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12166a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12166a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12166af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12166b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12166b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12166c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12166c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12166ca30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.102.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1217079d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121707e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1217082b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121708720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121708b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121709000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121709470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1217098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121709d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12170a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12170a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12170acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12170b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12170bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12170c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12170cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12170d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12170dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12170e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12170ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12170f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12170fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121710180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1217108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121710fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121711280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1217119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121712c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1217130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121713360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1217137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121713c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1217140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121714520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121714990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121714e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121715270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1217156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121715b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121715fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121716430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1217168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121716d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121717180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1217175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121717a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121717ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121718340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1217187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121718c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121719090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121719500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121719a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121719f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12171a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12171a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12171acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12171b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12171b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12171ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12171be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12171c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12171c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12171cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12171d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12171d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12171d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12171dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12171e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12171e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12171eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12171ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12171f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12171f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12171fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121720110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121720580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1217209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121720e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1217212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121721740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121721bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121722020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121722490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121722900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121722d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1217231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121723650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121723ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121723f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1217243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121724810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121724c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1217250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121725560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1217259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121725e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1217262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121726720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121726b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121727000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121727470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1217278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121727d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1217281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121728630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121728aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121728f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121729380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1217297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121729c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12172a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12172a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12172a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12172ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12172b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12172b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12172bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12172bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12172c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12172c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12172cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12172d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12172d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12172da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12172def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12172e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12172e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12172ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12172f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12172f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12172f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12172fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121730270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1217306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121730b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121730fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121731430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1217318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121731d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121732180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1217325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121732a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121732ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121733340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1217337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121733c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121734090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121734500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121734970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121734de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121735250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1217356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121735b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121735fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121736410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121736880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121736cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121737160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1217375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121737a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121737eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121738ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121738da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121739060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1217394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121739940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121739db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12173a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12173a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12173ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12173af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12173b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12173b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12173bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12173c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12173c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12173ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12173ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12173d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12173d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12173dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12173e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12173e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12173e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12173ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12173f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12173f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12173fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12173ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1217403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121740830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121740ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121741110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121741580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1217419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121741e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1217422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121742830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121742d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1217431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121743620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121743a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121743f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121744420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121744930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1217454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121745760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121745d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1217462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1217468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121746e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121747420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1217479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121747fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121748560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121748b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1217490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1217496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12174a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12174a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12174ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12174b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12174b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12174bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12174c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12174ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12174d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12174d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12174dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12174e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12174e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12174ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12174f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12174f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12174fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1217503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1217509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121750f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121751ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1217520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121752660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121752c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1217531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1217537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121753d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121754320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1217548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121754ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121755460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121755a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121755fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1217565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121756b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121757120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1217576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121757ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121758260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121758820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121758de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1217593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121759960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121759e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12175a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12175a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12175ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12175b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12175b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12175bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12175c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12175c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12175cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12175d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12175d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12175da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12175df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12175e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12175ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12175f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12175fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1217603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121760690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121760e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121761140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121761750 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1228044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1228056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1228063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1228092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12280a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12280a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12280af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12280b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12280be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12280c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12280cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12280d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12280dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12280dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12280e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12280e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12280e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12280edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12280f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12280f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12280fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12280fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1228102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1228114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1228133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1228149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1228152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1228177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1228180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1228189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1228196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12281a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12281a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12281ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12281b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12281b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12281ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12281bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12281c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12281c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12281cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12281d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12281d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12281d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12281ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12281e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12281e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12281eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12281efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12281f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12281f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12281fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1228205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1228217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1228224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1228253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1228269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1228272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1228291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12282a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12282a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12282ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12282b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12282b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12282b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12282be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12282c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12282c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12282cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12282d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12282d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12282d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12282dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12282e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12282e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12282eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12282ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12282f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12282f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12282fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1228300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1228309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1228328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1228331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1228347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1228350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1228366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1228378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1228385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1228397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12283a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12283a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12283a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12283ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12283b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12283b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12283bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12283bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12283c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12283c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12283ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12283d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12283d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12283da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12283deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12283e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12283e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12283ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12283f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12283f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12283f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12283fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1228406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1228424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1228436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1228455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x122846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1228474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x122848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1228493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12284a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12284a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12284aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12284ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12284b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12284b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12284bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12284c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12284c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12284c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12284cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12284d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12284d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12284dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12284df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12284e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12284e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12284eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12284f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12284f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12284f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12284fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1228502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1228521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1228533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1228540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1228549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1228552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1228568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1228576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1228579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.967s
user	0m0.233s
sys	0m0.158s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.77 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.21 sec*proc (2 tests)

Total Test time (real) =   2.22 sec
        2.24 real         0.51 user         0.25 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.13 user         0.08 sys
```
