### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.60 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.69 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.46 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.19 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.20 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.21 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.24 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.30 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.24 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.47 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  182.73 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.93 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.58 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.36 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 226.96 sec*proc (28 tests)

Total Test time (real) = 226.97 sec

real	3m47.003s
user	7m49.243s
sys	0m6.747s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.30 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.95 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.22 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.20 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.25 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.34 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.69 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.39 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.21 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.23 sec*proc (28 tests)

Total Test time (real) =  52.25 sec

real	0m52.256s
user	1m12.784s
sys	0m5.659s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.117 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.122 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.974 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.984 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.027.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.985 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.027.986 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.027.987 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.988 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.992 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.993 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.994 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.994 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.997 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.998 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.998 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.999 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.999 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.028.000 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.028.001 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.033.153 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.034.535 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.538 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.034.538 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.034.539 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.034.539 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.034.540 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.034.540 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.034.541 I llama_model_loader: - type  f32:  124 tensors
0.00.034.542 I llama_model_loader: - type  f16:   73 tensors
0.00.039.462 I llm_load_vocab: special tokens cache size = 5
0.00.041.876 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.041.880 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.041.881 I llm_load_print_meta: arch             = bert
0.00.041.881 I llm_load_print_meta: vocab type       = WPM
0.00.041.881 I llm_load_print_meta: n_vocab          = 30522
0.00.041.882 I llm_load_print_meta: n_merges         = 0
0.00.041.882 I llm_load_print_meta: vocab_only       = 0
0.00.041.882 I llm_load_print_meta: n_ctx_train      = 512
0.00.041.882 I llm_load_print_meta: n_embd           = 384
0.00.041.882 I llm_load_print_meta: n_layer          = 12
0.00.041.898 I llm_load_print_meta: n_head           = 12
0.00.041.898 I llm_load_print_meta: n_head_kv        = 12
0.00.041.899 I llm_load_print_meta: n_rot            = 32
0.00.041.899 I llm_load_print_meta: n_swa            = 0
0.00.041.899 I llm_load_print_meta: n_embd_head_k    = 32
0.00.041.899 I llm_load_print_meta: n_embd_head_v    = 32
0.00.041.900 I llm_load_print_meta: n_gqa            = 1
0.00.041.901 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.041.902 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.041.903 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.041.903 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.041.903 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.041.904 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.041.904 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.041.906 I llm_load_print_meta: n_ff             = 1536
0.00.041.906 I llm_load_print_meta: n_expert         = 0
0.00.041.907 I llm_load_print_meta: n_expert_used    = 0
0.00.041.907 I llm_load_print_meta: causal attn      = 0
0.00.041.907 I llm_load_print_meta: pooling type     = 2
0.00.041.907 I llm_load_print_meta: rope type        = 2
0.00.041.907 I llm_load_print_meta: rope scaling     = linear
0.00.041.909 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.041.909 I llm_load_print_meta: freq_scale_train = 1
0.00.041.910 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.041.910 I llm_load_print_meta: rope_finetuned   = unknown
0.00.041.910 I llm_load_print_meta: ssm_d_conv       = 0
0.00.041.910 I llm_load_print_meta: ssm_d_inner      = 0
0.00.041.912 I llm_load_print_meta: ssm_d_state      = 0
0.00.041.913 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.041.913 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.041.913 I llm_load_print_meta: model type       = 33M
0.00.041.914 I llm_load_print_meta: model ftype      = F16
0.00.041.914 I llm_load_print_meta: model params     = 33.21 M
0.00.041.916 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.041.917 I llm_load_print_meta: general.name     = Bge Small
0.00.041.917 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.041.919 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.041.919 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.041.920 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.041.920 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.041.920 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.041.920 I llm_load_print_meta: max token length = 21
0.00.043.963 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.043.964 I llm_load_tensors: offloading output layer to GPU
0.00.043.965 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.043.991 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.993 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.568 I llama_new_context_with_model: n_seq_max     = 1
0.00.044.570 I llama_new_context_with_model: n_ctx         = 512
0.00.044.570 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.044.571 I llama_new_context_with_model: n_batch       = 2048
0.00.044.571 I llama_new_context_with_model: n_ubatch      = 2048
0.00.044.571 I llama_new_context_with_model: flash_attn    = 0
0.00.044.572 I llama_new_context_with_model: freq_base     = 10000.0
0.00.044.572 I llama_new_context_with_model: freq_scale    = 1
0.00.044.573 I ggml_metal_init: allocating
0.00.044.578 I ggml_metal_init: found device: Apple M4
0.00.044.585 I ggml_metal_init: picking default device: Apple M4
0.00.045.508 I ggml_metal_init: using embedded metal library
0.00.049.959 I ggml_metal_init: GPU name:   Apple M4
0.00.049.962 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.049.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.049.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.049.964 I ggml_metal_init: simdgroup reduction   = true
0.00.049.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.049.964 I ggml_metal_init: has bfloat            = true
0.00.049.964 I ggml_metal_init: use bfloat            = true
0.00.049.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.049.966 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.582 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.063.585 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.063.587 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.064.466 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.064.468 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.064.468 I llama_new_context_with_model: graph nodes  = 429
0.00.064.469 I llama_new_context_with_model: graph splits = 2
0.00.064.491 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.064.492 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.071.507 I 
0.00.071.536 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.072.216 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.077.355 I llama_perf_context_print:        load time =      48.38 ms
0.00.077.356 I llama_perf_context_print: prompt eval time =       4.99 ms /     9 tokens (    0.55 ms per token,  1802.88 tokens per second)
0.00.077.357 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.077.358 I llama_perf_context_print:       total time =       5.85 ms /    10 tokens
0.00.077.466 I ggml_metal_free: deallocating

real	0m0.287s
user	0m0.053s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.452 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.489 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.494 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.495 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.496 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.496 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.497 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.497 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.497 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.498 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.498 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.501 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.501 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.501 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.502 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.502 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.502 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.502 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.978 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.660 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.660 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.661 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.661 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.661 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.662 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.662 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.662 I llama_model_loader: - type  f32:  124 tensors
0.00.014.663 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.071 I llm_load_vocab: special tokens cache size = 5
0.00.018.400 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.402 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.403 I llm_load_print_meta: arch             = bert
0.00.018.403 I llm_load_print_meta: vocab type       = WPM
0.00.018.403 I llm_load_print_meta: n_vocab          = 30522
0.00.018.403 I llm_load_print_meta: n_merges         = 0
0.00.018.403 I llm_load_print_meta: vocab_only       = 0
0.00.018.403 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.404 I llm_load_print_meta: n_embd           = 384
0.00.018.404 I llm_load_print_meta: n_layer          = 12
0.00.018.413 I llm_load_print_meta: n_head           = 12
0.00.018.413 I llm_load_print_meta: n_head_kv        = 12
0.00.018.415 I llm_load_print_meta: n_rot            = 32
0.00.018.415 I llm_load_print_meta: n_swa            = 0
0.00.018.415 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.415 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.416 I llm_load_print_meta: n_gqa            = 1
0.00.018.416 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.418 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.419 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.419 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.419 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.420 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.420 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.420 I llm_load_print_meta: n_ff             = 1536
0.00.018.420 I llm_load_print_meta: n_expert         = 0
0.00.018.420 I llm_load_print_meta: n_expert_used    = 0
0.00.018.421 I llm_load_print_meta: causal attn      = 0
0.00.018.421 I llm_load_print_meta: pooling type     = 2
0.00.018.421 I llm_load_print_meta: rope type        = 2
0.00.018.421 I llm_load_print_meta: rope scaling     = linear
0.00.018.421 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.422 I llm_load_print_meta: freq_scale_train = 1
0.00.018.422 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.422 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.422 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.422 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.422 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.422 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.423 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.423 I llm_load_print_meta: model type       = 33M
0.00.018.423 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.423 I llm_load_print_meta: model params     = 33.21 M
0.00.018.424 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.424 I llm_load_print_meta: general.name     = Bge Small
0.00.018.424 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.425 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.425 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.425 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.425 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.425 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.425 I llm_load_print_meta: max token length = 21
0.00.019.725 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.725 I llm_load_tensors: offloading output layer to GPU
0.00.019.725 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.733 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.734 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.091 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.091 I llama_new_context_with_model: n_ctx         = 512
0.00.020.091 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.092 I llama_new_context_with_model: n_batch       = 2048
0.00.020.092 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.092 I llama_new_context_with_model: flash_attn    = 0
0.00.020.092 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.093 I llama_new_context_with_model: freq_scale    = 1
0.00.020.093 I ggml_metal_init: allocating
0.00.020.096 I ggml_metal_init: found device: Apple M4
0.00.020.098 I ggml_metal_init: picking default device: Apple M4
0.00.020.699 I ggml_metal_init: using embedded metal library
0.00.023.249 I ggml_metal_init: GPU name:   Apple M4
0.00.023.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.251 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.251 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.252 I ggml_metal_init: simdgroup reduction   = true
0.00.023.252 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.252 I ggml_metal_init: has bfloat            = true
0.00.023.252 I ggml_metal_init: use bfloat            = true
0.00.023.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.149 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.154 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.156 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.823 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.824 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.824 I llama_new_context_with_model: graph nodes  = 429
0.00.034.825 I llama_new_context_with_model: graph splits = 2
0.00.034.837 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.261 I 
0.00.040.286 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.861 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.393 I llama_perf_context_print:        load time =      30.80 ms
0.00.044.395 I llama_perf_context_print: prompt eval time =       3.40 ms /     9 tokens (    0.38 ms per token,  2650.96 tokens per second)
0.00.044.395 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.396 I llama_perf_context_print:       total time =       4.13 ms /    10 tokens
0.00.044.593 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.138 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.173 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.054 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.062 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.063 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.064 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.064 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.065 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.066 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.067 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.068 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.068 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.074 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.074 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.075 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.076 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.547 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.620 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.088 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.089 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.090 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.090 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.090 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.047.090 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.091 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.091 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.092 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.092 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.047.092 I llama_model_loader: - type  f32:   41 tensors
0.00.047.095 I llama_model_loader: - type  f16:   29 tensors
0.00.065.337 W llm_load_vocab: empty token at index 5
0.00.069.940 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.157 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.182 I llm_load_vocab: special tokens cache size = 5
0.00.339.639 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.339.650 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.339.651 I llm_load_print_meta: arch             = jina-bert-v2
0.00.339.651 I llm_load_print_meta: vocab type       = BPE
0.00.339.652 I llm_load_print_meta: n_vocab          = 61056
0.00.339.652 I llm_load_print_meta: n_merges         = 39382
0.00.339.652 I llm_load_print_meta: vocab_only       = 0
0.00.339.652 I llm_load_print_meta: n_ctx_train      = 8192
0.00.339.652 I llm_load_print_meta: n_embd           = 384
0.00.339.652 I llm_load_print_meta: n_layer          = 4
0.00.339.683 I llm_load_print_meta: n_head           = 12
0.00.339.685 I llm_load_print_meta: n_head_kv        = 12
0.00.339.685 I llm_load_print_meta: n_rot            = 32
0.00.339.685 I llm_load_print_meta: n_swa            = 0
0.00.339.685 I llm_load_print_meta: n_embd_head_k    = 32
0.00.339.685 I llm_load_print_meta: n_embd_head_v    = 32
0.00.339.686 I llm_load_print_meta: n_gqa            = 1
0.00.339.686 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.339.687 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.339.687 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.339.688 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.339.688 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.339.688 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.339.688 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.339.689 I llm_load_print_meta: n_ff             = 1536
0.00.339.689 I llm_load_print_meta: n_expert         = 0
0.00.339.689 I llm_load_print_meta: n_expert_used    = 0
0.00.339.694 I llm_load_print_meta: causal attn      = 0
0.00.339.694 I llm_load_print_meta: pooling type     = -1
0.00.339.694 I llm_load_print_meta: rope type        = -1
0.00.339.694 I llm_load_print_meta: rope scaling     = linear
0.00.339.695 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.339.695 I llm_load_print_meta: freq_scale_train = 1
0.00.339.695 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.339.696 I llm_load_print_meta: rope_finetuned   = unknown
0.00.339.696 I llm_load_print_meta: ssm_d_conv       = 0
0.00.339.696 I llm_load_print_meta: ssm_d_inner      = 0
0.00.339.696 I llm_load_print_meta: ssm_d_state      = 0
0.00.339.696 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.339.696 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.339.697 I llm_load_print_meta: model type       = 33M
0.00.339.697 I llm_load_print_meta: model ftype      = F16
0.00.339.697 I llm_load_print_meta: model params     = 32.90 M
0.00.339.698 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.339.698 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.339.698 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.339.699 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.339.699 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.339.699 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.339.699 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.339.699 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.339.699 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.339.700 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.339.700 I llm_load_print_meta: max token length = 45
0.00.340.702 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.340.702 I llm_load_tensors: offloading output layer to GPU
0.00.340.702 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.340.724 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.340.725 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.341.447 I llama_new_context_with_model: n_seq_max     = 1
0.00.341.448 I llama_new_context_with_model: n_ctx         = 8192
0.00.341.448 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.341.448 I llama_new_context_with_model: n_batch       = 2048
0.00.341.449 I llama_new_context_with_model: n_ubatch      = 2048
0.00.341.449 I llama_new_context_with_model: flash_attn    = 0
0.00.341.449 I llama_new_context_with_model: freq_base     = 10000.0
0.00.341.449 I llama_new_context_with_model: freq_scale    = 1
0.00.341.450 I ggml_metal_init: allocating
0.00.341.453 I ggml_metal_init: found device: Apple M4
0.00.341.457 I ggml_metal_init: picking default device: Apple M4
0.00.342.175 I ggml_metal_init: using embedded metal library
0.00.344.919 I ggml_metal_init: GPU name:   Apple M4
0.00.344.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.922 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.922 I ggml_metal_init: simdgroup reduction   = true
0.00.344.922 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.922 I ggml_metal_init: has bfloat            = true
0.00.344.922 I ggml_metal_init: use bfloat            = true
0.00.344.923 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.924 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.357.910 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.357.913 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.357.916 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.358.543 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.358.544 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.358.545 I llama_new_context_with_model: graph nodes  = 154
0.00.358.545 I llama_new_context_with_model: graph splits = 2
0.00.358.562 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.358.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.370.900 I 
0.00.370.942 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.371.200 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.371.201 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.371.206 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.371.207 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.371.209 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.371.210 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.371.823 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.375.417 I llama_perf_context_print:        load time =     347.72 ms
0.00.375.418 I llama_perf_context_print: prompt eval time =       3.58 ms /    62 tokens (    0.06 ms per token, 17318.44 tokens per second)
0.00.375.419 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.375.419 I llama_perf_context_print:       total time =       4.51 ms /    63 tokens
0.00.375.588 I ggml_metal_free: deallocating

real	0m1.186s
user	0m0.346s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.109 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.234 I main: llama backend init
0.00.000.240 I main: load the model and apply lora adapter, if any
0.00.031.145 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.608 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.630 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.634 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.635 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.636 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.645 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.646 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.833 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.181 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.181 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.183 I llama_model_loader: - type  f32:  194 tensors
0.00.061.183 I llama_model_loader: - type  f16:   98 tensors
0.00.096.818 I llm_load_vocab: special tokens cache size = 25
0.00.104.059 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.104.062 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.104.062 I llm_load_print_meta: arch             = gptneox
0.00.104.063 I llm_load_print_meta: vocab type       = BPE
0.00.104.063 I llm_load_print_meta: n_vocab          = 50304
0.00.104.063 I llm_load_print_meta: n_merges         = 50009
0.00.104.063 I llm_load_print_meta: vocab_only       = 0
0.00.104.064 I llm_load_print_meta: n_ctx_train      = 2048
0.00.104.064 I llm_load_print_meta: n_embd           = 2048
0.00.104.064 I llm_load_print_meta: n_layer          = 24
0.00.104.079 I llm_load_print_meta: n_head           = 16
0.00.104.080 I llm_load_print_meta: n_head_kv        = 16
0.00.104.080 I llm_load_print_meta: n_rot            = 32
0.00.104.081 I llm_load_print_meta: n_swa            = 0
0.00.104.081 I llm_load_print_meta: n_embd_head_k    = 128
0.00.104.081 I llm_load_print_meta: n_embd_head_v    = 128
0.00.104.082 I llm_load_print_meta: n_gqa            = 1
0.00.104.082 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.104.083 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.104.084 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.104.084 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.104.086 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.104.087 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.104.087 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.104.088 I llm_load_print_meta: n_ff             = 8192
0.00.104.088 I llm_load_print_meta: n_expert         = 0
0.00.104.089 I llm_load_print_meta: n_expert_used    = 0
0.00.104.089 I llm_load_print_meta: causal attn      = 1
0.00.104.089 I llm_load_print_meta: pooling type     = 0
0.00.104.090 I llm_load_print_meta: rope type        = 2
0.00.104.090 I llm_load_print_meta: rope scaling     = linear
0.00.104.090 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.104.091 I llm_load_print_meta: freq_scale_train = 1
0.00.104.091 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.104.091 I llm_load_print_meta: rope_finetuned   = unknown
0.00.104.091 I llm_load_print_meta: ssm_d_conv       = 0
0.00.104.091 I llm_load_print_meta: ssm_d_inner      = 0
0.00.104.091 I llm_load_print_meta: ssm_d_state      = 0
0.00.104.091 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.104.091 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.104.092 I llm_load_print_meta: model type       = 1.4B
0.00.104.092 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.104.093 I llm_load_print_meta: model params     = 1.41 B
0.00.104.093 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.104.093 I llm_load_print_meta: general.name     = 1.4B
0.00.104.094 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.104.094 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.104.094 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.104.094 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.104.095 I llm_load_print_meta: LF token         = 128 ''
0.00.104.095 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.104.095 I llm_load_print_meta: max token length = 1024
0.00.106.818 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.106.818 I llm_load_tensors: offloading output layer to GPU
0.00.106.818 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.106.837 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.106.838 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.107.845 I llama_new_context_with_model: n_seq_max     = 1
0.00.107.846 I llama_new_context_with_model: n_ctx         = 2048
0.00.107.846 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.107.846 I llama_new_context_with_model: n_batch       = 2048
0.00.107.846 I llama_new_context_with_model: n_ubatch      = 512
0.00.107.846 I llama_new_context_with_model: flash_attn    = 0
0.00.107.847 I llama_new_context_with_model: freq_base     = 10000.0
0.00.107.847 I llama_new_context_with_model: freq_scale    = 1
0.00.107.847 I ggml_metal_init: allocating
0.00.107.851 I ggml_metal_init: found device: Apple M4
0.00.107.853 I ggml_metal_init: picking default device: Apple M4
0.00.108.549 I ggml_metal_init: using embedded metal library
0.00.118.178 I ggml_metal_init: GPU name:   Apple M4
0.00.118.180 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.118.180 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.118.181 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.118.181 I ggml_metal_init: simdgroup reduction   = true
0.00.118.181 I ggml_metal_init: simdgroup matrix mul. = true
0.00.118.181 I ggml_metal_init: has bfloat            = true
0.00.118.181 I ggml_metal_init: use bfloat            = true
0.00.118.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.118.182 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.163.950 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.163.956 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.163.975 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.164.936 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.164.938 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.164.938 I llama_new_context_with_model: graph nodes  = 967
0.00.164.938 I llama_new_context_with_model: graph splits = 2
0.00.164.963 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.165.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.165.095 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.248.085 I main: llama threadpool init, n_threads = 4
0.00.248.119 I 
0.00.248.162 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.248.163 I 
0.00.248.248 I sampler seed: 1234
0.00.248.253 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.248.287 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.248.289 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.248.289 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.093.442 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.02.093.442 I llama_perf_context_print:        load time =     216.93 ms
0.02.093.443 I llama_perf_context_print: prompt eval time =      43.84 ms /     7 tokens (    6.26 ms per token,   159.66 tokens per second)
0.02.093.443 I llama_perf_context_print:        eval time =    1798.28 ms /    63 runs   (   28.54 ms per token,    35.03 tokens per second)
0.02.093.445 I llama_perf_context_print:       total time =    1845.36 ms /    70 tokens
0.02.093.631 I ggml_metal_free: deallocating

real	0m2.439s
user	0m0.149s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.583 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.190 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.984 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.005 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.007 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.007 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.014 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.015 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.017 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.017 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.021 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.022 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.022 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.156 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.823 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.827 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.829 I llama_model_loader: - type  f32:  194 tensors
0.00.054.830 I llama_model_loader: - type  f16:   98 tensors
0.00.088.337 I llm_load_vocab: special tokens cache size = 25
0.00.095.341 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.345 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.346 I llm_load_print_meta: arch             = gptneox
0.00.095.346 I llm_load_print_meta: vocab type       = BPE
0.00.095.348 I llm_load_print_meta: n_vocab          = 50304
0.00.095.348 I llm_load_print_meta: n_merges         = 50009
0.00.095.348 I llm_load_print_meta: vocab_only       = 0
0.00.095.348 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.349 I llm_load_print_meta: n_embd           = 2048
0.00.095.349 I llm_load_print_meta: n_layer          = 24
0.00.095.363 I llm_load_print_meta: n_head           = 16
0.00.095.365 I llm_load_print_meta: n_head_kv        = 16
0.00.095.365 I llm_load_print_meta: n_rot            = 32
0.00.095.365 I llm_load_print_meta: n_swa            = 0
0.00.095.365 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.365 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.367 I llm_load_print_meta: n_gqa            = 1
0.00.095.367 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.368 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.369 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.369 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.369 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.369 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.370 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.372 I llm_load_print_meta: n_ff             = 8192
0.00.095.372 I llm_load_print_meta: n_expert         = 0
0.00.095.372 I llm_load_print_meta: n_expert_used    = 0
0.00.095.372 I llm_load_print_meta: causal attn      = 1
0.00.095.372 I llm_load_print_meta: pooling type     = 0
0.00.095.373 I llm_load_print_meta: rope type        = 2
0.00.095.373 I llm_load_print_meta: rope scaling     = linear
0.00.095.373 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.373 I llm_load_print_meta: freq_scale_train = 1
0.00.095.373 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.374 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.374 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.374 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.374 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.374 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.374 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.375 I llm_load_print_meta: model type       = 1.4B
0.00.095.375 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.376 I llm_load_print_meta: model params     = 1.41 B
0.00.095.376 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.376 I llm_load_print_meta: general.name     = 1.4B
0.00.095.376 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.378 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.378 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.378 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.378 I llm_load_print_meta: LF token         = 128 ''
0.00.095.378 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.379 I llm_load_print_meta: max token length = 1024
0.00.098.049 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.098.049 I llm_load_tensors: offloading output layer to GPU
0.00.098.049 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.098.060 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.098.061 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.099.018 I llama_new_context_with_model: n_seq_max     = 1
0.00.099.019 I llama_new_context_with_model: n_ctx         = 128
0.00.099.020 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.099.020 I llama_new_context_with_model: n_batch       = 128
0.00.099.020 I llama_new_context_with_model: n_ubatch      = 128
0.00.099.020 I llama_new_context_with_model: flash_attn    = 0
0.00.099.021 I llama_new_context_with_model: freq_base     = 10000.0
0.00.099.021 I llama_new_context_with_model: freq_scale    = 1
0.00.099.021 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.099.022 I ggml_metal_init: allocating
0.00.099.024 I ggml_metal_init: found device: Apple M4
0.00.099.027 I ggml_metal_init: picking default device: Apple M4
0.00.099.669 I ggml_metal_init: using embedded metal library
0.00.102.352 I ggml_metal_init: GPU name:   Apple M4
0.00.102.354 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.354 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.355 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.355 I ggml_metal_init: simdgroup reduction   = true
0.00.102.355 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.355 I ggml_metal_init: has bfloat            = true
0.00.102.355 I ggml_metal_init: use bfloat            = true
0.00.102.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.778 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.113.783 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.113.796 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.655 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.114.656 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.114.656 I llama_new_context_with_model: graph nodes  = 967
0.00.114.657 I llama_new_context_with_model: graph splits = 2
0.00.114.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.114.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.015.969 I 
0.01.016.039 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.016.094 I perplexity: tokenizing the input ..
0.01.029.485 I perplexity: tokenization took 13.388 ms
0.01.029.492 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.151.489 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.153.182 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.153.219 I llama_perf_context_print:        load time =     991.76 ms
0.01.153.221 I llama_perf_context_print: prompt eval time =     121.13 ms /   128 tokens (    0.95 ms per token,  1056.73 tokens per second)
0.01.153.222 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.153.223 I llama_perf_context_print:       total time =     137.25 ms /   129 tokens
0.01.153.900 I ggml_metal_free: deallocating

real	0m1.361s
user	0m0.127s
sys	0m0.198s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.605 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.900 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.905 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.910 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.910 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.914 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.916 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.916 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.916 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.917 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.917 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.917 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.918 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.920 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.920 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.920 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.964 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.961 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.962 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.963 I llama_model_loader: - type  f32:  194 tensors
0.00.033.964 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.900 I llm_load_vocab: special tokens cache size = 25
0.00.062.894 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.897 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.898 I llm_load_print_meta: arch             = gptneox
0.00.062.898 I llm_load_print_meta: vocab type       = BPE
0.00.062.898 I llm_load_print_meta: n_vocab          = 50304
0.00.062.899 I llm_load_print_meta: n_merges         = 50009
0.00.062.899 I llm_load_print_meta: vocab_only       = 0
0.00.062.899 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.899 I llm_load_print_meta: n_embd           = 2048
0.00.062.899 I llm_load_print_meta: n_layer          = 24
0.00.062.916 I llm_load_print_meta: n_head           = 16
0.00.062.917 I llm_load_print_meta: n_head_kv        = 16
0.00.062.917 I llm_load_print_meta: n_rot            = 32
0.00.062.918 I llm_load_print_meta: n_swa            = 0
0.00.062.918 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.918 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.919 I llm_load_print_meta: n_gqa            = 1
0.00.062.919 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.920 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.921 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.921 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.921 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.921 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.921 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.922 I llm_load_print_meta: n_ff             = 8192
0.00.062.922 I llm_load_print_meta: n_expert         = 0
0.00.062.922 I llm_load_print_meta: n_expert_used    = 0
0.00.062.922 I llm_load_print_meta: causal attn      = 1
0.00.062.923 I llm_load_print_meta: pooling type     = 0
0.00.062.923 I llm_load_print_meta: rope type        = 2
0.00.062.923 I llm_load_print_meta: rope scaling     = linear
0.00.062.923 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.924 I llm_load_print_meta: freq_scale_train = 1
0.00.062.927 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.927 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.927 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.927 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.927 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.928 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.928 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.928 I llm_load_print_meta: model type       = 1.4B
0.00.062.928 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.929 I llm_load_print_meta: model params     = 1.41 B
0.00.062.929 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.929 I llm_load_print_meta: general.name     = 1.4B
0.00.062.933 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.933 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.933 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.933 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.933 I llm_load_print_meta: LF token         = 128 ''
0.00.062.934 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.934 I llm_load_print_meta: max token length = 1024
0.00.065.393 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.394 I llm_load_tensors: offloading output layer to GPU
0.00.065.394 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.406 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.407 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.368 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.369 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.369 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.369 I llama_new_context_with_model: n_batch       = 2048
0.00.066.370 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.370 I llama_new_context_with_model: flash_attn    = 0
0.00.066.370 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.371 I llama_new_context_with_model: freq_scale    = 1
0.00.066.371 I ggml_metal_init: allocating
0.00.066.375 I ggml_metal_init: found device: Apple M4
0.00.066.377 I ggml_metal_init: picking default device: Apple M4
0.00.067.122 I ggml_metal_init: using embedded metal library
0.00.069.781 I ggml_metal_init: GPU name:   Apple M4
0.00.069.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.784 I ggml_metal_init: simdgroup reduction   = true
0.00.069.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.784 I ggml_metal_init: has bfloat            = true
0.00.069.784 I ggml_metal_init: use bfloat            = true
0.00.069.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.786 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.305 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.318 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.341 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.437 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.439 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.440 I llama_new_context_with_model: graph nodes  = 967
0.00.107.440 I llama_new_context_with_model: graph splits = 2
0.00.107.463 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.724.571 I main: llama threadpool init, n_threads = 4
0.01.724.617 I 
0.01.724.671 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.724.672 I 
0.01.725.072 I sampler seed: 1234
0.01.725.078 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.725.116 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.725.118 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.725.118 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.812.148 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54741.71 tokens per second)
0.02.812.149 I llama_perf_context_print:        load time =    1714.96 ms
0.02.812.150 I llama_perf_context_print: prompt eval time =      40.22 ms /     7 tokens (    5.75 ms per token,   174.03 tokens per second)
0.02.812.154 I llama_perf_context_print:        eval time =    1043.90 ms /    63 runs   (   16.57 ms per token,    60.35 tokens per second)
0.02.812.155 I llama_perf_context_print:       total time =    1087.58 ms /    70 tokens
0.02.812.346 I ggml_metal_free: deallocating

real	0m2.830s
user	0m0.121s
sys	0m0.265s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.123 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.099 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.616 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.616 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.622 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.626 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.179 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.102 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.104 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.105 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.106 I llama_model_loader: - type  f32:  194 tensors
0.00.033.106 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.695 I llm_load_vocab: special tokens cache size = 25
0.00.063.750 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.753 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.754 I llm_load_print_meta: arch             = gptneox
0.00.063.754 I llm_load_print_meta: vocab type       = BPE
0.00.063.754 I llm_load_print_meta: n_vocab          = 50304
0.00.063.754 I llm_load_print_meta: n_merges         = 50009
0.00.063.755 I llm_load_print_meta: vocab_only       = 0
0.00.063.755 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.755 I llm_load_print_meta: n_embd           = 2048
0.00.063.755 I llm_load_print_meta: n_layer          = 24
0.00.063.771 I llm_load_print_meta: n_head           = 16
0.00.063.771 I llm_load_print_meta: n_head_kv        = 16
0.00.063.771 I llm_load_print_meta: n_rot            = 32
0.00.063.772 I llm_load_print_meta: n_swa            = 0
0.00.063.772 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.772 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.773 I llm_load_print_meta: n_gqa            = 1
0.00.063.773 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.774 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.776 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.776 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.776 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.776 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.776 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.777 I llm_load_print_meta: n_ff             = 8192
0.00.063.777 I llm_load_print_meta: n_expert         = 0
0.00.063.777 I llm_load_print_meta: n_expert_used    = 0
0.00.063.778 I llm_load_print_meta: causal attn      = 1
0.00.063.778 I llm_load_print_meta: pooling type     = 0
0.00.063.778 I llm_load_print_meta: rope type        = 2
0.00.063.779 I llm_load_print_meta: rope scaling     = linear
0.00.063.780 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.780 I llm_load_print_meta: freq_scale_train = 1
0.00.063.780 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.780 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.780 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.780 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.781 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.781 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.781 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.781 I llm_load_print_meta: model type       = 1.4B
0.00.063.785 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.786 I llm_load_print_meta: model params     = 1.41 B
0.00.063.786 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.786 I llm_load_print_meta: general.name     = 1.4B
0.00.063.787 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.787 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.787 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.788 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.788 I llm_load_print_meta: LF token         = 128 ''
0.00.063.789 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.789 I llm_load_print_meta: max token length = 1024
0.00.066.079 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.080 I llm_load_tensors: offloading output layer to GPU
0.00.066.080 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.091 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.093 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.044 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.045 I llama_new_context_with_model: n_ctx         = 128
0.00.067.045 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.045 I llama_new_context_with_model: n_batch       = 128
0.00.067.045 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.045 I llama_new_context_with_model: flash_attn    = 0
0.00.067.046 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.046 I llama_new_context_with_model: freq_scale    = 1
0.00.067.046 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.047 I ggml_metal_init: allocating
0.00.067.054 I ggml_metal_init: found device: Apple M4
0.00.067.057 I ggml_metal_init: picking default device: Apple M4
0.00.067.707 I ggml_metal_init: using embedded metal library
0.00.070.262 I ggml_metal_init: GPU name:   Apple M4
0.00.070.264 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.265 I ggml_metal_init: simdgroup reduction   = true
0.00.070.265 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.265 I ggml_metal_init: has bfloat            = true
0.00.070.265 I ggml_metal_init: use bfloat            = true
0.00.070.266 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.267 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.814 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.818 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.833 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.771 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.772 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.772 I llama_new_context_with_model: graph nodes  = 967
0.00.081.772 I llama_new_context_with_model: graph splits = 2
0.00.081.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.786 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.960.711 I 
0.00.960.739 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.960.774 I perplexity: tokenizing the input ..
0.00.968.513 I perplexity: tokenization took 7.738 ms
0.00.968.520 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.092.781 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.093.969 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.093.985 I llama_perf_context_print:        load time =     949.61 ms
0.01.093.987 I llama_perf_context_print: prompt eval time =     124.04 ms /   128 tokens (    0.97 ms per token,  1031.96 tokens per second)
0.01.093.987 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.093.988 I llama_perf_context_print:       total time =     133.28 ms /   129 tokens
0.01.094.406 I ggml_metal_free: deallocating

real	0m1.111s
user	0m0.092s
sys	0m0.154s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.017.142 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.103 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.106 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.106 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.107 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.107 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.107 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.110 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.111 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.111 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.119 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.119 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.119 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.120 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.122 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.123 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.123 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.048.432 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.433 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.433 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.048.434 I llama_model_loader: - type  f32:  194 tensors
0.00.048.434 I llama_model_loader: - type q4_0:   97 tensors
0.00.048.435 I llama_model_loader: - type q6_K:    1 tensors
0.00.081.492 I llm_load_vocab: special tokens cache size = 25
0.00.092.075 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.079 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.080 I llm_load_print_meta: arch             = gptneox
0.00.092.080 I llm_load_print_meta: vocab type       = BPE
0.00.092.081 I llm_load_print_meta: n_vocab          = 50304
0.00.092.081 I llm_load_print_meta: n_merges         = 50009
0.00.092.081 I llm_load_print_meta: vocab_only       = 0
0.00.092.083 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.084 I llm_load_print_meta: n_embd           = 2048
0.00.092.084 I llm_load_print_meta: n_layer          = 24
0.00.092.101 I llm_load_print_meta: n_head           = 16
0.00.092.102 I llm_load_print_meta: n_head_kv        = 16
0.00.092.102 I llm_load_print_meta: n_rot            = 32
0.00.092.102 I llm_load_print_meta: n_swa            = 0
0.00.092.105 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.105 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.106 I llm_load_print_meta: n_gqa            = 1
0.00.092.107 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.108 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.109 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.109 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.109 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.110 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.110 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.112 I llm_load_print_meta: n_ff             = 8192
0.00.092.112 I llm_load_print_meta: n_expert         = 0
0.00.092.113 I llm_load_print_meta: n_expert_used    = 0
0.00.092.113 I llm_load_print_meta: causal attn      = 1
0.00.092.113 I llm_load_print_meta: pooling type     = 0
0.00.092.113 I llm_load_print_meta: rope type        = 2
0.00.092.114 I llm_load_print_meta: rope scaling     = linear
0.00.092.114 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.114 I llm_load_print_meta: freq_scale_train = 1
0.00.092.115 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.115 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.115 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.117 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.117 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.117 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.117 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.118 I llm_load_print_meta: model type       = 1.4B
0.00.092.118 I llm_load_print_meta: model ftype      = Q4_0
0.00.092.119 I llm_load_print_meta: model params     = 1.41 B
0.00.092.119 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.092.119 I llm_load_print_meta: general.name     = 1.4B
0.00.092.120 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.120 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.120 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.121 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.121 I llm_load_print_meta: LF token         = 128 ''
0.00.092.121 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.121 I llm_load_print_meta: max token length = 1024
0.00.094.940 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.940 I llm_load_tensors: offloading output layer to GPU
0.00.094.940 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.952 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.094.954 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.096.343 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.344 I llama_new_context_with_model: n_ctx         = 2048
0.00.096.344 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.096.345 I llama_new_context_with_model: n_batch       = 2048
0.00.096.345 I llama_new_context_with_model: n_ubatch      = 512
0.00.096.345 I llama_new_context_with_model: flash_attn    = 0
0.00.096.346 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.346 I llama_new_context_with_model: freq_scale    = 1
0.00.096.347 I ggml_metal_init: allocating
0.00.096.356 I ggml_metal_init: found device: Apple M4
0.00.096.360 I ggml_metal_init: picking default device: Apple M4
0.00.097.328 I ggml_metal_init: using embedded metal library
0.00.101.072 I ggml_metal_init: GPU name:   Apple M4
0.00.101.074 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.076 I ggml_metal_init: simdgroup reduction   = true
0.00.101.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.076 I ggml_metal_init: has bfloat            = true
0.00.101.076 I ggml_metal_init: use bfloat            = true
0.00.101.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.078 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.138.369 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.138.377 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.138.412 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.139.472 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.139.474 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.139.474 I llama_new_context_with_model: graph nodes  = 967
0.00.139.475 I llama_new_context_with_model: graph splits = 2
0.00.139.494 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.139.634 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.139.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.842.738 I main: llama threadpool init, n_threads = 4
0.00.842.798 I 
0.00.842.848 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.842.849 I 
0.00.843.214 I sampler seed: 1234
0.00.843.220 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.843.259 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.843.261 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.843.261 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.522.560 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.522.561 I llama_perf_context_print:        load time =     825.59 ms
0.01.522.562 I llama_perf_context_print: prompt eval time =      46.33 ms /     7 tokens (    6.62 ms per token,   151.10 tokens per second)
0.01.522.562 I llama_perf_context_print:        eval time =     629.94 ms /    63 runs   (   10.00 ms per token,   100.01 tokens per second)
0.01.522.566 I llama_perf_context_print:       total time =     679.83 ms /    70 tokens
0.01.522.757 I ggml_metal_free: deallocating

real	0m1.549s
user	0m0.140s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.639 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.685 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.689 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.691 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.691 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.691 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.692 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.692 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.693 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.695 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.696 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.696 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.696 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.697 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.697 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.699 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.699 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.699 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.661 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.787 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.638 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.639 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.640 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.640 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.640 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.641 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.641 I llama_model_loader: - type  f32:  194 tensors
0.00.024.642 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.642 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.794 I llm_load_vocab: special tokens cache size = 25
0.00.051.571 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.574 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.575 I llm_load_print_meta: arch             = gptneox
0.00.051.575 I llm_load_print_meta: vocab type       = BPE
0.00.051.575 I llm_load_print_meta: n_vocab          = 50304
0.00.051.575 I llm_load_print_meta: n_merges         = 50009
0.00.051.576 I llm_load_print_meta: vocab_only       = 0
0.00.051.576 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.576 I llm_load_print_meta: n_embd           = 2048
0.00.051.576 I llm_load_print_meta: n_layer          = 24
0.00.051.590 I llm_load_print_meta: n_head           = 16
0.00.051.591 I llm_load_print_meta: n_head_kv        = 16
0.00.051.591 I llm_load_print_meta: n_rot            = 32
0.00.051.591 I llm_load_print_meta: n_swa            = 0
0.00.051.591 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.591 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.592 I llm_load_print_meta: n_gqa            = 1
0.00.051.593 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.593 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.594 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.594 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.595 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.595 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.595 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.596 I llm_load_print_meta: n_ff             = 8192
0.00.051.596 I llm_load_print_meta: n_expert         = 0
0.00.051.596 I llm_load_print_meta: n_expert_used    = 0
0.00.051.596 I llm_load_print_meta: causal attn      = 1
0.00.051.598 I llm_load_print_meta: pooling type     = 0
0.00.051.598 I llm_load_print_meta: rope type        = 2
0.00.051.598 I llm_load_print_meta: rope scaling     = linear
0.00.051.598 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.599 I llm_load_print_meta: freq_scale_train = 1
0.00.051.599 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.599 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.599 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.599 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.599 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.599 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.599 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.600 I llm_load_print_meta: model type       = 1.4B
0.00.051.600 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.600 I llm_load_print_meta: model params     = 1.41 B
0.00.051.601 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.601 I llm_load_print_meta: general.name     = 1.4B
0.00.051.601 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.601 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.602 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.602 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.603 I llm_load_print_meta: LF token         = 128 ''
0.00.051.604 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.604 I llm_load_print_meta: max token length = 1024
0.00.053.616 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.617 I llm_load_tensors: offloading output layer to GPU
0.00.053.617 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.627 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.628 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.517 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.518 I llama_new_context_with_model: n_ctx         = 128
0.00.054.518 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.518 I llama_new_context_with_model: n_batch       = 128
0.00.054.518 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.519 I llama_new_context_with_model: flash_attn    = 0
0.00.054.519 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.519 I llama_new_context_with_model: freq_scale    = 1
0.00.054.520 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.520 I ggml_metal_init: allocating
0.00.054.527 I ggml_metal_init: found device: Apple M4
0.00.054.529 I ggml_metal_init: picking default device: Apple M4
0.00.055.068 I ggml_metal_init: using embedded metal library
0.00.057.455 I ggml_metal_init: GPU name:   Apple M4
0.00.057.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.457 I ggml_metal_init: simdgroup reduction   = true
0.00.057.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.457 I ggml_metal_init: has bfloat            = true
0.00.057.457 I ggml_metal_init: use bfloat            = true
0.00.057.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.459 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.817 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.821 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.838 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.737 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.738 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.738 I llama_new_context_with_model: graph nodes  = 967
0.00.069.738 I llama_new_context_with_model: graph splits = 2
0.00.069.751 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.814 I 
0.00.630.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.869 I perplexity: tokenizing the input ..
0.00.638.516 I perplexity: tokenization took 7.646 ms
0.00.638.521 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.233 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.762.396 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.762.411 I llama_perf_context_print:        load time =     621.17 ms
0.00.762.412 I llama_perf_context_print: prompt eval time =     122.49 ms /   128 tokens (    0.96 ms per token,  1045.01 tokens per second)
0.00.762.413 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.413 I llama_perf_context_print:       total time =     131.60 ms /   129 tokens
0.00.762.841 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.079s
sys	0m0.106s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.360 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.326 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.331 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.332 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.337 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.337 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.340 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.340 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.341 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.341 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.341 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.342 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.342 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.344 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.344 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.344 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.398 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.597 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.597 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.598 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.599 I llama_model_loader: - type  f32:  194 tensors
0.00.025.599 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.599 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.940 I llm_load_vocab: special tokens cache size = 25
0.00.052.915 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.918 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.919 I llm_load_print_meta: arch             = gptneox
0.00.052.919 I llm_load_print_meta: vocab type       = BPE
0.00.052.919 I llm_load_print_meta: n_vocab          = 50304
0.00.052.920 I llm_load_print_meta: n_merges         = 50009
0.00.052.920 I llm_load_print_meta: vocab_only       = 0
0.00.052.920 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.920 I llm_load_print_meta: n_embd           = 2048
0.00.052.920 I llm_load_print_meta: n_layer          = 24
0.00.052.934 I llm_load_print_meta: n_head           = 16
0.00.052.935 I llm_load_print_meta: n_head_kv        = 16
0.00.052.935 I llm_load_print_meta: n_rot            = 32
0.00.052.938 I llm_load_print_meta: n_swa            = 0
0.00.052.938 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.938 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.939 I llm_load_print_meta: n_gqa            = 1
0.00.052.940 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.940 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.941 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.941 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.941 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.942 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.942 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.942 I llm_load_print_meta: n_ff             = 8192
0.00.052.943 I llm_load_print_meta: n_expert         = 0
0.00.052.943 I llm_load_print_meta: n_expert_used    = 0
0.00.052.943 I llm_load_print_meta: causal attn      = 1
0.00.052.943 I llm_load_print_meta: pooling type     = 0
0.00.052.943 I llm_load_print_meta: rope type        = 2
0.00.052.943 I llm_load_print_meta: rope scaling     = linear
0.00.052.944 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.944 I llm_load_print_meta: freq_scale_train = 1
0.00.052.946 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.946 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.946 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.946 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.946 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.947 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.947 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.947 I llm_load_print_meta: model type       = 1.4B
0.00.052.947 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.948 I llm_load_print_meta: model params     = 1.41 B
0.00.052.948 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.949 I llm_load_print_meta: general.name     = 1.4B
0.00.052.949 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.950 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.950 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.951 I llm_load_print_meta: LF token         = 128 ''
0.00.052.951 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.951 I llm_load_print_meta: max token length = 1024
0.00.055.032 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.032 I llm_load_tensors: offloading output layer to GPU
0.00.055.032 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.043 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.044 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.988 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.989 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.989 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.990 I llama_new_context_with_model: n_batch       = 2048
0.00.055.990 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.990 I llama_new_context_with_model: flash_attn    = 0
0.00.055.990 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.991 I llama_new_context_with_model: freq_scale    = 1
0.00.055.991 I ggml_metal_init: allocating
0.00.055.997 I ggml_metal_init: found device: Apple M4
0.00.055.999 I ggml_metal_init: picking default device: Apple M4
0.00.056.580 I ggml_metal_init: using embedded metal library
0.00.058.945 I ggml_metal_init: GPU name:   Apple M4
0.00.058.946 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.947 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.947 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.947 I ggml_metal_init: simdgroup reduction   = true
0.00.058.947 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.948 I ggml_metal_init: has bfloat            = true
0.00.058.948 I ggml_metal_init: use bfloat            = true
0.00.058.948 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.949 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.679 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.684 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.702 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.681 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.682 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.683 I llama_new_context_with_model: graph nodes  = 967
0.00.089.683 I llama_new_context_with_model: graph splits = 2
0.00.089.699 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.816 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.742 I main: llama threadpool init, n_threads = 4
0.00.730.783 I 
0.00.730.813 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.813 I 
0.00.731.030 I sampler seed: 1234
0.00.731.035 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.079 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.096 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.096 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.463.986 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63620.07 tokens per second)
0.01.463.987 I llama_perf_context_print:        load time =     721.38 ms
0.01.463.988 I llama_perf_context_print: prompt eval time =      45.57 ms /     7 tokens (    6.51 ms per token,   153.62 tokens per second)
0.01.463.989 I llama_perf_context_print:        eval time =     684.48 ms /    63 runs   (   10.86 ms per token,    92.04 tokens per second)
0.01.463.989 I llama_perf_context_print:       total time =     733.25 ms /    70 tokens
0.01.464.183 I ggml_metal_free: deallocating

real	0m1.481s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.810 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.587 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.597 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.598 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.598 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.598 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.599 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.601 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.601 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.602 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.602 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.603 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.603 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.606 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.606 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.606 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.560 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.506 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.507 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.508 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.508 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.508 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.509 I llama_model_loader: - type  f32:  194 tensors
0.00.023.509 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.510 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.657 I llm_load_vocab: special tokens cache size = 25
0.00.050.563 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.566 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.566 I llm_load_print_meta: arch             = gptneox
0.00.050.566 I llm_load_print_meta: vocab type       = BPE
0.00.050.566 I llm_load_print_meta: n_vocab          = 50304
0.00.050.566 I llm_load_print_meta: n_merges         = 50009
0.00.050.567 I llm_load_print_meta: vocab_only       = 0
0.00.050.567 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.567 I llm_load_print_meta: n_embd           = 2048
0.00.050.567 I llm_load_print_meta: n_layer          = 24
0.00.050.582 I llm_load_print_meta: n_head           = 16
0.00.050.582 I llm_load_print_meta: n_head_kv        = 16
0.00.050.583 I llm_load_print_meta: n_rot            = 32
0.00.050.583 I llm_load_print_meta: n_swa            = 0
0.00.050.583 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.583 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.584 I llm_load_print_meta: n_gqa            = 1
0.00.050.584 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.585 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.585 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.586 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.586 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.586 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.586 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.587 I llm_load_print_meta: n_ff             = 8192
0.00.050.587 I llm_load_print_meta: n_expert         = 0
0.00.050.588 I llm_load_print_meta: n_expert_used    = 0
0.00.050.588 I llm_load_print_meta: causal attn      = 1
0.00.050.588 I llm_load_print_meta: pooling type     = 0
0.00.050.588 I llm_load_print_meta: rope type        = 2
0.00.050.589 I llm_load_print_meta: rope scaling     = linear
0.00.050.589 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.590 I llm_load_print_meta: freq_scale_train = 1
0.00.050.590 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.590 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.590 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.590 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.590 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.591 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.591 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.592 I llm_load_print_meta: model type       = 1.4B
0.00.050.592 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.592 I llm_load_print_meta: model params     = 1.41 B
0.00.050.593 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.593 I llm_load_print_meta: general.name     = 1.4B
0.00.050.593 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.593 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.593 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.594 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.594 I llm_load_print_meta: LF token         = 128 ''
0.00.050.595 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.595 I llm_load_print_meta: max token length = 1024
0.00.052.614 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.614 I llm_load_tensors: offloading output layer to GPU
0.00.052.614 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.624 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.626 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.524 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.525 I llama_new_context_with_model: n_ctx         = 128
0.00.053.525 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.525 I llama_new_context_with_model: n_batch       = 128
0.00.053.525 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.525 I llama_new_context_with_model: flash_attn    = 0
0.00.053.526 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.526 I llama_new_context_with_model: freq_scale    = 1
0.00.053.527 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.527 I ggml_metal_init: allocating
0.00.053.534 I ggml_metal_init: found device: Apple M4
0.00.053.536 I ggml_metal_init: picking default device: Apple M4
0.00.054.115 I ggml_metal_init: using embedded metal library
0.00.056.433 I ggml_metal_init: GPU name:   Apple M4
0.00.056.435 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.435 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.436 I ggml_metal_init: simdgroup reduction   = true
0.00.056.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.436 I ggml_metal_init: has bfloat            = true
0.00.056.436 I ggml_metal_init: use bfloat            = true
0.00.056.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.241 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.243 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.264 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.123 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.125 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.125 I llama_new_context_with_model: graph nodes  = 967
0.00.068.125 I llama_new_context_with_model: graph splits = 2
0.00.068.138 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.139 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.695 I 
0.00.679.776 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.796 I perplexity: tokenizing the input ..
0.00.687.586 I perplexity: tokenization took 7.788 ms
0.00.687.590 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.110 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.811.304 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.811.322 I llama_perf_context_print:        load time =     670.88 ms
0.00.811.323 I llama_perf_context_print: prompt eval time =     122.29 ms /   128 tokens (    0.96 ms per token,  1046.72 tokens per second)
0.00.811.324 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.325 I llama_perf_context_print:       total time =     131.63 ms /   129 tokens
0.00.811.659 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.079s
sys	0m0.117s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.340 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.978 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.990 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.990 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.991 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.991 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.996 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.890 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.909 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.807 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.808 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.808 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.809 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.809 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.809 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.810 I llama_model_loader: - type  f32:  194 tensors
0.00.024.810 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.810 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.075 I llm_load_vocab: special tokens cache size = 25
0.00.052.049 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.052 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.052 I llm_load_print_meta: arch             = gptneox
0.00.052.052 I llm_load_print_meta: vocab type       = BPE
0.00.052.053 I llm_load_print_meta: n_vocab          = 50304
0.00.052.053 I llm_load_print_meta: n_merges         = 50009
0.00.052.053 I llm_load_print_meta: vocab_only       = 0
0.00.052.053 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.053 I llm_load_print_meta: n_embd           = 2048
0.00.052.054 I llm_load_print_meta: n_layer          = 24
0.00.052.068 I llm_load_print_meta: n_head           = 16
0.00.052.068 I llm_load_print_meta: n_head_kv        = 16
0.00.052.068 I llm_load_print_meta: n_rot            = 32
0.00.052.069 I llm_load_print_meta: n_swa            = 0
0.00.052.069 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.072 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.072 I llm_load_print_meta: n_gqa            = 1
0.00.052.073 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.074 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.075 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.076 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.077 I llm_load_print_meta: n_ff             = 8192
0.00.052.077 I llm_load_print_meta: n_expert         = 0
0.00.052.077 I llm_load_print_meta: n_expert_used    = 0
0.00.052.079 I llm_load_print_meta: causal attn      = 1
0.00.052.080 I llm_load_print_meta: pooling type     = 0
0.00.052.080 I llm_load_print_meta: rope type        = 2
0.00.052.080 I llm_load_print_meta: rope scaling     = linear
0.00.052.080 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.081 I llm_load_print_meta: freq_scale_train = 1
0.00.052.081 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.081 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.081 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.081 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.081 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.081 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.081 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.082 I llm_load_print_meta: model type       = 1.4B
0.00.052.082 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.082 I llm_load_print_meta: model params     = 1.41 B
0.00.052.083 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.083 I llm_load_print_meta: general.name     = 1.4B
0.00.052.083 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.083 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.083 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: LF token         = 128 ''
0.00.052.084 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: max token length = 1024
0.00.054.151 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.151 I llm_load_tensors: offloading output layer to GPU
0.00.054.151 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.162 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.163 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.075 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.076 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.076 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.077 I llama_new_context_with_model: n_batch       = 2048
0.00.055.077 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.077 I llama_new_context_with_model: flash_attn    = 0
0.00.055.078 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.078 I llama_new_context_with_model: freq_scale    = 1
0.00.055.078 I ggml_metal_init: allocating
0.00.055.086 I ggml_metal_init: found device: Apple M4
0.00.055.089 I ggml_metal_init: picking default device: Apple M4
0.00.055.663 I ggml_metal_init: using embedded metal library
0.00.057.994 I ggml_metal_init: GPU name:   Apple M4
0.00.057.996 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.996 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.996 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.996 I ggml_metal_init: simdgroup reduction   = true
0.00.057.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.997 I ggml_metal_init: has bfloat            = true
0.00.057.997 I ggml_metal_init: use bfloat            = true
0.00.057.997 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.999 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.454 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.462 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.481 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.534 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.535 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.535 I llama_new_context_with_model: graph nodes  = 967
0.00.088.535 I llama_new_context_with_model: graph splits = 2
0.00.088.550 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.672 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.673 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.801.318 I main: llama threadpool init, n_threads = 4
0.00.801.354 I 
0.00.801.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.384 I 
0.00.801.626 I sampler seed: 1234
0.00.801.630 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.645 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.647 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.647 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.587.682 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61631.94 tokens per second)
0.01.587.683 I llama_perf_context_print:        load time =     791.97 ms
0.01.587.684 I llama_perf_context_print: prompt eval time =      43.09 ms /     7 tokens (    6.16 ms per token,   162.47 tokens per second)
0.01.587.684 I llama_perf_context_print:        eval time =     740.09 ms /    63 runs   (   11.75 ms per token,    85.13 tokens per second)
0.01.587.685 I llama_perf_context_print:       total time =     786.37 ms /    70 tokens
0.01.587.876 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.110s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.820 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.589 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.589 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.590 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.593 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.640 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.556 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.557 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.557 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.557 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.558 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.558 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.559 I llama_model_loader: - type  f32:  194 tensors
0.00.024.559 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.559 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.070 I llm_load_vocab: special tokens cache size = 25
0.00.051.221 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.223 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.224 I llm_load_print_meta: arch             = gptneox
0.00.051.224 I llm_load_print_meta: vocab type       = BPE
0.00.051.224 I llm_load_print_meta: n_vocab          = 50304
0.00.051.224 I llm_load_print_meta: n_merges         = 50009
0.00.051.225 I llm_load_print_meta: vocab_only       = 0
0.00.051.225 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.225 I llm_load_print_meta: n_embd           = 2048
0.00.051.225 I llm_load_print_meta: n_layer          = 24
0.00.051.240 I llm_load_print_meta: n_head           = 16
0.00.051.243 I llm_load_print_meta: n_head_kv        = 16
0.00.051.243 I llm_load_print_meta: n_rot            = 32
0.00.051.243 I llm_load_print_meta: n_swa            = 0
0.00.051.243 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.243 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.244 I llm_load_print_meta: n_gqa            = 1
0.00.051.245 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.245 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.246 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.246 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.247 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.247 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.247 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.247 I llm_load_print_meta: n_ff             = 8192
0.00.051.248 I llm_load_print_meta: n_expert         = 0
0.00.051.248 I llm_load_print_meta: n_expert_used    = 0
0.00.051.248 I llm_load_print_meta: causal attn      = 1
0.00.051.248 I llm_load_print_meta: pooling type     = 0
0.00.051.248 I llm_load_print_meta: rope type        = 2
0.00.051.248 I llm_load_print_meta: rope scaling     = linear
0.00.051.249 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.249 I llm_load_print_meta: freq_scale_train = 1
0.00.051.249 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.249 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.249 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.250 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.250 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.250 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.250 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.250 I llm_load_print_meta: model type       = 1.4B
0.00.051.250 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.251 I llm_load_print_meta: model params     = 1.41 B
0.00.051.251 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.251 I llm_load_print_meta: general.name     = 1.4B
0.00.051.252 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.252 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.252 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.252 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.252 I llm_load_print_meta: LF token         = 128 ''
0.00.051.253 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.253 I llm_load_print_meta: max token length = 1024
0.00.053.223 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.223 I llm_load_tensors: offloading output layer to GPU
0.00.053.223 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.234 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.235 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.116 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.117 I llama_new_context_with_model: n_ctx         = 128
0.00.054.117 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.118 I llama_new_context_with_model: n_batch       = 128
0.00.054.118 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.118 I llama_new_context_with_model: flash_attn    = 0
0.00.054.118 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.119 I llama_new_context_with_model: freq_scale    = 1
0.00.054.119 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.119 I ggml_metal_init: allocating
0.00.054.122 I ggml_metal_init: found device: Apple M4
0.00.054.124 I ggml_metal_init: picking default device: Apple M4
0.00.054.700 I ggml_metal_init: using embedded metal library
0.00.057.024 I ggml_metal_init: GPU name:   Apple M4
0.00.057.025 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.026 I ggml_metal_init: simdgroup reduction   = true
0.00.057.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.026 I ggml_metal_init: has bfloat            = true
0.00.057.026 I ggml_metal_init: use bfloat            = true
0.00.057.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.027 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.027 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.029 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.042 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.956 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.957 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.957 I llama_new_context_with_model: graph nodes  = 967
0.00.068.957 I llama_new_context_with_model: graph splits = 2
0.00.068.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.565 I 
0.00.748.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.612 I perplexity: tokenizing the input ..
0.00.755.918 I perplexity: tokenization took 7.305 ms
0.00.755.921 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.891.207 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.892.448 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.892.463 I llama_perf_context_print:        load time =     738.74 ms
0.00.892.464 I llama_perf_context_print: prompt eval time =     135.05 ms /   128 tokens (    1.06 ms per token,   947.77 tokens per second)
0.00.892.465 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.892.465 I llama_perf_context_print:       total time =     143.90 ms /   129 tokens
0.00.892.883 I ggml_metal_free: deallocating

real	0m0.909s
user	0m0.078s
sys	0m0.114s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.842 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.066 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.072 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.078 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.081 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.082 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.088 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.088 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.089 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.155 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.209 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.210 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.210 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.211 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.211 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.211 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.212 I llama_model_loader: - type  f32:  194 tensors
0.00.025.212 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.212 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.520 I llm_load_vocab: special tokens cache size = 25
0.00.052.537 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.540 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.540 I llm_load_print_meta: arch             = gptneox
0.00.052.541 I llm_load_print_meta: vocab type       = BPE
0.00.052.541 I llm_load_print_meta: n_vocab          = 50304
0.00.052.541 I llm_load_print_meta: n_merges         = 50009
0.00.052.541 I llm_load_print_meta: vocab_only       = 0
0.00.052.542 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.542 I llm_load_print_meta: n_embd           = 2048
0.00.052.542 I llm_load_print_meta: n_layer          = 24
0.00.052.556 I llm_load_print_meta: n_head           = 16
0.00.052.557 I llm_load_print_meta: n_head_kv        = 16
0.00.052.557 I llm_load_print_meta: n_rot            = 32
0.00.052.558 I llm_load_print_meta: n_swa            = 0
0.00.052.558 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.558 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.559 I llm_load_print_meta: n_gqa            = 1
0.00.052.559 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.560 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.561 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.561 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.563 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.563 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.563 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.564 I llm_load_print_meta: n_ff             = 8192
0.00.052.565 I llm_load_print_meta: n_expert         = 0
0.00.052.565 I llm_load_print_meta: n_expert_used    = 0
0.00.052.565 I llm_load_print_meta: causal attn      = 1
0.00.052.566 I llm_load_print_meta: pooling type     = 0
0.00.052.566 I llm_load_print_meta: rope type        = 2
0.00.052.566 I llm_load_print_meta: rope scaling     = linear
0.00.052.566 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.566 I llm_load_print_meta: freq_scale_train = 1
0.00.052.567 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.567 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.567 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.567 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.567 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.567 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.568 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.568 I llm_load_print_meta: model type       = 1.4B
0.00.052.568 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.569 I llm_load_print_meta: model params     = 1.41 B
0.00.052.570 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.570 I llm_load_print_meta: general.name     = 1.4B
0.00.052.570 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.570 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.571 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.571 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.571 I llm_load_print_meta: LF token         = 128 ''
0.00.052.571 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.571 I llm_load_print_meta: max token length = 1024
0.00.054.675 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.676 I llm_load_tensors: offloading output layer to GPU
0.00.054.676 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.686 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.688 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.592 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.593 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.593 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.593 I llama_new_context_with_model: n_batch       = 2048
0.00.055.593 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.594 I llama_new_context_with_model: flash_attn    = 0
0.00.055.594 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.594 I llama_new_context_with_model: freq_scale    = 1
0.00.055.595 I ggml_metal_init: allocating
0.00.055.598 I ggml_metal_init: found device: Apple M4
0.00.055.600 I ggml_metal_init: picking default device: Apple M4
0.00.056.204 I ggml_metal_init: using embedded metal library
0.00.058.580 I ggml_metal_init: GPU name:   Apple M4
0.00.058.582 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.582 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.583 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.583 I ggml_metal_init: simdgroup reduction   = true
0.00.058.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.583 I ggml_metal_init: has bfloat            = true
0.00.058.583 I ggml_metal_init: use bfloat            = true
0.00.058.584 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.584 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.960 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.965 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.989 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.992 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.994 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.994 I llama_new_context_with_model: graph nodes  = 967
0.00.090.994 I llama_new_context_with_model: graph splits = 2
0.00.091.004 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.146 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.364 I main: llama threadpool init, n_threads = 4
0.00.727.409 I 
0.00.727.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.441 I 
0.00.727.758 I sampler seed: 1234
0.00.727.767 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.727.795 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.727.796 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.727.796 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.572.251 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48168.25 tokens per second)
0.01.572.252 I llama_perf_context_print:        load time =     718.52 ms
0.01.572.253 I llama_perf_context_print: prompt eval time =      42.34 ms /     7 tokens (    6.05 ms per token,   165.35 tokens per second)
0.01.572.254 I llama_perf_context_print:        eval time =     799.45 ms /    63 runs   (   12.69 ms per token,    78.80 tokens per second)
0.01.572.255 I llama_perf_context_print:       total time =     844.89 ms /    70 tokens
0.01.572.455 I ggml_metal_free: deallocating

real	0m1.591s
user	0m0.110s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.988 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.057 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.062 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.064 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.065 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.065 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.067 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.072 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.022 I llama_model_loader: - type  f32:  194 tensors
0.00.024.023 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.023 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.488 I llm_load_vocab: special tokens cache size = 25
0.00.051.453 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.456 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.456 I llm_load_print_meta: arch             = gptneox
0.00.051.457 I llm_load_print_meta: vocab type       = BPE
0.00.051.457 I llm_load_print_meta: n_vocab          = 50304
0.00.051.457 I llm_load_print_meta: n_merges         = 50009
0.00.051.458 I llm_load_print_meta: vocab_only       = 0
0.00.051.458 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.458 I llm_load_print_meta: n_embd           = 2048
0.00.051.458 I llm_load_print_meta: n_layer          = 24
0.00.051.472 I llm_load_print_meta: n_head           = 16
0.00.051.475 I llm_load_print_meta: n_head_kv        = 16
0.00.051.475 I llm_load_print_meta: n_rot            = 32
0.00.051.476 I llm_load_print_meta: n_swa            = 0
0.00.051.477 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.477 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.478 I llm_load_print_meta: n_gqa            = 1
0.00.051.479 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.479 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.480 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.480 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.480 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.480 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.480 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.481 I llm_load_print_meta: n_ff             = 8192
0.00.051.481 I llm_load_print_meta: n_expert         = 0
0.00.051.481 I llm_load_print_meta: n_expert_used    = 0
0.00.051.482 I llm_load_print_meta: causal attn      = 1
0.00.051.482 I llm_load_print_meta: pooling type     = 0
0.00.051.482 I llm_load_print_meta: rope type        = 2
0.00.051.482 I llm_load_print_meta: rope scaling     = linear
0.00.051.482 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.483 I llm_load_print_meta: freq_scale_train = 1
0.00.051.483 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.483 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.483 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.483 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.483 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.484 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.484 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.484 I llm_load_print_meta: model type       = 1.4B
0.00.051.485 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.485 I llm_load_print_meta: model params     = 1.41 B
0.00.051.485 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.486 I llm_load_print_meta: general.name     = 1.4B
0.00.051.486 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.486 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.486 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.487 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.487 I llm_load_print_meta: LF token         = 128 ''
0.00.051.487 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.487 I llm_load_print_meta: max token length = 1024
0.00.053.603 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.604 I llm_load_tensors: offloading output layer to GPU
0.00.053.604 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.614 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.615 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.549 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.550 I llama_new_context_with_model: n_ctx         = 128
0.00.054.550 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.550 I llama_new_context_with_model: n_batch       = 128
0.00.054.550 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.551 I llama_new_context_with_model: flash_attn    = 0
0.00.054.551 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.551 I llama_new_context_with_model: freq_scale    = 1
0.00.054.552 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.552 I ggml_metal_init: allocating
0.00.054.555 I ggml_metal_init: found device: Apple M4
0.00.054.557 I ggml_metal_init: picking default device: Apple M4
0.00.055.145 I ggml_metal_init: using embedded metal library
0.00.057.586 I ggml_metal_init: GPU name:   Apple M4
0.00.057.587 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.588 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.588 I ggml_metal_init: simdgroup reduction   = true
0.00.057.588 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.589 I ggml_metal_init: has bfloat            = true
0.00.057.589 I ggml_metal_init: use bfloat            = true
0.00.057.589 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.590 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.936 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.941 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.955 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.932 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.933 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.933 I llama_new_context_with_model: graph nodes  = 967
0.00.069.933 I llama_new_context_with_model: graph splits = 2
0.00.069.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.121 I 
0.00.676.195 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.222 I perplexity: tokenizing the input ..
0.00.684.096 I perplexity: tokenization took 7.871 ms
0.00.684.103 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.008 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.820.175 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.820.192 I llama_perf_context_print:        load time =     667.12 ms
0.00.820.193 I llama_perf_context_print: prompt eval time =     134.68 ms /   128 tokens (    1.05 ms per token,   950.42 tokens per second)
0.00.820.195 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.196 I llama_perf_context_print:       total time =     144.08 ms /   129 tokens
0.00.820.612 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.080s
sys	0m0.116s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.993 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.657 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.662 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.664 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.664 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.665 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.665 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.665 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.666 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.666 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.667 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.667 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.668 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.668 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.670 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.671 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.611 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.534 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.536 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.536 I llama_model_loader: - type  f32:  194 tensors
0.00.024.537 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.537 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.537 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.200 I llm_load_vocab: special tokens cache size = 25
0.00.050.947 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.950 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.950 I llm_load_print_meta: arch             = gptneox
0.00.050.951 I llm_load_print_meta: vocab type       = BPE
0.00.050.951 I llm_load_print_meta: n_vocab          = 50304
0.00.050.951 I llm_load_print_meta: n_merges         = 50009
0.00.050.951 I llm_load_print_meta: vocab_only       = 0
0.00.050.952 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.952 I llm_load_print_meta: n_embd           = 2048
0.00.050.952 I llm_load_print_meta: n_layer          = 24
0.00.050.966 I llm_load_print_meta: n_head           = 16
0.00.050.967 I llm_load_print_meta: n_head_kv        = 16
0.00.050.967 I llm_load_print_meta: n_rot            = 32
0.00.050.967 I llm_load_print_meta: n_swa            = 0
0.00.050.968 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.968 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.969 I llm_load_print_meta: n_gqa            = 1
0.00.050.969 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.970 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.971 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.971 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.971 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.971 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.972 I llm_load_print_meta: n_ff             = 8192
0.00.050.972 I llm_load_print_meta: n_expert         = 0
0.00.050.972 I llm_load_print_meta: n_expert_used    = 0
0.00.050.974 I llm_load_print_meta: causal attn      = 1
0.00.050.974 I llm_load_print_meta: pooling type     = 0
0.00.050.974 I llm_load_print_meta: rope type        = 2
0.00.050.975 I llm_load_print_meta: rope scaling     = linear
0.00.050.975 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.976 I llm_load_print_meta: freq_scale_train = 1
0.00.050.976 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.976 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.976 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.976 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.977 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.977 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.978 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.978 I llm_load_print_meta: model type       = 1.4B
0.00.050.979 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.979 I llm_load_print_meta: model params     = 1.41 B
0.00.050.980 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.980 I llm_load_print_meta: general.name     = 1.4B
0.00.050.980 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.980 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.980 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.980 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.981 I llm_load_print_meta: LF token         = 128 ''
0.00.050.981 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.982 I llm_load_print_meta: max token length = 1024
0.00.052.945 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.946 I llm_load_tensors: offloading output layer to GPU
0.00.052.946 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.956 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.958 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.934 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.935 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.935 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.935 I llama_new_context_with_model: n_batch       = 2048
0.00.053.935 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.935 I llama_new_context_with_model: flash_attn    = 0
0.00.053.936 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.936 I llama_new_context_with_model: freq_scale    = 1
0.00.053.936 I ggml_metal_init: allocating
0.00.053.939 I ggml_metal_init: found device: Apple M4
0.00.053.943 I ggml_metal_init: picking default device: Apple M4
0.00.054.527 I ggml_metal_init: using embedded metal library
0.00.056.918 I ggml_metal_init: GPU name:   Apple M4
0.00.056.919 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.920 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.920 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.920 I ggml_metal_init: simdgroup reduction   = true
0.00.056.920 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.920 I ggml_metal_init: has bfloat            = true
0.00.056.921 I ggml_metal_init: use bfloat            = true
0.00.056.921 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.922 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.006 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.011 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.030 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.091 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.093 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.093 I llama_new_context_with_model: graph nodes  = 967
0.00.088.093 I llama_new_context_with_model: graph splits = 2
0.00.088.109 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.237 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.238 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.451.069 I main: llama threadpool init, n_threads = 4
0.00.451.103 I 
0.00.451.137 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.451.138 I 
0.00.451.357 I sampler seed: 1234
0.00.451.362 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.451.376 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.451.378 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.451.378 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.136.061 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.136.062 I llama_perf_context_print:        load time =     441.07 ms
0.01.136.063 I llama_perf_context_print: prompt eval time =      39.66 ms /     7 tokens (    5.67 ms per token,   176.48 tokens per second)
0.01.136.063 I llama_perf_context_print:        eval time =     642.10 ms /    63 runs   (   10.19 ms per token,    98.12 tokens per second)
0.01.136.064 I llama_perf_context_print:       total time =     684.99 ms /    70 tokens
0.01.136.264 I ggml_metal_free: deallocating

real	0m1.153s
user	0m0.110s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.771 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.792 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.797 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.799 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.799 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.800 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.800 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.802 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.804 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.805 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.805 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.805 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.827 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.737 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.738 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.739 I llama_model_loader: - type  f32:  194 tensors
0.00.024.739 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.739 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.740 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.193 I llm_load_vocab: special tokens cache size = 25
0.00.051.175 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.178 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.178 I llm_load_print_meta: arch             = gptneox
0.00.051.179 I llm_load_print_meta: vocab type       = BPE
0.00.051.179 I llm_load_print_meta: n_vocab          = 50304
0.00.051.179 I llm_load_print_meta: n_merges         = 50009
0.00.051.179 I llm_load_print_meta: vocab_only       = 0
0.00.051.179 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.180 I llm_load_print_meta: n_embd           = 2048
0.00.051.180 I llm_load_print_meta: n_layer          = 24
0.00.051.194 I llm_load_print_meta: n_head           = 16
0.00.051.195 I llm_load_print_meta: n_head_kv        = 16
0.00.051.195 I llm_load_print_meta: n_rot            = 32
0.00.051.195 I llm_load_print_meta: n_swa            = 0
0.00.051.195 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.195 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.196 I llm_load_print_meta: n_gqa            = 1
0.00.051.199 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.200 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.201 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.201 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.201 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.202 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.202 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.203 I llm_load_print_meta: n_ff             = 8192
0.00.051.203 I llm_load_print_meta: n_expert         = 0
0.00.051.203 I llm_load_print_meta: n_expert_used    = 0
0.00.051.203 I llm_load_print_meta: causal attn      = 1
0.00.051.203 I llm_load_print_meta: pooling type     = 0
0.00.051.204 I llm_load_print_meta: rope type        = 2
0.00.051.204 I llm_load_print_meta: rope scaling     = linear
0.00.051.204 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.204 I llm_load_print_meta: freq_scale_train = 1
0.00.051.206 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.206 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.206 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.206 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.206 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.207 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.207 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.207 I llm_load_print_meta: model type       = 1.4B
0.00.051.207 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.208 I llm_load_print_meta: model params     = 1.41 B
0.00.051.208 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.209 I llm_load_print_meta: general.name     = 1.4B
0.00.051.209 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.210 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: LF token         = 128 ''
0.00.051.211 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: max token length = 1024
0.00.053.105 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.105 I llm_load_tensors: offloading output layer to GPU
0.00.053.105 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.116 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.117 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.024 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.025 I llama_new_context_with_model: n_ctx         = 128
0.00.054.025 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.025 I llama_new_context_with_model: n_batch       = 128
0.00.054.025 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.026 I llama_new_context_with_model: flash_attn    = 0
0.00.054.026 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.026 I llama_new_context_with_model: freq_scale    = 1
0.00.054.027 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.027 I ggml_metal_init: allocating
0.00.054.030 I ggml_metal_init: found device: Apple M4
0.00.054.032 I ggml_metal_init: picking default device: Apple M4
0.00.054.596 I ggml_metal_init: using embedded metal library
0.00.056.914 I ggml_metal_init: GPU name:   Apple M4
0.00.056.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.915 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.916 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.916 I ggml_metal_init: simdgroup reduction   = true
0.00.056.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.916 I ggml_metal_init: has bfloat            = true
0.00.056.916 I ggml_metal_init: use bfloat            = true
0.00.056.918 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.920 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.943 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.945 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.976 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.892 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.893 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.894 I llama_new_context_with_model: graph nodes  = 967
0.00.068.894 I llama_new_context_with_model: graph splits = 2
0.00.068.906 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.392.757 I 
0.00.392.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.392.810 I perplexity: tokenizing the input ..
0.00.400.626 I perplexity: tokenization took 7.815 ms
0.00.400.633 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.532.560 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.533.899 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.533.918 I llama_perf_context_print:        load time =     382.98 ms
0.00.533.919 I llama_perf_context_print: prompt eval time =     131.70 ms /   128 tokens (    1.03 ms per token,   971.91 tokens per second)
0.00.533.920 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.533.921 I llama_perf_context_print:       total time =     141.16 ms /   129 tokens
0.00.534.398 I ggml_metal_free: deallocating

real	0m0.550s
user	0m0.078s
sys	0m0.070s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.361 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.074 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.079 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.081 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.083 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.084 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.087 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.088 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.089 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.089 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.368 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.383 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.384 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.384 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.384 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.385 I llama_model_loader: - type  f32:  194 tensors
0.00.025.385 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.386 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.386 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.386 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.743 I llm_load_vocab: special tokens cache size = 25
0.00.052.678 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.681 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.681 I llm_load_print_meta: arch             = gptneox
0.00.052.681 I llm_load_print_meta: vocab type       = BPE
0.00.052.682 I llm_load_print_meta: n_vocab          = 50304
0.00.052.682 I llm_load_print_meta: n_merges         = 50009
0.00.052.682 I llm_load_print_meta: vocab_only       = 0
0.00.052.682 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.682 I llm_load_print_meta: n_embd           = 2048
0.00.052.683 I llm_load_print_meta: n_layer          = 24
0.00.052.696 I llm_load_print_meta: n_head           = 16
0.00.052.697 I llm_load_print_meta: n_head_kv        = 16
0.00.052.697 I llm_load_print_meta: n_rot            = 32
0.00.052.698 I llm_load_print_meta: n_swa            = 0
0.00.052.698 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.698 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.699 I llm_load_print_meta: n_gqa            = 1
0.00.052.700 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.702 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.704 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.704 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.704 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.704 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.706 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.707 I llm_load_print_meta: n_ff             = 8192
0.00.052.707 I llm_load_print_meta: n_expert         = 0
0.00.052.708 I llm_load_print_meta: n_expert_used    = 0
0.00.052.708 I llm_load_print_meta: causal attn      = 1
0.00.052.709 I llm_load_print_meta: pooling type     = 0
0.00.052.709 I llm_load_print_meta: rope type        = 2
0.00.052.709 I llm_load_print_meta: rope scaling     = linear
0.00.052.709 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.709 I llm_load_print_meta: freq_scale_train = 1
0.00.052.710 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.710 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.710 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.710 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.710 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.710 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.710 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.711 I llm_load_print_meta: model type       = 1.4B
0.00.052.711 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.711 I llm_load_print_meta: model params     = 1.41 B
0.00.052.712 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.712 I llm_load_print_meta: general.name     = 1.4B
0.00.052.712 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.712 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.714 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.714 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.714 I llm_load_print_meta: LF token         = 128 ''
0.00.052.715 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.715 I llm_load_print_meta: max token length = 1024
0.00.054.765 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.765 I llm_load_tensors: offloading output layer to GPU
0.00.054.765 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.776 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.777 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.703 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.704 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.704 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.704 I llama_new_context_with_model: n_batch       = 2048
0.00.055.705 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.705 I llama_new_context_with_model: flash_attn    = 0
0.00.055.707 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.707 I llama_new_context_with_model: freq_scale    = 1
0.00.055.707 I ggml_metal_init: allocating
0.00.055.711 I ggml_metal_init: found device: Apple M4
0.00.055.715 I ggml_metal_init: picking default device: Apple M4
0.00.056.395 I ggml_metal_init: using embedded metal library
0.00.058.867 I ggml_metal_init: GPU name:   Apple M4
0.00.058.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.870 I ggml_metal_init: simdgroup reduction   = true
0.00.058.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.870 I ggml_metal_init: has bfloat            = true
0.00.058.870 I ggml_metal_init: use bfloat            = true
0.00.058.871 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.872 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.267 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.275 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.301 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.266 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.267 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.267 I llama_new_context_with_model: graph nodes  = 967
0.00.089.268 I llama_new_context_with_model: graph splits = 2
0.00.089.280 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.919 I main: llama threadpool init, n_threads = 4
0.00.538.958 I 
0.00.538.989 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.538.989 I 
0.00.539.226 I sampler seed: 1234
0.00.539.229 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.539.272 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.539.273 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.539.273 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.289.145 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.289.146 I llama_perf_context_print:        load time =     529.55 ms
0.01.289.146 I llama_perf_context_print: prompt eval time =      43.50 ms /     7 tokens (    6.21 ms per token,   160.93 tokens per second)
0.01.289.147 I llama_perf_context_print:        eval time =     703.26 ms /    63 runs   (   11.16 ms per token,    89.58 tokens per second)
0.01.289.150 I llama_perf_context_print:       total time =     750.23 ms /    70 tokens
0.01.289.331 I ggml_metal_free: deallocating

real	0m1.307s
user	0m0.112s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.248 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.248 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.254 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.255 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.255 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.256 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.256 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.256 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.257 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.258 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.259 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.259 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.259 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.260 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.262 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.264 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.265 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.266 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.062 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.062 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.063 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.064 I llama_model_loader: - type  f32:  194 tensors
0.00.024.064 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.064 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.065 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.065 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.371 I llm_load_vocab: special tokens cache size = 25
0.00.051.322 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.325 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.325 I llm_load_print_meta: arch             = gptneox
0.00.051.326 I llm_load_print_meta: vocab type       = BPE
0.00.051.326 I llm_load_print_meta: n_vocab          = 50304
0.00.051.326 I llm_load_print_meta: n_merges         = 50009
0.00.051.326 I llm_load_print_meta: vocab_only       = 0
0.00.051.326 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.327 I llm_load_print_meta: n_embd           = 2048
0.00.051.329 I llm_load_print_meta: n_layer          = 24
0.00.051.344 I llm_load_print_meta: n_head           = 16
0.00.051.346 I llm_load_print_meta: n_head_kv        = 16
0.00.051.346 I llm_load_print_meta: n_rot            = 32
0.00.051.346 I llm_load_print_meta: n_swa            = 0
0.00.051.346 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.347 I llm_load_print_meta: n_gqa            = 1
0.00.051.348 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.348 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.349 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.349 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.349 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.349 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.351 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.352 I llm_load_print_meta: n_ff             = 8192
0.00.051.352 I llm_load_print_meta: n_expert         = 0
0.00.051.352 I llm_load_print_meta: n_expert_used    = 0
0.00.051.352 I llm_load_print_meta: causal attn      = 1
0.00.051.352 I llm_load_print_meta: pooling type     = 0
0.00.051.352 I llm_load_print_meta: rope type        = 2
0.00.051.352 I llm_load_print_meta: rope scaling     = linear
0.00.051.353 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.353 I llm_load_print_meta: freq_scale_train = 1
0.00.051.353 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.353 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.353 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.353 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.357 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.357 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.358 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.358 I llm_load_print_meta: model type       = 1.4B
0.00.051.358 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.359 I llm_load_print_meta: model params     = 1.41 B
0.00.051.359 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.359 I llm_load_print_meta: general.name     = 1.4B
0.00.051.359 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.360 I llm_load_print_meta: LF token         = 128 ''
0.00.051.361 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.361 I llm_load_print_meta: max token length = 1024
0.00.053.365 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.365 I llm_load_tensors: offloading output layer to GPU
0.00.053.365 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.376 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.377 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.248 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.249 I llama_new_context_with_model: n_ctx         = 128
0.00.054.249 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.249 I llama_new_context_with_model: n_batch       = 128
0.00.054.249 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.250 I llama_new_context_with_model: flash_attn    = 0
0.00.054.250 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.250 I llama_new_context_with_model: freq_scale    = 1
0.00.054.251 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.251 I ggml_metal_init: allocating
0.00.054.258 I ggml_metal_init: found device: Apple M4
0.00.054.260 I ggml_metal_init: picking default device: Apple M4
0.00.054.842 I ggml_metal_init: using embedded metal library
0.00.057.189 I ggml_metal_init: GPU name:   Apple M4
0.00.057.191 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.192 I ggml_metal_init: simdgroup reduction   = true
0.00.057.192 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.192 I ggml_metal_init: has bfloat            = true
0.00.057.193 I ggml_metal_init: use bfloat            = true
0.00.057.193 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.194 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.638 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.647 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.662 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.580 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.581 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.582 I llama_new_context_with_model: graph nodes  = 967
0.00.069.582 I llama_new_context_with_model: graph splits = 2
0.00.069.595 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.484.907 I 
0.00.484.944 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.484.954 I perplexity: tokenizing the input ..
0.00.492.254 I perplexity: tokenization took 7.299 ms
0.00.492.258 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.624.725 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.625.953 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.625.976 I llama_perf_context_print:        load time =     475.65 ms
0.00.625.977 I llama_perf_context_print: prompt eval time =     132.23 ms /   128 tokens (    1.03 ms per token,   968.00 tokens per second)
0.00.625.978 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.625.981 I llama_perf_context_print:       total time =     141.07 ms /   129 tokens
0.00.626.542 I ggml_metal_free: deallocating

real	0m0.641s
user	0m0.079s
sys	0m0.086s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.010.319 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.045 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.046 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.046 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.046 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.047 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.049 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.049 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.027 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.028 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.028 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.029 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.029 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.029 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.030 I llama_model_loader: - type  f32:  194 tensors
0.00.026.030 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.031 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.031 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.363 I llm_load_vocab: special tokens cache size = 25
0.00.053.262 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.265 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.266 I llm_load_print_meta: arch             = gptneox
0.00.053.266 I llm_load_print_meta: vocab type       = BPE
0.00.053.266 I llm_load_print_meta: n_vocab          = 50304
0.00.053.266 I llm_load_print_meta: n_merges         = 50009
0.00.053.267 I llm_load_print_meta: vocab_only       = 0
0.00.053.267 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.267 I llm_load_print_meta: n_embd           = 2048
0.00.053.267 I llm_load_print_meta: n_layer          = 24
0.00.053.276 I llm_load_print_meta: n_head           = 16
0.00.053.277 I llm_load_print_meta: n_head_kv        = 16
0.00.053.277 I llm_load_print_meta: n_rot            = 32
0.00.053.279 I llm_load_print_meta: n_swa            = 0
0.00.053.279 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.280 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.280 I llm_load_print_meta: n_gqa            = 1
0.00.053.281 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.282 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.282 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.283 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.283 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.283 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.283 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.287 I llm_load_print_meta: n_ff             = 8192
0.00.053.288 I llm_load_print_meta: n_expert         = 0
0.00.053.288 I llm_load_print_meta: n_expert_used    = 0
0.00.053.288 I llm_load_print_meta: causal attn      = 1
0.00.053.288 I llm_load_print_meta: pooling type     = 0
0.00.053.288 I llm_load_print_meta: rope type        = 2
0.00.053.288 I llm_load_print_meta: rope scaling     = linear
0.00.053.289 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.289 I llm_load_print_meta: freq_scale_train = 1
0.00.053.289 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.289 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.290 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.290 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.290 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.290 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.290 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.290 I llm_load_print_meta: model type       = 1.4B
0.00.053.291 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.291 I llm_load_print_meta: model params     = 1.41 B
0.00.053.291 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.292 I llm_load_print_meta: general.name     = 1.4B
0.00.053.292 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.292 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.292 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.292 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.293 I llm_load_print_meta: LF token         = 128 ''
0.00.053.293 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.293 I llm_load_print_meta: max token length = 1024
0.00.055.125 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.126 I llm_load_tensors: offloading output layer to GPU
0.00.055.126 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.131 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.132 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.163 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.163 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.164 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.164 I llama_new_context_with_model: n_batch       = 2048
0.00.056.164 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.164 I llama_new_context_with_model: flash_attn    = 0
0.00.056.165 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.165 I llama_new_context_with_model: freq_scale    = 1
0.00.056.165 I ggml_metal_init: allocating
0.00.056.172 I ggml_metal_init: found device: Apple M4
0.00.056.175 I ggml_metal_init: picking default device: Apple M4
0.00.056.780 I ggml_metal_init: using embedded metal library
0.00.059.160 I ggml_metal_init: GPU name:   Apple M4
0.00.059.162 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.163 I ggml_metal_init: simdgroup reduction   = true
0.00.059.163 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.163 I ggml_metal_init: has bfloat            = true
0.00.059.163 I ggml_metal_init: use bfloat            = true
0.00.059.164 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.164 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.312 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.317 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.334 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.362 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.364 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.364 I llama_new_context_with_model: graph nodes  = 967
0.00.091.364 I llama_new_context_with_model: graph splits = 2
0.00.091.379 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.520 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.521 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.972 I main: llama threadpool init, n_threads = 4
0.00.618.008 I 
0.00.618.034 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.034 I 
0.00.618.252 I sampler seed: 1234
0.00.618.257 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.618.294 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.618.307 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.618.308 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.383.375 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.01.383.375 I llama_perf_context_print:        load time =     607.65 ms
0.01.383.376 I llama_perf_context_print: prompt eval time =      50.98 ms /     7 tokens (    7.28 ms per token,   137.30 tokens per second)
0.01.383.377 I llama_perf_context_print:        eval time =     710.98 ms /    63 runs   (   11.29 ms per token,    88.61 tokens per second)
0.01.383.378 I llama_perf_context_print:       total time =     765.40 ms /    70 tokens
0.01.383.557 I ggml_metal_free: deallocating

real	0m1.400s
user	0m0.111s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.809 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.849 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.854 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.856 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.856 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.856 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.857 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.857 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.858 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.858 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.859 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.859 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.859 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.860 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.860 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.861 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.862 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.862 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.922 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.079 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.080 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.080 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.081 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.081 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.081 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.082 I llama_model_loader: - type  f32:  194 tensors
0.00.024.082 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.082 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.082 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.227 I llm_load_vocab: special tokens cache size = 25
0.00.051.218 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.221 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.221 I llm_load_print_meta: arch             = gptneox
0.00.051.221 I llm_load_print_meta: vocab type       = BPE
0.00.051.222 I llm_load_print_meta: n_vocab          = 50304
0.00.051.222 I llm_load_print_meta: n_merges         = 50009
0.00.051.222 I llm_load_print_meta: vocab_only       = 0
0.00.051.222 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.223 I llm_load_print_meta: n_embd           = 2048
0.00.051.223 I llm_load_print_meta: n_layer          = 24
0.00.051.237 I llm_load_print_meta: n_head           = 16
0.00.051.237 I llm_load_print_meta: n_head_kv        = 16
0.00.051.238 I llm_load_print_meta: n_rot            = 32
0.00.051.238 I llm_load_print_meta: n_swa            = 0
0.00.051.238 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.238 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.239 I llm_load_print_meta: n_gqa            = 1
0.00.051.240 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.243 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.244 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.244 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.244 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.244 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.245 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.245 I llm_load_print_meta: n_ff             = 8192
0.00.051.245 I llm_load_print_meta: n_expert         = 0
0.00.051.245 I llm_load_print_meta: n_expert_used    = 0
0.00.051.245 I llm_load_print_meta: causal attn      = 1
0.00.051.246 I llm_load_print_meta: pooling type     = 0
0.00.051.246 I llm_load_print_meta: rope type        = 2
0.00.051.246 I llm_load_print_meta: rope scaling     = linear
0.00.051.246 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.247 I llm_load_print_meta: freq_scale_train = 1
0.00.051.247 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.247 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.247 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.248 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.248 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.249 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.249 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.249 I llm_load_print_meta: model type       = 1.4B
0.00.051.249 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.250 I llm_load_print_meta: model params     = 1.41 B
0.00.051.250 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.250 I llm_load_print_meta: general.name     = 1.4B
0.00.051.250 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.251 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.251 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.251 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.251 I llm_load_print_meta: LF token         = 128 ''
0.00.051.251 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.251 I llm_load_print_meta: max token length = 1024
0.00.053.293 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.293 I llm_load_tensors: offloading output layer to GPU
0.00.053.294 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.304 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.305 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.255 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.256 I llama_new_context_with_model: n_ctx         = 128
0.00.054.256 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.256 I llama_new_context_with_model: n_batch       = 128
0.00.054.256 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.256 I llama_new_context_with_model: flash_attn    = 0
0.00.054.257 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.257 I llama_new_context_with_model: freq_scale    = 1
0.00.054.258 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.258 I ggml_metal_init: allocating
0.00.054.263 I ggml_metal_init: found device: Apple M4
0.00.054.266 I ggml_metal_init: picking default device: Apple M4
0.00.054.820 I ggml_metal_init: using embedded metal library
0.00.057.208 I ggml_metal_init: GPU name:   Apple M4
0.00.057.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.210 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.211 I ggml_metal_init: simdgroup reduction   = true
0.00.057.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.212 I ggml_metal_init: has bfloat            = true
0.00.057.212 I ggml_metal_init: use bfloat            = true
0.00.057.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.213 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.968 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.971 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.984 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.874 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.875 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.876 I llama_new_context_with_model: graph nodes  = 967
0.00.068.876 I llama_new_context_with_model: graph splits = 2
0.00.068.888 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.563.944 I 
0.00.563.997 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.009 I perplexity: tokenizing the input ..
0.00.571.593 I perplexity: tokenization took 7.581 ms
0.00.571.597 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.056 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.706.580 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.706.596 I llama_perf_context_print:        load time =     555.13 ms
0.00.706.597 I llama_perf_context_print: prompt eval time =     133.23 ms /   128 tokens (    1.04 ms per token,   960.76 tokens per second)
0.00.706.598 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.706.602 I llama_perf_context_print:       total time =     142.66 ms /   129 tokens
0.00.706.952 I ggml_metal_free: deallocating

real	0m0.720s
user	0m0.079s
sys	0m0.090s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.639 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.239 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.241 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.241 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.242 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.244 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.245 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.245 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.249 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.250 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.250 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.250 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.253 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.255 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.282 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.366 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.368 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.368 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.368 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.369 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.369 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.369 I llama_model_loader: - type  f32:  194 tensors
0.00.024.370 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.370 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.875 I llm_load_vocab: special tokens cache size = 25
0.00.050.770 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.773 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.773 I llm_load_print_meta: arch             = gptneox
0.00.050.774 I llm_load_print_meta: vocab type       = BPE
0.00.050.774 I llm_load_print_meta: n_vocab          = 50304
0.00.050.774 I llm_load_print_meta: n_merges         = 50009
0.00.050.774 I llm_load_print_meta: vocab_only       = 0
0.00.050.775 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.775 I llm_load_print_meta: n_embd           = 2048
0.00.050.775 I llm_load_print_meta: n_layer          = 24
0.00.050.789 I llm_load_print_meta: n_head           = 16
0.00.050.791 I llm_load_print_meta: n_head_kv        = 16
0.00.050.791 I llm_load_print_meta: n_rot            = 32
0.00.050.791 I llm_load_print_meta: n_swa            = 0
0.00.050.791 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.791 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.792 I llm_load_print_meta: n_gqa            = 1
0.00.050.794 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.795 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.795 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.796 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.796 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.796 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.796 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.797 I llm_load_print_meta: n_ff             = 8192
0.00.050.797 I llm_load_print_meta: n_expert         = 0
0.00.050.797 I llm_load_print_meta: n_expert_used    = 0
0.00.050.799 I llm_load_print_meta: causal attn      = 1
0.00.050.800 I llm_load_print_meta: pooling type     = 0
0.00.050.800 I llm_load_print_meta: rope type        = 2
0.00.050.800 I llm_load_print_meta: rope scaling     = linear
0.00.050.801 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.801 I llm_load_print_meta: freq_scale_train = 1
0.00.050.801 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.802 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.802 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.802 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.803 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.803 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.803 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.803 I llm_load_print_meta: model type       = 1.4B
0.00.050.803 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.804 I llm_load_print_meta: model params     = 1.41 B
0.00.050.804 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.804 I llm_load_print_meta: general.name     = 1.4B
0.00.050.805 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.805 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.805 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.805 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.805 I llm_load_print_meta: LF token         = 128 ''
0.00.050.806 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.806 I llm_load_print_meta: max token length = 1024
0.00.052.856 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.856 I llm_load_tensors: offloading output layer to GPU
0.00.052.856 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.866 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.867 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.803 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.803 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.804 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.804 I llama_new_context_with_model: n_batch       = 2048
0.00.053.804 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.804 I llama_new_context_with_model: flash_attn    = 0
0.00.053.805 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.805 I llama_new_context_with_model: freq_scale    = 1
0.00.053.805 I ggml_metal_init: allocating
0.00.053.808 I ggml_metal_init: found device: Apple M4
0.00.053.810 I ggml_metal_init: picking default device: Apple M4
0.00.054.416 I ggml_metal_init: using embedded metal library
0.00.056.739 I ggml_metal_init: GPU name:   Apple M4
0.00.056.740 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.742 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.742 I ggml_metal_init: simdgroup reduction   = true
0.00.056.743 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.743 I ggml_metal_init: has bfloat            = true
0.00.056.743 I ggml_metal_init: use bfloat            = true
0.00.056.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.258 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.278 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.302 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.303 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.303 I llama_new_context_with_model: graph nodes  = 967
0.00.087.304 I llama_new_context_with_model: graph splits = 2
0.00.087.319 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.463 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.464 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.310 I main: llama threadpool init, n_threads = 4
0.00.689.348 I 
0.00.689.396 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.397 I 
0.00.689.633 I sampler seed: 1234
0.00.689.637 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.653 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.655 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.655 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.542.855 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.542.856 I llama_perf_context_print:        load time =     680.67 ms
0.01.542.857 I llama_perf_context_print: prompt eval time =      55.55 ms /     7 tokens (    7.94 ms per token,   126.01 tokens per second)
0.01.542.857 I llama_perf_context_print:        eval time =     794.55 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.542.859 I llama_perf_context_print:       total time =     853.55 ms /    70 tokens
0.01.543.053 I ggml_metal_free: deallocating

real	0m1.559s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.161 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.502 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.507 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.512 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.513 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.513 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.515 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.515 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.516 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.518 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.609 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.597 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.598 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.599 I llama_model_loader: - type  f32:  194 tensors
0.00.025.599 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.599 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.025 I llm_load_vocab: special tokens cache size = 25
0.00.053.115 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.119 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.119 I llm_load_print_meta: arch             = gptneox
0.00.053.119 I llm_load_print_meta: vocab type       = BPE
0.00.053.120 I llm_load_print_meta: n_vocab          = 50304
0.00.053.120 I llm_load_print_meta: n_merges         = 50009
0.00.053.120 I llm_load_print_meta: vocab_only       = 0
0.00.053.120 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.122 I llm_load_print_meta: n_embd           = 2048
0.00.053.122 I llm_load_print_meta: n_layer          = 24
0.00.053.138 I llm_load_print_meta: n_head           = 16
0.00.053.140 I llm_load_print_meta: n_head_kv        = 16
0.00.053.140 I llm_load_print_meta: n_rot            = 32
0.00.053.140 I llm_load_print_meta: n_swa            = 0
0.00.053.140 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.140 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.141 I llm_load_print_meta: n_gqa            = 1
0.00.053.142 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.142 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.142 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.143 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.143 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.143 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.143 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.144 I llm_load_print_meta: n_ff             = 8192
0.00.053.145 I llm_load_print_meta: n_expert         = 0
0.00.053.145 I llm_load_print_meta: n_expert_used    = 0
0.00.053.145 I llm_load_print_meta: causal attn      = 1
0.00.053.145 I llm_load_print_meta: pooling type     = 0
0.00.053.145 I llm_load_print_meta: rope type        = 2
0.00.053.146 I llm_load_print_meta: rope scaling     = linear
0.00.053.146 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.146 I llm_load_print_meta: freq_scale_train = 1
0.00.053.146 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.147 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.147 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.147 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.147 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.147 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.147 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.147 I llm_load_print_meta: model type       = 1.4B
0.00.053.148 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.148 I llm_load_print_meta: model params     = 1.41 B
0.00.053.149 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.149 I llm_load_print_meta: general.name     = 1.4B
0.00.053.149 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.149 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.149 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.149 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.151 I llm_load_print_meta: LF token         = 128 ''
0.00.053.151 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.152 I llm_load_print_meta: max token length = 1024
0.00.055.269 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.269 I llm_load_tensors: offloading output layer to GPU
0.00.055.269 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.281 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.282 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.258 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.259 I llama_new_context_with_model: n_ctx         = 128
0.00.056.259 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.259 I llama_new_context_with_model: n_batch       = 128
0.00.056.259 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.259 I llama_new_context_with_model: flash_attn    = 0
0.00.056.260 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.260 I llama_new_context_with_model: freq_scale    = 1
0.00.056.260 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.261 I ggml_metal_init: allocating
0.00.056.265 I ggml_metal_init: found device: Apple M4
0.00.056.269 I ggml_metal_init: picking default device: Apple M4
0.00.056.876 I ggml_metal_init: using embedded metal library
0.00.059.290 I ggml_metal_init: GPU name:   Apple M4
0.00.059.292 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.292 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.293 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.293 I ggml_metal_init: simdgroup reduction   = true
0.00.059.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.294 I ggml_metal_init: has bfloat            = true
0.00.059.294 I ggml_metal_init: use bfloat            = true
0.00.059.294 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.634 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.640 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.657 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.618 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.619 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.620 I llama_new_context_with_model: graph nodes  = 967
0.00.070.620 I llama_new_context_with_model: graph splits = 2
0.00.070.633 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.492 I 
0.00.623.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.623.539 I perplexity: tokenizing the input ..
0.00.631.168 I perplexity: tokenization took 7.627 ms
0.00.631.172 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.770.939 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.772.271 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.772.291 I llama_perf_context_print:        load time =     613.33 ms
0.00.772.293 I llama_perf_context_print: prompt eval time =     139.53 ms /   128 tokens (    1.09 ms per token,   917.35 tokens per second)
0.00.772.293 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.772.294 I llama_perf_context_print:       total time =     148.80 ms /   129 tokens
0.00.772.667 I ggml_metal_free: deallocating

real	0m0.789s
user	0m0.080s
sys	0m0.095s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.170 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.473 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.484 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.484 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.485 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.487 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.487 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.488 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.488 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.489 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.489 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.489 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.490 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.490 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.492 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.492 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.520 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.688 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.702 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.702 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.703 I llama_model_loader: - type  f32:  194 tensors
0.00.026.703 I llama_model_loader: - type q6_K:   98 tensors
0.00.048.068 I llm_load_vocab: special tokens cache size = 25
0.00.054.032 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.035 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.035 I llm_load_print_meta: arch             = gptneox
0.00.054.035 I llm_load_print_meta: vocab type       = BPE
0.00.054.036 I llm_load_print_meta: n_vocab          = 50304
0.00.054.036 I llm_load_print_meta: n_merges         = 50009
0.00.054.036 I llm_load_print_meta: vocab_only       = 0
0.00.054.036 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.036 I llm_load_print_meta: n_embd           = 2048
0.00.054.037 I llm_load_print_meta: n_layer          = 24
0.00.054.051 I llm_load_print_meta: n_head           = 16
0.00.054.052 I llm_load_print_meta: n_head_kv        = 16
0.00.054.053 I llm_load_print_meta: n_rot            = 32
0.00.054.053 I llm_load_print_meta: n_swa            = 0
0.00.054.053 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.053 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.054 I llm_load_print_meta: n_gqa            = 1
0.00.054.055 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.059 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.060 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.060 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.060 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.060 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.060 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.061 I llm_load_print_meta: n_ff             = 8192
0.00.054.061 I llm_load_print_meta: n_expert         = 0
0.00.054.061 I llm_load_print_meta: n_expert_used    = 0
0.00.054.063 I llm_load_print_meta: causal attn      = 1
0.00.054.063 I llm_load_print_meta: pooling type     = 0
0.00.054.063 I llm_load_print_meta: rope type        = 2
0.00.054.063 I llm_load_print_meta: rope scaling     = linear
0.00.054.063 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.064 I llm_load_print_meta: freq_scale_train = 1
0.00.054.064 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.064 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.064 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.064 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.064 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.064 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.065 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.065 I llm_load_print_meta: model type       = 1.4B
0.00.054.065 I llm_load_print_meta: model ftype      = Q6_K
0.00.054.065 I llm_load_print_meta: model params     = 1.41 B
0.00.054.066 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.054.066 I llm_load_print_meta: general.name     = 1.4B
0.00.054.066 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.066 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.066 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.067 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.067 I llm_load_print_meta: LF token         = 128 ''
0.00.054.067 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.067 I llm_load_print_meta: max token length = 1024
0.00.056.161 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.162 I llm_load_tensors: offloading output layer to GPU
0.00.056.162 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.173 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.056.174 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.057.155 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.156 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.156 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.156 I llama_new_context_with_model: n_batch       = 2048
0.00.057.157 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.157 I llama_new_context_with_model: flash_attn    = 0
0.00.057.157 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.158 I llama_new_context_with_model: freq_scale    = 1
0.00.057.158 I ggml_metal_init: allocating
0.00.057.165 I ggml_metal_init: found device: Apple M4
0.00.057.168 I ggml_metal_init: picking default device: Apple M4
0.00.057.765 I ggml_metal_init: using embedded metal library
0.00.060.085 I ggml_metal_init: GPU name:   Apple M4
0.00.060.086 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.086 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.087 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.087 I ggml_metal_init: simdgroup reduction   = true
0.00.060.087 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.087 I ggml_metal_init: has bfloat            = true
0.00.060.087 I ggml_metal_init: use bfloat            = true
0.00.060.088 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.089 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.176 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.185 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.204 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.162 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.163 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.163 I llama_new_context_with_model: graph nodes  = 967
0.00.091.163 I llama_new_context_with_model: graph splits = 2
0.00.091.172 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.289 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.860 I main: llama threadpool init, n_threads = 4
0.00.753.896 I 
0.00.753.942 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.943 I 
0.00.754.165 I sampler seed: 1234
0.00.754.170 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.223 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.225 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.226 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.640.807 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50969.13 tokens per second)
0.01.640.808 I llama_perf_context_print:        load time =     743.69 ms
0.01.640.809 I llama_perf_context_print: prompt eval time =      54.41 ms /     7 tokens (    7.77 ms per token,   128.65 tokens per second)
0.01.640.809 I llama_perf_context_print:        eval time =     829.45 ms /    63 runs   (   13.17 ms per token,    75.95 tokens per second)
0.01.640.810 I llama_perf_context_print:       total time =     886.95 ms /    70 tokens
0.01.641.009 I ggml_metal_free: deallocating

real	0m1.658s
user	0m0.112s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4354 (0e70ba68) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.713 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.625 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.632 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.635 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.636 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.636 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.637 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.637 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.638 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.641 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.642 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.642 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.560 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.710 I llama_model_loader: - type  f32:  194 tensors
0.00.023.710 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.930 I llm_load_vocab: special tokens cache size = 25
0.00.050.818 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.821 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.821 I llm_load_print_meta: arch             = gptneox
0.00.050.822 I llm_load_print_meta: vocab type       = BPE
0.00.050.822 I llm_load_print_meta: n_vocab          = 50304
0.00.050.822 I llm_load_print_meta: n_merges         = 50009
0.00.050.823 I llm_load_print_meta: vocab_only       = 0
0.00.050.823 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.823 I llm_load_print_meta: n_embd           = 2048
0.00.050.823 I llm_load_print_meta: n_layer          = 24
0.00.050.838 I llm_load_print_meta: n_head           = 16
0.00.050.840 I llm_load_print_meta: n_head_kv        = 16
0.00.050.840 I llm_load_print_meta: n_rot            = 32
0.00.050.840 I llm_load_print_meta: n_swa            = 0
0.00.050.840 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.840 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.844 I llm_load_print_meta: n_gqa            = 1
0.00.050.845 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.845 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.845 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.846 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.846 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.846 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.846 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.847 I llm_load_print_meta: n_ff             = 8192
0.00.050.847 I llm_load_print_meta: n_expert         = 0
0.00.050.847 I llm_load_print_meta: n_expert_used    = 0
0.00.050.847 I llm_load_print_meta: causal attn      = 1
0.00.050.848 I llm_load_print_meta: pooling type     = 0
0.00.050.848 I llm_load_print_meta: rope type        = 2
0.00.050.848 I llm_load_print_meta: rope scaling     = linear
0.00.050.848 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.849 I llm_load_print_meta: freq_scale_train = 1
0.00.050.849 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.849 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.849 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.849 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.849 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.849 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.850 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.850 I llm_load_print_meta: model type       = 1.4B
0.00.050.850 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.851 I llm_load_print_meta: model params     = 1.41 B
0.00.050.851 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.851 I llm_load_print_meta: general.name     = 1.4B
0.00.050.851 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.852 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.852 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.852 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.852 I llm_load_print_meta: LF token         = 128 ''
0.00.050.852 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.854 I llm_load_print_meta: max token length = 1024
0.00.052.564 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.564 I llm_load_tensors: offloading output layer to GPU
0.00.052.564 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.574 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.575 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.452 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.453 I llama_new_context_with_model: n_ctx         = 128
0.00.053.453 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.453 I llama_new_context_with_model: n_batch       = 128
0.00.053.453 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.453 I llama_new_context_with_model: flash_attn    = 0
0.00.053.454 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.454 I llama_new_context_with_model: freq_scale    = 1
0.00.053.454 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.455 I ggml_metal_init: allocating
0.00.053.458 I ggml_metal_init: found device: Apple M4
0.00.053.460 I ggml_metal_init: picking default device: Apple M4
0.00.054.077 I ggml_metal_init: using embedded metal library
0.00.056.561 I ggml_metal_init: GPU name:   Apple M4
0.00.056.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.563 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.564 I ggml_metal_init: simdgroup reduction   = true
0.00.056.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.564 I ggml_metal_init: has bfloat            = true
0.00.056.564 I ggml_metal_init: use bfloat            = true
0.00.056.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.105 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.108 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.123 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.045 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.046 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.047 I llama_new_context_with_model: graph nodes  = 967
0.00.069.047 I llama_new_context_with_model: graph splits = 2
0.00.069.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.061 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.275.182 I 
0.00.275.234 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.275.251 I perplexity: tokenizing the input ..
0.00.283.046 I perplexity: tokenization took 7.794 ms
0.00.283.053 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.423.431 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.424.618 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.424.629 I llama_perf_context_print:        load time =     266.46 ms
0.00.424.630 I llama_perf_context_print: prompt eval time =     140.10 ms /   128 tokens (    1.09 ms per token,   913.61 tokens per second)
0.00.424.631 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.424.631 I llama_perf_context_print:       total time =     149.45 ms /   129 tokens
0.00.425.021 I ggml_metal_free: deallocating

real	0m0.439s
user	0m0.081s
sys	0m0.052s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4354 (0e70ba68)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b607590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b607ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b608250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b608800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b608db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b609360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b609910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b609ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b60a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b60a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b60ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b60b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b60be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b60c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b60ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b60d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b60dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b60e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b60ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b60f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b60f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b6100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b610800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b6110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b6117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b611a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b612090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b612d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b613240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b613500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b6139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b613c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b6144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b614a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b614cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b615190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b615630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b615ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b615f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b616410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b6168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b616d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b6171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b617950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b617f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b618e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b6194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b619ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b61a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b61a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b61ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b61b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b61bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b61bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b61c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b61c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b61ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b61d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b61d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b61dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b61e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b61e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b61ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b61eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b61f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b61f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b61fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b620140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b6205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b620f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b6219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b621f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b6229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b622f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b623450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b6239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b623ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b624ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b625430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b625980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b626420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b626970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b627410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b627960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b627eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b628950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b628ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b618b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b629310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b629ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b62a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b62a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b62aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b62b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b62b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b62baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b62bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b62c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b62ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b62cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b62d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b62da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b62dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b62e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b62e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b62edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b62f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b62f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b62fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b630030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b6304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b630970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b630e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b6312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b631750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b631bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b632090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b632530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b6329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b632e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b633310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b6337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b633c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b6340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b634a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b634ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b635370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b635810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b635cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b636150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b6365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b636a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b636f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b6373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b637870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b637d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b6381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b638650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b638af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b638f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b639430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b6398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b639d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b63a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b63a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b63ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b63aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b63b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b63b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b63bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b63c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b63c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b63cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b63d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b63d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b63d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b63de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b63e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b63e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b63ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b63f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b63f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b63f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b63fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b640330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b6407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b640c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b641110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b6415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b641a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b641ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b642830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b642cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b643170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b643610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b643ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b643f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b6443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b644890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b644d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b6451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b645720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b645c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b6461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b646710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b6469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b6475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b647c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b6483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b648890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b648b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b649160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b649f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b64a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b64a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b64ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b64b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b64ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b64bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b64c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b64ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b64cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b64d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b64da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b64df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b64e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b64ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b64ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b64f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b64fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b64ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b6504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b6509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b650f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b651490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b6519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b651f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b652480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b6529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b652f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b653470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b6539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b653f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b654460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b6549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b654f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b655450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b6559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b655ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b656440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b656990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b656ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b657430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b657980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b657ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b658420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b658970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b658ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b659410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b659960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b659eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b65a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b65a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b65aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b65b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b65b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b65be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b65c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b65c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b65ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b65d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b65d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b65de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b65e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b65e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b65ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b65f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b65f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b65fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b65fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b660810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b660cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b661150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b6615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b661a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b661f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b6623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b662920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b663040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b663760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b663e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b6645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b664860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b665310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b665920 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.145.246 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.145.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b608eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b609320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b609790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b609c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b60a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b60a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b60a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b606f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b6073a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b607810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b60b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b60b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b60be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b60c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b60ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b60d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b60dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b60e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b60ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b60f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b60f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b60fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b610710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b610e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b611550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b611810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b611ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b611f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b6123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b612820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b612d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b613230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b6136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b613960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b613dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b614240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b6147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b614ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b6151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b6156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b615ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b6160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b6165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b616aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b616fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b617410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b617880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b617cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b618160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b6185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b618a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b618eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b619320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b619790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b619c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b61a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b61a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b61ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b61b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b61b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b61bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b61c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b61c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b61cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b61d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b61d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b61d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b61de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b61e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b61e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b61ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b61f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b61f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13b61faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13b61fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13b620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13b620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13b620fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13b621530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13b621a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13b621fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13b622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13b622a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13b622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13b623510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13b623a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13b623fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13b624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13b624a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13b624fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13b6254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13b625a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13b625f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13b6264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13b626a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13b626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13b6274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13b627a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13b627f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13b6284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13b628a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13b628f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13b6294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13b629a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13b629f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13b62a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13b62a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13b62af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13b62b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13b62b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13b62bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13b62c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13b62c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b62ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b62d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b62d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b62dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b62e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b62e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b62ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b62eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b62f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b62f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b62fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b630150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b6305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b630a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b630f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b6313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b631870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b631d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b6321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b632650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b632af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b632f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b633430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b6338d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b633d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b634210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b6346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b634b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b634ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b635490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b635930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b635dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b636270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b636bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b637050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b6374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b637e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b6382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b638c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b6390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b639550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b6399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b639e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b63a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b63a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b63ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b63b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b63b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b63ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b63bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b63c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b63c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b63ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b63d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b63d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b63dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b63df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b63e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b63e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b63ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b63f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b63f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b63fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b63ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b640450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b6408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b641230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b6416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b641b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b642010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b6424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b642950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b642df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b643730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b643bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b644120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b644670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b644bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b645110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b6453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b6459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b645ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b646600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13b646df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13b647290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b647550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b647b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13b648170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b648960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b648e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b6492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b649740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b649ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b64a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b64a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b64aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b64b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b64b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b64bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b64c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b64c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b64cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b64d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b64d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b64deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b64e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b64e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b64eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b64f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b64f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b64fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b6503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b650930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b650e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b6513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b651920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b651e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b6523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b652910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b652e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b6533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b653900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b653e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b6543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b6548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b654e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b655390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b6558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b655e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b656380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b6568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b656e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b657370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b6578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b657e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b658360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b6588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b658e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b659350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b6598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b659df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b65a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b65a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b65ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b65b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b65b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b65bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b65c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b65c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13b65cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13b65d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b65d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b65daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b65df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b65e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b65e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b65ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b65f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b65f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b65fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b65fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b660490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b660930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b660dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b661320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b661a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b662160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b662880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b662fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b663260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13b663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b663d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b664320 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b609310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b609780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b609bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b60a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b60a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b60a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b60adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b60b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b60b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b60bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b60bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b60c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b60ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b60d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b60dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b60e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b60eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b60f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b60f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b6102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b6109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b6110c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b6117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b611ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b612590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b612a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b612e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b6132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b613750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b613bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b614030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b6144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b614910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b614bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b615040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b6154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b615920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b615d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b616670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b616ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b616f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b6173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b617830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b617ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b618110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b618580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b6189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b618e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b6192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b619740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b619bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b61a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b61a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b61a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b61ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b61b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b61b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b61bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b61bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b61c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b61c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b61cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b61d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b61d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b61d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b61de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b61e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b61e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b61eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b61f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b61f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b61f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13b61fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13b6201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13b620630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13b620aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13b620f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13b621380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13b6217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13b621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13b6220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13b622540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13b6229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13b622e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13b623290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13b623700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13b623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13b623fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13b624450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13b6248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13b624d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13b6251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13b625610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13b625a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13b625ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13b626360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13b6267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13b626c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13b6270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13b627520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13b627990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13b627e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13b628270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13b6286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13b628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13b628fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13b629430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13b6298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13b629d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13b62a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13b62a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13b62aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b62aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b62b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b62b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b62bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b62c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b62c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b62c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b62cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b62d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b62d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b62db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b62dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b62e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b62e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b62ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b62f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b62f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b62fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b62feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b630320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b630790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b630c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b631070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b6314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b631950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b631dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b632230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b6326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b632b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b632f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b6333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b633860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b633cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b634140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b6345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b634a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b634e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b635300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b635be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b6364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b636930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b636da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b637680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b637af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b637f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b6383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b638840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b638cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b639120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b639590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b639a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b639e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b63a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b63a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b63abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b63b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b63b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b63b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b63bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b63c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b63c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b63cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b63cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b63d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b63d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b63dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b63e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b63e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b63e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b63ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b63f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b63f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b63fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b640010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b640480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b6408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b640d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b6411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b641640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b641ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b641f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b642800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b642c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b6430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13b643550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13b6439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b643e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b6442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13b644710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b644b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b644ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b645460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b6458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b646050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b6464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b646930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b646da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b647210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b647680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b647af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b647f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b6483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b648840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b648cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b649120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b649590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b649a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b649e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b64a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b64a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b64abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b64b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b64b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b64b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b64bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b64c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b64c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b64cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b64cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b64d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b64d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b64dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b64e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b64e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b64e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b64ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b64f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b64f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b64fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b650010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b650480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b6508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b650d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b6511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b651640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b651ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b651f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b652800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b652c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b6530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b653550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b6539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b653e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b6542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b654710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b654b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b654ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b655460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b6558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13b655d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13b6561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b656620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b656a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b656f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b657370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b6577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b657c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b6580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b658530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b6589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b658e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b659280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b6596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b659fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b65a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b65af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b65b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b65bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b65c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13b65c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b65ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b65cec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.801s
user	0m0.294s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4354 (0e70ba68)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c70ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c70f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c70f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c70ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c710530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c710ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c711090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c711640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c711bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c7120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c7125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c712af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c713610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c713dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c7145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c714cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c715410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c715b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c716250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c716a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c717140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c717860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c718820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c718f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c719200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c719810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c71a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c71a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c71ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c71b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c71b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c71bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c71c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c71c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c71c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c71cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c71d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c71d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c71db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c71e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c71e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c71e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c71ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c71f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c71f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c71fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c720610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c720c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c721230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c721840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c721e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c722460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c722a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c723260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c723700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c723ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c723e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c724470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c724c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c724f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c7253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c725860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c725d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c7261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c726640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c726ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c726f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c727420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c7278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c727d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c728200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c7286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c728bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c729140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c729690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c729be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c72a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c72a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c72abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c72b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c72b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c72bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c72c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c72c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c72cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c72d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c72d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c72dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c72e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c72e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c72eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c72f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c72f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c72fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c7300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c730620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c720300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c730a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c731240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c731ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c732230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c732780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c732cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c733220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c733770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c733cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c734210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c734760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c734cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c735200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c735750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c735bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c736090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c736530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c7369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c736e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c737310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c7377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c737c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c7380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c738590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c738a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c738ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c739370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c739810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c73a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c73a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c73aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c73af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c73b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c73b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c73bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c73c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c73c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c73caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c73cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c73d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c73d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c73dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c73e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c73e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c73eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c73eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c73f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c73f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c73fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c740270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c740710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c740bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c741050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c7414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c741990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c741e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c7422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c742770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c742c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c7430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c743550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c7439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c743e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c744330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c7447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c744c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c745110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c7455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c745a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c745ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c746390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c746830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c746cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c747170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c747610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c747ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c747f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c7483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c748890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c748d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c7491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c749670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c749b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c749fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c74a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c74a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c74ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c74b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c74b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c74bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c74c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c74c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c74c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c74cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c74d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c74d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c74de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c74e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c74e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c74ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c74f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c74fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c750010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c7502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c7508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c750ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c7516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c751b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c752020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c7524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c752c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c7531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c753710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c753c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c7541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c754700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c754c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c7551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c7556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c755c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c756190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c7566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c756c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c757180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c7576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c757c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c758170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c7586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c758c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c759160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c7596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c759c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c75a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c75a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c75abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c75b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c75b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c75bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c75c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c75c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c75cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c75d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c75d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c75dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c75e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c75e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c75ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c75f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c75f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c75fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c7600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c760640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c760b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c7610e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c761630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c761b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c7620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c762620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c762b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c7630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c763610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c763b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c7640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c764600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c764b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c7650a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c7655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c765a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c765f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c7663d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c766870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c766d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c7671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c767650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c767af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c767f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c768430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c7688d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c768d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c769210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c7696b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c769b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c76a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c76a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c76aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c76b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c76bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c76bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c76c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c76ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c76d0a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.429 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c605a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c605ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c6064b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c606a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c607010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c6075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c607b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c608120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c6086d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c608bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c6090d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c6095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c60a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c60a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c60b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c60b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c60bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c60c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c60cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c60d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c60dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c60e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c60ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c60f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c60f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c60fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c610170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c610780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c610d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c611580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c611a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c611ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c612570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c612ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c612d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c613210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c6136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c613b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c613ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c614490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c614930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c614dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c615270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c615710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c6159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c615fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c6165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c617820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c617e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c618440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c618a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c619850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c619cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c61a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c61a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c61aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c61b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c61b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c61bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c61c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c61c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c61c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c61ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c61d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c61d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c61dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c61e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c61e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c61e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c61ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c61f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c61f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c61fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c6203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c620900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c620e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c6213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c6218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c621e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c622390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c6228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c622e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c6238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c623e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c624370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c6248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c624e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c625360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c6258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c625e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c626350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c6268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c626df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c627340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c627890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c628330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c628880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c628dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c629320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c629870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c629dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c62a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c62a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c62adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c62b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c62b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c62bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c62c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c62c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c62cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c62d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c62d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c62da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c62deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c62e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c62e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c62ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c62f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c62f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c62fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c62ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c6303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c630850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c630cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c631190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c631630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c631ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c631f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c632410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c6328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c632d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c6331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c633690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c633b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c633fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c634470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c634910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c634db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c635250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c6356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c635b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c6364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c636970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c636e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c6372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c637750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c637bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c638090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c638530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c6389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c638e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c639310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c6397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c639c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c63a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c63a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c63aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c63aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c63b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c63b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c63bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c63c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c63c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c63ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c63cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c63d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c63d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c63dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c63e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c63e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c63eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c63ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c63f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c63f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c63fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c640210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c6406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c640b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c640ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c641490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c641930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c641dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c642270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c642710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c642bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c6434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c643a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c643f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c6444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c644a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c644cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c645300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c645910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c645f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c646710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c646bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c646e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c647480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c647a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c648280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c648720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c648bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c649060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c649810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c649d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c64a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c64a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c64ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c64b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c64b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c64bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c64c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c64c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c64cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c64d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c64d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c64dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c64e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c64e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c64ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c64f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c64f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c64fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c650250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c6507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c650cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c651240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c651790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c651ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c652230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c652780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c652cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c653220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c653770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c653cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c654210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c654760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c654cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c655200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c655750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c655ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c6561f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c656740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c656c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c6571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c657730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c657c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c6581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c658720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c658c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c6591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c659710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c659c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c65a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c65a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c65ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c65b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c65b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c65bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c65c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c65c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c65cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c65cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c65d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c65d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c65dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c65e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c65e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c65eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c65efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c65f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c65f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c65fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c660250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c6606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c660c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c661a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c6621a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c6628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c662b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c663370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c663630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c663c40 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1398046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1398058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1398065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139807a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139808580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139808d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139809540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139809c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13980a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13980aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13980b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13980b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13980c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13980c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13980ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13980d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13980dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13980df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13980e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13980e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13980eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13980ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13980f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13980f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13980fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139810030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1398104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139810910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139810d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1398111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139811660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139811ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139811f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1398123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139812820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139812c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139813100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139813570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1398139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139813e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1398142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139814730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139814ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139815010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139815480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1398158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139815d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1398161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139816740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139816c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1398170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139817520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139817990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139817e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139818270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1398186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139818b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139818fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139819430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1398198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139819d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13981a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13981a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13981aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13981aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13981b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13981b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13981bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13981c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13981c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13981c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13981cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13981d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13981d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13981db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13981dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13981e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13981e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13981ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13981f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13981f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13981fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13981feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139820320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139820790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139820c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139821070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1398214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139821950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139821dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139822230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1398226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139822b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139822f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1398233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139823860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139823cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139824140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1398245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139824a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139824e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139825300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139825770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139825be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139826050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1398264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139826930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139826da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139827210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139827680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139827af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139827f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1398283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139828840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139828cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139829120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139829590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139829a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139829e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13982a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13982a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13982abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13982b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13982b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13982b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13982bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13982c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13982c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13982cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13982cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13982d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13982d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13982dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13982e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13982e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13982e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13982ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13982f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13982f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13982fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139830010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139830480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1398308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139830d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1398311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139831640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139831ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139831f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139832390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139832800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139832c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1398330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139833550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1398339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139833e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1398342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139834710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139834b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139834ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139835460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1398358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139835d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1398361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139836620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139836a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139836f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139837370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1398377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139837c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1398380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139838530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1398389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139838e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139839280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1398396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139839b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139839fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13983a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13983a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13983ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13983b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13983b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13983ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13983bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13983c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13983c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13983cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13983d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13983d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13983d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13983ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13983e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13983e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13983eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13983efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13983f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13983f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13983fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139840170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139840700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139840b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139840fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139841b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139841df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1398420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139842520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139842990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139842e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139843270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1398436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139843b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139843fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139844430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1398448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139844d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139845180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1398455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139845a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139845ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139846340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1398467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139846c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139847090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139847500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139847970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139847de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139848250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1398486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139848b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139848fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139849410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139849880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139849cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13984a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13984a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13984aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13984b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13984b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13984bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13984bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13984c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13984c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13984cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13984d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13984d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13984d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13984de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13984e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13984e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13984ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13984f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13984f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13984f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13984fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1398501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139850650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139850ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139850f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1398513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139851810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139851c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1398520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139852560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1398529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139852e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1398532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139853720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139853b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139854000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139854470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1398548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139854d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1398551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139855630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139855aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139856510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139856c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139857350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139857a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139857d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1398581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1398587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139858db0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.935s
user	0m0.246s
sys	0m0.148s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.57 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.61 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.17 sec*proc (2 tests)

Total Test time (real) =   1.18 sec
        1.20 real         0.75 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.36 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.62 sec*proc (2 tests)

Total Test time (real) =   0.63 sec
        0.63 real         0.15 user         0.05 sys
```
