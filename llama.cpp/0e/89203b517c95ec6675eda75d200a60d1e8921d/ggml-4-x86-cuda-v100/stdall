rm: cannot remove '/home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/*.log': No such file or directory
rm: cannot remove '/home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/*.exit': No such file or directory
rm: cannot remove '/home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/*.md': No such file or directory
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: numpy==1.24.4 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/requirements.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece==0.1.98 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/requirements.txt (line 2)) (0.1.98)
Requirement already satisfied: gguf>=0.1.0 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/requirements.txt (line 3)) (0.4.4)
Defaulting to user installation because normal site-packages is not writeable
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /home/ggml/.local/lib/python3.10/site-packages (from gguf==0.4.4) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.4.4-py3-none-any.whl size=2717 sha256=c0f27b2c25b9b61b4ea705104484e15c25dd1b9711346807ffdebb267916e2b8
  Stored in directory: /tmp/pip-ephem-wheel-cache-6mr8qtkn/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.4.4
    Uninstalling gguf-0.4.4:
      Successfully uninstalled gguf-0.4.4
Successfully installed gguf-0.4.4
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.6s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.802s
user	0m0.472s
sys	0m0.165s
+ tee -a /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/ctest_debug-make.log
+ make -j
[  1%] Built target BUILD_INFO
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  5%] Building C object CMakeFiles/ggml.dir/k_quants.c.o
[  5%] Built target ggml
[  7%] Linking C static library libggml_static.a
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Built target ggml_static
[  8%] Linking CXX static library libllama.a
[  8%] Built target llama
[ 10%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 11%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 13%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 13%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 16%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 19%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 20%] Linking CXX executable ../bin/test-c
[ 20%] Built target test-c
[ 21%] Linking CXX executable ../../bin/quantize
[ 21%] Built target quantize
[ 22%] Linking CXX executable ../../bin/benchmark
[ 22%] Built target benchmark
[ 23%] Linking CXX executable ../../bin/quantize-stats
[ 23%] Built target quantize-stats
[ 23%] Built target common
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 37%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 38%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 39%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 40%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 41%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 42%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 43%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 44%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 46%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 47%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 48%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 49%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 50%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 51%] Building CXX object examples/embd-input/CMakeFiles/embdinput.dir/embd-input-lib.cpp.o
[ 52%] Building CXX object examples/llava/CMakeFiles/clip.dir/clip.cpp.o
[ 53%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 55%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 56%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 57%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 58%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 59%] Linking CXX executable ../bin/test-grad0
[ 60%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-grammar-parser
[ 65%] Linking CXX executable ../bin/test-sampling
[ 65%] Built target test-grad0
[ 66%] Linking CXX executable ../../bin/baby-llama
[ 66%] Built target test-quantize-fns
[ 67%] Linking CXX executable ../../bin/q8dot
[ 67%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/vdot
[ 69%] Linking CXX executable ../../bin/embedding
[ 70%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 70%] Built target test-grammar-parser
[ 71%] Linking CXX executable ../../bin/simple
[ 71%] Built target test-sampling
[ 73%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 74%] Linking CXX executable ../../bin/save-load-state
[ 75%] Linking CXX executable ../bin/test-quantize-perf
[ 76%] Linking CXX executable ../../bin/batched-bench
[ 77%] Linking CXX executable ../../bin/beam-search
[ 78%] Linking CXX executable ../../bin/batched
[ 78%] Built target baby-llama
[ 78%] Built target simple
[ 78%] Built target q8dot
[ 79%] Linking CXX executable ../../bin/parallel
[ 80%] Linking CXX executable ../../bin/speculative
[ 80%] Built target embedding
[ 82%] Linking CXX static library libembdinput.a
[ 83%] Linking CXX executable ../../bin/train-text-from-scratch
[ 84%] Linking CXX executable ../../bin/perplexity
[ 84%] Built target vdot
[ 84%] Built target test-tokenizer-0-llama
[ 85%] Linking CXX executable ../../bin/export-lora
[ 86%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 86%] Built target embdinput
[ 87%] Linking CXX executable ../../bin/finetune
[ 87%] Built target batched
[ 88%] Building CXX object examples/embd-input/CMakeFiles/embd-input-test.dir/embd-input-test.cpp.o
[ 88%] Built target save-load-state
[ 88%] Built target speculative
[ 88%] Built target test-tokenizer-0-falcon
[ 88%] Built target test-quantize-perf
[ 89%] Linking CXX executable ../../bin/main
[ 89%] Built target beam-search
[ 89%] Built target batched-bench
[ 89%] Built target parallel
[ 89%] Built target perplexity
[ 89%] Built target train-text-from-scratch
[ 89%] Built target export-lora
[ 91%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 91%] Built target convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 92%] Built target finetune
[ 93%] Linking CXX executable ../../bin/embd-input-test
[ 93%] Built target main
[ 93%] Built target test-tokenizer-1-llama
[ 93%] Built target test-tokenizer-1-bpe
[ 93%] Built target embd-input-test
[ 94%] Linking CXX static library libclip.a
[ 94%] Built target clip
[ 95%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-bench
[ 97%] Linking CXX executable ../../bin/llava
[ 97%] Built target llama-bench
[ 97%] Built target llava
[ 98%] Linking CXX executable ../bin/test-llama-grammar
[ 98%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	0m34.326s
user	1m27.609s
sys	0m8.461s
+ tee -a /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/ctest_debug-ctest.log
+ ctest --output-on-failure -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-quantize-fns
 1/12 Test  #1: test-quantize-fns ................   Passed    0.04 sec
      Start  2: test-quantize-perf
 2/12 Test  #2: test-quantize-perf ...............   Passed    0.26 sec
      Start  3: test-sampling
 3/12 Test  #3: test-sampling ....................   Passed    0.00 sec
      Start  4: test-tokenizer-0-llama
 4/12 Test  #4: test-tokenizer-0-llama ...........   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/12 Test  #5: test-tokenizer-0-falcon ..........   Passed    0.83 sec
      Start  6: test-tokenizer-1-llama
 6/12 Test  #6: test-tokenizer-1-llama ...........   Passed   11.48 sec
      Start  7: test-tokenizer-1-falcon
 7/12 Test  #7: test-tokenizer-1-falcon ..........   Passed   10.70 sec
      Start  8: test-tokenizer-1-aquila
 8/12 Test  #8: test-tokenizer-1-aquila ..........   Passed   12.92 sec
      Start  9: test-grammar-parser
 9/12 Test  #9: test-grammar-parser ..............   Passed    0.00 sec
      Start 10: test-llama-grammar
10/12 Test #10: test-llama-grammar ...............   Passed    0.00 sec
      Start 11: test-grad0
11/12 Test #11: test-grad0 .......................   Passed    6.35 sec
      Start 12: test-rope
12/12 Test #12: test-rope ........................   Passed    0.08 sec

100% tests passed, 0 tests failed out of 12

Total Test time (real) =  42.76 sec

real	0m42.820s
user	0m43.182s
sys	0m7.204s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.4s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.590s
user	0m0.445s
sys	0m0.150s
+ tee -a /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/ctest_release-make.log
+ make -j
[  1%] Built target BUILD_INFO
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Building C object CMakeFiles/ggml.dir/k_quants.c.o
[  5%] Built target ggml
[  6%] Linking C static library libggml_static.a
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Built target ggml_static
[  8%] Linking CXX static library libllama.a
[  8%] Built target llama
[ 10%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 12%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 12%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 13%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 19%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 20%] Linking CXX executable ../bin/test-c
[ 20%] Built target test-c
[ 21%] Linking CXX executable ../../bin/quantize
[ 21%] Built target quantize
[ 22%] Linking CXX executable ../../bin/benchmark
[ 22%] Built target benchmark
[ 23%] Linking CXX executable ../../bin/quantize-stats
[ 23%] Built target quantize-stats
[ 23%] Built target common
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 37%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 39%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 39%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 40%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 41%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 42%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 43%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 44%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 46%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 47%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 48%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 49%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 50%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 51%] Building CXX object examples/embd-input/CMakeFiles/embdinput.dir/embd-input-lib.cpp.o
[ 52%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 53%] Building CXX object examples/llava/CMakeFiles/clip.dir/clip.cpp.o
[ 55%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 56%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 57%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 58%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 59%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 60%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Built target test-rope
[ 61%] Built target test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-grammar-parser
[ 64%] Linking CXX executable ../bin/test-sampling
[ 65%] Linking CXX executable ../../bin/q8dot
[ 65%] Built target test-grammar-parser
[ 66%] Linking CXX executable ../../bin/baby-llama
[ 66%] Built target test-sampling
[ 66%] Built target baby-llama
[ 66%] Built target q8dot
[ 67%] Linking CXX executable ../../bin/vdot
[ 68%] Linking CXX executable ../../bin/embedding
[ 68%] Built target embedding
[ 68%] Built target vdot
[ 69%] Linking CXX executable ../../bin/beam-search
[ 70%] Linking CXX executable ../../bin/save-load-state
[ 71%] Linking CXX executable ../bin/test-grad0
[ 73%] Linking CXX executable ../../bin/batched
[ 73%] Built target save-load-state
[ 73%] Built target beam-search
[ 74%] Linking CXX executable ../../bin/simple
[ 75%] Linking CXX executable ../../bin/batched-bench
[ 75%] Built target test-grad0
[ 75%] Built target batched
[ 76%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 76%] Built target simple
[ 77%] Linking CXX executable ../bin/test-quantize-perf
[ 77%] Built target batched-bench
[ 78%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 79%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 80%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 80%] Built target test-quantize-perf
[ 80%] Built target test-tokenizer-0-llama
[ 80%] Built target test-tokenizer-1-llama
[ 82%] Linking CXX executable ../../bin/export-lora
[ 82%] Built target test-tokenizer-1-bpe
[ 82%] Built target test-tokenizer-0-falcon
[ 83%] Linking CXX static library libembdinput.a
[ 83%] Built target export-lora
[ 84%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 85%] Linking CXX executable ../../bin/train-text-from-scratch
[ 85%] Built target embdinput
[ 86%] Linking CXX executable ../../bin/parallel
[ 87%] Linking CXX executable ../../bin/speculative
[ 88%] Building CXX object examples/embd-input/CMakeFiles/embd-input-test.dir/embd-input-test.cpp.o
[ 88%] Built target train-text-from-scratch
[ 88%] Built target convert-llama2c-to-ggml
[ 89%] Linking CXX executable ../../bin/perplexity
[ 89%] Built target speculative
[ 89%] Built target parallel
[ 89%] Built target perplexity
[ 91%] Linking CXX executable ../../bin/embd-input-test
[ 92%] Linking CXX executable ../../bin/finetune
[ 92%] Built target embd-input-test
[ 93%] Linking CXX executable ../../bin/main
[ 93%] Built target finetune
[ 93%] Built target main
[ 94%] Linking CXX executable ../../bin/llama-bench
[ 94%] Built target llama-bench
[ 95%] Linking CXX static library libclip.a
[ 95%] Built target clip
[ 96%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 97%] Linking CXX executable ../../bin/llava
[ 97%] Built target llava
[ 98%] Linking CXX executable ../bin/test-llama-grammar
[ 98%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	0m54.937s
user	2m6.565s
sys	0m6.246s
+ '[' -z ']'
+ tee -a /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/ctest_release-ctest.log
+ ctest --output-on-failure
Test project /home/ggml/work/llama.cpp/build-ci-release
      Start  1: test-quantize-fns
 1/12 Test  #1: test-quantize-fns ................   Passed    0.01 sec
      Start  2: test-quantize-perf
 2/12 Test  #2: test-quantize-perf ...............   Passed    0.05 sec
      Start  3: test-sampling
 3/12 Test  #3: test-sampling ....................   Passed    0.00 sec
      Start  4: test-tokenizer-0-llama
 4/12 Test  #4: test-tokenizer-0-llama ...........   Passed    0.03 sec
      Start  5: test-tokenizer-0-falcon
 5/12 Test  #5: test-tokenizer-0-falcon ..........   Passed    0.27 sec
      Start  6: test-tokenizer-1-llama
 6/12 Test  #6: test-tokenizer-1-llama ...........   Passed    3.10 sec
      Start  7: test-tokenizer-1-falcon
 7/12 Test  #7: test-tokenizer-1-falcon ..........   Passed    2.26 sec
      Start  8: test-tokenizer-1-aquila
 8/12 Test  #8: test-tokenizer-1-aquila ..........   Passed    2.82 sec
      Start  9: test-grammar-parser
 9/12 Test  #9: test-grammar-parser ..............   Passed    0.00 sec
      Start 10: test-llama-grammar
10/12 Test #10: test-llama-grammar ...............   Passed    0.00 sec
      Start 11: test-grad0
11/12 Test #11: test-grad0 .......................   Passed    6.38 sec
      Start 12: test-rope
12/12 Test #12: test-rope ........................   Passed    0.06 sec

100% tests passed, 0 tests failed out of 12

Total Test time (real) =  15.00 sec

real	0m15.031s
user	0m15.612s
sys	0m6.790s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_open_llama_7b_v2
+ tee /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2023-10-18 15:36:54 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json [502/502] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2023-10-18 15:36:55 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json [593/593] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2023-10-18 15:36:55 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json [330/330] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
Last-modified header missing -- time-stamps turned off.
2023-10-18 15:36:55 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json [26788/26788] -> "pytorch_model.bin.index.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
Last-modified header missing -- time-stamps turned off.
2023-10-18 15:36:57 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json [132/132] -> "generation_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
+ cd /home/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ path_models=../models-mnt/open-llama/7B-v2
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.91") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.91
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.1s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.260s
user	0m2.394s
sys	0m0.699s
+ tee -a /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-make.log
+ make -j
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Built target BUILD_INFO
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  6%] Building C object CMakeFiles/ggml.dir/k_quants.c.o
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_clamp(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, CUstream_st* const&)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:6525:20: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
 6525 |     const float min = ((float *) dst->op_params)[0];
      |                   ~^~~~~~~~~~~~~~~~~~~~~~~~~~
[  6%] Built target ggml
[  7%] Linking CUDA static library libggml_static.a
[  8%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  8%] Built target ggml_static
[ 10%] Linking CXX static library libllama.a
[ 10%] Built target llama
[ 13%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 13%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 14%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 15%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 18%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 20%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 21%] Linking CXX executable ../bin/test-c
[ 21%] Built target test-c
[ 22%] Linking CXX executable ../../bin/quantize
[ 23%] Linking CXX executable ../../bin/benchmark
[ 23%] Built target quantize
[ 23%] Built target benchmark
[ 24%] Linking CXX executable ../../bin/quantize-stats
[ 24%] Built target quantize-stats
[ 24%] Built target common
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 37%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 38%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 41%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 41%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 42%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 43%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 44%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 45%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 46%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 47%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 48%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 50%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 51%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 52%] Building CXX object examples/embd-input/CMakeFiles/embdinput.dir/embd-input-lib.cpp.o
[ 53%] Building CXX object examples/llava/CMakeFiles/clip.dir/clip.cpp.o
[ 54%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 55%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 56%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 57%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 58%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 60%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 61%] Linking CXX executable ../bin/test-rope
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-rope
[ 62%] Built target test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-sampling
[ 64%] Linking CXX executable ../bin/test-grammar-parser
[ 65%] Linking CXX executable ../../bin/q8dot
[ 65%] Built target test-sampling
[ 65%] Built target test-grammar-parser
[ 66%] Linking CXX executable ../../bin/baby-llama
[ 67%] Linking CXX executable ../../bin/embedding
[ 68%] Linking CXX executable ../../bin/vdot
[ 68%] Built target q8dot
[ 70%] Linking CXX executable ../../bin/save-load-state
[ 70%] Built target baby-llama
[ 70%] Built target vdot
[ 70%] Built target embedding
[ 71%] Linking CXX executable ../../bin/beam-search
[ 72%] Linking CXX executable ../../bin/simple
[ 72%] Built target save-load-state
[ 73%] Linking CXX executable ../bin/test-grad0
[ 74%] Linking CXX executable ../../bin/batched
[ 74%] Built target beam-search
[ 75%] Linking CXX executable ../../bin/batched-bench
[ 75%] Built target simple
[ 76%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 76%] Built target batched-bench
[ 77%] Linking CXX executable ../bin/test-quantize-perf
[ 78%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 78%] Built target test-grad0
[ 80%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 80%] Built target test-tokenizer-1-llama
[ 80%] Built target batched
[ 81%] Linking CXX executable ../../bin/export-lora
[ 82%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 82%] Built target test-quantize-perf
[ 82%] Built target test-tokenizer-1-bpe
[ 82%] Built target test-tokenizer-0-falcon
[ 83%] Linking CXX static library libembdinput.a
[ 83%] Built target export-lora
[ 84%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 84%] Built target embdinput
[ 85%] Building CXX object examples/embd-input/CMakeFiles/embd-input-test.dir/embd-input-test.cpp.o
[ 85%] Built target test-tokenizer-0-llama
[ 86%] Linking CXX executable ../../bin/parallel
[ 86%] Built target convert-llama2c-to-ggml
[ 87%] Linking CXX executable ../../bin/speculative
[ 87%] Built target parallel
[ 88%] Linking CXX executable ../../bin/perplexity
[ 88%] Built target speculative
[ 90%] Linking CXX executable ../../bin/train-text-from-scratch
[ 90%] Built target perplexity
[ 90%] Built target train-text-from-scratch
[ 91%] Linking CXX executable ../../bin/finetune
[ 92%] Linking CXX executable ../../bin/embd-input-test
[ 92%] Built target finetune
[ 92%] Built target embd-input-test
[ 93%] Linking CXX executable ../../bin/main
[ 93%] Built target main
[ 94%] Linking CXX executable ../../bin/llama-bench
[ 94%] Built target llama-bench
[ 95%] Linking CXX static library libclip.a
[ 95%] Built target clip
[ 96%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 97%] Linking CXX executable ../../bin/llava
[ 97%] Built target llava
[ 98%] Linking CXX executable ../bin/test-llama-grammar
[ 98%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	1m44.072s
user	3m4.983s
sys	0m7.582s
+ python3 ../convert.py ../models-mnt/open-llama/7B-v2
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00002-of-00002.bin
params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, f_norm_eps=1e-06, f_rope_freq_base=None, f_rope_scale=None, ftype=None, path_model=PosixPath('../models-mnt/open-llama/7B-v2'))
Loading vocab file '../models-mnt/open-llama/7B-v2/tokenizer.model', type 'spm'
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
Permuting layer 26
Permuting layer 27
Permuting layer 28
Permuting layer 29
Permuting layer 30
Permuting layer 31
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.0.attn_rot_embd
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.1.attn_rot_embd
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.2.attn_rot_embd
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.3.attn_rot_embd
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.4.attn_rot_embd
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.5.attn_rot_embd
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.6.attn_rot_embd
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.7.attn_rot_embd
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.8.attn_rot_embd
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.9.attn_rot_embd
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.10.attn_rot_embd
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.11.attn_rot_embd
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.12.attn_rot_embd
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.13.attn_rot_embd
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.14.attn_rot_embd
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.15.attn_rot_embd
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.16.attn_rot_embd
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.17.attn_rot_embd
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.18.attn_rot_embd
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.19.attn_rot_embd
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.20.attn_rot_embd
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.21.attn_rot_embd
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.22.attn_rot_embd
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.23.attn_rot_embd
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.24.attn_rot_embd
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.25.attn_rot_embd
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]
model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.26.attn_rot_embd
model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]
model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]
model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.27.attn_rot_embd
model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]
model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]
model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.28.attn_rot_embd
model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]
model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]
model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.29.attn_rot_embd
model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]
model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]
model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.30.attn_rot_embd
model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]
model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]
model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.31.attn_rot_embd
model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]
model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]
model.norm.weight                                -> output_norm.weight                       | F16    | [4096]
lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]
Writing ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf, format 1
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting special token type pad to 0
[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   2
[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[  7/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[  8/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   4
[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   5
[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   5
[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   5
[ 16/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   5
[ 17/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   5
[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   6
[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   6
[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   6
[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   6
[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   7
[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   7
[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   7
[ 25/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   7
[ 26/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   7
[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   7
[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   7
[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   7
[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   8
[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   8
[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   8
[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   9
[ 34/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   9
[ 35/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   9
[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   9
[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   9
[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   9
[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  12
[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  12
[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  12
[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  17
[ 43/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  20
[ 44/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  20
[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  20
[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  20
[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  20
[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  20
[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  20
[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  21
[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  22
[ 52/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  22
[ 53/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  22
[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  22
[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  22
[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  22
[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  22
[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  22
[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  23
[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  23
[ 61/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  23
[ 62/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  23
[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  24
[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  24
[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  24
[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  24
[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  26
[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  26
[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  26
[ 70/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  26
[ 71/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  28
[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  28
[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  28
[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  28
[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  29
[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  33
[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  33
[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  33
[ 79/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  33
[ 80/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  34
[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  34
[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  34
[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  34
[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  34
[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  34
[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  35
[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  35
[ 88/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  35
[ 89/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  36
[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  36
[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  36
[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  36
[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  36
[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  36
[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  36
[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  37
[ 97/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  38
[ 98/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  38
[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  38
[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  38
[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  38
[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  38
[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  39
[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  40
[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  47
[106/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  47
[107/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  47
[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  47
[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  47
[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  47
[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  47
[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  47
[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  47
[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  48
[115/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  48
[116/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  48
[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  49
[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  49
[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  49
[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  49
[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  50
[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  50
[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  50
[124/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  50
[125/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  50
[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  51
[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  51
[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  51
[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  51
[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  51
[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  51
[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  51
[133/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  52
[134/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  52
[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  52
[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  52
[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  52
[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  53
[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  53
[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  53
[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  53
[142/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  54
[143/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  54
[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  54
[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  54
[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  56
[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  56
[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  56
[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  56
[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  56
[151/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  56
[152/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  56
[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  56
[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  56
[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  57
[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  57
[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  57
[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  57
[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  57
[160/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  57
[161/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  58
[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  58
[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  58
[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  59
[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  59
[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  59
[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  59
[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  59
[169/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  59
[170/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  60
[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  60
[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  60
[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  61
[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  61
[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  61
[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  61
[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  61
[178/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  62
[179/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  62
[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  62
[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  62
[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  62
[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  62
[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  63
[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  63
[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  63
[187/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  63
[188/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  64
[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  64
[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  64
[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  64
[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  64
[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  65
[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  65
[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  65
[196/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  65
[197/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  65
[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  66
[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  66
[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  66
[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  66
[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  66
[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  66
[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  67
[205/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  67
[206/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  68
[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  68
[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  68
[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  68
[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  68
[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  68
[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  68
[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  69
[214/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  69
[215/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  70
[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  70
[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  70
[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  70
[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  70
[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  70
[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  70
[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  70
[223/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  71
[224/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  71
[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  71
[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  71
[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  71
[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  71
[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  73
[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  73
[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  73
[232/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  73
[233/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  73
[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  73
[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  73
[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  73
[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  73
[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  74
[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  74
[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  74
[241/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  75
[242/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  75
[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  75
[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  75
[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  75
[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  76
[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  76
[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  76
[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  76
[250/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  76
[251/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  77
[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  77
[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  77
[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  77
[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  77
[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  77
[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  77
[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  78
[259/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  80
[260/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  84
[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  84
[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  84
[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  84
[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  84
[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  85
[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  86
[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  89
[268/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  94
[269/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  94
[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  95
[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  95
[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  95
[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  96
[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 105
[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 105
[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 105
[277/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 105
[278/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 106
[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 106
[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 106
[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 106
[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 106
[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 108
[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 109
[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 111
[286/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 117
[287/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 117
[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 117
[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 117
[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 117
[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+ 118
Wrote ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_f16=../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf
+ wiki_test=../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf q8_0

CUDA error 100 at /home/ggml/work/llama.cpp/ggml-cuda.cu:5661: no CUDA-capable device is detected
current device: 21852
+ cur=1
+ echo 1
+ set +x
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-lora-ppl.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-save-load-state.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-f16.log: No such file or directory
cat: /home/ggml/results/llama.cpp/0e/89203b517c95ec6675eda75d200a60d1e8921d/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-lora-f16.log: No such file or directory
