### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.42 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.82 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.70 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.49 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.35 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.01 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.37 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.29 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.23 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.19 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.20 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  176.05 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.99 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   26.35 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.34 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.26 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 219.56 sec*proc (27 tests)

Total Test time (real) = 219.57 sec

real	3m39.598s
user	7m26.593s
sys	0m5.649s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.28 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    1.00 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.20 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   28.56 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.28 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.08 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.13 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.41 sec*proc (27 tests)

Total Test time (real) =  50.42 sec

real	0m50.425s
user	1m10.708s
sys	0m4.976s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.068 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.629 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.515 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.018.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.524 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.018.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.526 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.018.527 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.018.527 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.018.529 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.018.530 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.018.530 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.018.531 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.018.531 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.018.535 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.018.535 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.018.536 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.018.536 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.018.537 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.018.538 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.018.538 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.106 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.365 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.367 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.368 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.368 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.369 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.024.370 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.370 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.024.371 I llama_model_loader: - type  f32:  124 tensors
0.00.024.372 I llama_model_loader: - type  f16:   73 tensors
0.00.028.503 I llm_load_vocab: special tokens cache size = 5
0.00.030.595 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.030.598 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.030.599 I llm_load_print_meta: arch             = bert
0.00.030.600 I llm_load_print_meta: vocab type       = WPM
0.00.030.600 I llm_load_print_meta: n_vocab          = 30522
0.00.030.600 I llm_load_print_meta: n_merges         = 0
0.00.030.601 I llm_load_print_meta: vocab_only       = 0
0.00.030.601 I llm_load_print_meta: n_ctx_train      = 512
0.00.030.601 I llm_load_print_meta: n_embd           = 384
0.00.030.602 I llm_load_print_meta: n_layer          = 12
0.00.030.605 I llm_load_print_meta: n_head           = 12
0.00.030.606 I llm_load_print_meta: n_head_kv        = 12
0.00.030.606 I llm_load_print_meta: n_rot            = 32
0.00.030.606 I llm_load_print_meta: n_swa            = 0
0.00.030.609 I llm_load_print_meta: n_embd_head_k    = 32
0.00.030.609 I llm_load_print_meta: n_embd_head_v    = 32
0.00.030.610 I llm_load_print_meta: n_gqa            = 1
0.00.030.611 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.030.612 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.030.613 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.030.613 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.030.614 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.030.614 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.030.614 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.030.615 I llm_load_print_meta: n_ff             = 1536
0.00.030.616 I llm_load_print_meta: n_expert         = 0
0.00.030.616 I llm_load_print_meta: n_expert_used    = 0
0.00.030.616 I llm_load_print_meta: causal attn      = 0
0.00.030.616 I llm_load_print_meta: pooling type     = 2
0.00.030.617 I llm_load_print_meta: rope type        = 2
0.00.030.617 I llm_load_print_meta: rope scaling     = linear
0.00.030.617 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.030.618 I llm_load_print_meta: freq_scale_train = 1
0.00.030.618 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.030.618 I llm_load_print_meta: rope_finetuned   = unknown
0.00.030.619 I llm_load_print_meta: ssm_d_conv       = 0
0.00.030.619 I llm_load_print_meta: ssm_d_inner      = 0
0.00.030.621 I llm_load_print_meta: ssm_d_state      = 0
0.00.030.621 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.030.621 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.030.635 I llm_load_print_meta: model type       = 33M
0.00.030.636 I llm_load_print_meta: model ftype      = F16
0.00.030.636 I llm_load_print_meta: model params     = 33.21 M
0.00.030.637 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.030.637 I llm_load_print_meta: general.name     = Bge Small
0.00.030.638 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.030.638 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.030.638 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.030.639 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.030.639 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.030.639 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.030.640 I llm_load_print_meta: max token length = 21
0.00.032.694 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.032.694 I llm_load_tensors: offloading output layer to GPU
0.00.032.695 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.032.721 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.032.722 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.033.333 I llama_new_context_with_model: n_seq_max     = 1
0.00.033.335 I llama_new_context_with_model: n_ctx         = 512
0.00.033.335 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.033.335 I llama_new_context_with_model: n_batch       = 2048
0.00.033.336 I llama_new_context_with_model: n_ubatch      = 2048
0.00.033.336 I llama_new_context_with_model: flash_attn    = 0
0.00.033.337 I llama_new_context_with_model: freq_base     = 10000.0
0.00.033.337 I llama_new_context_with_model: freq_scale    = 1
0.00.033.338 I ggml_metal_init: allocating
0.00.033.350 I ggml_metal_init: found device: Apple M4
0.00.033.356 I ggml_metal_init: picking default device: Apple M4
0.00.034.191 I ggml_metal_init: using embedded metal library
0.00.037.582 I ggml_metal_init: GPU name:   Apple M4
0.00.037.585 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.037.585 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.037.586 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.037.586 I ggml_metal_init: simdgroup reduction   = true
0.00.037.587 I ggml_metal_init: simdgroup matrix mul. = true
0.00.037.587 I ggml_metal_init: has bfloat            = true
0.00.037.587 I ggml_metal_init: use bfloat            = true
0.00.037.587 I ggml_metal_init: hasUnifiedMemory      = true
0.00.037.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.048.094 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.048.096 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.048.097 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.048.898 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.048.899 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.048.900 I llama_new_context_with_model: graph nodes  = 429
0.00.048.900 I llama_new_context_with_model: graph splits = 2
0.00.048.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.055.349 I 
0.00.055.362 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.056.035 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.059.567 I llama_perf_context_print:        load time =      40.72 ms
0.00.059.568 I llama_perf_context_print: prompt eval time =       3.38 ms /     9 tokens (    0.38 ms per token,  2664.30 tokens per second)
0.00.059.569 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.059.569 I llama_perf_context_print:       total time =       4.22 ms /    10 tokens
0.00.059.697 I ggml_metal_free: deallocating

real	0m0.242s
user	0m0.046s
sys	0m0.027s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.814 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.818 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.818 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.819 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.819 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.819 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.820 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.820 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.821 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.821 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.821 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.823 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.824 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.010.824 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.010.824 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.010.825 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.825 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.010.825 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.842 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.843 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.844 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.844 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.844 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.844 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.845 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.845 I llama_model_loader: - type  f32:  124 tensors
0.00.013.845 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.221 I llm_load_vocab: special tokens cache size = 5
0.00.017.459 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.461 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.461 I llm_load_print_meta: arch             = bert
0.00.017.461 I llm_load_print_meta: vocab type       = WPM
0.00.017.462 I llm_load_print_meta: n_vocab          = 30522
0.00.017.462 I llm_load_print_meta: n_merges         = 0
0.00.017.462 I llm_load_print_meta: vocab_only       = 0
0.00.017.462 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.462 I llm_load_print_meta: n_embd           = 384
0.00.017.463 I llm_load_print_meta: n_layer          = 12
0.00.017.464 I llm_load_print_meta: n_head           = 12
0.00.017.465 I llm_load_print_meta: n_head_kv        = 12
0.00.017.465 I llm_load_print_meta: n_rot            = 32
0.00.017.465 I llm_load_print_meta: n_swa            = 0
0.00.017.466 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.466 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.466 I llm_load_print_meta: n_gqa            = 1
0.00.017.467 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.470 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.470 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.471 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.471 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.471 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.471 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.472 I llm_load_print_meta: n_ff             = 1536
0.00.017.472 I llm_load_print_meta: n_expert         = 0
0.00.017.472 I llm_load_print_meta: n_expert_used    = 0
0.00.017.472 I llm_load_print_meta: causal attn      = 0
0.00.017.472 I llm_load_print_meta: pooling type     = 2
0.00.017.474 I llm_load_print_meta: rope type        = 2
0.00.017.474 I llm_load_print_meta: rope scaling     = linear
0.00.017.474 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.475 I llm_load_print_meta: freq_scale_train = 1
0.00.017.475 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.475 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.475 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.475 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.476 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.476 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.476 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.483 I llm_load_print_meta: model type       = 33M
0.00.017.483 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.483 I llm_load_print_meta: model params     = 33.21 M
0.00.017.484 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.484 I llm_load_print_meta: general.name     = Bge Small
0.00.017.484 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.485 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.485 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.485 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.485 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.485 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.485 I llm_load_print_meta: max token length = 21
0.00.018.772 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.772 I llm_load_tensors: offloading output layer to GPU
0.00.018.772 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.779 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.780 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.138 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.139 I llama_new_context_with_model: n_ctx         = 512
0.00.019.139 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.140 I llama_new_context_with_model: n_batch       = 2048
0.00.019.140 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.140 I llama_new_context_with_model: flash_attn    = 0
0.00.019.140 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.141 I llama_new_context_with_model: freq_scale    = 1
0.00.019.141 I ggml_metal_init: allocating
0.00.019.147 I ggml_metal_init: found device: Apple M4
0.00.019.150 I ggml_metal_init: picking default device: Apple M4
0.00.019.680 I ggml_metal_init: using embedded metal library
0.00.021.824 I ggml_metal_init: GPU name:   Apple M4
0.00.021.826 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.826 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.827 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.827 I ggml_metal_init: simdgroup reduction   = true
0.00.021.827 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.827 I ggml_metal_init: has bfloat            = true
0.00.021.827 I ggml_metal_init: use bfloat            = true
0.00.021.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.030.699 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.030.701 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.030.702 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.031.348 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.031.349 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.031.350 I llama_new_context_with_model: graph nodes  = 429
0.00.031.350 I llama_new_context_with_model: graph splits = 2
0.00.031.363 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.035.065 I 
0.00.035.082 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.035.605 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.038.976 I llama_perf_context_print:        load time =      26.26 ms
0.00.038.977 I llama_perf_context_print: prompt eval time =       3.24 ms /     9 tokens (    0.36 ms per token,  2780.35 tokens per second)
0.00.038.978 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.038.978 I llama_perf_context_print:       total time =       3.91 ms /    10 tokens
0.00.039.161 I ggml_metal_free: deallocating

real	0m0.051s
user	0m0.028s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.180 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.281 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.934 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.942 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.943 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.947 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.948 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.948 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.950 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.950 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.951 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.951 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.952 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.956 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.957 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.957 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.958 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.959 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.949 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.413 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.246 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.248 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.249 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.249 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.250 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.250 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.250 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.251 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.251 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.251 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.252 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.252 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.253 I llama_model_loader: - type  f32:   41 tensors
0.00.050.253 I llama_model_loader: - type  f16:   29 tensors
0.00.069.288 W llm_load_vocab: empty token at index 5
0.00.073.926 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.075.266 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.292 I llm_load_vocab: special tokens cache size = 5
0.00.322.921 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.322.935 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.322.936 I llm_load_print_meta: arch             = jina-bert-v2
0.00.322.936 I llm_load_print_meta: vocab type       = BPE
0.00.322.937 I llm_load_print_meta: n_vocab          = 61056
0.00.322.937 I llm_load_print_meta: n_merges         = 39382
0.00.322.937 I llm_load_print_meta: vocab_only       = 0
0.00.322.937 I llm_load_print_meta: n_ctx_train      = 8192
0.00.322.937 I llm_load_print_meta: n_embd           = 384
0.00.322.938 I llm_load_print_meta: n_layer          = 4
0.00.322.946 I llm_load_print_meta: n_head           = 12
0.00.322.947 I llm_load_print_meta: n_head_kv        = 12
0.00.322.947 I llm_load_print_meta: n_rot            = 32
0.00.322.947 I llm_load_print_meta: n_swa            = 0
0.00.322.947 I llm_load_print_meta: n_embd_head_k    = 32
0.00.322.947 I llm_load_print_meta: n_embd_head_v    = 32
0.00.322.948 I llm_load_print_meta: n_gqa            = 1
0.00.322.949 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.322.949 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.322.951 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.322.953 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.322.953 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.322.954 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.322.954 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.322.954 I llm_load_print_meta: n_ff             = 1536
0.00.322.955 I llm_load_print_meta: n_expert         = 0
0.00.322.955 I llm_load_print_meta: n_expert_used    = 0
0.00.322.955 I llm_load_print_meta: causal attn      = 0
0.00.322.955 I llm_load_print_meta: pooling type     = -1
0.00.322.955 I llm_load_print_meta: rope type        = -1
0.00.322.956 I llm_load_print_meta: rope scaling     = linear
0.00.322.956 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.322.956 I llm_load_print_meta: freq_scale_train = 1
0.00.322.956 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.322.957 I llm_load_print_meta: rope_finetuned   = unknown
0.00.322.957 I llm_load_print_meta: ssm_d_conv       = 0
0.00.322.957 I llm_load_print_meta: ssm_d_inner      = 0
0.00.322.957 I llm_load_print_meta: ssm_d_state      = 0
0.00.322.957 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.322.957 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.322.986 I llm_load_print_meta: model type       = 33M
0.00.322.986 I llm_load_print_meta: model ftype      = F16
0.00.322.987 I llm_load_print_meta: model params     = 32.90 M
0.00.322.987 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.322.987 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.322.988 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.322.988 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.322.988 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.322.988 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.322.989 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.322.990 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.322.990 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.322.990 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.322.990 I llm_load_print_meta: max token length = 45
0.00.323.964 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.323.964 I llm_load_tensors: offloading output layer to GPU
0.00.323.964 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.323.979 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.323.980 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.733 I llama_new_context_with_model: n_seq_max     = 1
0.00.324.734 I llama_new_context_with_model: n_ctx         = 8192
0.00.324.734 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.324.735 I llama_new_context_with_model: n_batch       = 2048
0.00.324.735 I llama_new_context_with_model: n_ubatch      = 2048
0.00.324.735 I llama_new_context_with_model: flash_attn    = 0
0.00.324.736 I llama_new_context_with_model: freq_base     = 10000.0
0.00.324.736 I llama_new_context_with_model: freq_scale    = 1
0.00.324.737 I ggml_metal_init: allocating
0.00.324.742 I ggml_metal_init: found device: Apple M4
0.00.324.745 I ggml_metal_init: picking default device: Apple M4
0.00.325.673 I ggml_metal_init: using embedded metal library
0.00.328.211 I ggml_metal_init: GPU name:   Apple M4
0.00.328.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.328.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.328.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.328.213 I ggml_metal_init: simdgroup reduction   = true
0.00.328.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.328.214 I ggml_metal_init: has bfloat            = true
0.00.328.214 I ggml_metal_init: use bfloat            = true
0.00.328.214 I ggml_metal_init: hasUnifiedMemory      = true
0.00.328.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.338.850 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.338.852 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.338.854 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.339.396 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.339.396 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.339.397 I llama_new_context_with_model: graph nodes  = 154
0.00.339.397 I llama_new_context_with_model: graph splits = 2
0.00.339.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.349.416 I 
0.00.349.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.349.576 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.349.576 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.349.579 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.349.579 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.349.583 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.349.583 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.350.076 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.353.743 I llama_perf_context_print:        load time =     326.13 ms
0.00.353.744 I llama_perf_context_print: prompt eval time =       3.66 ms /    62 tokens (    0.06 ms per token, 16949.15 tokens per second)
0.00.353.745 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.353.745 I llama_perf_context_print:       total time =       4.33 ms /    63 tokens
0.00.353.936 I ggml_metal_free: deallocating

real	0m1.053s
user	0m0.330s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.172 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.319 I main: llama backend init
0.00.000.343 I main: load the model and apply lora adapter, if any
0.00.033.740 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.047.423 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.431 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.434 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.435 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.436 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.437 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.437 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.440 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.440 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.441 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.441 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.442 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.443 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.446 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.446 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.447 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.055.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.063.995 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.996 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.996 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.997 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.998 I llama_model_loader: - type  f32:  194 tensors
0.00.063.998 I llama_model_loader: - type  f16:   98 tensors
0.00.092.622 I llm_load_vocab: special tokens cache size = 25
0.00.099.253 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.256 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.256 I llm_load_print_meta: arch             = gptneox
0.00.099.256 I llm_load_print_meta: vocab type       = BPE
0.00.099.257 I llm_load_print_meta: n_vocab          = 50304
0.00.099.257 I llm_load_print_meta: n_merges         = 50009
0.00.099.257 I llm_load_print_meta: vocab_only       = 0
0.00.099.257 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.257 I llm_load_print_meta: n_embd           = 2048
0.00.099.257 I llm_load_print_meta: n_layer          = 24
0.00.099.261 I llm_load_print_meta: n_head           = 16
0.00.099.261 I llm_load_print_meta: n_head_kv        = 16
0.00.099.262 I llm_load_print_meta: n_rot            = 32
0.00.099.262 I llm_load_print_meta: n_swa            = 0
0.00.099.262 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.262 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.263 I llm_load_print_meta: n_gqa            = 1
0.00.099.263 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.264 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.264 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.265 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.266 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.266 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.266 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.267 I llm_load_print_meta: n_ff             = 8192
0.00.099.267 I llm_load_print_meta: n_expert         = 0
0.00.099.267 I llm_load_print_meta: n_expert_used    = 0
0.00.099.268 I llm_load_print_meta: causal attn      = 1
0.00.099.268 I llm_load_print_meta: pooling type     = 0
0.00.099.268 I llm_load_print_meta: rope type        = 2
0.00.099.268 I llm_load_print_meta: rope scaling     = linear
0.00.099.268 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.269 I llm_load_print_meta: freq_scale_train = 1
0.00.099.269 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.269 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.269 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.269 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.270 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.270 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.270 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.281 I llm_load_print_meta: model type       = 1.4B
0.00.099.282 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.282 I llm_load_print_meta: model params     = 1.41 B
0.00.099.282 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.283 I llm_load_print_meta: general.name     = 1.4B
0.00.099.283 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.283 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.283 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.283 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.284 I llm_load_print_meta: LF token         = 128 ''
0.00.099.284 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.284 I llm_load_print_meta: max token length = 1024
0.00.101.187 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.187 I llm_load_tensors: offloading output layer to GPU
0.00.101.187 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.204 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.204 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.071 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.073 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.073 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.073 I llama_new_context_with_model: n_batch       = 2048
0.00.102.073 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.073 I llama_new_context_with_model: flash_attn    = 0
0.00.102.074 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.074 I llama_new_context_with_model: freq_scale    = 1
0.00.102.074 I ggml_metal_init: allocating
0.00.102.082 I ggml_metal_init: found device: Apple M4
0.00.102.085 I ggml_metal_init: picking default device: Apple M4
0.00.102.689 I ggml_metal_init: using embedded metal library
0.00.118.255 I ggml_metal_init: GPU name:   Apple M4
0.00.118.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.118.257 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.118.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.118.258 I ggml_metal_init: simdgroup reduction   = true
0.00.118.258 I ggml_metal_init: simdgroup matrix mul. = true
0.00.118.258 I ggml_metal_init: has bfloat            = true
0.00.118.258 I ggml_metal_init: use bfloat            = true
0.00.118.258 I ggml_metal_init: hasUnifiedMemory      = true
0.00.118.259 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.170.601 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.170.605 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.170.624 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.171.527 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.171.528 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.171.528 I llama_new_context_with_model: graph nodes  = 967
0.00.171.528 I llama_new_context_with_model: graph splits = 2
0.00.171.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.248.674 I main: llama threadpool init, n_threads = 4
0.00.248.706 I 
0.00.248.723 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.248.724 I 
0.00.248.795 I sampler seed: 1234
0.00.248.800 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.248.822 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.248.824 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.248.824 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.041.681 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.02.041.682 I llama_perf_context_print:        load time =     214.92 ms
0.02.041.683 I llama_perf_context_print: prompt eval time =      37.79 ms /     7 tokens (    5.40 ms per token,   185.25 tokens per second)
0.02.041.684 I llama_perf_context_print:        eval time =    1752.13 ms /    63 runs   (   27.81 ms per token,    35.96 tokens per second)
0.02.041.684 I llama_perf_context_print:       total time =    1793.01 ms /    70 tokens
0.02.041.871 I ggml_metal_free: deallocating

real	0m2.417s
user	0m0.145s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.530 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.387 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.604 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.609 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.612 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.613 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.613 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.615 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.615 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.615 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.616 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.617 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.661 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.173 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.049.175 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.176 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.176 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.176 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.177 I llama_model_loader: - type  f32:  194 tensors
0.00.049.178 I llama_model_loader: - type  f16:   98 tensors
0.00.077.156 I llm_load_vocab: special tokens cache size = 25
0.00.083.832 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.835 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.835 I llm_load_print_meta: arch             = gptneox
0.00.083.835 I llm_load_print_meta: vocab type       = BPE
0.00.083.836 I llm_load_print_meta: n_vocab          = 50304
0.00.083.836 I llm_load_print_meta: n_merges         = 50009
0.00.083.836 I llm_load_print_meta: vocab_only       = 0
0.00.083.836 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.836 I llm_load_print_meta: n_embd           = 2048
0.00.083.836 I llm_load_print_meta: n_layer          = 24
0.00.083.839 I llm_load_print_meta: n_head           = 16
0.00.083.839 I llm_load_print_meta: n_head_kv        = 16
0.00.083.840 I llm_load_print_meta: n_rot            = 32
0.00.083.840 I llm_load_print_meta: n_swa            = 0
0.00.083.840 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.840 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.841 I llm_load_print_meta: n_gqa            = 1
0.00.083.843 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.844 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.844 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.845 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.845 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.846 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.846 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.847 I llm_load_print_meta: n_ff             = 8192
0.00.083.847 I llm_load_print_meta: n_expert         = 0
0.00.083.847 I llm_load_print_meta: n_expert_used    = 0
0.00.083.849 I llm_load_print_meta: causal attn      = 1
0.00.083.849 I llm_load_print_meta: pooling type     = 0
0.00.083.849 I llm_load_print_meta: rope type        = 2
0.00.083.850 I llm_load_print_meta: rope scaling     = linear
0.00.083.850 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.850 I llm_load_print_meta: freq_scale_train = 1
0.00.083.850 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.851 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.851 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.851 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.851 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.851 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.851 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.863 I llm_load_print_meta: model type       = 1.4B
0.00.083.864 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.864 I llm_load_print_meta: model params     = 1.41 B
0.00.083.864 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.865 I llm_load_print_meta: general.name     = 1.4B
0.00.083.865 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.865 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.866 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.866 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.866 I llm_load_print_meta: LF token         = 128 ''
0.00.083.867 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.867 I llm_load_print_meta: max token length = 1024
0.00.085.693 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.693 I llm_load_tensors: offloading output layer to GPU
0.00.085.693 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.702 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.085.703 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.086.578 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.578 I llama_new_context_with_model: n_ctx         = 128
0.00.086.579 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.086.579 I llama_new_context_with_model: n_batch       = 128
0.00.086.579 I llama_new_context_with_model: n_ubatch      = 128
0.00.086.579 I llama_new_context_with_model: flash_attn    = 0
0.00.086.580 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.580 I llama_new_context_with_model: freq_scale    = 1
0.00.086.580 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.086.581 I ggml_metal_init: allocating
0.00.086.587 I ggml_metal_init: found device: Apple M4
0.00.086.589 I ggml_metal_init: picking default device: Apple M4
0.00.087.136 I ggml_metal_init: using embedded metal library
0.00.089.209 I ggml_metal_init: GPU name:   Apple M4
0.00.089.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.211 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.212 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.212 I ggml_metal_init: simdgroup reduction   = true
0.00.089.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.212 I ggml_metal_init: has bfloat            = true
0.00.089.212 I ggml_metal_init: use bfloat            = true
0.00.089.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.213 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.660 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.099.664 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.099.678 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.553 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.100.554 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.100.554 I llama_new_context_with_model: graph nodes  = 967
0.00.100.554 I llama_new_context_with_model: graph splits = 2
0.00.100.566 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.136.628 I 
0.01.136.652 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.01.136.655 I perplexity: tokenizing the input ..
0.01.146.315 I perplexity: tokenization took 9.659 ms
0.01.146.340 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.266.389 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.268.025 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.268.047 I llama_perf_context_print:        load time =    1117.23 ms
0.01.268.048 I llama_perf_context_print: prompt eval time =     119.80 ms /   128 tokens (    0.94 ms per token,  1068.45 tokens per second)
0.01.268.052 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.268.053 I llama_perf_context_print:       total time =     131.42 ms /   129 tokens
0.01.268.693 I ggml_metal_free: deallocating

real	0m1.480s
user	0m0.123s
sys	0m0.262s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.492 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.532 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.539 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.539 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.541 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.543 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.543 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.544 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.546 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.546 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.547 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.481 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.600 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.583 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.584 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.585 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.585 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.586 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.586 I llama_model_loader: - type  f32:  194 tensors
0.00.034.587 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.835 I llm_load_vocab: special tokens cache size = 25
0.00.064.079 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.082 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.083 I llm_load_print_meta: arch             = gptneox
0.00.064.083 I llm_load_print_meta: vocab type       = BPE
0.00.064.084 I llm_load_print_meta: n_vocab          = 50304
0.00.064.084 I llm_load_print_meta: n_merges         = 50009
0.00.064.084 I llm_load_print_meta: vocab_only       = 0
0.00.064.084 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.084 I llm_load_print_meta: n_embd           = 2048
0.00.064.085 I llm_load_print_meta: n_layer          = 24
0.00.064.089 I llm_load_print_meta: n_head           = 16
0.00.064.089 I llm_load_print_meta: n_head_kv        = 16
0.00.064.090 I llm_load_print_meta: n_rot            = 32
0.00.064.090 I llm_load_print_meta: n_swa            = 0
0.00.064.090 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.093 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.094 I llm_load_print_meta: n_gqa            = 1
0.00.064.094 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.095 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.096 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.096 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.096 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.097 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.097 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.098 I llm_load_print_meta: n_ff             = 8192
0.00.064.098 I llm_load_print_meta: n_expert         = 0
0.00.064.098 I llm_load_print_meta: n_expert_used    = 0
0.00.064.098 I llm_load_print_meta: causal attn      = 1
0.00.064.098 I llm_load_print_meta: pooling type     = 0
0.00.064.099 I llm_load_print_meta: rope type        = 2
0.00.064.099 I llm_load_print_meta: rope scaling     = linear
0.00.064.100 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.101 I llm_load_print_meta: freq_scale_train = 1
0.00.064.101 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.101 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.101 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.101 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.101 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.102 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.102 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.119 I llm_load_print_meta: model type       = 1.4B
0.00.064.119 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.119 I llm_load_print_meta: model params     = 1.41 B
0.00.064.120 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.121 I llm_load_print_meta: general.name     = 1.4B
0.00.064.121 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.121 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.121 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.121 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.122 I llm_load_print_meta: LF token         = 128 ''
0.00.064.123 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.123 I llm_load_print_meta: max token length = 1024
0.00.066.091 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.092 I llm_load_tensors: offloading output layer to GPU
0.00.066.092 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.102 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.103 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.011 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.012 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.012 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.012 I llama_new_context_with_model: n_batch       = 2048
0.00.067.012 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.013 I llama_new_context_with_model: flash_attn    = 0
0.00.067.013 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.013 I llama_new_context_with_model: freq_scale    = 1
0.00.067.014 I ggml_metal_init: allocating
0.00.067.018 I ggml_metal_init: found device: Apple M4
0.00.067.020 I ggml_metal_init: picking default device: Apple M4
0.00.067.702 I ggml_metal_init: using embedded metal library
0.00.069.846 I ggml_metal_init: GPU name:   Apple M4
0.00.069.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.848 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.849 I ggml_metal_init: simdgroup reduction   = true
0.00.069.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.849 I ggml_metal_init: has bfloat            = true
0.00.069.849 I ggml_metal_init: use bfloat            = true
0.00.069.850 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.852 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.806 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.813 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.838 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.900 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.902 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.902 I llama_new_context_with_model: graph nodes  = 967
0.00.105.902 I llama_new_context_with_model: graph splits = 2
0.00.105.925 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.375.755 I main: llama threadpool init, n_threads = 4
0.01.375.820 I 
0.01.375.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.01.375.867 I 
0.01.376.165 I sampler seed: 1234
0.01.376.173 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.376.228 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.376.230 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.376.230 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.438.599 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 46926.64 tokens per second)
0.02.438.600 I llama_perf_context_print:        load time =    1366.26 ms
0.02.438.600 I llama_perf_context_print: prompt eval time =      33.93 ms /     7 tokens (    4.85 ms per token,   206.29 tokens per second)
0.02.438.601 I llama_perf_context_print:        eval time =    1025.09 ms /    63 runs   (   16.27 ms per token,    61.46 tokens per second)
0.02.438.602 I llama_perf_context_print:       total time =    1062.85 ms /    70 tokens
0.02.438.770 I ggml_metal_free: deallocating

real	0m2.456s
user	0m0.129s
sys	0m0.301s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.139 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.229 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.211 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.217 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.219 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.219 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.220 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.228 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.228 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.229 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.230 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.232 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.610 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.612 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.613 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.613 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.613 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.614 I llama_model_loader: - type  f32:  194 tensors
0.00.032.615 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.529 I llm_load_vocab: special tokens cache size = 25
0.00.062.583 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.586 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.586 I llm_load_print_meta: arch             = gptneox
0.00.062.586 I llm_load_print_meta: vocab type       = BPE
0.00.062.586 I llm_load_print_meta: n_vocab          = 50304
0.00.062.587 I llm_load_print_meta: n_merges         = 50009
0.00.062.587 I llm_load_print_meta: vocab_only       = 0
0.00.062.587 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.587 I llm_load_print_meta: n_embd           = 2048
0.00.062.587 I llm_load_print_meta: n_layer          = 24
0.00.062.590 I llm_load_print_meta: n_head           = 16
0.00.062.593 I llm_load_print_meta: n_head_kv        = 16
0.00.062.593 I llm_load_print_meta: n_rot            = 32
0.00.062.593 I llm_load_print_meta: n_swa            = 0
0.00.062.599 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.599 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.600 I llm_load_print_meta: n_gqa            = 1
0.00.062.601 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.602 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.604 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.605 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.605 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.605 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.605 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.606 I llm_load_print_meta: n_ff             = 8192
0.00.062.606 I llm_load_print_meta: n_expert         = 0
0.00.062.606 I llm_load_print_meta: n_expert_used    = 0
0.00.062.607 I llm_load_print_meta: causal attn      = 1
0.00.062.607 I llm_load_print_meta: pooling type     = 0
0.00.062.607 I llm_load_print_meta: rope type        = 2
0.00.062.607 I llm_load_print_meta: rope scaling     = linear
0.00.062.607 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.608 I llm_load_print_meta: freq_scale_train = 1
0.00.062.609 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.609 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.609 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.610 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.610 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.610 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.610 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.622 I llm_load_print_meta: model type       = 1.4B
0.00.062.622 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.622 I llm_load_print_meta: model params     = 1.41 B
0.00.062.623 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.623 I llm_load_print_meta: general.name     = 1.4B
0.00.062.623 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.623 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.624 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.624 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.624 I llm_load_print_meta: LF token         = 128 ''
0.00.062.624 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.624 I llm_load_print_meta: max token length = 1024
0.00.064.471 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.471 I llm_load_tensors: offloading output layer to GPU
0.00.064.472 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.481 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.482 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.355 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.356 I llama_new_context_with_model: n_ctx         = 128
0.00.065.356 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.356 I llama_new_context_with_model: n_batch       = 128
0.00.065.356 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.357 I llama_new_context_with_model: flash_attn    = 0
0.00.065.357 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.357 I llama_new_context_with_model: freq_scale    = 1
0.00.065.357 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.358 I ggml_metal_init: allocating
0.00.065.363 I ggml_metal_init: found device: Apple M4
0.00.065.366 I ggml_metal_init: picking default device: Apple M4
0.00.065.949 I ggml_metal_init: using embedded metal library
0.00.067.938 I ggml_metal_init: GPU name:   Apple M4
0.00.067.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.940 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.941 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.941 I ggml_metal_init: simdgroup reduction   = true
0.00.067.941 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.941 I ggml_metal_init: has bfloat            = true
0.00.067.941 I ggml_metal_init: use bfloat            = true
0.00.067.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.942 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.811 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.815 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.831 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.077.757 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.077.758 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.077.759 I llama_new_context_with_model: graph nodes  = 967
0.00.077.759 I llama_new_context_with_model: graph splits = 2
0.00.077.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.060.956 I 
0.01.060.979 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.01.060.983 I perplexity: tokenizing the input ..
0.01.068.701 I perplexity: tokenization took 7.717 ms
0.01.068.714 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.190.061 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.191.155 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.191.162 I llama_perf_context_print:        load time =    1049.72 ms
0.01.191.164 I llama_perf_context_print: prompt eval time =     121.13 ms /   128 tokens (    0.95 ms per token,  1056.74 tokens per second)
0.01.191.165 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.191.165 I llama_perf_context_print:       total time =     130.21 ms /   129 tokens
0.01.191.431 I ggml_metal_free: deallocating

real	0m1.209s
user	0m0.091s
sys	0m0.224s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.016.502 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.249 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.255 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.256 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.259 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.259 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.259 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.260 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.261 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.261 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.261 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.262 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.262 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.264 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.264 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.265 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.637 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.884 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.184 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.185 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.186 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.186 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.186 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.187 I llama_model_loader: - type  f32:  194 tensors
0.00.034.187 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.187 I llama_model_loader: - type q6_K:    1 tensors
0.00.062.249 I llm_load_vocab: special tokens cache size = 25
0.00.071.800 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.804 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.804 I llm_load_print_meta: arch             = gptneox
0.00.071.804 I llm_load_print_meta: vocab type       = BPE
0.00.071.805 I llm_load_print_meta: n_vocab          = 50304
0.00.071.805 I llm_load_print_meta: n_merges         = 50009
0.00.071.805 I llm_load_print_meta: vocab_only       = 0
0.00.071.805 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.806 I llm_load_print_meta: n_embd           = 2048
0.00.071.806 I llm_load_print_meta: n_layer          = 24
0.00.071.810 I llm_load_print_meta: n_head           = 16
0.00.071.811 I llm_load_print_meta: n_head_kv        = 16
0.00.071.811 I llm_load_print_meta: n_rot            = 32
0.00.071.811 I llm_load_print_meta: n_swa            = 0
0.00.071.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.812 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.813 I llm_load_print_meta: n_gqa            = 1
0.00.071.814 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.815 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.815 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.816 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.816 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.816 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.817 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.817 I llm_load_print_meta: n_ff             = 8192
0.00.071.818 I llm_load_print_meta: n_expert         = 0
0.00.071.818 I llm_load_print_meta: n_expert_used    = 0
0.00.071.818 I llm_load_print_meta: causal attn      = 1
0.00.071.818 I llm_load_print_meta: pooling type     = 0
0.00.071.818 I llm_load_print_meta: rope type        = 2
0.00.071.821 I llm_load_print_meta: rope scaling     = linear
0.00.071.821 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.822 I llm_load_print_meta: freq_scale_train = 1
0.00.071.822 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.822 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.823 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.823 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.823 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.823 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.823 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.836 I llm_load_print_meta: model type       = 1.4B
0.00.071.836 I llm_load_print_meta: model ftype      = Q4_0
0.00.071.836 I llm_load_print_meta: model params     = 1.41 B
0.00.071.837 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.071.838 I llm_load_print_meta: general.name     = 1.4B
0.00.071.838 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.841 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.841 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.841 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.842 I llm_load_print_meta: LF token         = 128 ''
0.00.071.842 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.842 I llm_load_print_meta: max token length = 1024
0.00.074.445 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.445 I llm_load_tensors: offloading output layer to GPU
0.00.074.446 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.456 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.074.458 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.075.782 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.783 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.784 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.784 I llama_new_context_with_model: n_batch       = 2048
0.00.075.784 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.785 I llama_new_context_with_model: flash_attn    = 0
0.00.075.785 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.786 I llama_new_context_with_model: freq_scale    = 1
0.00.075.786 I ggml_metal_init: allocating
0.00.075.795 I ggml_metal_init: found device: Apple M4
0.00.075.798 I ggml_metal_init: picking default device: Apple M4
0.00.076.666 I ggml_metal_init: using embedded metal library
0.00.079.900 I ggml_metal_init: GPU name:   Apple M4
0.00.079.902 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.904 I ggml_metal_init: simdgroup reduction   = true
0.00.079.904 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.904 I ggml_metal_init: has bfloat            = true
0.00.079.904 I ggml_metal_init: use bfloat            = true
0.00.079.905 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.906 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.115.094 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.115.102 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.115.123 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.116.152 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.116.154 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.116.155 I llama_new_context_with_model: graph nodes  = 967
0.00.116.155 I llama_new_context_with_model: graph splits = 2
0.00.116.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.980.107 I main: llama threadpool init, n_threads = 4
0.00.980.176 I 
0.00.980.218 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.980.218 I 
0.00.980.551 I sampler seed: 1234
0.00.980.559 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.980.608 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.980.613 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.980.613 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.658.513 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.658.514 I llama_perf_context_print:        load time =     963.60 ms
0.01.658.514 I llama_perf_context_print: prompt eval time =      40.55 ms /     7 tokens (    5.79 ms per token,   172.63 tokens per second)
0.01.658.515 I llama_perf_context_print:        eval time =     634.14 ms /    63 runs   (   10.07 ms per token,    99.35 tokens per second)
0.01.658.516 I llama_perf_context_print:       total time =     678.41 ms /    70 tokens
0.01.658.697 I ggml_metal_free: deallocating

real	0m1.686s
user	0m0.129s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.526 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.361 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.367 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.369 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.369 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.369 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.370 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.370 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.371 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.371 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.371 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.372 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.373 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.374 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.226 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.273 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.083 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.085 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.085 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.085 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.086 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.086 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.087 I llama_model_loader: - type  f32:  194 tensors
0.00.024.087 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.087 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.172 I llm_load_vocab: special tokens cache size = 25
0.00.050.106 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.110 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.110 I llm_load_print_meta: arch             = gptneox
0.00.050.110 I llm_load_print_meta: vocab type       = BPE
0.00.050.111 I llm_load_print_meta: n_vocab          = 50304
0.00.050.111 I llm_load_print_meta: n_merges         = 50009
0.00.050.111 I llm_load_print_meta: vocab_only       = 0
0.00.050.111 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.111 I llm_load_print_meta: n_embd           = 2048
0.00.050.112 I llm_load_print_meta: n_layer          = 24
0.00.050.114 I llm_load_print_meta: n_head           = 16
0.00.050.114 I llm_load_print_meta: n_head_kv        = 16
0.00.050.115 I llm_load_print_meta: n_rot            = 32
0.00.050.115 I llm_load_print_meta: n_swa            = 0
0.00.050.115 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.115 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.116 I llm_load_print_meta: n_gqa            = 1
0.00.050.117 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.117 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.118 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.118 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.119 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.119 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.119 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.125 I llm_load_print_meta: n_ff             = 8192
0.00.050.125 I llm_load_print_meta: n_expert         = 0
0.00.050.125 I llm_load_print_meta: n_expert_used    = 0
0.00.050.126 I llm_load_print_meta: causal attn      = 1
0.00.050.126 I llm_load_print_meta: pooling type     = 0
0.00.050.127 I llm_load_print_meta: rope type        = 2
0.00.050.127 I llm_load_print_meta: rope scaling     = linear
0.00.050.127 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.128 I llm_load_print_meta: freq_scale_train = 1
0.00.050.128 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.128 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.129 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.129 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.129 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.129 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.129 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.141 I llm_load_print_meta: model type       = 1.4B
0.00.050.141 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.141 I llm_load_print_meta: model params     = 1.41 B
0.00.050.142 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.142 I llm_load_print_meta: general.name     = 1.4B
0.00.050.142 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.142 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.143 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.143 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.143 I llm_load_print_meta: LF token         = 128 ''
0.00.050.143 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.143 I llm_load_print_meta: max token length = 1024
0.00.051.867 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.867 I llm_load_tensors: offloading output layer to GPU
0.00.051.868 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.877 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.878 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.726 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.726 I llama_new_context_with_model: n_ctx         = 128
0.00.052.727 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.727 I llama_new_context_with_model: n_batch       = 128
0.00.052.727 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.727 I llama_new_context_with_model: flash_attn    = 0
0.00.052.728 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.728 I llama_new_context_with_model: freq_scale    = 1
0.00.052.728 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.729 I ggml_metal_init: allocating
0.00.052.736 I ggml_metal_init: found device: Apple M4
0.00.052.739 I ggml_metal_init: picking default device: Apple M4
0.00.053.293 I ggml_metal_init: using embedded metal library
0.00.055.258 I ggml_metal_init: GPU name:   Apple M4
0.00.055.260 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.260 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.261 I ggml_metal_init: simdgroup reduction   = true
0.00.055.261 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.261 I ggml_metal_init: has bfloat            = true
0.00.055.261 I ggml_metal_init: use bfloat            = true
0.00.055.261 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.262 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.376 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.379 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.394 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.338 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.339 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.339 I llama_new_context_with_model: graph nodes  = 967
0.00.065.339 I llama_new_context_with_model: graph splits = 2
0.00.065.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.223 I 
0.00.693.245 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.693.249 I perplexity: tokenizing the input ..
0.00.700.875 I perplexity: tokenization took 7.625 ms
0.00.700.888 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.065 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.825.150 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.825.163 I llama_perf_context_print:        load time =     683.69 ms
0.00.825.164 I llama_perf_context_print: prompt eval time =     122.96 ms /   128 tokens (    0.96 ms per token,  1041.02 tokens per second)
0.00.825.165 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.166 I llama_perf_context_print:       total time =     131.94 ms /   129 tokens
0.00.825.540 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.077s
sys	0m0.144s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.014.789 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.438 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.030.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.443 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.448 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.449 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.449 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.450 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.453 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.453 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.454 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.454 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.456 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.900 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.041.335 I llama_model_loader: - type  f32:  194 tensors
0.00.041.335 I llama_model_loader: - type q4_1:   97 tensors
0.00.041.336 I llama_model_loader: - type q6_K:    1 tensors
0.00.071.639 I llm_load_vocab: special tokens cache size = 25
0.00.082.004 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.082.008 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.082.008 I llm_load_print_meta: arch             = gptneox
0.00.082.009 I llm_load_print_meta: vocab type       = BPE
0.00.082.009 I llm_load_print_meta: n_vocab          = 50304
0.00.082.009 I llm_load_print_meta: n_merges         = 50009
0.00.082.009 I llm_load_print_meta: vocab_only       = 0
0.00.082.010 I llm_load_print_meta: n_ctx_train      = 2048
0.00.082.010 I llm_load_print_meta: n_embd           = 2048
0.00.082.010 I llm_load_print_meta: n_layer          = 24
0.00.082.013 I llm_load_print_meta: n_head           = 16
0.00.082.014 I llm_load_print_meta: n_head_kv        = 16
0.00.082.015 I llm_load_print_meta: n_rot            = 32
0.00.082.017 I llm_load_print_meta: n_swa            = 0
0.00.082.017 I llm_load_print_meta: n_embd_head_k    = 128
0.00.082.017 I llm_load_print_meta: n_embd_head_v    = 128
0.00.082.018 I llm_load_print_meta: n_gqa            = 1
0.00.082.019 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.082.020 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.082.021 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.082.021 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.082.022 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.082.022 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.082.022 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.082.023 I llm_load_print_meta: n_ff             = 8192
0.00.082.023 I llm_load_print_meta: n_expert         = 0
0.00.082.023 I llm_load_print_meta: n_expert_used    = 0
0.00.082.024 I llm_load_print_meta: causal attn      = 1
0.00.082.024 I llm_load_print_meta: pooling type     = 0
0.00.082.024 I llm_load_print_meta: rope type        = 2
0.00.082.024 I llm_load_print_meta: rope scaling     = linear
0.00.082.025 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.082.025 I llm_load_print_meta: freq_scale_train = 1
0.00.082.026 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.082.026 I llm_load_print_meta: rope_finetuned   = unknown
0.00.082.026 I llm_load_print_meta: ssm_d_conv       = 0
0.00.082.026 I llm_load_print_meta: ssm_d_inner      = 0
0.00.082.027 I llm_load_print_meta: ssm_d_state      = 0
0.00.082.028 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.082.034 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.082.047 I llm_load_print_meta: model type       = 1.4B
0.00.082.047 I llm_load_print_meta: model ftype      = Q4_1
0.00.082.048 I llm_load_print_meta: model params     = 1.41 B
0.00.082.048 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.082.049 I llm_load_print_meta: general.name     = 1.4B
0.00.082.049 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.082.049 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.082.050 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.082.050 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.082.050 I llm_load_print_meta: LF token         = 128 ''
0.00.082.051 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.082.053 I llm_load_print_meta: max token length = 1024
0.00.084.850 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.084.850 I llm_load_tensors: offloading output layer to GPU
0.00.084.851 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.084.861 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.084.863 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.086.163 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.164 I llama_new_context_with_model: n_ctx         = 2048
0.00.086.165 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.086.165 I llama_new_context_with_model: n_batch       = 2048
0.00.086.165 I llama_new_context_with_model: n_ubatch      = 512
0.00.086.166 I llama_new_context_with_model: flash_attn    = 0
0.00.086.166 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.167 I llama_new_context_with_model: freq_scale    = 1
0.00.086.167 I ggml_metal_init: allocating
0.00.086.171 I ggml_metal_init: found device: Apple M4
0.00.086.174 I ggml_metal_init: picking default device: Apple M4
0.00.086.965 I ggml_metal_init: using embedded metal library
0.00.089.997 I ggml_metal_init: GPU name:   Apple M4
0.00.089.999 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.000 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.000 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.001 I ggml_metal_init: simdgroup reduction   = true
0.00.090.001 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.001 I ggml_metal_init: has bfloat            = true
0.00.090.001 I ggml_metal_init: use bfloat            = true
0.00.090.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.122.638 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.646 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.665 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.123.640 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.123.642 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.123.642 I llama_new_context_with_model: graph nodes  = 967
0.00.123.642 I llama_new_context_with_model: graph splits = 2
0.00.123.664 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.944.882 I main: llama threadpool init, n_threads = 4
0.00.944.956 I 
0.00.944.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.944.991 I 
0.00.945.276 I sampler seed: 1234
0.00.945.284 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.945.333 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.945.336 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.945.337 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.671.711 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.671.711 I llama_perf_context_print:        load time =     930.09 ms
0.01.671.712 I llama_perf_context_print: prompt eval time =      41.40 ms /     7 tokens (    5.91 ms per token,   169.08 tokens per second)
0.01.671.714 I llama_perf_context_print:        eval time =     681.85 ms /    63 runs   (   10.82 ms per token,    92.40 tokens per second)
0.01.671.714 I llama_perf_context_print:       total time =     726.83 ms /    70 tokens
0.01.671.886 I ggml_metal_free: deallocating

real	0m1.698s
user	0m0.138s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.145 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.253 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.258 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.259 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.259 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.260 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.260 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.261 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.263 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.264 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.264 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.265 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.267 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.217 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.060 I llama_model_loader: - type  f32:  194 tensors
0.00.024.060 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.061 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.908 I llm_load_vocab: special tokens cache size = 25
0.00.051.102 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.105 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.105 I llm_load_print_meta: arch             = gptneox
0.00.051.106 I llm_load_print_meta: vocab type       = BPE
0.00.051.106 I llm_load_print_meta: n_vocab          = 50304
0.00.051.106 I llm_load_print_meta: n_merges         = 50009
0.00.051.106 I llm_load_print_meta: vocab_only       = 0
0.00.051.106 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.106 I llm_load_print_meta: n_embd           = 2048
0.00.051.107 I llm_load_print_meta: n_layer          = 24
0.00.051.110 I llm_load_print_meta: n_head           = 16
0.00.051.110 I llm_load_print_meta: n_head_kv        = 16
0.00.051.110 I llm_load_print_meta: n_rot            = 32
0.00.051.111 I llm_load_print_meta: n_swa            = 0
0.00.051.111 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.111 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.112 I llm_load_print_meta: n_gqa            = 1
0.00.051.113 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.113 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.114 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.114 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.114 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.115 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.115 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.115 I llm_load_print_meta: n_ff             = 8192
0.00.051.116 I llm_load_print_meta: n_expert         = 0
0.00.051.116 I llm_load_print_meta: n_expert_used    = 0
0.00.051.116 I llm_load_print_meta: causal attn      = 1
0.00.051.116 I llm_load_print_meta: pooling type     = 0
0.00.051.116 I llm_load_print_meta: rope type        = 2
0.00.051.116 I llm_load_print_meta: rope scaling     = linear
0.00.051.117 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.117 I llm_load_print_meta: freq_scale_train = 1
0.00.051.117 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.118 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.118 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.118 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.118 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.118 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.118 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.130 I llm_load_print_meta: model type       = 1.4B
0.00.051.130 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.130 I llm_load_print_meta: model params     = 1.41 B
0.00.051.131 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.132 I llm_load_print_meta: general.name     = 1.4B
0.00.051.132 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.134 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.134 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.134 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.134 I llm_load_print_meta: LF token         = 128 ''
0.00.051.134 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.134 I llm_load_print_meta: max token length = 1024
0.00.052.870 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.870 I llm_load_tensors: offloading output layer to GPU
0.00.052.871 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.880 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.881 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.746 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.747 I llama_new_context_with_model: n_ctx         = 128
0.00.053.747 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.747 I llama_new_context_with_model: n_batch       = 128
0.00.053.747 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.748 I llama_new_context_with_model: flash_attn    = 0
0.00.053.748 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.748 I llama_new_context_with_model: freq_scale    = 1
0.00.053.749 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.749 I ggml_metal_init: allocating
0.00.053.756 I ggml_metal_init: found device: Apple M4
0.00.053.759 I ggml_metal_init: picking default device: Apple M4
0.00.054.295 I ggml_metal_init: using embedded metal library
0.00.056.207 I ggml_metal_init: GPU name:   Apple M4
0.00.056.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.209 I ggml_metal_init: simdgroup reduction   = true
0.00.056.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.209 I ggml_metal_init: has bfloat            = true
0.00.056.210 I ggml_metal_init: use bfloat            = true
0.00.056.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.329 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.331 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.345 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.233 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.234 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.234 I llama_new_context_with_model: graph nodes  = 967
0.00.066.234 I llama_new_context_with_model: graph splits = 2
0.00.066.246 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.749 I 
0.00.731.771 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.731.774 I perplexity: tokenizing the input ..
0.00.739.463 I perplexity: tokenization took 7.687 ms
0.00.739.478 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.863.054 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.864.201 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.864.215 I llama_perf_context_print:        load time =     722.60 ms
0.00.864.216 I llama_perf_context_print: prompt eval time =     123.36 ms /   128 tokens (    0.96 ms per token,  1037.63 tokens per second)
0.00.864.217 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.218 I llama_perf_context_print:       total time =     132.47 ms /   129 tokens
0.00.864.607 I ggml_metal_free: deallocating

real	0m0.877s
user	0m0.078s
sys	0m0.144s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.883 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.147 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.150 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.151 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.151 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.153 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.154 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.156 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.157 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.157 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.124 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.212 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.158 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.158 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.159 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.159 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.037.160 I llama_model_loader: - type  f32:  194 tensors
0.00.037.160 I llama_model_loader: - type q5_0:   97 tensors
0.00.037.160 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.246 I llm_load_vocab: special tokens cache size = 25
0.00.067.864 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.867 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.867 I llm_load_print_meta: arch             = gptneox
0.00.067.867 I llm_load_print_meta: vocab type       = BPE
0.00.067.867 I llm_load_print_meta: n_vocab          = 50304
0.00.067.868 I llm_load_print_meta: n_merges         = 50009
0.00.067.868 I llm_load_print_meta: vocab_only       = 0
0.00.067.868 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.868 I llm_load_print_meta: n_embd           = 2048
0.00.067.868 I llm_load_print_meta: n_layer          = 24
0.00.067.871 I llm_load_print_meta: n_head           = 16
0.00.067.871 I llm_load_print_meta: n_head_kv        = 16
0.00.067.872 I llm_load_print_meta: n_rot            = 32
0.00.067.872 I llm_load_print_meta: n_swa            = 0
0.00.067.874 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.874 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.875 I llm_load_print_meta: n_gqa            = 1
0.00.067.875 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.876 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.876 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.877 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.877 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.877 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.878 I llm_load_print_meta: n_ff             = 8192
0.00.067.878 I llm_load_print_meta: n_expert         = 0
0.00.067.878 I llm_load_print_meta: n_expert_used    = 0
0.00.067.879 I llm_load_print_meta: causal attn      = 1
0.00.067.879 I llm_load_print_meta: pooling type     = 0
0.00.067.879 I llm_load_print_meta: rope type        = 2
0.00.067.880 I llm_load_print_meta: rope scaling     = linear
0.00.067.880 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.880 I llm_load_print_meta: freq_scale_train = 1
0.00.067.880 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.881 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.881 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.883 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.883 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.883 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.883 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.894 I llm_load_print_meta: model type       = 1.4B
0.00.067.894 I llm_load_print_meta: model ftype      = Q5_0
0.00.067.896 I llm_load_print_meta: model params     = 1.41 B
0.00.067.896 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.067.896 I llm_load_print_meta: general.name     = 1.4B
0.00.067.897 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.897 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.897 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.897 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.897 I llm_load_print_meta: LF token         = 128 ''
0.00.067.898 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.898 I llm_load_print_meta: max token length = 1024
0.00.069.779 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.779 I llm_load_tensors: offloading output layer to GPU
0.00.069.779 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.789 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.069.790 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.070.710 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.711 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.711 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.711 I llama_new_context_with_model: n_batch       = 2048
0.00.070.712 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.712 I llama_new_context_with_model: flash_attn    = 0
0.00.070.712 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.712 I llama_new_context_with_model: freq_scale    = 1
0.00.070.713 I ggml_metal_init: allocating
0.00.070.715 I ggml_metal_init: found device: Apple M4
0.00.070.718 I ggml_metal_init: picking default device: Apple M4
0.00.071.306 I ggml_metal_init: using embedded metal library
0.00.073.507 I ggml_metal_init: GPU name:   Apple M4
0.00.073.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.510 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.510 I ggml_metal_init: simdgroup reduction   = true
0.00.073.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.510 I ggml_metal_init: has bfloat            = true
0.00.073.510 I ggml_metal_init: use bfloat            = true
0.00.073.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.175 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.184 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.201 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.304 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.306 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.306 I llama_new_context_with_model: graph nodes  = 967
0.00.106.306 I llama_new_context_with_model: graph splits = 2
0.00.106.329 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.917.419 I main: llama threadpool init, n_threads = 4
0.00.917.493 I 
0.00.917.513 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.917.513 I 
0.00.917.665 I sampler seed: 1234
0.00.917.669 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.917.706 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.917.707 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.917.707 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.693.504 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.693.505 I llama_perf_context_print:        load time =     907.53 ms
0.01.693.505 I llama_perf_context_print: prompt eval time =      36.59 ms /     7 tokens (    5.23 ms per token,   191.31 tokens per second)
0.01.693.506 I llama_perf_context_print:        eval time =     736.11 ms /    63 runs   (   11.68 ms per token,    85.58 tokens per second)
0.01.693.506 I llama_perf_context_print:       total time =     776.09 ms /    70 tokens
0.01.693.675 I ggml_metal_free: deallocating

real	0m1.709s
user	0m0.116s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.989 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.651 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.660 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.661 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.661 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.662 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.662 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.663 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.663 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.664 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.664 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.664 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.665 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.665 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.667 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.667 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.485 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.269 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.270 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.271 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.271 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.271 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.271 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.272 I llama_model_loader: - type  f32:  194 tensors
0.00.026.272 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.273 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.207 I llm_load_vocab: special tokens cache size = 25
0.00.052.275 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.278 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.278 I llm_load_print_meta: arch             = gptneox
0.00.052.278 I llm_load_print_meta: vocab type       = BPE
0.00.052.278 I llm_load_print_meta: n_vocab          = 50304
0.00.052.279 I llm_load_print_meta: n_merges         = 50009
0.00.052.279 I llm_load_print_meta: vocab_only       = 0
0.00.052.279 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.279 I llm_load_print_meta: n_embd           = 2048
0.00.052.279 I llm_load_print_meta: n_layer          = 24
0.00.052.282 I llm_load_print_meta: n_head           = 16
0.00.052.283 I llm_load_print_meta: n_head_kv        = 16
0.00.052.283 I llm_load_print_meta: n_rot            = 32
0.00.052.283 I llm_load_print_meta: n_swa            = 0
0.00.052.283 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.284 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.284 I llm_load_print_meta: n_gqa            = 1
0.00.052.285 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.286 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.286 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.286 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.287 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.287 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.287 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.288 I llm_load_print_meta: n_ff             = 8192
0.00.052.288 I llm_load_print_meta: n_expert         = 0
0.00.052.288 I llm_load_print_meta: n_expert_used    = 0
0.00.052.288 I llm_load_print_meta: causal attn      = 1
0.00.052.288 I llm_load_print_meta: pooling type     = 0
0.00.052.288 I llm_load_print_meta: rope type        = 2
0.00.052.289 I llm_load_print_meta: rope scaling     = linear
0.00.052.289 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.289 I llm_load_print_meta: freq_scale_train = 1
0.00.052.289 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.290 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.292 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.292 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.292 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.292 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.292 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.304 I llm_load_print_meta: model type       = 1.4B
0.00.052.304 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.304 I llm_load_print_meta: model params     = 1.41 B
0.00.052.305 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.305 I llm_load_print_meta: general.name     = 1.4B
0.00.052.306 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.307 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.307 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.307 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.307 I llm_load_print_meta: LF token         = 128 ''
0.00.052.308 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.308 I llm_load_print_meta: max token length = 1024
0.00.054.039 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.039 I llm_load_tensors: offloading output layer to GPU
0.00.054.040 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.049 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.050 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.878 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.879 I llama_new_context_with_model: n_ctx         = 128
0.00.054.879 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.879 I llama_new_context_with_model: n_batch       = 128
0.00.054.880 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.880 I llama_new_context_with_model: flash_attn    = 0
0.00.054.880 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.880 I llama_new_context_with_model: freq_scale    = 1
0.00.054.881 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.881 I ggml_metal_init: allocating
0.00.054.886 I ggml_metal_init: found device: Apple M4
0.00.054.888 I ggml_metal_init: picking default device: Apple M4
0.00.055.415 I ggml_metal_init: using embedded metal library
0.00.057.286 I ggml_metal_init: GPU name:   Apple M4
0.00.057.287 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.288 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.288 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.288 I ggml_metal_init: simdgroup reduction   = true
0.00.057.288 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.289 I ggml_metal_init: has bfloat            = true
0.00.057.289 I ggml_metal_init: use bfloat            = true
0.00.057.289 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.290 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.386 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.390 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.411 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.281 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.282 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.283 I llama_new_context_with_model: graph nodes  = 967
0.00.067.283 I llama_new_context_with_model: graph splits = 2
0.00.067.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.258 I 
0.00.762.282 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.762.285 I perplexity: tokenizing the input ..
0.00.769.616 I perplexity: tokenization took 7.328 ms
0.00.769.630 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.905.226 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.906.368 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.906.384 I llama_perf_context_print:        load time =     750.26 ms
0.00.906.385 I llama_perf_context_print: prompt eval time =     135.38 ms /   128 tokens (    1.06 ms per token,   945.51 tokens per second)
0.00.906.386 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.388 I llama_perf_context_print:       total time =     144.12 ms /   129 tokens
0.00.906.795 I ggml_metal_free: deallocating

real	0m0.923s
user	0m0.077s
sys	0m0.160s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.146 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.501 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.505 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.514 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.515 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.515 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.517 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.518 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.406 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.492 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.314 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.315 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.315 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.316 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.316 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.317 I llama_model_loader: - type  f32:  194 tensors
0.00.025.317 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.317 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.552 I llm_load_vocab: special tokens cache size = 25
0.00.051.708 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.711 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.711 I llm_load_print_meta: arch             = gptneox
0.00.051.711 I llm_load_print_meta: vocab type       = BPE
0.00.051.712 I llm_load_print_meta: n_vocab          = 50304
0.00.051.712 I llm_load_print_meta: n_merges         = 50009
0.00.051.712 I llm_load_print_meta: vocab_only       = 0
0.00.051.712 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.712 I llm_load_print_meta: n_embd           = 2048
0.00.051.712 I llm_load_print_meta: n_layer          = 24
0.00.051.715 I llm_load_print_meta: n_head           = 16
0.00.051.716 I llm_load_print_meta: n_head_kv        = 16
0.00.051.716 I llm_load_print_meta: n_rot            = 32
0.00.051.716 I llm_load_print_meta: n_swa            = 0
0.00.051.718 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.718 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.719 I llm_load_print_meta: n_gqa            = 1
0.00.051.720 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.721 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.726 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.726 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.726 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.727 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.727 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.728 I llm_load_print_meta: n_ff             = 8192
0.00.051.728 I llm_load_print_meta: n_expert         = 0
0.00.051.728 I llm_load_print_meta: n_expert_used    = 0
0.00.051.728 I llm_load_print_meta: causal attn      = 1
0.00.051.728 I llm_load_print_meta: pooling type     = 0
0.00.051.729 I llm_load_print_meta: rope type        = 2
0.00.051.729 I llm_load_print_meta: rope scaling     = linear
0.00.051.729 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.729 I llm_load_print_meta: freq_scale_train = 1
0.00.051.730 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.732 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.732 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.732 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.732 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.732 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.732 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.743 I llm_load_print_meta: model type       = 1.4B
0.00.051.744 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.744 I llm_load_print_meta: model params     = 1.41 B
0.00.051.745 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.745 I llm_load_print_meta: general.name     = 1.4B
0.00.051.747 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.747 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.747 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.747 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.748 I llm_load_print_meta: LF token         = 128 ''
0.00.051.748 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.748 I llm_load_print_meta: max token length = 1024
0.00.053.532 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.532 I llm_load_tensors: offloading output layer to GPU
0.00.053.532 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.542 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.543 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.436 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.437 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.437 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.437 I llama_new_context_with_model: n_batch       = 2048
0.00.054.437 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.437 I llama_new_context_with_model: flash_attn    = 0
0.00.054.438 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.438 I llama_new_context_with_model: freq_scale    = 1
0.00.054.439 I ggml_metal_init: allocating
0.00.054.443 I ggml_metal_init: found device: Apple M4
0.00.054.446 I ggml_metal_init: picking default device: Apple M4
0.00.054.976 I ggml_metal_init: using embedded metal library
0.00.056.902 I ggml_metal_init: GPU name:   Apple M4
0.00.056.904 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.904 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.904 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.905 I ggml_metal_init: simdgroup reduction   = true
0.00.056.905 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.905 I ggml_metal_init: has bfloat            = true
0.00.056.905 I ggml_metal_init: use bfloat            = true
0.00.056.905 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.906 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.536 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.542 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.559 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.556 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.557 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.557 I llama_new_context_with_model: graph nodes  = 967
0.00.085.558 I llama_new_context_with_model: graph splits = 2
0.00.085.582 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.624 I main: llama threadpool init, n_threads = 4
0.00.780.661 I 
0.00.780.677 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.780.678 I 
0.00.780.846 I sampler seed: 1234
0.00.780.851 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.860 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.861 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.861 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.617.203 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48830.81 tokens per second)
0.01.617.204 I llama_perf_context_print:        load time =     770.48 ms
0.01.617.205 I llama_perf_context_print: prompt eval time =      36.58 ms /     7 tokens (    5.23 ms per token,   191.37 tokens per second)
0.01.617.206 I llama_perf_context_print:        eval time =     796.42 ms /    63 runs   (   12.64 ms per token,    79.10 tokens per second)
0.01.617.206 I llama_perf_context_print:       total time =     836.58 ms /    70 tokens
0.01.617.390 I ggml_metal_free: deallocating

real	0m1.634s
user	0m0.108s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.832 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.621 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.625 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.627 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.628 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.628 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.629 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.635 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.637 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.329 I llama_model_loader: - type  f32:  194 tensors
0.00.023.329 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.329 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.346 I llm_load_vocab: special tokens cache size = 25
0.00.049.454 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.457 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.457 I llm_load_print_meta: arch             = gptneox
0.00.049.458 I llm_load_print_meta: vocab type       = BPE
0.00.049.458 I llm_load_print_meta: n_vocab          = 50304
0.00.049.458 I llm_load_print_meta: n_merges         = 50009
0.00.049.458 I llm_load_print_meta: vocab_only       = 0
0.00.049.458 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.459 I llm_load_print_meta: n_embd           = 2048
0.00.049.459 I llm_load_print_meta: n_layer          = 24
0.00.049.461 I llm_load_print_meta: n_head           = 16
0.00.049.462 I llm_load_print_meta: n_head_kv        = 16
0.00.049.462 I llm_load_print_meta: n_rot            = 32
0.00.049.462 I llm_load_print_meta: n_swa            = 0
0.00.049.463 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.463 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.463 I llm_load_print_meta: n_gqa            = 1
0.00.049.464 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.465 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.466 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.466 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.466 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.466 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.466 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.467 I llm_load_print_meta: n_ff             = 8192
0.00.049.468 I llm_load_print_meta: n_expert         = 0
0.00.049.468 I llm_load_print_meta: n_expert_used    = 0
0.00.049.468 I llm_load_print_meta: causal attn      = 1
0.00.049.468 I llm_load_print_meta: pooling type     = 0
0.00.049.468 I llm_load_print_meta: rope type        = 2
0.00.049.468 I llm_load_print_meta: rope scaling     = linear
0.00.049.469 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.469 I llm_load_print_meta: freq_scale_train = 1
0.00.049.469 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.469 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.470 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.470 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.470 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.470 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.470 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.482 I llm_load_print_meta: model type       = 1.4B
0.00.049.482 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.482 I llm_load_print_meta: model params     = 1.41 B
0.00.049.483 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.484 I llm_load_print_meta: general.name     = 1.4B
0.00.049.485 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.485 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.485 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.485 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.485 I llm_load_print_meta: LF token         = 128 ''
0.00.049.486 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.486 I llm_load_print_meta: max token length = 1024
0.00.051.226 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.226 I llm_load_tensors: offloading output layer to GPU
0.00.051.227 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.236 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.237 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.094 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.095 I llama_new_context_with_model: n_ctx         = 128
0.00.052.095 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.095 I llama_new_context_with_model: n_batch       = 128
0.00.052.096 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.096 I llama_new_context_with_model: flash_attn    = 0
0.00.052.096 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.097 I llama_new_context_with_model: freq_scale    = 1
0.00.052.097 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.097 I ggml_metal_init: allocating
0.00.052.101 I ggml_metal_init: found device: Apple M4
0.00.052.102 I ggml_metal_init: picking default device: Apple M4
0.00.052.630 I ggml_metal_init: using embedded metal library
0.00.054.587 I ggml_metal_init: GPU name:   Apple M4
0.00.054.589 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.590 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.590 I ggml_metal_init: simdgroup reduction   = true
0.00.054.590 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.590 I ggml_metal_init: has bfloat            = true
0.00.054.591 I ggml_metal_init: use bfloat            = true
0.00.054.591 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.592 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.676 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.680 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.695 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.565 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.567 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.567 I llama_new_context_with_model: graph nodes  = 967
0.00.064.567 I llama_new_context_with_model: graph splits = 2
0.00.064.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.000 I 
0.00.726.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.726.029 I perplexity: tokenizing the input ..
0.00.733.578 I perplexity: tokenization took 7.546 ms
0.00.733.592 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.868.552 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.869.646 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.869.664 I llama_perf_context_print:        load time =     717.16 ms
0.00.869.665 I llama_perf_context_print: prompt eval time =     134.74 ms /   128 tokens (    1.05 ms per token,   949.96 tokens per second)
0.00.869.665 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.869.666 I llama_perf_context_print:       total time =     143.67 ms /   129 tokens
0.00.870.029 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.077s
sys	0m0.163s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.191 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.865 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.870 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.871 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.872 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.872 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.872 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.873 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.874 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.874 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.877 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.877 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.878 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.880 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.880 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.880 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.707 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.633 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.635 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.635 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.635 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.636 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.637 I llama_model_loader: - type  f32:  194 tensors
0.00.024.637 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.637 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.637 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.033 I llm_load_vocab: special tokens cache size = 25
0.00.051.247 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.250 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.250 I llm_load_print_meta: arch             = gptneox
0.00.051.251 I llm_load_print_meta: vocab type       = BPE
0.00.051.251 I llm_load_print_meta: n_vocab          = 50304
0.00.051.251 I llm_load_print_meta: n_merges         = 50009
0.00.051.251 I llm_load_print_meta: vocab_only       = 0
0.00.051.251 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.252 I llm_load_print_meta: n_embd           = 2048
0.00.051.252 I llm_load_print_meta: n_layer          = 24
0.00.051.255 I llm_load_print_meta: n_head           = 16
0.00.051.256 I llm_load_print_meta: n_head_kv        = 16
0.00.051.257 I llm_load_print_meta: n_rot            = 32
0.00.051.257 I llm_load_print_meta: n_swa            = 0
0.00.051.257 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.257 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.258 I llm_load_print_meta: n_gqa            = 1
0.00.051.259 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.259 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.260 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.260 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.260 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.260 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.261 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.261 I llm_load_print_meta: n_ff             = 8192
0.00.051.261 I llm_load_print_meta: n_expert         = 0
0.00.051.262 I llm_load_print_meta: n_expert_used    = 0
0.00.051.262 I llm_load_print_meta: causal attn      = 1
0.00.051.262 I llm_load_print_meta: pooling type     = 0
0.00.051.262 I llm_load_print_meta: rope type        = 2
0.00.051.262 I llm_load_print_meta: rope scaling     = linear
0.00.051.264 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.264 I llm_load_print_meta: freq_scale_train = 1
0.00.051.265 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.265 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.265 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.265 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.265 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.266 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.266 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.277 I llm_load_print_meta: model type       = 1.4B
0.00.051.277 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.278 I llm_load_print_meta: model params     = 1.41 B
0.00.051.278 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.278 I llm_load_print_meta: general.name     = 1.4B
0.00.051.279 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.279 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.279 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.279 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.279 I llm_load_print_meta: LF token         = 128 ''
0.00.051.280 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.280 I llm_load_print_meta: max token length = 1024
0.00.053.043 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.043 I llm_load_tensors: offloading output layer to GPU
0.00.053.043 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.053 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.054 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.916 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.917 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.917 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.918 I llama_new_context_with_model: n_batch       = 2048
0.00.053.918 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.918 I llama_new_context_with_model: flash_attn    = 0
0.00.053.919 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.919 I llama_new_context_with_model: freq_scale    = 1
0.00.053.919 I ggml_metal_init: allocating
0.00.053.927 I ggml_metal_init: found device: Apple M4
0.00.053.929 I ggml_metal_init: picking default device: Apple M4
0.00.054.484 I ggml_metal_init: using embedded metal library
0.00.056.450 I ggml_metal_init: GPU name:   Apple M4
0.00.056.453 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.453 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.453 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.454 I ggml_metal_init: simdgroup reduction   = true
0.00.056.454 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.454 I ggml_metal_init: has bfloat            = true
0.00.056.454 I ggml_metal_init: use bfloat            = true
0.00.056.455 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.455 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.158 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.164 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.181 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.216 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.217 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.217 I llama_new_context_with_model: graph nodes  = 967
0.00.085.218 I llama_new_context_with_model: graph splits = 2
0.00.085.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.187 I main: llama threadpool init, n_threads = 4
0.00.476.222 I 
0.00.476.241 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.476.244 I 
0.00.476.408 I sampler seed: 1234
0.00.476.413 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.476.432 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.476.432 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.476.432 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.155.051 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.155.052 I llama_perf_context_print:        load time =     465.99 ms
0.01.155.053 I llama_perf_context_print: prompt eval time =      35.68 ms /     7 tokens (    5.10 ms per token,   196.18 tokens per second)
0.01.155.053 I llama_perf_context_print:        eval time =     639.76 ms /    63 runs   (   10.15 ms per token,    98.47 tokens per second)
0.01.155.054 I llama_perf_context_print:       total time =     678.87 ms /    70 tokens
0.01.155.218 I ggml_metal_free: deallocating

real	0m1.173s
user	0m0.110s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.246 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.873 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.877 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.879 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.879 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.880 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.880 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.880 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.883 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.883 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.700 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.547 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.548 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.549 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.549 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.550 I llama_model_loader: - type  f32:  194 tensors
0.00.024.550 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.550 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.551 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.481 I llm_load_vocab: special tokens cache size = 25
0.00.051.457 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.460 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.460 I llm_load_print_meta: arch             = gptneox
0.00.051.460 I llm_load_print_meta: vocab type       = BPE
0.00.051.461 I llm_load_print_meta: n_vocab          = 50304
0.00.051.461 I llm_load_print_meta: n_merges         = 50009
0.00.051.461 I llm_load_print_meta: vocab_only       = 0
0.00.051.461 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.461 I llm_load_print_meta: n_embd           = 2048
0.00.051.461 I llm_load_print_meta: n_layer          = 24
0.00.051.464 I llm_load_print_meta: n_head           = 16
0.00.051.465 I llm_load_print_meta: n_head_kv        = 16
0.00.051.465 I llm_load_print_meta: n_rot            = 32
0.00.051.465 I llm_load_print_meta: n_swa            = 0
0.00.051.465 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.468 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.468 I llm_load_print_meta: n_gqa            = 1
0.00.051.469 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.470 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.470 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.471 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.471 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.471 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.471 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.472 I llm_load_print_meta: n_ff             = 8192
0.00.051.472 I llm_load_print_meta: n_expert         = 0
0.00.051.472 I llm_load_print_meta: n_expert_used    = 0
0.00.051.472 I llm_load_print_meta: causal attn      = 1
0.00.051.472 I llm_load_print_meta: pooling type     = 0
0.00.051.472 I llm_load_print_meta: rope type        = 2
0.00.051.473 I llm_load_print_meta: rope scaling     = linear
0.00.051.473 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.473 I llm_load_print_meta: freq_scale_train = 1
0.00.051.474 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.474 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.480 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.482 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.482 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.482 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.482 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.494 I llm_load_print_meta: model type       = 1.4B
0.00.051.495 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.495 I llm_load_print_meta: model params     = 1.41 B
0.00.051.496 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.496 I llm_load_print_meta: general.name     = 1.4B
0.00.051.496 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.497 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.497 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.497 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.497 I llm_load_print_meta: LF token         = 128 ''
0.00.051.498 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.498 I llm_load_print_meta: max token length = 1024
0.00.053.193 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.194 I llm_load_tensors: offloading output layer to GPU
0.00.053.194 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.203 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.204 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.052 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.053 I llama_new_context_with_model: n_ctx         = 128
0.00.054.053 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.053 I llama_new_context_with_model: n_batch       = 128
0.00.054.053 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.053 I llama_new_context_with_model: flash_attn    = 0
0.00.054.054 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.054 I llama_new_context_with_model: freq_scale    = 1
0.00.054.054 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.055 I ggml_metal_init: allocating
0.00.054.058 I ggml_metal_init: found device: Apple M4
0.00.054.060 I ggml_metal_init: picking default device: Apple M4
0.00.054.580 I ggml_metal_init: using embedded metal library
0.00.056.503 I ggml_metal_init: GPU name:   Apple M4
0.00.056.504 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.504 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.505 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.505 I ggml_metal_init: simdgroup reduction   = true
0.00.056.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.505 I ggml_metal_init: has bfloat            = true
0.00.056.505 I ggml_metal_init: use bfloat            = true
0.00.056.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.371 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.373 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.388 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.237 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.238 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.238 I llama_new_context_with_model: graph nodes  = 967
0.00.066.238 I llama_new_context_with_model: graph splits = 2
0.00.066.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.432.265 I 
0.00.432.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.432.295 I perplexity: tokenizing the input ..
0.00.439.947 I perplexity: tokenization took 7.651 ms
0.00.439.961 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.572.661 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.573.773 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.573.785 I llama_perf_context_print:        load time =     422.02 ms
0.00.573.786 I llama_perf_context_print: prompt eval time =     132.48 ms /   128 tokens (    1.04 ms per token,   966.18 tokens per second)
0.00.573.787 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.573.787 I llama_perf_context_print:       total time =     141.52 ms /   129 tokens
0.00.574.226 I ggml_metal_free: deallocating

real	0m0.591s
user	0m0.078s
sys	0m0.098s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.145 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.741 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.742 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.742 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.743 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.743 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.744 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.799 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.677 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.678 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.678 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.678 I llama_model_loader: - type  f32:  194 tensors
0.00.024.679 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.679 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.679 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.679 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.769 I llm_load_vocab: special tokens cache size = 25
0.00.051.979 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.982 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.982 I llm_load_print_meta: arch             = gptneox
0.00.051.982 I llm_load_print_meta: vocab type       = BPE
0.00.051.983 I llm_load_print_meta: n_vocab          = 50304
0.00.051.983 I llm_load_print_meta: n_merges         = 50009
0.00.051.983 I llm_load_print_meta: vocab_only       = 0
0.00.051.983 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.983 I llm_load_print_meta: n_embd           = 2048
0.00.051.983 I llm_load_print_meta: n_layer          = 24
0.00.051.986 I llm_load_print_meta: n_head           = 16
0.00.051.986 I llm_load_print_meta: n_head_kv        = 16
0.00.051.986 I llm_load_print_meta: n_rot            = 32
0.00.051.987 I llm_load_print_meta: n_swa            = 0
0.00.051.987 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.987 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.988 I llm_load_print_meta: n_gqa            = 1
0.00.051.989 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.989 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.990 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.990 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.991 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.991 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.991 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.991 I llm_load_print_meta: n_ff             = 8192
0.00.051.993 I llm_load_print_meta: n_expert         = 0
0.00.051.995 I llm_load_print_meta: n_expert_used    = 0
0.00.051.995 I llm_load_print_meta: causal attn      = 1
0.00.051.995 I llm_load_print_meta: pooling type     = 0
0.00.051.995 I llm_load_print_meta: rope type        = 2
0.00.051.995 I llm_load_print_meta: rope scaling     = linear
0.00.051.996 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.996 I llm_load_print_meta: freq_scale_train = 1
0.00.051.996 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.998 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.998 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.998 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.999 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.999 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.999 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.010 I llm_load_print_meta: model type       = 1.4B
0.00.052.012 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.012 I llm_load_print_meta: model params     = 1.41 B
0.00.052.013 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.013 I llm_load_print_meta: general.name     = 1.4B
0.00.052.013 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.013 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.013 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.013 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.014 I llm_load_print_meta: LF token         = 128 ''
0.00.052.014 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.014 I llm_load_print_meta: max token length = 1024
0.00.053.769 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.770 I llm_load_tensors: offloading output layer to GPU
0.00.053.770 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.779 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.780 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.647 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.648 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.648 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.649 I llama_new_context_with_model: n_batch       = 2048
0.00.054.649 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.649 I llama_new_context_with_model: flash_attn    = 0
0.00.054.649 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.650 I llama_new_context_with_model: freq_scale    = 1
0.00.054.650 I ggml_metal_init: allocating
0.00.054.654 I ggml_metal_init: found device: Apple M4
0.00.054.656 I ggml_metal_init: picking default device: Apple M4
0.00.055.207 I ggml_metal_init: using embedded metal library
0.00.057.138 I ggml_metal_init: GPU name:   Apple M4
0.00.057.142 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.142 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.143 I ggml_metal_init: simdgroup reduction   = true
0.00.057.143 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.143 I ggml_metal_init: has bfloat            = true
0.00.057.143 I ggml_metal_init: use bfloat            = true
0.00.057.143 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.144 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.060 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.067 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.083 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.988 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.990 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.990 I llama_new_context_with_model: graph nodes  = 967
0.00.084.990 I llama_new_context_with_model: graph splits = 2
0.00.085.012 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.154 I main: llama threadpool init, n_threads = 4
0.00.591.195 I 
0.00.591.216 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.591.216 I 
0.00.591.377 I sampler seed: 1234
0.00.591.381 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.591.413 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.591.414 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.591.414 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.326.819 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.326.820 I llama_perf_context_print:        load time =     582.00 ms
0.01.326.821 I llama_perf_context_print: prompt eval time =      35.63 ms /     7 tokens (    5.09 ms per token,   196.44 tokens per second)
0.01.326.825 I llama_perf_context_print:        eval time =     696.61 ms /    63 runs   (   11.06 ms per token,    90.44 tokens per second)
0.01.326.825 I llama_perf_context_print:       total time =     735.67 ms /    70 tokens
0.01.326.995 I ggml_metal_free: deallocating

real	0m1.341s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.441 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.384 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.389 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.392 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.395 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.395 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.396 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.396 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.401 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.291 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.132 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.134 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.134 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.136 I llama_model_loader: - type  f32:  194 tensors
0.00.025.136 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.136 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.137 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.137 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.862 I llm_load_vocab: special tokens cache size = 25
0.00.051.916 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.919 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.919 I llm_load_print_meta: arch             = gptneox
0.00.051.920 I llm_load_print_meta: vocab type       = BPE
0.00.051.920 I llm_load_print_meta: n_vocab          = 50304
0.00.051.920 I llm_load_print_meta: n_merges         = 50009
0.00.051.920 I llm_load_print_meta: vocab_only       = 0
0.00.051.920 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.921 I llm_load_print_meta: n_embd           = 2048
0.00.051.921 I llm_load_print_meta: n_layer          = 24
0.00.051.923 I llm_load_print_meta: n_head           = 16
0.00.051.924 I llm_load_print_meta: n_head_kv        = 16
0.00.051.926 I llm_load_print_meta: n_rot            = 32
0.00.051.926 I llm_load_print_meta: n_swa            = 0
0.00.051.926 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.926 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.927 I llm_load_print_meta: n_gqa            = 1
0.00.051.928 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.929 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.929 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.930 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.930 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.930 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.930 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.931 I llm_load_print_meta: n_ff             = 8192
0.00.051.931 I llm_load_print_meta: n_expert         = 0
0.00.051.932 I llm_load_print_meta: n_expert_used    = 0
0.00.051.932 I llm_load_print_meta: causal attn      = 1
0.00.051.932 I llm_load_print_meta: pooling type     = 0
0.00.051.932 I llm_load_print_meta: rope type        = 2
0.00.051.934 I llm_load_print_meta: rope scaling     = linear
0.00.051.934 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.934 I llm_load_print_meta: freq_scale_train = 1
0.00.051.935 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.935 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.935 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.935 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.935 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.936 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.936 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.947 I llm_load_print_meta: model type       = 1.4B
0.00.051.947 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.948 I llm_load_print_meta: model params     = 1.41 B
0.00.051.948 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.948 I llm_load_print_meta: general.name     = 1.4B
0.00.051.949 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.949 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.949 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.949 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.949 I llm_load_print_meta: LF token         = 128 ''
0.00.051.950 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.950 I llm_load_print_meta: max token length = 1024
0.00.053.725 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.725 I llm_load_tensors: offloading output layer to GPU
0.00.053.725 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.734 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.735 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.578 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.579 I llama_new_context_with_model: n_ctx         = 128
0.00.054.579 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.579 I llama_new_context_with_model: n_batch       = 128
0.00.054.580 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.580 I llama_new_context_with_model: flash_attn    = 0
0.00.054.580 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.581 I llama_new_context_with_model: freq_scale    = 1
0.00.054.581 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.581 I ggml_metal_init: allocating
0.00.054.587 I ggml_metal_init: found device: Apple M4
0.00.054.590 I ggml_metal_init: picking default device: Apple M4
0.00.055.115 I ggml_metal_init: using embedded metal library
0.00.057.068 I ggml_metal_init: GPU name:   Apple M4
0.00.057.070 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.071 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.071 I ggml_metal_init: simdgroup reduction   = true
0.00.057.071 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.071 I ggml_metal_init: has bfloat            = true
0.00.057.071 I ggml_metal_init: use bfloat            = true
0.00.057.072 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.943 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.947 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.960 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.829 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.830 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.830 I llama_new_context_with_model: graph nodes  = 967
0.00.066.830 I llama_new_context_with_model: graph splits = 2
0.00.066.842 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.029 I 
0.00.538.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.538.061 I perplexity: tokenizing the input ..
0.00.545.549 I perplexity: tokenization took 7.486 ms
0.00.545.563 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.677.864 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.678.965 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.678.982 I llama_perf_context_print:        load time =     527.58 ms
0.00.678.982 I llama_perf_context_print: prompt eval time =     132.08 ms /   128 tokens (    1.03 ms per token,   969.10 tokens per second)
0.00.678.983 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.678.983 I llama_perf_context_print:       total time =     140.95 ms /   129 tokens
0.00.679.379 I ggml_metal_free: deallocating

real	0m0.692s
user	0m0.078s
sys	0m0.121s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.883 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.603 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.613 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.613 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.613 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.614 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.614 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.615 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.615 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.616 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.616 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.616 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.617 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.617 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.619 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.619 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.540 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.415 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.417 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.417 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.417 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.418 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.418 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.419 I llama_model_loader: - type  f32:  194 tensors
0.00.025.419 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.419 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.419 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.043 I llm_load_vocab: special tokens cache size = 25
0.00.052.165 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.168 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.168 I llm_load_print_meta: arch             = gptneox
0.00.052.168 I llm_load_print_meta: vocab type       = BPE
0.00.052.169 I llm_load_print_meta: n_vocab          = 50304
0.00.052.169 I llm_load_print_meta: n_merges         = 50009
0.00.052.169 I llm_load_print_meta: vocab_only       = 0
0.00.052.169 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.169 I llm_load_print_meta: n_embd           = 2048
0.00.052.170 I llm_load_print_meta: n_layer          = 24
0.00.052.172 I llm_load_print_meta: n_head           = 16
0.00.052.173 I llm_load_print_meta: n_head_kv        = 16
0.00.052.173 I llm_load_print_meta: n_rot            = 32
0.00.052.173 I llm_load_print_meta: n_swa            = 0
0.00.052.174 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.174 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.174 I llm_load_print_meta: n_gqa            = 1
0.00.052.175 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.176 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.177 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.177 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.177 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.180 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.180 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.181 I llm_load_print_meta: n_ff             = 8192
0.00.052.181 I llm_load_print_meta: n_expert         = 0
0.00.052.181 I llm_load_print_meta: n_expert_used    = 0
0.00.052.181 I llm_load_print_meta: causal attn      = 1
0.00.052.181 I llm_load_print_meta: pooling type     = 0
0.00.052.181 I llm_load_print_meta: rope type        = 2
0.00.052.182 I llm_load_print_meta: rope scaling     = linear
0.00.052.182 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.182 I llm_load_print_meta: freq_scale_train = 1
0.00.052.183 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.183 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.183 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.183 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.185 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.185 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.185 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.197 I llm_load_print_meta: model type       = 1.4B
0.00.052.197 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.197 I llm_load_print_meta: model params     = 1.41 B
0.00.052.198 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.198 I llm_load_print_meta: general.name     = 1.4B
0.00.052.198 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.199 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.199 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.199 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.199 I llm_load_print_meta: LF token         = 128 ''
0.00.052.199 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.200 I llm_load_print_meta: max token length = 1024
0.00.053.961 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.961 I llm_load_tensors: offloading output layer to GPU
0.00.053.961 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.970 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.971 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.850 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.850 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.851 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.851 I llama_new_context_with_model: n_batch       = 2048
0.00.054.851 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.851 I llama_new_context_with_model: flash_attn    = 0
0.00.054.852 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.852 I llama_new_context_with_model: freq_scale    = 1
0.00.054.852 I ggml_metal_init: allocating
0.00.054.855 I ggml_metal_init: found device: Apple M4
0.00.054.857 I ggml_metal_init: picking default device: Apple M4
0.00.055.431 I ggml_metal_init: using embedded metal library
0.00.057.412 I ggml_metal_init: GPU name:   Apple M4
0.00.057.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.414 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.414 I ggml_metal_init: simdgroup reduction   = true
0.00.057.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.415 I ggml_metal_init: has bfloat            = true
0.00.057.415 I ggml_metal_init: use bfloat            = true
0.00.057.415 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.416 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.315 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.320 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.339 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.319 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.321 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.321 I llama_new_context_with_model: graph nodes  = 967
0.00.086.321 I llama_new_context_with_model: graph splits = 2
0.00.086.344 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.753 I main: llama threadpool init, n_threads = 4
0.00.690.790 I 
0.00.690.810 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.690.810 I 
0.00.690.961 I sampler seed: 1234
0.00.690.967 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.995 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.004 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.004 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.434.759 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.434.761 I llama_perf_context_print:        load time =     680.87 ms
0.01.434.762 I llama_perf_context_print: prompt eval time =      36.87 ms /     7 tokens (    5.27 ms per token,   189.88 tokens per second)
0.01.434.762 I llama_perf_context_print:        eval time =     703.79 ms /    63 runs   (   11.17 ms per token,    89.52 tokens per second)
0.01.434.763 I llama_perf_context_print:       total time =     744.01 ms /    70 tokens
0.01.434.961 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.110s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.366 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.059 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.063 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.065 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.066 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.066 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.066 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.067 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.068 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.068 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.068 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.069 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.069 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.069 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.071 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.071 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.072 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.897 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.898 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.900 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.900 I llama_model_loader: - type  f32:  194 tensors
0.00.024.901 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.901 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.901 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.787 I llm_load_vocab: special tokens cache size = 25
0.00.051.943 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.946 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.946 I llm_load_print_meta: arch             = gptneox
0.00.051.946 I llm_load_print_meta: vocab type       = BPE
0.00.051.947 I llm_load_print_meta: n_vocab          = 50304
0.00.051.947 I llm_load_print_meta: n_merges         = 50009
0.00.051.947 I llm_load_print_meta: vocab_only       = 0
0.00.051.947 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.947 I llm_load_print_meta: n_embd           = 2048
0.00.051.947 I llm_load_print_meta: n_layer          = 24
0.00.051.950 I llm_load_print_meta: n_head           = 16
0.00.051.950 I llm_load_print_meta: n_head_kv        = 16
0.00.051.950 I llm_load_print_meta: n_rot            = 32
0.00.051.952 I llm_load_print_meta: n_swa            = 0
0.00.051.952 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.952 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.953 I llm_load_print_meta: n_gqa            = 1
0.00.051.954 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.954 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.955 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.955 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.957 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.957 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.957 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.958 I llm_load_print_meta: n_ff             = 8192
0.00.051.958 I llm_load_print_meta: n_expert         = 0
0.00.051.958 I llm_load_print_meta: n_expert_used    = 0
0.00.051.958 I llm_load_print_meta: causal attn      = 1
0.00.051.958 I llm_load_print_meta: pooling type     = 0
0.00.051.959 I llm_load_print_meta: rope type        = 2
0.00.051.959 I llm_load_print_meta: rope scaling     = linear
0.00.051.959 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.959 I llm_load_print_meta: freq_scale_train = 1
0.00.051.960 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.960 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.960 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.960 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.960 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.960 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.961 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.972 I llm_load_print_meta: model type       = 1.4B
0.00.051.972 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.973 I llm_load_print_meta: model params     = 1.41 B
0.00.051.973 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.973 I llm_load_print_meta: general.name     = 1.4B
0.00.051.973 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.974 I llm_load_print_meta: LF token         = 128 ''
0.00.051.974 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.975 I llm_load_print_meta: max token length = 1024
0.00.053.679 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.680 I llm_load_tensors: offloading output layer to GPU
0.00.053.680 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.689 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.690 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.499 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.500 I llama_new_context_with_model: n_ctx         = 128
0.00.054.500 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.500 I llama_new_context_with_model: n_batch       = 128
0.00.054.501 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.501 I llama_new_context_with_model: flash_attn    = 0
0.00.054.501 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.502 I llama_new_context_with_model: freq_scale    = 1
0.00.054.502 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.502 I ggml_metal_init: allocating
0.00.054.505 I ggml_metal_init: found device: Apple M4
0.00.054.507 I ggml_metal_init: picking default device: Apple M4
0.00.055.032 I ggml_metal_init: using embedded metal library
0.00.057.002 I ggml_metal_init: GPU name:   Apple M4
0.00.057.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.004 I ggml_metal_init: simdgroup reduction   = true
0.00.057.005 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.005 I ggml_metal_init: has bfloat            = true
0.00.057.005 I ggml_metal_init: use bfloat            = true
0.00.057.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.006 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.872 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.876 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.891 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.768 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.769 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.770 I llama_new_context_with_model: graph nodes  = 967
0.00.066.770 I llama_new_context_with_model: graph splits = 2
0.00.066.782 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.244 I 
0.00.640.262 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.640.266 I perplexity: tokenizing the input ..
0.00.647.401 I perplexity: tokenization took 7.134 ms
0.00.647.413 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.781.112 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.782.416 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.782.426 I llama_perf_context_print:        load time =     629.87 ms
0.00.782.427 I llama_perf_context_print: prompt eval time =     133.48 ms /   128 tokens (    1.04 ms per token,   958.97 tokens per second)
0.00.782.428 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.428 I llama_perf_context_print:       total time =     142.18 ms /   129 tokens
0.00.782.705 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.076s
sys	0m0.147s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.122 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.471 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.476 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.477 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.478 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.478 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.479 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.487 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.489 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.685 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.686 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.686 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.687 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.687 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.688 I llama_model_loader: - type  f32:  194 tensors
0.00.024.688 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.689 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.903 I llm_load_vocab: special tokens cache size = 25
0.00.051.967 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.970 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.971 I llm_load_print_meta: arch             = gptneox
0.00.051.971 I llm_load_print_meta: vocab type       = BPE
0.00.051.971 I llm_load_print_meta: n_vocab          = 50304
0.00.051.972 I llm_load_print_meta: n_merges         = 50009
0.00.051.972 I llm_load_print_meta: vocab_only       = 0
0.00.051.972 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.972 I llm_load_print_meta: n_embd           = 2048
0.00.051.972 I llm_load_print_meta: n_layer          = 24
0.00.051.975 I llm_load_print_meta: n_head           = 16
0.00.051.975 I llm_load_print_meta: n_head_kv        = 16
0.00.051.976 I llm_load_print_meta: n_rot            = 32
0.00.051.976 I llm_load_print_meta: n_swa            = 0
0.00.051.978 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.978 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.979 I llm_load_print_meta: n_gqa            = 1
0.00.051.979 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.980 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.982 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.983 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.983 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.983 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.983 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.984 I llm_load_print_meta: n_ff             = 8192
0.00.051.984 I llm_load_print_meta: n_expert         = 0
0.00.051.984 I llm_load_print_meta: n_expert_used    = 0
0.00.051.984 I llm_load_print_meta: causal attn      = 1
0.00.051.986 I llm_load_print_meta: pooling type     = 0
0.00.051.986 I llm_load_print_meta: rope type        = 2
0.00.051.986 I llm_load_print_meta: rope scaling     = linear
0.00.051.987 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.987 I llm_load_print_meta: freq_scale_train = 1
0.00.051.987 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.988 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.988 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.988 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.988 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.988 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.988 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.000 I llm_load_print_meta: model type       = 1.4B
0.00.052.000 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.000 I llm_load_print_meta: model params     = 1.41 B
0.00.052.001 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.002 I llm_load_print_meta: general.name     = 1.4B
0.00.052.002 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.004 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.004 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.004 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.004 I llm_load_print_meta: LF token         = 128 ''
0.00.052.004 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.004 I llm_load_print_meta: max token length = 1024
0.00.053.775 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.775 I llm_load_tensors: offloading output layer to GPU
0.00.053.776 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.786 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.786 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.592 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.593 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.593 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.593 I llama_new_context_with_model: n_batch       = 2048
0.00.054.593 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.593 I llama_new_context_with_model: flash_attn    = 0
0.00.054.594 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.594 I llama_new_context_with_model: freq_scale    = 1
0.00.054.594 I ggml_metal_init: allocating
0.00.054.597 I ggml_metal_init: found device: Apple M4
0.00.054.599 I ggml_metal_init: picking default device: Apple M4
0.00.055.185 I ggml_metal_init: using embedded metal library
0.00.057.104 I ggml_metal_init: GPU name:   Apple M4
0.00.057.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.106 I ggml_metal_init: simdgroup reduction   = true
0.00.057.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.106 I ggml_metal_init: has bfloat            = true
0.00.057.107 I ggml_metal_init: use bfloat            = true
0.00.057.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.304 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.310 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.332 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.297 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.299 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.299 I llama_new_context_with_model: graph nodes  = 967
0.00.085.299 I llama_new_context_with_model: graph splits = 2
0.00.085.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.060 I main: llama threadpool init, n_threads = 4
0.00.763.100 I 
0.00.763.126 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.763.126 I 
0.00.763.294 I sampler seed: 1234
0.00.763.298 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.331 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.332 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.332 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.594.062 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.01.594.062 I llama_perf_context_print:        load time =     753.93 ms
0.01.594.063 I llama_perf_context_print: prompt eval time =      38.97 ms /     7 tokens (    5.57 ms per token,   179.64 tokens per second)
0.01.594.064 I llama_perf_context_print:        eval time =     788.82 ms /    63 runs   (   12.52 ms per token,    79.87 tokens per second)
0.01.594.064 I llama_perf_context_print:       total time =     831.01 ms /    70 tokens
0.01.594.239 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.109s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.890 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.635 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.640 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.646 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.647 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.649 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.650 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.652 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.501 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.357 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.359 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.359 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.359 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.360 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.360 I llama_model_loader: - type  f32:  194 tensors
0.00.023.361 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.361 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.366 I llm_load_vocab: special tokens cache size = 25
0.00.050.357 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.362 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.362 I llm_load_print_meta: arch             = gptneox
0.00.050.363 I llm_load_print_meta: vocab type       = BPE
0.00.050.363 I llm_load_print_meta: n_vocab          = 50304
0.00.050.363 I llm_load_print_meta: n_merges         = 50009
0.00.050.363 I llm_load_print_meta: vocab_only       = 0
0.00.050.363 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.364 I llm_load_print_meta: n_embd           = 2048
0.00.050.364 I llm_load_print_meta: n_layer          = 24
0.00.050.367 I llm_load_print_meta: n_head           = 16
0.00.050.368 I llm_load_print_meta: n_head_kv        = 16
0.00.050.368 I llm_load_print_meta: n_rot            = 32
0.00.050.368 I llm_load_print_meta: n_swa            = 0
0.00.050.368 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.368 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.369 I llm_load_print_meta: n_gqa            = 1
0.00.050.370 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.371 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.371 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.372 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.372 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.372 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.372 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.373 I llm_load_print_meta: n_ff             = 8192
0.00.050.373 I llm_load_print_meta: n_expert         = 0
0.00.050.373 I llm_load_print_meta: n_expert_used    = 0
0.00.050.373 I llm_load_print_meta: causal attn      = 1
0.00.050.374 I llm_load_print_meta: pooling type     = 0
0.00.050.374 I llm_load_print_meta: rope type        = 2
0.00.050.374 I llm_load_print_meta: rope scaling     = linear
0.00.050.375 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.375 I llm_load_print_meta: freq_scale_train = 1
0.00.050.375 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.375 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.376 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.376 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.377 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.377 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.378 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.389 I llm_load_print_meta: model type       = 1.4B
0.00.050.390 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.390 I llm_load_print_meta: model params     = 1.41 B
0.00.050.391 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.391 I llm_load_print_meta: general.name     = 1.4B
0.00.050.391 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.391 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.391 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.391 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.392 I llm_load_print_meta: LF token         = 128 ''
0.00.050.392 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.392 I llm_load_print_meta: max token length = 1024
0.00.052.227 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.227 I llm_load_tensors: offloading output layer to GPU
0.00.052.228 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.237 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.238 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.070 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.071 I llama_new_context_with_model: n_ctx         = 128
0.00.053.071 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.071 I llama_new_context_with_model: n_batch       = 128
0.00.053.072 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.072 I llama_new_context_with_model: flash_attn    = 0
0.00.053.072 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.072 I llama_new_context_with_model: freq_scale    = 1
0.00.053.073 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.073 I ggml_metal_init: allocating
0.00.053.076 I ggml_metal_init: found device: Apple M4
0.00.053.078 I ggml_metal_init: picking default device: Apple M4
0.00.053.663 I ggml_metal_init: using embedded metal library
0.00.055.610 I ggml_metal_init: GPU name:   Apple M4
0.00.055.612 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.613 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.613 I ggml_metal_init: simdgroup reduction   = true
0.00.055.613 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.613 I ggml_metal_init: has bfloat            = true
0.00.055.613 I ggml_metal_init: use bfloat            = true
0.00.055.614 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.615 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.408 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.412 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.428 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.342 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.345 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.345 I llama_new_context_with_model: graph nodes  = 967
0.00.066.345 I llama_new_context_with_model: graph splits = 2
0.00.066.358 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.112 I 
0.00.716.138 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.716.143 I perplexity: tokenizing the input ..
0.00.723.689 I perplexity: tokenization took 7.544 ms
0.00.723.702 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.864.904 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.866.039 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.866.050 I llama_perf_context_print:        load time =     707.22 ms
0.00.866.051 I llama_perf_context_print: prompt eval time =     140.98 ms /   128 tokens (    1.10 ms per token,   907.92 tokens per second)
0.00.866.052 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.052 I llama_perf_context_print:       total time =     149.94 ms /   129 tokens
0.00.866.492 I ggml_metal_free: deallocating

real	0m0.880s
user	0m0.079s
sys	0m0.159s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.584 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.463 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.464 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.465 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.465 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.466 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.468 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.468 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.362 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.414 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.261 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.261 I llama_model_loader: - type  f32:  194 tensors
0.00.025.262 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.288 I llm_load_vocab: special tokens cache size = 25
0.00.052.436 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.439 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.439 I llm_load_print_meta: arch             = gptneox
0.00.052.440 I llm_load_print_meta: vocab type       = BPE
0.00.052.440 I llm_load_print_meta: n_vocab          = 50304
0.00.052.440 I llm_load_print_meta: n_merges         = 50009
0.00.052.440 I llm_load_print_meta: vocab_only       = 0
0.00.052.440 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.441 I llm_load_print_meta: n_embd           = 2048
0.00.052.441 I llm_load_print_meta: n_layer          = 24
0.00.052.444 I llm_load_print_meta: n_head           = 16
0.00.052.444 I llm_load_print_meta: n_head_kv        = 16
0.00.052.445 I llm_load_print_meta: n_rot            = 32
0.00.052.445 I llm_load_print_meta: n_swa            = 0
0.00.052.445 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.445 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.446 I llm_load_print_meta: n_gqa            = 1
0.00.052.448 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.448 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.449 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.449 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.449 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.449 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.450 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.450 I llm_load_print_meta: n_ff             = 8192
0.00.052.450 I llm_load_print_meta: n_expert         = 0
0.00.052.451 I llm_load_print_meta: n_expert_used    = 0
0.00.052.451 I llm_load_print_meta: causal attn      = 1
0.00.052.451 I llm_load_print_meta: pooling type     = 0
0.00.052.456 I llm_load_print_meta: rope type        = 2
0.00.052.456 I llm_load_print_meta: rope scaling     = linear
0.00.052.457 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.459 I llm_load_print_meta: freq_scale_train = 1
0.00.052.459 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.459 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.459 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.460 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.460 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.460 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.460 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.471 I llm_load_print_meta: model type       = 1.4B
0.00.052.472 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.472 I llm_load_print_meta: model params     = 1.41 B
0.00.052.472 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.472 I llm_load_print_meta: general.name     = 1.4B
0.00.052.473 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.473 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.473 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.473 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.473 I llm_load_print_meta: LF token         = 128 ''
0.00.052.474 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.474 I llm_load_print_meta: max token length = 1024
0.00.054.292 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.293 I llm_load_tensors: offloading output layer to GPU
0.00.054.293 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.302 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.303 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.152 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.153 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.153 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.153 I llama_new_context_with_model: n_batch       = 2048
0.00.055.153 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.154 I llama_new_context_with_model: flash_attn    = 0
0.00.055.154 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.154 I llama_new_context_with_model: freq_scale    = 1
0.00.055.155 I ggml_metal_init: allocating
0.00.055.161 I ggml_metal_init: found device: Apple M4
0.00.055.163 I ggml_metal_init: picking default device: Apple M4
0.00.055.706 I ggml_metal_init: using embedded metal library
0.00.057.651 I ggml_metal_init: GPU name:   Apple M4
0.00.057.652 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.652 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.653 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.653 I ggml_metal_init: simdgroup reduction   = true
0.00.057.653 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.653 I ggml_metal_init: has bfloat            = true
0.00.057.653 I ggml_metal_init: use bfloat            = true
0.00.057.654 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.770 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.780 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.798 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.860 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.862 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.862 I llama_new_context_with_model: graph nodes  = 967
0.00.085.862 I llama_new_context_with_model: graph splits = 2
0.00.085.903 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.832.839 I main: llama threadpool init, n_threads = 4
0.00.832.874 I 
0.00.832.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.832.904 I 
0.00.833.043 I sampler seed: 1234
0.00.833.049 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.833.092 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.833.122 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.833.124 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.685.478 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.685.479 I llama_perf_context_print:        load time =     823.25 ms
0.01.685.479 I llama_perf_context_print: prompt eval time =      38.86 ms /     7 tokens (    5.55 ms per token,   180.15 tokens per second)
0.01.685.480 I llama_perf_context_print:        eval time =     810.58 ms /    63 runs   (   12.87 ms per token,    77.72 tokens per second)
0.01.685.480 I llama_perf_context_print:       total time =     852.64 ms /    70 tokens
0.01.685.654 I ggml_metal_free: deallocating

real	0m1.705s
user	0m0.110s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4175 (b83cae08) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.631 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.238 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.240 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.240 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.241 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.241 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.241 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.242 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.243 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.243 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.244 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.244 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.245 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.246 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.248 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.995 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.996 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.996 I llama_model_loader: - type  f32:  194 tensors
0.00.024.997 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.808 I llm_load_vocab: special tokens cache size = 25
0.00.050.844 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.848 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.848 I llm_load_print_meta: arch             = gptneox
0.00.050.848 I llm_load_print_meta: vocab type       = BPE
0.00.050.848 I llm_load_print_meta: n_vocab          = 50304
0.00.050.849 I llm_load_print_meta: n_merges         = 50009
0.00.050.849 I llm_load_print_meta: vocab_only       = 0
0.00.050.849 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.849 I llm_load_print_meta: n_embd           = 2048
0.00.050.849 I llm_load_print_meta: n_layer          = 24
0.00.050.852 I llm_load_print_meta: n_head           = 16
0.00.050.853 I llm_load_print_meta: n_head_kv        = 16
0.00.050.853 I llm_load_print_meta: n_rot            = 32
0.00.050.853 I llm_load_print_meta: n_swa            = 0
0.00.050.853 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.853 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.854 I llm_load_print_meta: n_gqa            = 1
0.00.050.855 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.856 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.856 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.856 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.857 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.857 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.857 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.858 I llm_load_print_meta: n_ff             = 8192
0.00.050.859 I llm_load_print_meta: n_expert         = 0
0.00.050.859 I llm_load_print_meta: n_expert_used    = 0
0.00.050.860 I llm_load_print_meta: causal attn      = 1
0.00.050.860 I llm_load_print_meta: pooling type     = 0
0.00.050.860 I llm_load_print_meta: rope type        = 2
0.00.050.861 I llm_load_print_meta: rope scaling     = linear
0.00.050.861 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.861 I llm_load_print_meta: freq_scale_train = 1
0.00.050.862 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.862 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.862 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.862 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.862 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.862 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.863 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.874 I llm_load_print_meta: model type       = 1.4B
0.00.050.874 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.875 I llm_load_print_meta: model params     = 1.41 B
0.00.050.875 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.875 I llm_load_print_meta: general.name     = 1.4B
0.00.050.876 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.876 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.876 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.876 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.876 I llm_load_print_meta: LF token         = 128 ''
0.00.050.876 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.877 I llm_load_print_meta: max token length = 1024
0.00.052.608 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.608 I llm_load_tensors: offloading output layer to GPU
0.00.052.609 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.618 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.619 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.456 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.457 I llama_new_context_with_model: n_ctx         = 128
0.00.053.457 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.457 I llama_new_context_with_model: n_batch       = 128
0.00.053.458 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.458 I llama_new_context_with_model: flash_attn    = 0
0.00.053.458 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.458 I llama_new_context_with_model: freq_scale    = 1
0.00.053.459 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.459 I ggml_metal_init: allocating
0.00.053.462 I ggml_metal_init: found device: Apple M4
0.00.053.464 I ggml_metal_init: picking default device: Apple M4
0.00.053.989 I ggml_metal_init: using embedded metal library
0.00.055.901 I ggml_metal_init: GPU name:   Apple M4
0.00.055.902 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.903 I ggml_metal_init: simdgroup reduction   = true
0.00.055.903 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.903 I ggml_metal_init: has bfloat            = true
0.00.055.904 I ggml_metal_init: use bfloat            = true
0.00.055.904 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.904 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.987 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.989 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.004 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.864 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.865 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.865 I llama_new_context_with_model: graph nodes  = 967
0.00.065.865 I llama_new_context_with_model: graph splits = 2
0.00.065.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.721 I 
0.00.366.742 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.366.746 I perplexity: tokenizing the input ..
0.00.374.293 I perplexity: tokenization took 7.546 ms
0.00.374.306 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.514.797 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.515.900 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.515.923 I llama_perf_context_print:        load time =     356.09 ms
0.00.515.924 I llama_perf_context_print: prompt eval time =     140.27 ms /   128 tokens (    1.10 ms per token,   912.53 tokens per second)
0.00.515.925 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.515.926 I llama_perf_context_print:       total time =     149.20 ms /   129 tokens
0.00.516.391 I ggml_metal_free: deallocating

real	0m0.533s
user	0m0.077s
sys	0m0.100s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4175 (b83cae08)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155a07930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155a08060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155a08610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155a08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155a09170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155a09720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155a09cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155a0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155a0a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155a0ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155a0b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155a0b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155a0c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155a0ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155a0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155a0d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155a0e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155a0e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155a0ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155a0f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155a0fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155a104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155a10bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155a11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155a11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155a11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155a12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155a130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155a13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155a138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155a13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155a14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155a148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155a14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155a150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155a15550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155a159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155a15e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155a16330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155a167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155a16c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155a17110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155a175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155a17a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155a17d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155a18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155a18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155a19250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155a19860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155a19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155a1a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155a1aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155a1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155a1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155a1bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155a1c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155a1c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155a1caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155a1d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155a1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155a1db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155a1e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155a1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155a1e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155a1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155a1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155a1f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155a1fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155a20060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155a20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155a209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155a20e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155a212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155a21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155a21c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155a220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155a22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155a22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155a22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155a23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155a237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155a23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155a24120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155a245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155a24a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155a24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155a253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155a25840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155a25ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155a26180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155a26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155a26ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155a26f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155a27400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155a278a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155a27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155a281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155a18f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155a28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155a28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155a29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155a29610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155a29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155a29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155a2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155a2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155a2ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155a2b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155a2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155a2bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155a2bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155a2c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155a2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155a2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155a2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155a2d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155a2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155a2e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155a2e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155a2e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155a2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155a2f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155a2f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155a2fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155a30070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155a30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155a309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155a30e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155a312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155a31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155a31c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155a320d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155a32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155a32a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155a32eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155a33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155a337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155a33c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155a34130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155a345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155a34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155a34f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155a353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155a35850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155a35cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155a36190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155a36630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155a36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155a36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155a37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155a378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155a37d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155a381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155a38740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155a38c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155a391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155a39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155a399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155a3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155a3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155a3ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155a3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155a3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155a3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155a3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155a3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155a3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155a3d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155a3db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155a3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155a3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155a3eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155a3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155a3f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155a3faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155a40040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155a40590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155a40ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155a41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155a41580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155a41ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155a42020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155a42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155a42ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155a43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155a43560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155a43ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155a44000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155a44550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155a44aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155a44ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155a45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155a45a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155a45fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155a46530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155a46a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155a46fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155a47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155a47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155a47fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155a48510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155a48a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155a48fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155a49500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155a49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155a49fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155a4a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155a4aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155a4af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155a4b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155a4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155a4bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155a4c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155a4ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155a4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155a4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155a4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155a4df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155a4e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155a4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155a4ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155a4f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155a4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155a4ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155a503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155a50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155a50d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155a511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155a51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155a51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155a51fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155a52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155a528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155a52d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155a53220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155a536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155a53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155a540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155a547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155a54ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155a55610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155a55d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155a55ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155a56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155a56c10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.144.567 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155a47250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155a476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155a47b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155a47fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155a48410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155a48880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155a48cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155a49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155a495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155a49a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155a49eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155a4a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155a4ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155a4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155a4bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155a4c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155a4cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155a4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155a4d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155a4e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155a4e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155a4f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155a4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155a4fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155a504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155a50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155a50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155a51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155a51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155a51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155a51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155a523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155a52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155a52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155a52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155a533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155a53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155a53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155a54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155a545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155a54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155a54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155a55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155a55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155a55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155a56050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155a564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155a56930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155a56da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155a08bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155a09710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155a08610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155a09170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155a07240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155a07ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155a14f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155a15400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155a15870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155a15ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155a16150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155a165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155a16a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155a16ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155a17310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155a17780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155a17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155a18060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155a184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155a18940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155a18db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155a19220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155a19690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155a19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155a19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155a1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155a1a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155a1acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155a1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155a1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155a1ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155a1be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155a1c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155a1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155a1cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155a1d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155a1d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155a1d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155a1dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155a1e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155a1e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155a1eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155a1ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155a1f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155a1f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155a1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155a20110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155a20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155a209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155a20e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155a212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155a21740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155a21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155a22020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155a22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155a22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155a22d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155a231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155a23650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155a23ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155a23f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155a243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155a24810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155a24c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155a250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155a25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155a259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155a25e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155a262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155a26720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155a26b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155a27000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155a27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155a278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155a27d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155a281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155a28630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155a28aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155a28f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155a29380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155a297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155a29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155a2a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155a2a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155a2a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155a2ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155a2b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155a2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155a2bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155a2bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155a2c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155a2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155a2cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155a2d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155a2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155a2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155a2def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155a2e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155a2e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155a2ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155a2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155a2f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155a2f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155a2fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155a30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155a306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155a30b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155a30fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155a31430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155a318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155a31d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155a32180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155a325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155a32a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155a32ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155a33340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155a337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155a33c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155a343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155a34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155a34c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155a350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155a35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155a359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155a35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155a362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155a36720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155a36b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155a37000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155a37470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155a378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155a37d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155a381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155a38630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155a38aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155a38f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155a39380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155a397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155a39c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155a3a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155a3a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155a3a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155a3ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155a3b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155a3b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155a3bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155a3bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155a3c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155a3c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155a3cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155a3d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155a3d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155a3da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155a3def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155a3e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155a3e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155a3ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155a3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155a3f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155a3f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155a3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155a40270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155a406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155a40b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155a40fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155a41430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155a418a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155a41d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155a42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155a425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155a42a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155a42ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155a43340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155a437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155a43c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155a44090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155a44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155a44970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155a44de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155a45250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155a456c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155a45b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155a45fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155a46410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155a137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155a13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155a14080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155a144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155a0acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155a0b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155a0b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155a0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155a0c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155a0cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155a0cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155a0d450 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155a135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155a13a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155a13ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155a14340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155a147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155a14f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155a15400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155a15870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155a15ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155a16150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155a165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155a16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155a17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155a17c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155a183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155a18ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155a191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155a198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155a19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155a1a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155a1b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155a1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155a1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155a1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155a1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155a1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155a1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155a1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155a1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155a1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155a1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155a1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155a1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155a1f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155a1f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155a1fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155a1ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155a203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155a20850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155a20cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155a21130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155a215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155a21a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155a21e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155a222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155a22760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155a22bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155a23040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155a234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155a23920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155a23d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155a24200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155a24670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155a24ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155a24f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155a253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155a25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155a25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155a26110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155a26580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155a269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155a26e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155a272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155a27740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155a27bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155a28020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155a28490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155a28900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155a28d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155a291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155a29650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155a29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155a29f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155a2a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155a2a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155a2ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155a2b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155a2b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155a2b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155a2be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155a2c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155a2c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155a2cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155a2d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155a2d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155a2d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155a2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155a2e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155a2e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155a2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155a2ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155a2f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155a2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155a2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155a300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155a30540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155a309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155a30e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155a31290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155a31700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155a31b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155a31fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155a32450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155a328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155a32d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155a331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155a33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155a33a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155a33ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155a34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155a347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155a34c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155a350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155a35520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155a35990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155a35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155a36270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155a366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155a36b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155a36fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155a37430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155a378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155a37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155a38180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155a385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155a38a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155a38ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155a39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155a397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155a39c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155a3a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155a3a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155a3a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155a3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155a3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155a3b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155a3bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155a3bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155a3c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155a3c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155a3ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155a3d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155a3d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155a3da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155a3deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155a3e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155a3e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155a3ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155a3f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155a3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155a3f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155a3fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155a40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155a406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155a40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155a40f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155a413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155a41860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155a41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155a42140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155a425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155a42a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155a42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155a43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155a43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155a43be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155a44050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155a447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155a44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155a450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155a45520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155a45990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155a45e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155a46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155a07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155a08fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155a08460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155a09560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155a08a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155a46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155a47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155a476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155a47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155a47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155a48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155a48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155a48d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155a49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155a495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155a49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155a49ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155a4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155a4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155a4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155a4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155a4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155a4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155a4bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155a4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155a4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155a4cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155a4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155a4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155a4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155a4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155a4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155a4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155a4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155a4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155a4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155a4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155a4fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155a50060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155a504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155a50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155a50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155a51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155a51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155a51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155a51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155a523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155a52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155a52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155a53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155a535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155a53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155a53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155a542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155a54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155a54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155a55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155a554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155a55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155a55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155a56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155a56670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155a56ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155a0acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155a0b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155a0bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155a0c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155a0c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155a0cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155a0d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155a0d600 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.912s
user	0m0.307s
sys	0m0.309s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4175 (b83cae08)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149e0cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149e0d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149e0dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149e0e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149e0e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149e0edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149e0f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149e0f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149e0fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149e103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149e108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149e10dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149e118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149e12090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149e128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149e12fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149e136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149e13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149e14520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149e14cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149e15410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149e15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149e16250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149e16af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149e17210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149e174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149e17ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149e18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149e18c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149e18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149e193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149e196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149e19f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149e1a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149e1a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149e1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149e1b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149e1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149e1b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149e1be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149e1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149e1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149e1cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149e1d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149e1d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149e1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149e1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149e1e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149e1eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149e1f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149e1fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149e20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149e20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149e20d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149e21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149e219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149e21e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149e22130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149e22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149e22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149e231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149e23690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149e23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149e23fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149e24470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149e24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149e24db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149e25250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149e256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149e25b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149e26030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149e264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149e26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149e26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149e272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149e27750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149e27bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149e28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149e28530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149e289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149e28e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149e29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149e297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149e29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149e2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149e2a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149e2aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149e2aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149e2b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149e2b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149e2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149e2c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149e2c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149e2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149e2cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149e2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149e2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149e1e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149e2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149e2e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149e2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149e2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149e2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149e2f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149e2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149e2ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149e303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149e30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149e30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149e311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149e31640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149e31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149e31f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149e32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149e328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149e32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149e33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149e336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149e33b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149e33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149e34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149e34920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149e34dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149e35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149e35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149e35ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149e36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149e364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149e36980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149e36e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149e372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149e37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149e37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149e380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149e38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149e389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149e38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149e39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149e397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149e39c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149e3a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149e3a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149e3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149e3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149e3b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149e3b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149e3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149e3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149e3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149e3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149e3cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149e3d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149e3d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149e3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149e3e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149e3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149e3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149e3f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149e3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149e3fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149e402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149e408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149e40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149e416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149e41b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149e42000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149e424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149e42c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149e431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149e436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149e43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149e44190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149e446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149e44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149e45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149e456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149e45c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149e46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149e466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149e46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149e47160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149e476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149e47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149e48150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149e486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149e48bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149e49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149e49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149e49be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149e4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149e4a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149e4abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149e4b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149e4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149e4bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149e4c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149e4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149e4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149e4d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149e4d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149e4dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149e4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149e4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149e4eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149e4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149e4f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149e4fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149e500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149e50620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149e50b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149e510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149e51610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149e51b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149e520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149e52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149e52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149e530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149e535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149e53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149e54090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149e545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149e54b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149e55080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149e555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149e55a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149e55f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149e563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149e56850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149e56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149e57190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149e57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149e57ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149e57f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149e58410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149e588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149e58d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149e591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149e59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149e59e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149e5a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149e5aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149e5b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149e5b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149e5bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149e5c2a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106d04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106d05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106d054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106d05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106d05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106d06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106d06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106d06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106d06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106d073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106d07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106d07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106d08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106d091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106d09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106d0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106d0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106d0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106d0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106d0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106d0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106d0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106d0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106d0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106d0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106d0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106d0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106d0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106d0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106d0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106d0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106d0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106d10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106d104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106d10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106d10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106d11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106d116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106d11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106d11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106d12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106d12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106d12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106d13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106d135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106d13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106d13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106d14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106d14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106d14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106d15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106d154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106d15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106d15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106d16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106d16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106d16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106d17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106d17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106d179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106d17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106d182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106d18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106d18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106d19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106d19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106d198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106d19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106d1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x106d1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x106d1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106d1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106d1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106d1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106d1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106d1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106d1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106d1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106d1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106d1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106d1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106d1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106d1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106d1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106d1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106d1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106d1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106d1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106d1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106d1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106d20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106d207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106d20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106d210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106d21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106d219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106d21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106d22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106d226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106d22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106d22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106d23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106d238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106d23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106d24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106d24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106d24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106d24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106d25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106d257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106d25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106d260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106d26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106d26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106d26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106d27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106d276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106d27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106d27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106d28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106d28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106d28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106d29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106d295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106d29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106d29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106d2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106d2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106d2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106d2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106d2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106d2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106d2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106d2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106d2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106d2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106d2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106d2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106d2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106d2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106d2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106d2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106d2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106d2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106d2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106d2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106d2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106d30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106d304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106d30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106d30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106d31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106d31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106d31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106d31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106d323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106d32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106d32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106d33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106d335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106d33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106d33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106d342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106d34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106d34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106d35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106d354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106d36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106d36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106d365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106d36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106d36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106d37310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106d37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106d37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106d38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106d384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106d38940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106d38db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106d39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106d39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106d39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106d39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106d3a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106d3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106d3acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106d3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106d3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106d3ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106d3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106d3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106d3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106d3cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106d3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106d3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106d3d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106d3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106d3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106d3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106d3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106d3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106d3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106d3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106d3fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106d40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106d40580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106d409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106d40e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106d412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106d41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106d41bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106d42020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106d42490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106d42900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106d42d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106d431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106d43650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106d43ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106d43f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106d443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106d44810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106d44c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106d450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106d45560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106d459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106d45e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106d462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106d46720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106d46b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106d47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106d47470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106d478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106d47d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106d481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106d48630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106d48aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106d48f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106d49380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106d49ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106d4a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106d4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106d4b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106d4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106d4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106d4be10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149e0dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149e0eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149e0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149e0e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149e0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149e0d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149e4c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149e4c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149e4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149e4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149e4d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149e4d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149e4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149e4e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149e4f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149e4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149e4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149e50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149e50c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149e515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149e51ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149e523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149e52ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149e531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149e538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149e53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149e54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149e545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149e54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149e54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149e55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149e557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149e55c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149e55ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149e56350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149e567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149e56c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149e570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149e57510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149e57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149e57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149e58260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149e586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149e58b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149e58fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149e59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149e59890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149e59d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149e5a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149e5a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149e5aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149e5aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149e5b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149e5b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149e5bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149e5c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149e1a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149e1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149e1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149e1b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149e1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149e1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149e1bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149e1c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149e1c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149e1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149e1cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149e1d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149e1d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149e1dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149e1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149e1e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149e1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149e1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149e1f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149e1f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149e1fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149e1fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149e20460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149e208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149e20d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149e211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149e21620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149e21a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149e21f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149e22370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149e227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149e22c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149e230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149e23530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149e239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149e23e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149e24280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149e246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149e24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149e24fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149e25440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149e258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149e25d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149e26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149e26600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149e26a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149e26ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149e27350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149e277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149e27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149e280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149e28510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149e28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149e28df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149e29260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149e296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149e29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149e29fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149e2a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149e2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149e2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149e2b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149e2b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149e2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149e2bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149e2c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149e2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149e2cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149e2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149e2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149e2d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149e2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149e2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149e2e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149e2eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149e2ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149e2f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149e2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149e2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149e30150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149e305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149e30a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149e30ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149e31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149e31780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149e31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149e32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149e324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149e32940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149e32db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149e33220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149e33690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149e33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149e33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149e343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149e34850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149e34cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149e35130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149e355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149e35a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149e35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149e362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149e36760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149e36bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149e37040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149e374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149e37920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149e37d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149e38200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149e38670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149e38ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149e39260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149e396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149e39b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149e39fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149e3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149e3a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149e3ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149e3b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149e3b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149e3ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149e3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149e3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149e3c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149e3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149e3d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149e3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149e3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149e3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149e3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149e3e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149e3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149e3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149e3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149e3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149e3fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149e40150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149e405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149e40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149e40ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149e41310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149e41780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149e41bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149e42060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149e424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149e42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149e42db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149e43220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149e43690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149e43b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149e43f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149e443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149e44850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149e44cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149e45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149e455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149e45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149e45e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149e462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149e46760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149e46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149e47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149e474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149e47920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149e47d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149e48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149e48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149e48ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149e48f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149e493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149e49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149e49ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149e4a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149e4a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149e4a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149e4ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149e4b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149e4b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149e4bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149e18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149e192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149e19710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149e19e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149e107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149e10ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149e11590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149e11a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149e11e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149e122e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.885s
user	0m0.239s
sys	0m0.118s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.56 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.26 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.15 user         0.04 sys
```
