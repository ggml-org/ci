Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.717s
user	0m0.894s
sys	0m1.342s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target common
[ 36%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Built target test-log
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Built target test-json-schema-to-grammar
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Linking CXX executable ../bin/test-autorelease
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 61%] Built target test-chat-template
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-backend-ops
[ 62%] Built target test-gguf
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-quantize-fns
[ 70%] Built target test-quantize-perf
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target llama-batched-bench
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-batched
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Built target llama-imatrix
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Built target llama-infill
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-cli
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Built target llama-passkey
[ 87%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-speculative
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-speculative-simple
[ 90%] Built target llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-run
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-gen-docs
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Built target llama-tts
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Built target llama-cvector-generator
[ 97%] Built target llama-minicpmv-cli
[ 97%] Built target llama-export-lora
[ 97%] Built target llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.284s
user	0m6.685s
sys	0m10.151s

main: quantize time =  2339.11 ms
main:    total time =  2339.11 ms

main: quantize time =  1969.79 ms
main:    total time =  1969.79 ms

main: quantize time =  2486.67 ms
main:    total time =  2486.67 ms

main: quantize time =  3010.09 ms
main:    total time =  3010.09 ms

main: quantize time =  2027.50 ms
main:    total time =  2027.50 ms

main: quantize time =  5002.13 ms
main:    total time =  5002.13 ms

main: quantize time =  6105.10 ms
main:    total time =  6105.10 ms

main: quantize time =  7117.35 ms
main:    total time =  7117.35 ms

main: quantize time =  6171.24 ms
main:    total time =  6171.24 ms

main: quantize time =  4651.74 ms
main:    total time =  4651.74 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.225 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.417 I main: llama backend init
0.00.000.424 I main: load the model and apply lora adapter, if any
0.00.080.643 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.094.038 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.094.056 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.094.061 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.094.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.094.063 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.094.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.094.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.094.067 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.094.068 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.094.069 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.094.069 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.094.070 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.094.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.094.072 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.094.076 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.094.076 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.094.077 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.101.129 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.103.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.110.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.110.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.110.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.110.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.110.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.110.336 I llama_model_loader: - type  f32:  194 tensors
0.00.110.337 I llama_model_loader: - type  f16:   98 tensors
0.00.110.339 I print_info: file format = GGUF V3 (latest)
0.00.110.341 I print_info: file type   = all F32 (guessed)
0.00.110.343 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.128.934 I load: special tokens cache size = 25
0.00.139.155 I load: token to piece cache size = 0.2984 MB
0.00.139.161 I print_info: arch             = gptneox
0.00.139.161 I print_info: vocab_only       = 0
0.00.139.161 I print_info: n_ctx_train      = 2048
0.00.139.164 I print_info: n_embd           = 2048
0.00.139.164 I print_info: n_layer          = 24
0.00.139.168 I print_info: n_head           = 16
0.00.139.169 I print_info: n_head_kv        = 16
0.00.139.170 I print_info: n_rot            = 32
0.00.139.170 I print_info: n_swa            = 0
0.00.139.170 I print_info: n_embd_head_k    = 128
0.00.139.170 I print_info: n_embd_head_v    = 128
0.00.139.171 I print_info: n_gqa            = 1
0.00.139.172 I print_info: n_embd_k_gqa     = 2048
0.00.139.173 I print_info: n_embd_v_gqa     = 2048
0.00.139.173 I print_info: f_norm_eps       = 1.0e-05
0.00.139.174 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.139.174 I print_info: f_clamp_kqv      = 0.0e+00
0.00.139.174 I print_info: f_max_alibi_bias = 0.0e+00
0.00.139.175 I print_info: f_logit_scale    = 0.0e+00
0.00.139.176 I print_info: n_ff             = 8192
0.00.139.176 I print_info: n_expert         = 0
0.00.139.176 I print_info: n_expert_used    = 0
0.00.139.176 I print_info: causal attn      = 1
0.00.139.177 I print_info: pooling type     = 0
0.00.139.177 I print_info: rope type        = 2
0.00.139.178 I print_info: rope scaling     = linear
0.00.139.181 I print_info: freq_base_train  = 10000.0
0.00.139.182 I print_info: freq_scale_train = 1
0.00.139.182 I print_info: n_ctx_orig_yarn  = 2048
0.00.139.182 I print_info: rope_finetuned   = unknown
0.00.139.182 I print_info: ssm_d_conv       = 0
0.00.139.182 I print_info: ssm_d_inner      = 0
0.00.139.182 I print_info: ssm_d_state      = 0
0.00.139.183 I print_info: ssm_dt_rank      = 0
0.00.139.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.139.183 I print_info: model type       = 1.4B
0.00.139.183 I print_info: model params     = 1.41 B
0.00.139.184 I print_info: general.name     = 1.4B
0.00.139.184 I print_info: vocab type       = BPE
0.00.139.185 I print_info: n_vocab          = 50304
0.00.139.185 I print_info: n_merges         = 50009
0.00.139.185 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.139.185 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.139.186 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.139.186 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.139.186 I print_info: LF token         = 187 'Ċ'
0.00.139.187 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.139.191 I print_info: max token length = 1024
0.00.184.474 I load_tensors: offloading 24 repeating layers to GPU
0.00.184.478 I load_tensors: offloading output layer to GPU
0.00.184.478 I load_tensors: offloaded 25/25 layers to GPU
0.00.184.493 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.184.495 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.184.807 I llama_context: n_seq_max     = 1
0.00.184.809 I llama_context: n_ctx         = 2048
0.00.184.809 I llama_context: n_ctx_per_seq = 2048
0.00.184.809 I llama_context: n_batch       = 2048
0.00.184.809 I llama_context: n_ubatch      = 512
0.00.184.809 I llama_context: flash_attn    = 0
0.00.184.810 I llama_context: freq_base     = 10000.0
0.00.184.810 I llama_context: freq_scale    = 1
0.00.184.811 I ggml_metal_init: allocating
0.00.184.828 I ggml_metal_init: found device: Apple M4
0.00.184.833 I ggml_metal_init: picking default device: Apple M4
0.00.185.404 I ggml_metal_init: using embedded metal library
0.00.207.961 I ggml_metal_init: GPU name:   Apple M4
0.00.207.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.207.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.207.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.207.964 I ggml_metal_init: simdgroup reduction   = true
0.00.207.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.207.964 I ggml_metal_init: has residency sets    = true
0.00.207.964 I ggml_metal_init: has bfloat            = true
0.00.207.965 I ggml_metal_init: use bfloat            = true
0.00.207.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.207.966 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.258.923 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.288.901 I init:      Metal KV buffer size =   384.00 MiB
0.00.288.907 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.288.928 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.293.141 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.293.143 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.293.144 I llama_context: graph nodes  = 967
0.00.293.144 I llama_context: graph splits = 2
0.00.293.170 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.293.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.293.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.384 I main: llama threadpool init, n_threads = 4
0.00.360.429 I 
0.00.360.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.461 I 
0.00.360.511 I sampler seed: 1234
0.00.360.515 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.360.544 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.360.545 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.360.545 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.189.051 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.02.189.052 I llama_perf_context_print:        load time =     278.87 ms
0.02.189.053 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.45 tokens per second)
0.02.189.054 I llama_perf_context_print:        eval time =    1781.97 ms /    63 runs   (   28.29 ms per token,    35.35 tokens per second)
0.02.189.054 I llama_perf_context_print:       total time =    1829.52 ms /    70 tokens
0.02.193.012 I ggml_metal_free: deallocating

real	0m2.607s
user	0m0.137s
sys	0m0.148s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.009.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.994 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.008 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.009 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.010 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.011 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.011 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.011 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.012 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.012 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.015 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.015 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.015 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.128 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.020 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.021 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.021 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.022 I llama_model_loader: - type  f32:  194 tensors
0.00.035.022 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.023 I print_info: file format = GGUF V3 (latest)
0.00.035.024 I print_info: file type   = Q8_0
0.00.035.025 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.886 I load: special tokens cache size = 25
0.00.050.731 I load: token to piece cache size = 0.2984 MB
0.00.050.735 I print_info: arch             = gptneox
0.00.050.735 I print_info: vocab_only       = 0
0.00.050.735 I print_info: n_ctx_train      = 2048
0.00.050.735 I print_info: n_embd           = 2048
0.00.050.736 I print_info: n_layer          = 24
0.00.050.741 I print_info: n_head           = 16
0.00.050.742 I print_info: n_head_kv        = 16
0.00.050.742 I print_info: n_rot            = 32
0.00.050.743 I print_info: n_swa            = 0
0.00.050.743 I print_info: n_embd_head_k    = 128
0.00.050.743 I print_info: n_embd_head_v    = 128
0.00.050.743 I print_info: n_gqa            = 1
0.00.050.744 I print_info: n_embd_k_gqa     = 2048
0.00.050.745 I print_info: n_embd_v_gqa     = 2048
0.00.050.745 I print_info: f_norm_eps       = 1.0e-05
0.00.050.746 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.746 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.746 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.746 I print_info: f_logit_scale    = 0.0e+00
0.00.050.747 I print_info: n_ff             = 8192
0.00.050.747 I print_info: n_expert         = 0
0.00.050.747 I print_info: n_expert_used    = 0
0.00.050.748 I print_info: causal attn      = 1
0.00.050.749 I print_info: pooling type     = 0
0.00.050.749 I print_info: rope type        = 2
0.00.050.750 I print_info: rope scaling     = linear
0.00.050.750 I print_info: freq_base_train  = 10000.0
0.00.050.750 I print_info: freq_scale_train = 1
0.00.050.750 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.751 I print_info: rope_finetuned   = unknown
0.00.050.753 I print_info: ssm_d_conv       = 0
0.00.050.753 I print_info: ssm_d_inner      = 0
0.00.050.753 I print_info: ssm_d_state      = 0
0.00.050.753 I print_info: ssm_dt_rank      = 0
0.00.050.753 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.753 I print_info: model type       = 1.4B
0.00.050.753 I print_info: model params     = 1.41 B
0.00.050.754 I print_info: general.name     = 1.4B
0.00.050.754 I print_info: vocab type       = BPE
0.00.050.754 I print_info: n_vocab          = 50304
0.00.050.754 I print_info: n_merges         = 50009
0.00.050.755 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.755 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.755 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.755 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.755 I print_info: LF token         = 187 'Ċ'
0.00.050.760 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.760 I print_info: max token length = 1024
0.01.140.793 I load_tensors: offloading 24 repeating layers to GPU
0.01.140.797 I load_tensors: offloading output layer to GPU
0.01.140.799 I load_tensors: offloaded 25/25 layers to GPU
0.01.140.822 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.140.823 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.141.778 I llama_context: n_seq_max     = 1
0.01.141.780 I llama_context: n_ctx         = 2048
0.01.141.781 I llama_context: n_ctx_per_seq = 2048
0.01.141.781 I llama_context: n_batch       = 2048
0.01.141.781 I llama_context: n_ubatch      = 512
0.01.141.782 I llama_context: flash_attn    = 0
0.01.141.782 I llama_context: freq_base     = 10000.0
0.01.141.783 I llama_context: freq_scale    = 1
0.01.141.784 I ggml_metal_init: allocating
0.01.141.794 I ggml_metal_init: found device: Apple M4
0.01.141.800 I ggml_metal_init: picking default device: Apple M4
0.01.143.127 I ggml_metal_init: using embedded metal library
0.01.148.307 I ggml_metal_init: GPU name:   Apple M4
0.01.148.310 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.148.311 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.148.313 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.148.313 I ggml_metal_init: simdgroup reduction   = true
0.01.148.314 I ggml_metal_init: simdgroup matrix mul. = true
0.01.148.314 I ggml_metal_init: has residency sets    = true
0.01.148.314 I ggml_metal_init: has bfloat            = true
0.01.148.314 I ggml_metal_init: use bfloat            = true
0.01.148.315 I ggml_metal_init: hasUnifiedMemory      = true
0.01.148.316 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.162.963 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.204.301 I init:      Metal KV buffer size =   384.00 MiB
0.01.204.307 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.204.331 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.209.346 I llama_context:      Metal compute buffer size =   102.25 MiB
0.01.209.348 I llama_context:        CPU compute buffer size =     8.01 MiB
0.01.209.348 I llama_context: graph nodes  = 967
0.01.209.348 I llama_context: graph splits = 2
0.01.209.365 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.209.486 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.209.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.265.789 I main: llama threadpool init, n_threads = 4
0.01.265.826 I 
0.01.265.845 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.265.846 I 
0.01.266.026 I sampler seed: 1234
0.01.266.030 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.266.071 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.266.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.266.074 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.362.755 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.02.362.756 I llama_perf_context_print:        load time =    1255.14 ms
0.02.362.756 I llama_perf_context_print: prompt eval time =      48.89 ms /     7 tokens (    6.98 ms per token,   143.17 tokens per second)
0.02.362.757 I llama_perf_context_print:        eval time =    1044.71 ms /    63 runs   (   16.58 ms per token,    60.30 tokens per second)
0.02.362.757 I llama_perf_context_print:       total time =    1097.67 ms /    70 tokens
0.02.366.810 I ggml_metal_free: deallocating

real	0m2.386s
user	0m0.107s
sys	0m0.250s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.017.246 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.731 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.038.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.738 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.743 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.942 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.619 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.621 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.621 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.622 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.622 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.622 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.048.623 I llama_model_loader: - type  f32:  194 tensors
0.00.048.623 I llama_model_loader: - type q4_0:   97 tensors
0.00.048.624 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.624 I print_info: file format = GGUF V3 (latest)
0.00.048.625 I print_info: file type   = Q4_0
0.00.048.626 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.057.922 I load: special tokens cache size = 25
0.00.064.901 I load: token to piece cache size = 0.2984 MB
0.00.064.905 I print_info: arch             = gptneox
0.00.064.905 I print_info: vocab_only       = 0
0.00.064.905 I print_info: n_ctx_train      = 2048
0.00.064.905 I print_info: n_embd           = 2048
0.00.064.906 I print_info: n_layer          = 24
0.00.064.910 I print_info: n_head           = 16
0.00.064.911 I print_info: n_head_kv        = 16
0.00.064.911 I print_info: n_rot            = 32
0.00.064.912 I print_info: n_swa            = 0
0.00.064.912 I print_info: n_embd_head_k    = 128
0.00.064.912 I print_info: n_embd_head_v    = 128
0.00.064.913 I print_info: n_gqa            = 1
0.00.064.914 I print_info: n_embd_k_gqa     = 2048
0.00.064.915 I print_info: n_embd_v_gqa     = 2048
0.00.064.917 I print_info: f_norm_eps       = 1.0e-05
0.00.064.917 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.918 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.918 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.920 I print_info: f_logit_scale    = 0.0e+00
0.00.064.921 I print_info: n_ff             = 8192
0.00.064.921 I print_info: n_expert         = 0
0.00.064.921 I print_info: n_expert_used    = 0
0.00.064.921 I print_info: causal attn      = 1
0.00.064.921 I print_info: pooling type     = 0
0.00.064.921 I print_info: rope type        = 2
0.00.064.922 I print_info: rope scaling     = linear
0.00.064.922 I print_info: freq_base_train  = 10000.0
0.00.064.923 I print_info: freq_scale_train = 1
0.00.064.923 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.928 I print_info: rope_finetuned   = unknown
0.00.064.928 I print_info: ssm_d_conv       = 0
0.00.064.929 I print_info: ssm_d_inner      = 0
0.00.064.929 I print_info: ssm_d_state      = 0
0.00.064.929 I print_info: ssm_dt_rank      = 0
0.00.064.929 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.929 I print_info: model type       = 1.4B
0.00.064.930 I print_info: model params     = 1.41 B
0.00.064.930 I print_info: general.name     = 1.4B
0.00.064.931 I print_info: vocab type       = BPE
0.00.064.931 I print_info: n_vocab          = 50304
0.00.064.931 I print_info: n_merges         = 50009
0.00.064.932 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.933 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.934 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.934 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.934 I print_info: LF token         = 187 'Ċ'
0.00.064.935 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.935 I print_info: max token length = 1024
0.00.717.783 I load_tensors: offloading 24 repeating layers to GPU
0.00.717.795 I load_tensors: offloading output layer to GPU
0.00.717.796 I load_tensors: offloaded 25/25 layers to GPU
0.00.717.824 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.717.826 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.719.293 I llama_context: n_seq_max     = 1
0.00.719.301 I llama_context: n_ctx         = 2048
0.00.719.301 I llama_context: n_ctx_per_seq = 2048
0.00.719.302 I llama_context: n_batch       = 2048
0.00.719.302 I llama_context: n_ubatch      = 512
0.00.719.302 I llama_context: flash_attn    = 0
0.00.719.303 I llama_context: freq_base     = 10000.0
0.00.719.304 I llama_context: freq_scale    = 1
0.00.719.306 I ggml_metal_init: allocating
0.00.719.354 I ggml_metal_init: found device: Apple M4
0.00.719.366 I ggml_metal_init: picking default device: Apple M4
0.00.721.052 I ggml_metal_init: using embedded metal library
0.00.726.643 I ggml_metal_init: GPU name:   Apple M4
0.00.726.661 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.726.662 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.726.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.726.663 I ggml_metal_init: simdgroup reduction   = true
0.00.726.663 I ggml_metal_init: simdgroup matrix mul. = true
0.00.726.664 I ggml_metal_init: has residency sets    = true
0.00.726.664 I ggml_metal_init: has bfloat            = true
0.00.726.664 I ggml_metal_init: use bfloat            = true
0.00.726.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.726.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.747.295 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.805.171 I init:      Metal KV buffer size =   384.00 MiB
0.00.805.187 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.805.216 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.809.454 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.809.456 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.809.456 I llama_context: graph nodes  = 967
0.00.809.456 I llama_context: graph splits = 2
0.00.809.469 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.809.593 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.809.593 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.867.222 I main: llama threadpool init, n_threads = 4
0.00.867.261 I 
0.00.867.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.867.284 I 
0.00.867.460 I sampler seed: 1234
0.00.867.466 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.867.480 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.867.482 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.867.482 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.551.865 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50859.60 tokens per second)
0.01.551.865 I llama_perf_context_print:        load time =     849.24 ms
0.01.551.866 I llama_perf_context_print: prompt eval time =      49.44 ms /     7 tokens (    7.06 ms per token,   141.57 tokens per second)
0.01.551.867 I llama_perf_context_print:        eval time =     632.03 ms /    63 runs   (   10.03 ms per token,    99.68 tokens per second)
0.01.551.867 I llama_perf_context_print:       total time =     685.37 ms /    70 tokens
0.01.555.637 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.115s
sys	0m0.220s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.688 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.278 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.284 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.285 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.286 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.286 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.287 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.287 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.288 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.288 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.289 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.289 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.291 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.292 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.159 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.034 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.035 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.035 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.036 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.036 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.036 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.037 I llama_model_loader: - type  f32:  194 tensors
0.00.027.037 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.037 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.038 I print_info: file format = GGUF V3 (latest)
0.00.027.039 I print_info: file type   = Q4_1
0.00.027.040 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.207 I load: special tokens cache size = 25
0.00.041.327 I load: token to piece cache size = 0.2984 MB
0.00.041.330 I print_info: arch             = gptneox
0.00.041.330 I print_info: vocab_only       = 0
0.00.041.331 I print_info: n_ctx_train      = 2048
0.00.041.331 I print_info: n_embd           = 2048
0.00.041.331 I print_info: n_layer          = 24
0.00.041.334 I print_info: n_head           = 16
0.00.041.337 I print_info: n_head_kv        = 16
0.00.041.337 I print_info: n_rot            = 32
0.00.041.337 I print_info: n_swa            = 0
0.00.041.338 I print_info: n_embd_head_k    = 128
0.00.041.338 I print_info: n_embd_head_v    = 128
0.00.041.339 I print_info: n_gqa            = 1
0.00.041.339 I print_info: n_embd_k_gqa     = 2048
0.00.041.340 I print_info: n_embd_v_gqa     = 2048
0.00.041.341 I print_info: f_norm_eps       = 1.0e-05
0.00.041.341 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.341 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.342 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.342 I print_info: f_logit_scale    = 0.0e+00
0.00.041.342 I print_info: n_ff             = 8192
0.00.041.343 I print_info: n_expert         = 0
0.00.041.343 I print_info: n_expert_used    = 0
0.00.041.343 I print_info: causal attn      = 1
0.00.041.343 I print_info: pooling type     = 0
0.00.041.343 I print_info: rope type        = 2
0.00.041.343 I print_info: rope scaling     = linear
0.00.041.344 I print_info: freq_base_train  = 10000.0
0.00.041.344 I print_info: freq_scale_train = 1
0.00.041.344 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.344 I print_info: rope_finetuned   = unknown
0.00.041.345 I print_info: ssm_d_conv       = 0
0.00.041.346 I print_info: ssm_d_inner      = 0
0.00.041.346 I print_info: ssm_d_state      = 0
0.00.041.346 I print_info: ssm_dt_rank      = 0
0.00.041.346 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.346 I print_info: model type       = 1.4B
0.00.041.347 I print_info: model params     = 1.41 B
0.00.041.347 I print_info: general.name     = 1.4B
0.00.041.347 I print_info: vocab type       = BPE
0.00.041.347 I print_info: n_vocab          = 50304
0.00.041.348 I print_info: n_merges         = 50009
0.00.041.348 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.348 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.348 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.348 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.349 I print_info: LF token         = 187 'Ċ'
0.00.041.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.350 I print_info: max token length = 1024
0.00.689.125 I load_tensors: offloading 24 repeating layers to GPU
0.00.689.141 I load_tensors: offloading output layer to GPU
0.00.689.142 I load_tensors: offloaded 25/25 layers to GPU
0.00.689.175 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.689.176 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.690.430 I llama_context: n_seq_max     = 1
0.00.690.437 I llama_context: n_ctx         = 2048
0.00.690.438 I llama_context: n_ctx_per_seq = 2048
0.00.690.438 I llama_context: n_batch       = 2048
0.00.690.439 I llama_context: n_ubatch      = 512
0.00.690.439 I llama_context: flash_attn    = 0
0.00.690.441 I llama_context: freq_base     = 10000.0
0.00.690.441 I llama_context: freq_scale    = 1
0.00.690.443 I ggml_metal_init: allocating
0.00.690.524 I ggml_metal_init: found device: Apple M4
0.00.690.539 I ggml_metal_init: picking default device: Apple M4
0.00.692.303 I ggml_metal_init: using embedded metal library
0.00.698.964 I ggml_metal_init: GPU name:   Apple M4
0.00.698.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.698.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.698.971 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.698.972 I ggml_metal_init: simdgroup reduction   = true
0.00.698.972 I ggml_metal_init: simdgroup matrix mul. = true
0.00.698.972 I ggml_metal_init: has residency sets    = true
0.00.698.973 I ggml_metal_init: has bfloat            = true
0.00.698.973 I ggml_metal_init: use bfloat            = true
0.00.698.974 I ggml_metal_init: hasUnifiedMemory      = true
0.00.698.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.716.768 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.776.428 I init:      Metal KV buffer size =   384.00 MiB
0.00.776.434 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.776.455 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.781.030 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.781.033 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.781.033 I llama_context: graph nodes  = 967
0.00.781.033 I llama_context: graph splits = 2
0.00.781.051 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.781.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.781.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.763 I main: llama threadpool init, n_threads = 4
0.00.828.806 I 
0.00.828.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.836 I 
0.00.828.991 I sampler seed: 1234
0.00.828.996 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.037 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.041 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.041 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.567.231 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.01.567.232 I llama_perf_context_print:        load time =     819.39 ms
0.01.567.233 I llama_perf_context_print: prompt eval time =      48.98 ms /     7 tokens (    7.00 ms per token,   142.92 tokens per second)
0.01.567.234 I llama_perf_context_print:        eval time =     686.37 ms /    63 runs   (   10.89 ms per token,    91.79 tokens per second)
0.01.567.234 I llama_perf_context_print:       total time =     739.15 ms /    70 tokens
0.01.571.195 I ggml_metal_free: deallocating

real	0m1.587s
user	0m0.108s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.018.192 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.029.412 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.414 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.414 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.415 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.415 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.417 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.417 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.417 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.418 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.418 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.424 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.866 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.919 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.920 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.920 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.040.921 I llama_model_loader: - type  f32:  194 tensors
0.00.040.921 I llama_model_loader: - type q5_0:   97 tensors
0.00.040.921 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.922 I print_info: file format = GGUF V3 (latest)
0.00.040.922 I print_info: file type   = Q5_0
0.00.040.923 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.053.314 I load: special tokens cache size = 25
0.00.063.340 I load: token to piece cache size = 0.2984 MB
0.00.063.344 I print_info: arch             = gptneox
0.00.063.344 I print_info: vocab_only       = 0
0.00.063.345 I print_info: n_ctx_train      = 2048
0.00.063.345 I print_info: n_embd           = 2048
0.00.063.345 I print_info: n_layer          = 24
0.00.063.348 I print_info: n_head           = 16
0.00.063.350 I print_info: n_head_kv        = 16
0.00.063.350 I print_info: n_rot            = 32
0.00.063.353 I print_info: n_swa            = 0
0.00.063.353 I print_info: n_embd_head_k    = 128
0.00.063.353 I print_info: n_embd_head_v    = 128
0.00.063.354 I print_info: n_gqa            = 1
0.00.063.355 I print_info: n_embd_k_gqa     = 2048
0.00.063.363 I print_info: n_embd_v_gqa     = 2048
0.00.063.364 I print_info: f_norm_eps       = 1.0e-05
0.00.063.365 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.365 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.367 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.367 I print_info: f_logit_scale    = 0.0e+00
0.00.063.369 I print_info: n_ff             = 8192
0.00.063.369 I print_info: n_expert         = 0
0.00.063.369 I print_info: n_expert_used    = 0
0.00.063.370 I print_info: causal attn      = 1
0.00.063.370 I print_info: pooling type     = 0
0.00.063.372 I print_info: rope type        = 2
0.00.063.374 I print_info: rope scaling     = linear
0.00.063.374 I print_info: freq_base_train  = 10000.0
0.00.063.375 I print_info: freq_scale_train = 1
0.00.063.375 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.375 I print_info: rope_finetuned   = unknown
0.00.063.376 I print_info: ssm_d_conv       = 0
0.00.063.376 I print_info: ssm_d_inner      = 0
0.00.063.376 I print_info: ssm_d_state      = 0
0.00.063.377 I print_info: ssm_dt_rank      = 0
0.00.063.377 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.377 I print_info: model type       = 1.4B
0.00.063.378 I print_info: model params     = 1.41 B
0.00.063.378 I print_info: general.name     = 1.4B
0.00.063.381 I print_info: vocab type       = BPE
0.00.063.381 I print_info: n_vocab          = 50304
0.00.063.381 I print_info: n_merges         = 50009
0.00.063.382 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.382 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.382 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.382 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.383 I print_info: LF token         = 187 'Ċ'
0.00.063.383 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.383 I print_info: max token length = 1024
0.00.736.240 I load_tensors: offloading 24 repeating layers to GPU
0.00.736.256 I load_tensors: offloading output layer to GPU
0.00.736.257 I load_tensors: offloaded 25/25 layers to GPU
0.00.736.290 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.736.292 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.737.626 I llama_context: n_seq_max     = 1
0.00.737.631 I llama_context: n_ctx         = 2048
0.00.737.632 I llama_context: n_ctx_per_seq = 2048
0.00.737.632 I llama_context: n_batch       = 2048
0.00.737.632 I llama_context: n_ubatch      = 512
0.00.737.633 I llama_context: flash_attn    = 0
0.00.737.634 I llama_context: freq_base     = 10000.0
0.00.737.635 I llama_context: freq_scale    = 1
0.00.737.641 I ggml_metal_init: allocating
0.00.737.716 I ggml_metal_init: found device: Apple M4
0.00.737.729 I ggml_metal_init: picking default device: Apple M4
0.00.739.595 I ggml_metal_init: using embedded metal library
0.00.746.131 I ggml_metal_init: GPU name:   Apple M4
0.00.746.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.746.137 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.746.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.746.138 I ggml_metal_init: simdgroup reduction   = true
0.00.746.138 I ggml_metal_init: simdgroup matrix mul. = true
0.00.746.139 I ggml_metal_init: has residency sets    = true
0.00.746.139 I ggml_metal_init: has bfloat            = true
0.00.746.139 I ggml_metal_init: use bfloat            = true
0.00.746.140 I ggml_metal_init: hasUnifiedMemory      = true
0.00.746.141 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.763.515 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.817.900 I init:      Metal KV buffer size =   384.00 MiB
0.00.817.908 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.817.945 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.822.196 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.822.198 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.822.198 I llama_context: graph nodes  = 967
0.00.822.199 I llama_context: graph splits = 2
0.00.822.216 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.822.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.822.347 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.878.456 I main: llama threadpool init, n_threads = 4
0.00.878.498 I 
0.00.878.520 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.878.522 I 
0.00.878.668 I sampler seed: 1234
0.00.878.672 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.878.688 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.878.689 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.878.689 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.669.665 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50283.29 tokens per second)
0.01.669.666 I llama_perf_context_print:        load time =     859.57 ms
0.01.669.666 I llama_perf_context_print: prompt eval time =      42.76 ms /     7 tokens (    6.11 ms per token,   163.69 tokens per second)
0.01.669.670 I llama_perf_context_print:        eval time =     745.25 ms /    63 runs   (   11.83 ms per token,    84.54 tokens per second)
0.01.669.671 I llama_perf_context_print:       total time =     791.90 ms /    70 tokens
0.01.673.428 I ggml_metal_free: deallocating

real	0m1.710s
user	0m0.123s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.693 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.136 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.141 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.142 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.142 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.145 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.146 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.147 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.147 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.148 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.150 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.151 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.151 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.984 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.038 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.804 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.804 I llama_model_loader: - type  f32:  194 tensors
0.00.024.805 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.805 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.806 I print_info: file format = GGUF V3 (latest)
0.00.024.806 I print_info: file type   = Q5_1
0.00.024.807 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.045 I load: special tokens cache size = 25
0.00.039.200 I load: token to piece cache size = 0.2984 MB
0.00.039.202 I print_info: arch             = gptneox
0.00.039.203 I print_info: vocab_only       = 0
0.00.039.203 I print_info: n_ctx_train      = 2048
0.00.039.203 I print_info: n_embd           = 2048
0.00.039.203 I print_info: n_layer          = 24
0.00.039.206 I print_info: n_head           = 16
0.00.039.207 I print_info: n_head_kv        = 16
0.00.039.207 I print_info: n_rot            = 32
0.00.039.207 I print_info: n_swa            = 0
0.00.039.208 I print_info: n_embd_head_k    = 128
0.00.039.208 I print_info: n_embd_head_v    = 128
0.00.039.208 I print_info: n_gqa            = 1
0.00.039.209 I print_info: n_embd_k_gqa     = 2048
0.00.039.210 I print_info: n_embd_v_gqa     = 2048
0.00.039.210 I print_info: f_norm_eps       = 1.0e-05
0.00.039.211 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.211 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.211 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.211 I print_info: f_logit_scale    = 0.0e+00
0.00.039.212 I print_info: n_ff             = 8192
0.00.039.212 I print_info: n_expert         = 0
0.00.039.212 I print_info: n_expert_used    = 0
0.00.039.212 I print_info: causal attn      = 1
0.00.039.213 I print_info: pooling type     = 0
0.00.039.213 I print_info: rope type        = 2
0.00.039.213 I print_info: rope scaling     = linear
0.00.039.214 I print_info: freq_base_train  = 10000.0
0.00.039.214 I print_info: freq_scale_train = 1
0.00.039.215 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.215 I print_info: rope_finetuned   = unknown
0.00.039.215 I print_info: ssm_d_conv       = 0
0.00.039.215 I print_info: ssm_d_inner      = 0
0.00.039.215 I print_info: ssm_d_state      = 0
0.00.039.215 I print_info: ssm_dt_rank      = 0
0.00.039.216 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.216 I print_info: model type       = 1.4B
0.00.039.216 I print_info: model params     = 1.41 B
0.00.039.217 I print_info: general.name     = 1.4B
0.00.039.217 I print_info: vocab type       = BPE
0.00.039.217 I print_info: n_vocab          = 50304
0.00.039.218 I print_info: n_merges         = 50009
0.00.039.218 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.218 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: LF token         = 187 'Ċ'
0.00.039.219 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.219 I print_info: max token length = 1024
0.00.598.096 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.100 I load_tensors: offloading output layer to GPU
0.00.598.101 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.124 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.598.126 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.599.505 I llama_context: n_seq_max     = 1
0.00.599.507 I llama_context: n_ctx         = 2048
0.00.599.507 I llama_context: n_ctx_per_seq = 2048
0.00.599.508 I llama_context: n_batch       = 2048
0.00.599.509 I llama_context: n_ubatch      = 512
0.00.599.509 I llama_context: flash_attn    = 0
0.00.599.510 I llama_context: freq_base     = 10000.0
0.00.599.510 I llama_context: freq_scale    = 1
0.00.599.512 I ggml_metal_init: allocating
0.00.599.533 I ggml_metal_init: found device: Apple M4
0.00.599.547 I ggml_metal_init: picking default device: Apple M4
0.00.600.999 I ggml_metal_init: using embedded metal library
0.00.607.146 I ggml_metal_init: GPU name:   Apple M4
0.00.607.150 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.151 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.152 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.152 I ggml_metal_init: simdgroup reduction   = true
0.00.607.153 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.153 I ggml_metal_init: has residency sets    = true
0.00.607.153 I ggml_metal_init: has bfloat            = true
0.00.607.153 I ggml_metal_init: use bfloat            = true
0.00.607.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.581 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.874 I init:      Metal KV buffer size =   384.00 MiB
0.00.681.881 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.905 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.685.985 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.685.987 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.685.988 I llama_context: graph nodes  = 967
0.00.685.988 I llama_context: graph splits = 2
0.00.686.008 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.686.136 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.686.137 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.145 I main: llama threadpool init, n_threads = 4
0.00.747.190 I 
0.00.747.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.215 I 
0.00.747.363 I sampler seed: 1234
0.00.747.368 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.383 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.385 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.385 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.593.239 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51749.27 tokens per second)
0.01.593.240 I llama_perf_context_print:        load time =     737.72 ms
0.01.593.241 I llama_perf_context_print: prompt eval time =      47.23 ms /     7 tokens (    6.75 ms per token,   148.22 tokens per second)
0.01.593.241 I llama_perf_context_print:        eval time =     795.69 ms /    63 runs   (   12.63 ms per token,    79.18 tokens per second)
0.01.593.242 I llama_perf_context_print:       total time =     846.83 ms /    70 tokens
0.01.597.184 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.107s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.659 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.314 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.320 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.321 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.323 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.323 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.324 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.324 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.324 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.325 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.325 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.327 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.327 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.143 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.961 I llama_model_loader: - type  f32:  194 tensors
0.00.024.962 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.962 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.962 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.963 I print_info: file format = GGUF V3 (latest)
0.00.024.963 I print_info: file type   = Q2_K - Medium
0.00.024.964 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.781 I load: special tokens cache size = 25
0.00.039.012 I load: token to piece cache size = 0.2984 MB
0.00.039.015 I print_info: arch             = gptneox
0.00.039.015 I print_info: vocab_only       = 0
0.00.039.015 I print_info: n_ctx_train      = 2048
0.00.039.015 I print_info: n_embd           = 2048
0.00.039.016 I print_info: n_layer          = 24
0.00.039.018 I print_info: n_head           = 16
0.00.039.019 I print_info: n_head_kv        = 16
0.00.039.019 I print_info: n_rot            = 32
0.00.039.020 I print_info: n_swa            = 0
0.00.039.020 I print_info: n_embd_head_k    = 128
0.00.039.020 I print_info: n_embd_head_v    = 128
0.00.039.023 I print_info: n_gqa            = 1
0.00.039.024 I print_info: n_embd_k_gqa     = 2048
0.00.039.025 I print_info: n_embd_v_gqa     = 2048
0.00.039.025 I print_info: f_norm_eps       = 1.0e-05
0.00.039.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.026 I print_info: f_logit_scale    = 0.0e+00
0.00.039.027 I print_info: n_ff             = 8192
0.00.039.027 I print_info: n_expert         = 0
0.00.039.027 I print_info: n_expert_used    = 0
0.00.039.027 I print_info: causal attn      = 1
0.00.039.027 I print_info: pooling type     = 0
0.00.039.029 I print_info: rope type        = 2
0.00.039.030 I print_info: rope scaling     = linear
0.00.039.030 I print_info: freq_base_train  = 10000.0
0.00.039.030 I print_info: freq_scale_train = 1
0.00.039.030 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.031 I print_info: rope_finetuned   = unknown
0.00.039.031 I print_info: ssm_d_conv       = 0
0.00.039.031 I print_info: ssm_d_inner      = 0
0.00.039.031 I print_info: ssm_d_state      = 0
0.00.039.031 I print_info: ssm_dt_rank      = 0
0.00.039.031 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.032 I print_info: model type       = 1.4B
0.00.039.032 I print_info: model params     = 1.41 B
0.00.039.032 I print_info: general.name     = 1.4B
0.00.039.036 I print_info: vocab type       = BPE
0.00.039.036 I print_info: n_vocab          = 50304
0.00.039.037 I print_info: n_merges         = 50009
0.00.039.037 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.037 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.037 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.037 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.038 I print_info: LF token         = 187 'Ċ'
0.00.039.038 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.038 I print_info: max token length = 1024
0.00.344.230 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.245 I load_tensors: offloading output layer to GPU
0.00.344.245 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.279 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.281 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.345.859 I llama_context: n_seq_max     = 1
0.00.345.865 I llama_context: n_ctx         = 2048
0.00.345.866 I llama_context: n_ctx_per_seq = 2048
0.00.345.866 I llama_context: n_batch       = 2048
0.00.345.866 I llama_context: n_ubatch      = 512
0.00.345.867 I llama_context: flash_attn    = 0
0.00.345.869 I llama_context: freq_base     = 10000.0
0.00.345.870 I llama_context: freq_scale    = 1
0.00.345.872 I ggml_metal_init: allocating
0.00.345.979 I ggml_metal_init: found device: Apple M4
0.00.345.993 I ggml_metal_init: picking default device: Apple M4
0.00.347.831 I ggml_metal_init: using embedded metal library
0.00.353.270 I ggml_metal_init: GPU name:   Apple M4
0.00.353.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.293 I ggml_metal_init: simdgroup reduction   = true
0.00.353.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.293 I ggml_metal_init: has residency sets    = true
0.00.353.294 I ggml_metal_init: has bfloat            = true
0.00.353.294 I ggml_metal_init: use bfloat            = true
0.00.353.295 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.300 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.374.652 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.429.463 I init:      Metal KV buffer size =   384.00 MiB
0.00.429.472 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.429.491 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.434.286 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.434.288 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.434.289 I llama_context: graph nodes  = 967
0.00.434.289 I llama_context: graph splits = 2
0.00.434.309 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.434.432 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.434.432 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.417 I main: llama threadpool init, n_threads = 4
0.00.490.463 I 
0.00.490.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.494 I 
0.00.490.671 I sampler seed: 1234
0.00.490.676 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.490.691 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.490.693 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.490.693 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.164.855 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.164.855 I llama_perf_context_print:        load time =     480.05 ms
0.01.164.856 I llama_perf_context_print: prompt eval time =      35.46 ms /     7 tokens (    5.07 ms per token,   197.42 tokens per second)
0.01.164.857 I llama_perf_context_print:        eval time =     635.94 ms /    63 runs   (   10.09 ms per token,    99.07 tokens per second)
0.01.164.857 I llama_perf_context_print:       total time =     675.15 ms /    70 tokens
0.01.168.797 I ggml_metal_free: deallocating

real	0m1.187s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.980 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.916 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.918 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.918 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.919 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.919 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.919 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.920 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.920 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.921 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.921 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.922 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.922 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.922 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.924 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.924 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.925 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.686 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.665 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.362 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.363 I llama_model_loader: - type  f32:  194 tensors
0.00.025.363 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.363 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.364 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.364 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.364 I print_info: file format = GGUF V3 (latest)
0.00.025.365 I print_info: file type   = Q3_K - Medium
0.00.025.366 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.526 I load: special tokens cache size = 25
0.00.039.593 I load: token to piece cache size = 0.2984 MB
0.00.039.596 I print_info: arch             = gptneox
0.00.039.596 I print_info: vocab_only       = 0
0.00.039.596 I print_info: n_ctx_train      = 2048
0.00.039.596 I print_info: n_embd           = 2048
0.00.039.596 I print_info: n_layer          = 24
0.00.039.599 I print_info: n_head           = 16
0.00.039.600 I print_info: n_head_kv        = 16
0.00.039.600 I print_info: n_rot            = 32
0.00.039.600 I print_info: n_swa            = 0
0.00.039.601 I print_info: n_embd_head_k    = 128
0.00.039.601 I print_info: n_embd_head_v    = 128
0.00.039.601 I print_info: n_gqa            = 1
0.00.039.602 I print_info: n_embd_k_gqa     = 2048
0.00.039.603 I print_info: n_embd_v_gqa     = 2048
0.00.039.604 I print_info: f_norm_eps       = 1.0e-05
0.00.039.605 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.605 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.605 I print_info: f_logit_scale    = 0.0e+00
0.00.039.606 I print_info: n_ff             = 8192
0.00.039.606 I print_info: n_expert         = 0
0.00.039.607 I print_info: n_expert_used    = 0
0.00.039.607 I print_info: causal attn      = 1
0.00.039.607 I print_info: pooling type     = 0
0.00.039.607 I print_info: rope type        = 2
0.00.039.607 I print_info: rope scaling     = linear
0.00.039.608 I print_info: freq_base_train  = 10000.0
0.00.039.608 I print_info: freq_scale_train = 1
0.00.039.608 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.608 I print_info: rope_finetuned   = unknown
0.00.039.609 I print_info: ssm_d_conv       = 0
0.00.039.609 I print_info: ssm_d_inner      = 0
0.00.039.609 I print_info: ssm_d_state      = 0
0.00.039.609 I print_info: ssm_dt_rank      = 0
0.00.039.609 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.609 I print_info: model type       = 1.4B
0.00.039.610 I print_info: model params     = 1.41 B
0.00.039.610 I print_info: general.name     = 1.4B
0.00.039.611 I print_info: vocab type       = BPE
0.00.039.611 I print_info: n_vocab          = 50304
0.00.039.613 I print_info: n_merges         = 50009
0.00.039.613 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: LF token         = 187 'Ċ'
0.00.039.614 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: max token length = 1024
0.00.452.551 I load_tensors: offloading 24 repeating layers to GPU
0.00.452.569 I load_tensors: offloading output layer to GPU
0.00.452.570 I load_tensors: offloaded 25/25 layers to GPU
0.00.452.602 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.452.604 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.454.004 I llama_context: n_seq_max     = 1
0.00.454.009 I llama_context: n_ctx         = 2048
0.00.454.009 I llama_context: n_ctx_per_seq = 2048
0.00.454.010 I llama_context: n_batch       = 2048
0.00.454.010 I llama_context: n_ubatch      = 512
0.00.454.010 I llama_context: flash_attn    = 0
0.00.454.012 I llama_context: freq_base     = 10000.0
0.00.454.013 I llama_context: freq_scale    = 1
0.00.454.015 I ggml_metal_init: allocating
0.00.454.097 I ggml_metal_init: found device: Apple M4
0.00.454.111 I ggml_metal_init: picking default device: Apple M4
0.00.455.943 I ggml_metal_init: using embedded metal library
0.00.461.515 I ggml_metal_init: GPU name:   Apple M4
0.00.461.520 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.461.521 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.461.521 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.461.522 I ggml_metal_init: simdgroup reduction   = true
0.00.461.522 I ggml_metal_init: simdgroup matrix mul. = true
0.00.461.523 I ggml_metal_init: has residency sets    = true
0.00.461.523 I ggml_metal_init: has bfloat            = true
0.00.461.523 I ggml_metal_init: use bfloat            = true
0.00.461.524 I ggml_metal_init: hasUnifiedMemory      = true
0.00.461.526 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.481.516 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.538.792 I init:      Metal KV buffer size =   384.00 MiB
0.00.538.799 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.538.822 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.543.289 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.543.291 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.543.291 I llama_context: graph nodes  = 967
0.00.543.291 I llama_context: graph splits = 2
0.00.543.311 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.543.436 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.543.437 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.599.624 I main: llama threadpool init, n_threads = 4
0.00.599.667 I 
0.00.599.690 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.599.690 I 
0.00.599.841 I sampler seed: 1234
0.00.599.846 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.599.890 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.599.894 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.599.894 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.345.004 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53625.38 tokens per second)
0.01.345.004 I llama_perf_context_print:        load time =     589.94 ms
0.01.345.005 I llama_perf_context_print: prompt eval time =      49.99 ms /     7 tokens (    7.14 ms per token,   140.04 tokens per second)
0.01.345.006 I llama_perf_context_print:        eval time =     692.22 ms /    63 runs   (   10.99 ms per token,    91.01 tokens per second)
0.01.345.006 I llama_perf_context_print:       total time =     746.09 ms /    70 tokens
0.01.348.953 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.110s
sys	0m0.195s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.009.387 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.153 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.163 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.166 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.171 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.172 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.962 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.668 I llama_model_loader: - type  f32:  194 tensors
0.00.025.668 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.668 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.669 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.669 I print_info: file format = GGUF V3 (latest)
0.00.025.670 I print_info: file type   = Q4_K - Medium
0.00.025.674 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.540 I load: special tokens cache size = 25
0.00.039.671 I load: token to piece cache size = 0.2984 MB
0.00.039.673 I print_info: arch             = gptneox
0.00.039.674 I print_info: vocab_only       = 0
0.00.039.674 I print_info: n_ctx_train      = 2048
0.00.039.674 I print_info: n_embd           = 2048
0.00.039.674 I print_info: n_layer          = 24
0.00.039.677 I print_info: n_head           = 16
0.00.039.678 I print_info: n_head_kv        = 16
0.00.039.678 I print_info: n_rot            = 32
0.00.039.679 I print_info: n_swa            = 0
0.00.039.679 I print_info: n_embd_head_k    = 128
0.00.039.679 I print_info: n_embd_head_v    = 128
0.00.039.680 I print_info: n_gqa            = 1
0.00.039.680 I print_info: n_embd_k_gqa     = 2048
0.00.039.681 I print_info: n_embd_v_gqa     = 2048
0.00.039.682 I print_info: f_norm_eps       = 1.0e-05
0.00.039.682 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.683 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.684 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.684 I print_info: f_logit_scale    = 0.0e+00
0.00.039.684 I print_info: n_ff             = 8192
0.00.039.685 I print_info: n_expert         = 0
0.00.039.685 I print_info: n_expert_used    = 0
0.00.039.685 I print_info: causal attn      = 1
0.00.039.685 I print_info: pooling type     = 0
0.00.039.685 I print_info: rope type        = 2
0.00.039.685 I print_info: rope scaling     = linear
0.00.039.687 I print_info: freq_base_train  = 10000.0
0.00.039.688 I print_info: freq_scale_train = 1
0.00.039.688 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.688 I print_info: rope_finetuned   = unknown
0.00.039.688 I print_info: ssm_d_conv       = 0
0.00.039.688 I print_info: ssm_d_inner      = 0
0.00.039.689 I print_info: ssm_d_state      = 0
0.00.039.690 I print_info: ssm_dt_rank      = 0
0.00.039.690 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.690 I print_info: model type       = 1.4B
0.00.039.691 I print_info: model params     = 1.41 B
0.00.039.691 I print_info: general.name     = 1.4B
0.00.039.692 I print_info: vocab type       = BPE
0.00.039.692 I print_info: n_vocab          = 50304
0.00.039.693 I print_info: n_merges         = 50009
0.00.039.693 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.693 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.693 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.694 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.694 I print_info: LF token         = 187 'Ċ'
0.00.039.694 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.694 I print_info: max token length = 1024
0.00.527.810 I load_tensors: offloading 24 repeating layers to GPU
0.00.527.825 I load_tensors: offloading output layer to GPU
0.00.527.825 I load_tensors: offloaded 25/25 layers to GPU
0.00.527.860 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.527.874 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.529.235 I llama_context: n_seq_max     = 1
0.00.529.240 I llama_context: n_ctx         = 2048
0.00.529.241 I llama_context: n_ctx_per_seq = 2048
0.00.529.241 I llama_context: n_batch       = 2048
0.00.529.242 I llama_context: n_ubatch      = 512
0.00.529.242 I llama_context: flash_attn    = 0
0.00.529.244 I llama_context: freq_base     = 10000.0
0.00.529.245 I llama_context: freq_scale    = 1
0.00.529.247 I ggml_metal_init: allocating
0.00.529.326 I ggml_metal_init: found device: Apple M4
0.00.529.341 I ggml_metal_init: picking default device: Apple M4
0.00.531.231 I ggml_metal_init: using embedded metal library
0.00.537.803 I ggml_metal_init: GPU name:   Apple M4
0.00.537.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.537.808 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.537.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.537.809 I ggml_metal_init: simdgroup reduction   = true
0.00.537.810 I ggml_metal_init: simdgroup matrix mul. = true
0.00.537.810 I ggml_metal_init: has residency sets    = true
0.00.537.810 I ggml_metal_init: has bfloat            = true
0.00.537.810 I ggml_metal_init: use bfloat            = true
0.00.537.811 I ggml_metal_init: hasUnifiedMemory      = true
0.00.537.813 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.555.785 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.377 I init:      Metal KV buffer size =   384.00 MiB
0.00.609.383 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.609.405 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.614.026 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.614.028 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.614.028 I llama_context: graph nodes  = 967
0.00.614.028 I llama_context: graph splits = 2
0.00.614.047 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.614.169 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.614.170 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.087 I main: llama threadpool init, n_threads = 4
0.00.670.132 I 
0.00.670.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.154 I 
0.00.670.306 I sampler seed: 1234
0.00.670.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.670.326 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.670.326 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.670.326 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.422.118 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48200.95 tokens per second)
0.01.422.119 I llama_perf_context_print:        load time =     659.99 ms
0.01.422.120 I llama_perf_context_print: prompt eval time =      46.94 ms /     7 tokens (    6.71 ms per token,   149.12 tokens per second)
0.01.422.120 I llama_perf_context_print:        eval time =     702.33 ms /    63 runs   (   11.15 ms per token,    89.70 tokens per second)
0.01.422.121 I llama_perf_context_print:       total time =     752.74 ms /    70 tokens
0.01.424.853 I ggml_metal_free: deallocating

real	0m1.439s
user	0m0.109s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.887 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.693 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.706 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.706 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.707 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.710 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.710 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.711 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.714 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.714 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.714 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.566 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.646 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.602 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.606 I llama_model_loader: - type  f32:  194 tensors
0.00.027.606 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.607 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.607 I print_info: file format = GGUF V3 (latest)
0.00.027.608 I print_info: file type   = Q5_K - Medium
0.00.027.609 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.676 I load: special tokens cache size = 25
0.00.041.792 I load: token to piece cache size = 0.2984 MB
0.00.041.797 I print_info: arch             = gptneox
0.00.041.797 I print_info: vocab_only       = 0
0.00.041.797 I print_info: n_ctx_train      = 2048
0.00.041.798 I print_info: n_embd           = 2048
0.00.041.798 I print_info: n_layer          = 24
0.00.041.802 I print_info: n_head           = 16
0.00.041.803 I print_info: n_head_kv        = 16
0.00.041.803 I print_info: n_rot            = 32
0.00.041.803 I print_info: n_swa            = 0
0.00.041.805 I print_info: n_embd_head_k    = 128
0.00.041.808 I print_info: n_embd_head_v    = 128
0.00.041.809 I print_info: n_gqa            = 1
0.00.041.810 I print_info: n_embd_k_gqa     = 2048
0.00.041.810 I print_info: n_embd_v_gqa     = 2048
0.00.041.811 I print_info: f_norm_eps       = 1.0e-05
0.00.041.811 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.811 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.813 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.813 I print_info: f_logit_scale    = 0.0e+00
0.00.041.814 I print_info: n_ff             = 8192
0.00.041.814 I print_info: n_expert         = 0
0.00.041.814 I print_info: n_expert_used    = 0
0.00.041.814 I print_info: causal attn      = 1
0.00.041.815 I print_info: pooling type     = 0
0.00.041.815 I print_info: rope type        = 2
0.00.041.815 I print_info: rope scaling     = linear
0.00.041.815 I print_info: freq_base_train  = 10000.0
0.00.041.815 I print_info: freq_scale_train = 1
0.00.041.816 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.816 I print_info: rope_finetuned   = unknown
0.00.041.816 I print_info: ssm_d_conv       = 0
0.00.041.816 I print_info: ssm_d_inner      = 0
0.00.041.816 I print_info: ssm_d_state      = 0
0.00.041.816 I print_info: ssm_dt_rank      = 0
0.00.041.816 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.817 I print_info: model type       = 1.4B
0.00.041.817 I print_info: model params     = 1.41 B
0.00.041.817 I print_info: general.name     = 1.4B
0.00.041.818 I print_info: vocab type       = BPE
0.00.041.818 I print_info: n_vocab          = 50304
0.00.041.818 I print_info: n_merges         = 50009
0.00.041.818 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.818 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.819 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.819 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.819 I print_info: LF token         = 187 'Ċ'
0.00.041.819 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.819 I print_info: max token length = 1024
0.00.601.473 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.482 I load_tensors: offloading output layer to GPU
0.00.601.482 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.511 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.516 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.602.663 I llama_context: n_seq_max     = 1
0.00.602.669 I llama_context: n_ctx         = 2048
0.00.602.670 I llama_context: n_ctx_per_seq = 2048
0.00.602.670 I llama_context: n_batch       = 2048
0.00.602.671 I llama_context: n_ubatch      = 512
0.00.602.671 I llama_context: flash_attn    = 0
0.00.602.672 I llama_context: freq_base     = 10000.0
0.00.602.682 I llama_context: freq_scale    = 1
0.00.602.684 I ggml_metal_init: allocating
0.00.602.750 I ggml_metal_init: found device: Apple M4
0.00.602.764 I ggml_metal_init: picking default device: Apple M4
0.00.604.290 I ggml_metal_init: using embedded metal library
0.00.610.969 I ggml_metal_init: GPU name:   Apple M4
0.00.610.972 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.973 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.974 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.975 I ggml_metal_init: simdgroup reduction   = true
0.00.610.975 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.975 I ggml_metal_init: has residency sets    = true
0.00.610.976 I ggml_metal_init: has bfloat            = true
0.00.610.976 I ggml_metal_init: use bfloat            = true
0.00.610.977 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.980 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.321 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.607 I init:      Metal KV buffer size =   384.00 MiB
0.00.689.618 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.689.642 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.694.066 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.694.068 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.694.068 I llama_context: graph nodes  = 967
0.00.694.068 I llama_context: graph splits = 2
0.00.694.088 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.694.215 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.694.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.508 I main: llama threadpool init, n_threads = 4
0.00.760.553 I 
0.00.760.577 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.577 I 
0.00.760.738 I sampler seed: 1234
0.00.760.743 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.789 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.793 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.793 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.601.186 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.601.187 I llama_perf_context_print:        load time =     748.89 ms
0.01.601.188 I llama_perf_context_print: prompt eval time =      51.39 ms /     7 tokens (    7.34 ms per token,   136.20 tokens per second)
0.01.601.188 I llama_perf_context_print:        eval time =     786.14 ms /    63 runs   (   12.48 ms per token,    80.14 tokens per second)
0.01.601.189 I llama_perf_context_print:       total time =     841.40 ms /    70 tokens
0.01.605.051 I ggml_metal_free: deallocating

real	0m1.625s
user	0m0.109s
sys	0m0.225s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.539 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.472 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.476 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.478 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.478 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.479 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.479 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.479 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.485 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.486 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.486 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.487 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.490 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.490 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.387 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.399 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.190 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.191 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.191 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.192 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.192 I llama_model_loader: - type  f32:  194 tensors
0.00.026.193 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.193 I print_info: file format = GGUF V3 (latest)
0.00.026.194 I print_info: file type   = Q6_K
0.00.026.198 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.181 I load: special tokens cache size = 25
0.00.040.197 I load: token to piece cache size = 0.2984 MB
0.00.040.200 I print_info: arch             = gptneox
0.00.040.201 I print_info: vocab_only       = 0
0.00.040.201 I print_info: n_ctx_train      = 2048
0.00.040.201 I print_info: n_embd           = 2048
0.00.040.201 I print_info: n_layer          = 24
0.00.040.204 I print_info: n_head           = 16
0.00.040.205 I print_info: n_head_kv        = 16
0.00.040.205 I print_info: n_rot            = 32
0.00.040.205 I print_info: n_swa            = 0
0.00.040.205 I print_info: n_embd_head_k    = 128
0.00.040.206 I print_info: n_embd_head_v    = 128
0.00.040.206 I print_info: n_gqa            = 1
0.00.040.207 I print_info: n_embd_k_gqa     = 2048
0.00.040.210 I print_info: n_embd_v_gqa     = 2048
0.00.040.211 I print_info: f_norm_eps       = 1.0e-05
0.00.040.212 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.212 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.212 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.214 I print_info: f_logit_scale    = 0.0e+00
0.00.040.216 I print_info: n_ff             = 8192
0.00.040.216 I print_info: n_expert         = 0
0.00.040.216 I print_info: n_expert_used    = 0
0.00.040.217 I print_info: causal attn      = 1
0.00.040.217 I print_info: pooling type     = 0
0.00.040.217 I print_info: rope type        = 2
0.00.040.220 I print_info: rope scaling     = linear
0.00.040.221 I print_info: freq_base_train  = 10000.0
0.00.040.221 I print_info: freq_scale_train = 1
0.00.040.221 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.222 I print_info: rope_finetuned   = unknown
0.00.040.232 I print_info: ssm_d_conv       = 0
0.00.040.233 I print_info: ssm_d_inner      = 0
0.00.040.233 I print_info: ssm_d_state      = 0
0.00.040.233 I print_info: ssm_dt_rank      = 0
0.00.040.233 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.233 I print_info: model type       = 1.4B
0.00.040.234 I print_info: model params     = 1.41 B
0.00.040.234 I print_info: general.name     = 1.4B
0.00.040.235 I print_info: vocab type       = BPE
0.00.040.235 I print_info: n_vocab          = 50304
0.00.040.235 I print_info: n_merges         = 50009
0.00.040.235 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.236 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.236 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.236 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.237 I print_info: LF token         = 187 'Ċ'
0.00.040.237 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.237 I print_info: max token length = 1024
0.00.650.739 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.744 I load_tensors: offloading output layer to GPU
0.00.650.745 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.769 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.650.770 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.652.016 I llama_context: n_seq_max     = 1
0.00.652.018 I llama_context: n_ctx         = 2048
0.00.652.022 I llama_context: n_ctx_per_seq = 2048
0.00.652.022 I llama_context: n_batch       = 2048
0.00.652.023 I llama_context: n_ubatch      = 512
0.00.652.023 I llama_context: flash_attn    = 0
0.00.652.024 I llama_context: freq_base     = 10000.0
0.00.652.025 I llama_context: freq_scale    = 1
0.00.652.030 I ggml_metal_init: allocating
0.00.652.042 I ggml_metal_init: found device: Apple M4
0.00.652.052 I ggml_metal_init: picking default device: Apple M4
0.00.653.467 I ggml_metal_init: using embedded metal library
0.00.659.356 I ggml_metal_init: GPU name:   Apple M4
0.00.659.360 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.362 I ggml_metal_init: simdgroup reduction   = true
0.00.659.362 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.362 I ggml_metal_init: has residency sets    = true
0.00.659.363 I ggml_metal_init: has bfloat            = true
0.00.659.363 I ggml_metal_init: use bfloat            = true
0.00.659.364 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.365 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.985 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.548 I init:      Metal KV buffer size =   384.00 MiB
0.00.730.557 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.581 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.735.981 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.735.983 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.735.984 I llama_context: graph nodes  = 967
0.00.735.984 I llama_context: graph splits = 2
0.00.736.004 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.736.132 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.736.132 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.015 I main: llama threadpool init, n_threads = 4
0.00.806.061 I 
0.00.806.082 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.806.084 I 
0.00.806.236 I sampler seed: 1234
0.00.806.241 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.256 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.257 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.257 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.678.157 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.01.678.158 I llama_perf_context_print:        load time =     795.78 ms
0.01.678.158 I llama_perf_context_print: prompt eval time =      54.38 ms /     7 tokens (    7.77 ms per token,   128.73 tokens per second)
0.01.678.160 I llama_perf_context_print:        eval time =     814.64 ms /    63 runs   (   12.93 ms per token,    77.33 tokens per second)
0.01.678.160 I llama_perf_context_print:       total time =     872.84 ms /    70 tokens
0.01.682.083 I ggml_metal_free: deallocating

real	0m1.698s
user	0m0.107s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.826 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.463 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.333 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.345 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.350 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.353 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.354 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.354 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.355 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.356 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.356 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.357 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.182 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.648 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.824 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.825 I llama_model_loader: - type  f32:  194 tensors
0.00.055.826 I llama_model_loader: - type  f16:   98 tensors
0.00.055.826 I print_info: file format = GGUF V3 (latest)
0.00.055.827 I print_info: file type   = all F32 (guessed)
0.00.055.830 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.558 I load: special tokens cache size = 25
0.00.079.866 I load: token to piece cache size = 0.2984 MB
0.00.079.870 I print_info: arch             = gptneox
0.00.079.870 I print_info: vocab_only       = 0
0.00.079.870 I print_info: n_ctx_train      = 2048
0.00.079.870 I print_info: n_embd           = 2048
0.00.079.870 I print_info: n_layer          = 24
0.00.079.874 I print_info: n_head           = 16
0.00.079.875 I print_info: n_head_kv        = 16
0.00.079.876 I print_info: n_rot            = 32
0.00.079.876 I print_info: n_swa            = 0
0.00.079.876 I print_info: n_embd_head_k    = 128
0.00.079.876 I print_info: n_embd_head_v    = 128
0.00.079.877 I print_info: n_gqa            = 1
0.00.079.878 I print_info: n_embd_k_gqa     = 2048
0.00.079.879 I print_info: n_embd_v_gqa     = 2048
0.00.079.879 I print_info: f_norm_eps       = 1.0e-05
0.00.079.880 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.880 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.880 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.880 I print_info: f_logit_scale    = 0.0e+00
0.00.079.881 I print_info: n_ff             = 8192
0.00.079.881 I print_info: n_expert         = 0
0.00.079.881 I print_info: n_expert_used    = 0
0.00.079.882 I print_info: causal attn      = 1
0.00.079.882 I print_info: pooling type     = 0
0.00.079.882 I print_info: rope type        = 2
0.00.079.882 I print_info: rope scaling     = linear
0.00.079.883 I print_info: freq_base_train  = 10000.0
0.00.079.883 I print_info: freq_scale_train = 1
0.00.079.883 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.884 I print_info: rope_finetuned   = unknown
0.00.079.884 I print_info: ssm_d_conv       = 0
0.00.079.884 I print_info: ssm_d_inner      = 0
0.00.079.885 I print_info: ssm_d_state      = 0
0.00.079.886 I print_info: ssm_dt_rank      = 0
0.00.079.886 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.888 I print_info: model type       = 1.4B
0.00.079.888 I print_info: model params     = 1.41 B
0.00.079.889 I print_info: general.name     = 1.4B
0.00.079.889 I print_info: vocab type       = BPE
0.00.079.889 I print_info: n_vocab          = 50304
0.00.079.890 I print_info: n_merges         = 50009
0.00.079.890 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.890 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.890 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.890 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.895 I print_info: LF token         = 187 'Ċ'
0.00.079.896 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.896 I print_info: max token length = 1024
0.01.277.332 I load_tensors: offloading 24 repeating layers to GPU
0.01.277.336 I load_tensors: offloading output layer to GPU
0.01.277.336 I load_tensors: offloaded 25/25 layers to GPU
0.01.277.363 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.277.364 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.278.588 I llama_context: n_seq_max     = 1
0.01.278.589 I llama_context: n_ctx         = 128
0.01.278.589 I llama_context: n_ctx_per_seq = 128
0.01.278.590 I llama_context: n_batch       = 128
0.01.278.590 I llama_context: n_ubatch      = 128
0.01.278.590 I llama_context: flash_attn    = 0
0.01.278.594 I llama_context: freq_base     = 10000.0
0.01.278.597 I llama_context: freq_scale    = 1
0.01.278.597 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.278.602 I ggml_metal_init: allocating
0.01.278.689 I ggml_metal_init: found device: Apple M4
0.01.278.702 I ggml_metal_init: picking default device: Apple M4
0.01.279.856 I ggml_metal_init: using embedded metal library
0.01.283.747 I ggml_metal_init: GPU name:   Apple M4
0.01.283.749 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.283.750 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.283.750 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.283.751 I ggml_metal_init: simdgroup reduction   = true
0.01.283.751 I ggml_metal_init: simdgroup matrix mul. = true
0.01.283.751 I ggml_metal_init: has residency sets    = true
0.01.283.751 I ggml_metal_init: has bfloat            = true
0.01.283.751 I ggml_metal_init: use bfloat            = true
0.01.283.752 I ggml_metal_init: hasUnifiedMemory      = true
0.01.283.754 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.294.145 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.295.841 I init:      Metal KV buffer size =    24.00 MiB
0.01.295.843 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.295.857 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.297.511 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.297.513 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.297.513 I llama_context: graph nodes  = 967
0.01.297.513 I llama_context: graph splits = 2
0.01.297.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.297.526 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.332.440 I 
0.01.332.494 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.332.498 I perplexity: tokenizing the input ..
0.01.337.296 I perplexity: tokenization took 4.796 ms
0.01.337.300 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.455.670 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.457.213 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.457.245 I llama_perf_context_print:        load time =    1307.97 ms
0.01.457.246 I llama_perf_context_print: prompt eval time =     118.11 ms /   128 tokens (    0.92 ms per token,  1083.78 tokens per second)
0.01.457.247 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.457.247 I llama_perf_context_print:       total time =     124.81 ms /   129 tokens
0.01.457.784 I ggml_metal_free: deallocating

real	0m1.722s
user	0m0.101s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.282 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.681 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.687 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.688 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.689 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.696 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.697 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.699 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.701 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.702 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.704 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.704 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.704 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.676 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.663 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.530 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.533 I llama_model_loader: - type  f32:  194 tensors
0.00.025.533 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.534 I print_info: file format = GGUF V3 (latest)
0.00.025.538 I print_info: file type   = Q8_0
0.00.025.539 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.006 I load: special tokens cache size = 25
0.00.040.227 I load: token to piece cache size = 0.2984 MB
0.00.040.231 I print_info: arch             = gptneox
0.00.040.232 I print_info: vocab_only       = 0
0.00.040.232 I print_info: n_ctx_train      = 2048
0.00.040.232 I print_info: n_embd           = 2048
0.00.040.232 I print_info: n_layer          = 24
0.00.040.236 I print_info: n_head           = 16
0.00.040.237 I print_info: n_head_kv        = 16
0.00.040.239 I print_info: n_rot            = 32
0.00.040.239 I print_info: n_swa            = 0
0.00.040.239 I print_info: n_embd_head_k    = 128
0.00.040.239 I print_info: n_embd_head_v    = 128
0.00.040.240 I print_info: n_gqa            = 1
0.00.040.241 I print_info: n_embd_k_gqa     = 2048
0.00.040.241 I print_info: n_embd_v_gqa     = 2048
0.00.040.242 I print_info: f_norm_eps       = 1.0e-05
0.00.040.242 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.242 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.243 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.243 I print_info: f_logit_scale    = 0.0e+00
0.00.040.243 I print_info: n_ff             = 8192
0.00.040.244 I print_info: n_expert         = 0
0.00.040.244 I print_info: n_expert_used    = 0
0.00.040.244 I print_info: causal attn      = 1
0.00.040.244 I print_info: pooling type     = 0
0.00.040.244 I print_info: rope type        = 2
0.00.040.244 I print_info: rope scaling     = linear
0.00.040.245 I print_info: freq_base_train  = 10000.0
0.00.040.245 I print_info: freq_scale_train = 1
0.00.040.245 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.245 I print_info: rope_finetuned   = unknown
0.00.040.245 I print_info: ssm_d_conv       = 0
0.00.040.247 I print_info: ssm_d_inner      = 0
0.00.040.247 I print_info: ssm_d_state      = 0
0.00.040.247 I print_info: ssm_dt_rank      = 0
0.00.040.247 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.248 I print_info: model type       = 1.4B
0.00.040.248 I print_info: model params     = 1.41 B
0.00.040.248 I print_info: general.name     = 1.4B
0.00.040.248 I print_info: vocab type       = BPE
0.00.040.249 I print_info: n_vocab          = 50304
0.00.040.249 I print_info: n_merges         = 50009
0.00.040.250 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.251 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.251 I print_info: LF token         = 187 'Ċ'
0.00.040.251 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.251 I print_info: max token length = 1024
0.00.897.264 I load_tensors: offloading 24 repeating layers to GPU
0.00.897.269 I load_tensors: offloading output layer to GPU
0.00.897.270 I load_tensors: offloaded 25/25 layers to GPU
0.00.897.299 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.897.300 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.898.514 I llama_context: n_seq_max     = 1
0.00.898.516 I llama_context: n_ctx         = 128
0.00.898.516 I llama_context: n_ctx_per_seq = 128
0.00.898.517 I llama_context: n_batch       = 128
0.00.898.520 I llama_context: n_ubatch      = 128
0.00.898.520 I llama_context: flash_attn    = 0
0.00.898.521 I llama_context: freq_base     = 10000.0
0.00.898.522 I llama_context: freq_scale    = 1
0.00.898.522 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.898.523 I ggml_metal_init: allocating
0.00.898.612 I ggml_metal_init: found device: Apple M4
0.00.898.623 I ggml_metal_init: picking default device: Apple M4
0.00.899.908 I ggml_metal_init: using embedded metal library
0.00.905.103 I ggml_metal_init: GPU name:   Apple M4
0.00.905.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.905.107 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.905.108 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.905.108 I ggml_metal_init: simdgroup reduction   = true
0.00.905.109 I ggml_metal_init: simdgroup matrix mul. = true
0.00.905.109 I ggml_metal_init: has residency sets    = true
0.00.905.109 I ggml_metal_init: has bfloat            = true
0.00.905.109 I ggml_metal_init: use bfloat            = true
0.00.905.110 I ggml_metal_init: hasUnifiedMemory      = true
0.00.905.119 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.920.088 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.923.424 I init:      Metal KV buffer size =    24.00 MiB
0.00.923.432 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.923.468 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.926.562 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.926.563 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.926.564 I llama_context: graph nodes  = 967
0.00.926.564 I llama_context: graph splits = 2
0.00.926.582 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.926.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.957.711 I 
0.00.957.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.957.802 I perplexity: tokenizing the input ..
0.00.965.109 I perplexity: tokenization took 7.305 ms
0.00.965.115 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.104.775 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.106.289 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.106.310 I llama_perf_context_print:        load time =     948.42 ms
0.01.106.311 I llama_perf_context_print: prompt eval time =     138.77 ms /   128 tokens (    1.08 ms per token,   922.38 tokens per second)
0.01.106.312 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.106.313 I llama_perf_context_print:       total time =     148.61 ms /   129 tokens
0.01.106.917 I ggml_metal_free: deallocating

real	0m1.122s
user	0m0.078s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.591 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.807 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.814 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.815 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.815 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.816 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.817 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.818 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.818 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.818 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.819 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.822 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.823 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.824 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.824 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.679 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.734 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.605 I llama_model_loader: - type  f32:  194 tensors
0.00.025.606 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.606 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.607 I print_info: file format = GGUF V3 (latest)
0.00.025.607 I print_info: file type   = Q4_0
0.00.025.608 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.847 I load: special tokens cache size = 25
0.00.039.576 I load: token to piece cache size = 0.2984 MB
0.00.039.580 I print_info: arch             = gptneox
0.00.039.580 I print_info: vocab_only       = 0
0.00.039.580 I print_info: n_ctx_train      = 2048
0.00.039.580 I print_info: n_embd           = 2048
0.00.039.580 I print_info: n_layer          = 24
0.00.039.585 I print_info: n_head           = 16
0.00.039.585 I print_info: n_head_kv        = 16
0.00.039.586 I print_info: n_rot            = 32
0.00.039.586 I print_info: n_swa            = 0
0.00.039.586 I print_info: n_embd_head_k    = 128
0.00.039.586 I print_info: n_embd_head_v    = 128
0.00.039.587 I print_info: n_gqa            = 1
0.00.039.588 I print_info: n_embd_k_gqa     = 2048
0.00.039.588 I print_info: n_embd_v_gqa     = 2048
0.00.039.589 I print_info: f_norm_eps       = 1.0e-05
0.00.039.590 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.590 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.590 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.590 I print_info: f_logit_scale    = 0.0e+00
0.00.039.593 I print_info: n_ff             = 8192
0.00.039.593 I print_info: n_expert         = 0
0.00.039.593 I print_info: n_expert_used    = 0
0.00.039.593 I print_info: causal attn      = 1
0.00.039.594 I print_info: pooling type     = 0
0.00.039.594 I print_info: rope type        = 2
0.00.039.594 I print_info: rope scaling     = linear
0.00.039.594 I print_info: freq_base_train  = 10000.0
0.00.039.595 I print_info: freq_scale_train = 1
0.00.039.595 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.596 I print_info: rope_finetuned   = unknown
0.00.039.597 I print_info: ssm_d_conv       = 0
0.00.039.597 I print_info: ssm_d_inner      = 0
0.00.039.597 I print_info: ssm_d_state      = 0
0.00.039.597 I print_info: ssm_dt_rank      = 0
0.00.039.597 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.598 I print_info: model type       = 1.4B
0.00.039.598 I print_info: model params     = 1.41 B
0.00.039.598 I print_info: general.name     = 1.4B
0.00.039.598 I print_info: vocab type       = BPE
0.00.039.599 I print_info: n_vocab          = 50304
0.00.039.599 I print_info: n_merges         = 50009
0.00.039.599 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.599 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.599 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.600 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.601 I print_info: LF token         = 187 'Ċ'
0.00.039.602 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.602 I print_info: max token length = 1024
0.00.601.786 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.802 I load_tensors: offloading output layer to GPU
0.00.601.803 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.835 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.601.836 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.603.354 I llama_context: n_seq_max     = 1
0.00.603.359 I llama_context: n_ctx         = 128
0.00.603.360 I llama_context: n_ctx_per_seq = 128
0.00.603.360 I llama_context: n_batch       = 128
0.00.603.361 I llama_context: n_ubatch      = 128
0.00.603.361 I llama_context: flash_attn    = 0
0.00.603.364 I llama_context: freq_base     = 10000.0
0.00.603.364 I llama_context: freq_scale    = 1
0.00.603.365 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.603.367 I ggml_metal_init: allocating
0.00.603.456 I ggml_metal_init: found device: Apple M4
0.00.603.470 I ggml_metal_init: picking default device: Apple M4
0.00.605.235 I ggml_metal_init: using embedded metal library
0.00.611.861 I ggml_metal_init: GPU name:   Apple M4
0.00.611.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.867 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.868 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.869 I ggml_metal_init: simdgroup reduction   = true
0.00.611.869 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.870 I ggml_metal_init: has residency sets    = true
0.00.611.870 I ggml_metal_init: has bfloat            = true
0.00.611.870 I ggml_metal_init: use bfloat            = true
0.00.611.871 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.463 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.995 I init:      Metal KV buffer size =    24.00 MiB
0.00.634.002 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.634.034 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.637.503 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.637.505 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.637.505 I llama_context: graph nodes  = 967
0.00.637.506 I llama_context: graph splits = 2
0.00.637.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.458 I 
0.00.666.537 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.545 I perplexity: tokenizing the input ..
0.00.673.480 I perplexity: tokenization took 6.934 ms
0.00.673.488 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.306 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.811.801 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.811.822 I llama_perf_context_print:        load time =     656.86 ms
0.00.811.824 I llama_perf_context_print: prompt eval time =     136.44 ms /   128 tokens (    1.07 ms per token,   938.18 tokens per second)
0.00.811.824 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.826 I llama_perf_context_print:       total time =     145.37 ms /   129 tokens
0.00.812.384 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.081s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.764 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.190 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.195 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.200 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.200 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.200 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.201 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.201 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.202 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.202 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.203 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.203 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.203 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.204 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.206 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.206 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.206 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.143 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.199 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.102 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.104 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.104 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.105 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.106 I llama_model_loader: - type  f32:  194 tensors
0.00.025.106 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.106 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.107 I print_info: file format = GGUF V3 (latest)
0.00.025.109 I print_info: file type   = Q4_1
0.00.025.110 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.399 I load: special tokens cache size = 25
0.00.039.403 I load: token to piece cache size = 0.2984 MB
0.00.039.406 I print_info: arch             = gptneox
0.00.039.406 I print_info: vocab_only       = 0
0.00.039.406 I print_info: n_ctx_train      = 2048
0.00.039.406 I print_info: n_embd           = 2048
0.00.039.407 I print_info: n_layer          = 24
0.00.039.410 I print_info: n_head           = 16
0.00.039.411 I print_info: n_head_kv        = 16
0.00.039.411 I print_info: n_rot            = 32
0.00.039.411 I print_info: n_swa            = 0
0.00.039.411 I print_info: n_embd_head_k    = 128
0.00.039.411 I print_info: n_embd_head_v    = 128
0.00.039.412 I print_info: n_gqa            = 1
0.00.039.413 I print_info: n_embd_k_gqa     = 2048
0.00.039.414 I print_info: n_embd_v_gqa     = 2048
0.00.039.414 I print_info: f_norm_eps       = 1.0e-05
0.00.039.415 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.420 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.421 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.421 I print_info: f_logit_scale    = 0.0e+00
0.00.039.422 I print_info: n_ff             = 8192
0.00.039.422 I print_info: n_expert         = 0
0.00.039.422 I print_info: n_expert_used    = 0
0.00.039.422 I print_info: causal attn      = 1
0.00.039.422 I print_info: pooling type     = 0
0.00.039.422 I print_info: rope type        = 2
0.00.039.423 I print_info: rope scaling     = linear
0.00.039.423 I print_info: freq_base_train  = 10000.0
0.00.039.425 I print_info: freq_scale_train = 1
0.00.039.425 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.425 I print_info: rope_finetuned   = unknown
0.00.039.426 I print_info: ssm_d_conv       = 0
0.00.039.426 I print_info: ssm_d_inner      = 0
0.00.039.426 I print_info: ssm_d_state      = 0
0.00.039.426 I print_info: ssm_dt_rank      = 0
0.00.039.426 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.426 I print_info: model type       = 1.4B
0.00.039.427 I print_info: model params     = 1.41 B
0.00.039.427 I print_info: general.name     = 1.4B
0.00.039.428 I print_info: vocab type       = BPE
0.00.039.428 I print_info: n_vocab          = 50304
0.00.039.428 I print_info: n_merges         = 50009
0.00.039.428 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.430 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.430 I print_info: LF token         = 187 'Ċ'
0.00.039.430 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.430 I print_info: max token length = 1024
0.00.658.517 I load_tensors: offloading 24 repeating layers to GPU
0.00.658.533 I load_tensors: offloading output layer to GPU
0.00.658.533 I load_tensors: offloaded 25/25 layers to GPU
0.00.658.566 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.658.567 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.660.059 I llama_context: n_seq_max     = 1
0.00.660.065 I llama_context: n_ctx         = 128
0.00.660.065 I llama_context: n_ctx_per_seq = 128
0.00.660.066 I llama_context: n_batch       = 128
0.00.660.067 I llama_context: n_ubatch      = 128
0.00.660.067 I llama_context: flash_attn    = 0
0.00.660.070 I llama_context: freq_base     = 10000.0
0.00.660.070 I llama_context: freq_scale    = 1
0.00.660.071 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.660.073 I ggml_metal_init: allocating
0.00.660.148 I ggml_metal_init: found device: Apple M4
0.00.660.162 I ggml_metal_init: picking default device: Apple M4
0.00.661.918 I ggml_metal_init: using embedded metal library
0.00.668.470 I ggml_metal_init: GPU name:   Apple M4
0.00.668.474 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.668.475 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.668.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.668.476 I ggml_metal_init: simdgroup reduction   = true
0.00.668.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.668.477 I ggml_metal_init: has residency sets    = true
0.00.668.477 I ggml_metal_init: has bfloat            = true
0.00.668.477 I ggml_metal_init: use bfloat            = true
0.00.668.478 I ggml_metal_init: hasUnifiedMemory      = true
0.00.668.480 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.672 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.118 I init:      Metal KV buffer size =    24.00 MiB
0.00.689.122 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.689.147 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.692.458 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.692.460 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.692.460 I llama_context: graph nodes  = 967
0.00.692.461 I llama_context: graph splits = 2
0.00.692.480 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.692.481 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.256 I 
0.00.716.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.327 I perplexity: tokenizing the input ..
0.00.722.664 I perplexity: tokenization took 6.336 ms
0.00.722.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.417 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.845.991 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.846.012 I llama_perf_context_print:        load time =     707.48 ms
0.00.846.013 I llama_perf_context_print: prompt eval time =     121.52 ms /   128 tokens (    0.95 ms per token,  1053.36 tokens per second)
0.00.846.014 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.014 I llama_perf_context_print:       total time =     129.76 ms /   129 tokens
0.00.846.540 I ggml_metal_free: deallocating

real	0m0.860s
user	0m0.079s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.081 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.092 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.098 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.105 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.106 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.106 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.107 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.108 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.108 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.109 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.110 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.110 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.000 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.072 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.924 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.926 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.926 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.926 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.927 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.928 I llama_model_loader: - type  f32:  194 tensors
0.00.024.928 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.928 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.929 I print_info: file format = GGUF V3 (latest)
0.00.024.929 I print_info: file type   = Q5_0
0.00.024.931 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.376 I load: special tokens cache size = 25
0.00.039.455 I load: token to piece cache size = 0.2984 MB
0.00.039.460 I print_info: arch             = gptneox
0.00.039.460 I print_info: vocab_only       = 0
0.00.039.460 I print_info: n_ctx_train      = 2048
0.00.039.460 I print_info: n_embd           = 2048
0.00.039.461 I print_info: n_layer          = 24
0.00.039.465 I print_info: n_head           = 16
0.00.039.465 I print_info: n_head_kv        = 16
0.00.039.466 I print_info: n_rot            = 32
0.00.039.466 I print_info: n_swa            = 0
0.00.039.466 I print_info: n_embd_head_k    = 128
0.00.039.466 I print_info: n_embd_head_v    = 128
0.00.039.467 I print_info: n_gqa            = 1
0.00.039.467 I print_info: n_embd_k_gqa     = 2048
0.00.039.468 I print_info: n_embd_v_gqa     = 2048
0.00.039.468 I print_info: f_norm_eps       = 1.0e-05
0.00.039.469 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.469 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.469 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.469 I print_info: f_logit_scale    = 0.0e+00
0.00.039.470 I print_info: n_ff             = 8192
0.00.039.470 I print_info: n_expert         = 0
0.00.039.470 I print_info: n_expert_used    = 0
0.00.039.470 I print_info: causal attn      = 1
0.00.039.470 I print_info: pooling type     = 0
0.00.039.473 I print_info: rope type        = 2
0.00.039.473 I print_info: rope scaling     = linear
0.00.039.474 I print_info: freq_base_train  = 10000.0
0.00.039.474 I print_info: freq_scale_train = 1
0.00.039.474 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.475 I print_info: rope_finetuned   = unknown
0.00.039.475 I print_info: ssm_d_conv       = 0
0.00.039.475 I print_info: ssm_d_inner      = 0
0.00.039.475 I print_info: ssm_d_state      = 0
0.00.039.475 I print_info: ssm_dt_rank      = 0
0.00.039.475 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.475 I print_info: model type       = 1.4B
0.00.039.476 I print_info: model params     = 1.41 B
0.00.039.476 I print_info: general.name     = 1.4B
0.00.039.476 I print_info: vocab type       = BPE
0.00.039.476 I print_info: n_vocab          = 50304
0.00.039.477 I print_info: n_merges         = 50009
0.00.039.477 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.477 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.477 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.477 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.477 I print_info: LF token         = 187 'Ċ'
0.00.039.478 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.478 I print_info: max token length = 1024
0.00.720.784 I load_tensors: offloading 24 repeating layers to GPU
0.00.720.801 I load_tensors: offloading output layer to GPU
0.00.720.802 I load_tensors: offloaded 25/25 layers to GPU
0.00.720.835 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.720.837 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.722.352 I llama_context: n_seq_max     = 1
0.00.722.356 I llama_context: n_ctx         = 128
0.00.722.356 I llama_context: n_ctx_per_seq = 128
0.00.722.357 I llama_context: n_batch       = 128
0.00.722.357 I llama_context: n_ubatch      = 128
0.00.722.358 I llama_context: flash_attn    = 0
0.00.722.359 I llama_context: freq_base     = 10000.0
0.00.722.359 I llama_context: freq_scale    = 1
0.00.722.360 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.722.362 I ggml_metal_init: allocating
0.00.722.384 I ggml_metal_init: found device: Apple M4
0.00.722.394 I ggml_metal_init: picking default device: Apple M4
0.00.723.733 I ggml_metal_init: using embedded metal library
0.00.729.982 I ggml_metal_init: GPU name:   Apple M4
0.00.729.987 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.729.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.729.988 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.729.992 I ggml_metal_init: simdgroup reduction   = true
0.00.729.992 I ggml_metal_init: simdgroup matrix mul. = true
0.00.729.993 I ggml_metal_init: has residency sets    = true
0.00.729.993 I ggml_metal_init: has bfloat            = true
0.00.729.993 I ggml_metal_init: use bfloat            = true
0.00.729.994 I ggml_metal_init: hasUnifiedMemory      = true
0.00.729.999 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.747.071 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.750.575 I init:      Metal KV buffer size =    24.00 MiB
0.00.750.579 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.750.611 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.753.724 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.753.725 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.753.726 I llama_context: graph nodes  = 967
0.00.753.726 I llama_context: graph splits = 2
0.00.753.744 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.753.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.785.919 I 
0.00.785.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.007 I perplexity: tokenizing the input ..
0.00.793.493 I perplexity: tokenization took 7.484 ms
0.00.793.503 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.939.366 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.940.897 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.940.927 I llama_perf_context_print:        load time =     776.83 ms
0.00.940.928 I llama_perf_context_print: prompt eval time =     144.95 ms /   128 tokens (    1.13 ms per token,   883.07 tokens per second)
0.00.940.928 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.940.929 I llama_perf_context_print:       total time =     155.01 ms /   129 tokens
0.00.941.495 I ggml_metal_free: deallocating

real	0m0.955s
user	0m0.081s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.168 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.980 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.982 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.983 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.984 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.984 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.984 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.991 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.992 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.863 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.752 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.753 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.754 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.754 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.754 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.754 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.755 I llama_model_loader: - type  f32:  194 tensors
0.00.025.755 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.755 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.756 I print_info: file format = GGUF V3 (latest)
0.00.025.757 I print_info: file type   = Q5_1
0.00.025.761 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.966 I load: special tokens cache size = 25
0.00.039.949 I load: token to piece cache size = 0.2984 MB
0.00.039.952 I print_info: arch             = gptneox
0.00.039.952 I print_info: vocab_only       = 0
0.00.039.953 I print_info: n_ctx_train      = 2048
0.00.039.953 I print_info: n_embd           = 2048
0.00.039.953 I print_info: n_layer          = 24
0.00.039.956 I print_info: n_head           = 16
0.00.039.957 I print_info: n_head_kv        = 16
0.00.039.957 I print_info: n_rot            = 32
0.00.039.957 I print_info: n_swa            = 0
0.00.039.957 I print_info: n_embd_head_k    = 128
0.00.039.959 I print_info: n_embd_head_v    = 128
0.00.039.960 I print_info: n_gqa            = 1
0.00.039.961 I print_info: n_embd_k_gqa     = 2048
0.00.039.961 I print_info: n_embd_v_gqa     = 2048
0.00.039.962 I print_info: f_norm_eps       = 1.0e-05
0.00.039.962 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.962 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.962 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.963 I print_info: f_logit_scale    = 0.0e+00
0.00.039.963 I print_info: n_ff             = 8192
0.00.039.964 I print_info: n_expert         = 0
0.00.039.965 I print_info: n_expert_used    = 0
0.00.039.965 I print_info: causal attn      = 1
0.00.039.965 I print_info: pooling type     = 0
0.00.039.966 I print_info: rope type        = 2
0.00.039.966 I print_info: rope scaling     = linear
0.00.039.966 I print_info: freq_base_train  = 10000.0
0.00.039.966 I print_info: freq_scale_train = 1
0.00.039.966 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.967 I print_info: rope_finetuned   = unknown
0.00.039.967 I print_info: ssm_d_conv       = 0
0.00.039.967 I print_info: ssm_d_inner      = 0
0.00.039.967 I print_info: ssm_d_state      = 0
0.00.039.967 I print_info: ssm_dt_rank      = 0
0.00.039.968 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.968 I print_info: model type       = 1.4B
0.00.039.968 I print_info: model params     = 1.41 B
0.00.039.968 I print_info: general.name     = 1.4B
0.00.039.969 I print_info: vocab type       = BPE
0.00.039.969 I print_info: n_vocab          = 50304
0.00.039.970 I print_info: n_merges         = 50009
0.00.039.971 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.971 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.971 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.971 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.972 I print_info: LF token         = 187 'Ċ'
0.00.039.972 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.972 I print_info: max token length = 1024
0.00.595.501 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.516 I load_tensors: offloading output layer to GPU
0.00.595.516 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.556 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.595.558 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.597.164 I llama_context: n_seq_max     = 1
0.00.597.168 I llama_context: n_ctx         = 128
0.00.597.168 I llama_context: n_ctx_per_seq = 128
0.00.597.169 I llama_context: n_batch       = 128
0.00.597.169 I llama_context: n_ubatch      = 128
0.00.597.170 I llama_context: flash_attn    = 0
0.00.597.172 I llama_context: freq_base     = 10000.0
0.00.597.172 I llama_context: freq_scale    = 1
0.00.597.173 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.180 I ggml_metal_init: allocating
0.00.597.258 I ggml_metal_init: found device: Apple M4
0.00.597.271 I ggml_metal_init: picking default device: Apple M4
0.00.599.074 I ggml_metal_init: using embedded metal library
0.00.605.904 I ggml_metal_init: GPU name:   Apple M4
0.00.605.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.915 I ggml_metal_init: simdgroup reduction   = true
0.00.605.915 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.916 I ggml_metal_init: has residency sets    = true
0.00.605.916 I ggml_metal_init: has bfloat            = true
0.00.605.916 I ggml_metal_init: use bfloat            = true
0.00.605.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.919 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.160 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.626.645 I init:      Metal KV buffer size =    24.00 MiB
0.00.626.649 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.626.681 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.629.885 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.629.887 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.629.887 I llama_context: graph nodes  = 967
0.00.629.888 I llama_context: graph splits = 2
0.00.629.906 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.629.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.319 I 
0.00.660.396 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.401 I perplexity: tokenizing the input ..
0.00.665.860 I perplexity: tokenization took 5.457 ms
0.00.665.866 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.722 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.803.255 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.803.278 I llama_perf_context_print:        load time =     650.14 ms
0.00.803.279 I llama_perf_context_print: prompt eval time =     135.63 ms /   128 tokens (    1.06 ms per token,   943.77 tokens per second)
0.00.803.280 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.280 I llama_perf_context_print:       total time =     142.96 ms /   129 tokens
0.00.803.811 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.078s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.043 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.836 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.845 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.845 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.846 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.847 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.847 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.848 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.848 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.848 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.852 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.842 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.709 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.710 I llama_model_loader: - type  f32:  194 tensors
0.00.024.711 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.711 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.711 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.712 I print_info: file format = GGUF V3 (latest)
0.00.024.713 I print_info: file type   = Q2_K - Medium
0.00.024.714 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.657 I load: special tokens cache size = 25
0.00.038.627 I load: token to piece cache size = 0.2984 MB
0.00.038.630 I print_info: arch             = gptneox
0.00.038.630 I print_info: vocab_only       = 0
0.00.038.630 I print_info: n_ctx_train      = 2048
0.00.038.630 I print_info: n_embd           = 2048
0.00.038.630 I print_info: n_layer          = 24
0.00.038.634 I print_info: n_head           = 16
0.00.038.635 I print_info: n_head_kv        = 16
0.00.038.635 I print_info: n_rot            = 32
0.00.038.635 I print_info: n_swa            = 0
0.00.038.636 I print_info: n_embd_head_k    = 128
0.00.038.636 I print_info: n_embd_head_v    = 128
0.00.038.639 I print_info: n_gqa            = 1
0.00.038.640 I print_info: n_embd_k_gqa     = 2048
0.00.038.640 I print_info: n_embd_v_gqa     = 2048
0.00.038.641 I print_info: f_norm_eps       = 1.0e-05
0.00.038.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.642 I print_info: f_logit_scale    = 0.0e+00
0.00.038.642 I print_info: n_ff             = 8192
0.00.038.642 I print_info: n_expert         = 0
0.00.038.643 I print_info: n_expert_used    = 0
0.00.038.643 I print_info: causal attn      = 1
0.00.038.643 I print_info: pooling type     = 0
0.00.038.643 I print_info: rope type        = 2
0.00.038.643 I print_info: rope scaling     = linear
0.00.038.644 I print_info: freq_base_train  = 10000.0
0.00.038.644 I print_info: freq_scale_train = 1
0.00.038.644 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.644 I print_info: rope_finetuned   = unknown
0.00.038.645 I print_info: ssm_d_conv       = 0
0.00.038.645 I print_info: ssm_d_inner      = 0
0.00.038.646 I print_info: ssm_d_state      = 0
0.00.038.647 I print_info: ssm_dt_rank      = 0
0.00.038.647 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.647 I print_info: model type       = 1.4B
0.00.038.647 I print_info: model params     = 1.41 B
0.00.038.647 I print_info: general.name     = 1.4B
0.00.038.648 I print_info: vocab type       = BPE
0.00.038.648 I print_info: n_vocab          = 50304
0.00.038.648 I print_info: n_merges         = 50009
0.00.038.649 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.649 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.649 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.649 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.649 I print_info: LF token         = 187 'Ċ'
0.00.038.650 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.651 I print_info: max token length = 1024
0.00.342.735 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.749 I load_tensors: offloading output layer to GPU
0.00.342.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.780 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.782 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.344.332 I llama_context: n_seq_max     = 1
0.00.344.338 I llama_context: n_ctx         = 128
0.00.344.338 I llama_context: n_ctx_per_seq = 128
0.00.344.339 I llama_context: n_batch       = 128
0.00.344.339 I llama_context: n_ubatch      = 128
0.00.344.339 I llama_context: flash_attn    = 0
0.00.344.342 I llama_context: freq_base     = 10000.0
0.00.344.342 I llama_context: freq_scale    = 1
0.00.344.343 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.344.357 I ggml_metal_init: allocating
0.00.344.437 I ggml_metal_init: found device: Apple M4
0.00.344.451 I ggml_metal_init: picking default device: Apple M4
0.00.346.295 I ggml_metal_init: using embedded metal library
0.00.351.723 I ggml_metal_init: GPU name:   Apple M4
0.00.351.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.742 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.743 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.744 I ggml_metal_init: simdgroup reduction   = true
0.00.351.744 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.744 I ggml_metal_init: has residency sets    = true
0.00.351.744 I ggml_metal_init: has bfloat            = true
0.00.351.745 I ggml_metal_init: use bfloat            = true
0.00.351.746 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.373.074 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.376.817 I init:      Metal KV buffer size =    24.00 MiB
0.00.376.821 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.376.848 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.380.255 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.380.257 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.380.257 I llama_context: graph nodes  = 967
0.00.380.258 I llama_context: graph splits = 2
0.00.380.276 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.380.280 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.409.566 I 
0.00.409.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.409.646 I perplexity: tokenizing the input ..
0.00.416.256 I perplexity: tokenization took 6.605 ms
0.00.416.263 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.558.610 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.560.228 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.560.255 I llama_perf_context_print:        load time =     400.51 ms
0.00.560.256 I llama_perf_context_print: prompt eval time =     141.41 ms /   128 tokens (    1.10 ms per token,   905.19 tokens per second)
0.00.560.256 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.560.256 I llama_perf_context_print:       total time =     150.69 ms /   129 tokens
0.00.560.778 I ggml_metal_free: deallocating

real	0m0.574s
user	0m0.082s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.047 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.982 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.990 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.991 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.991 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.992 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.993 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.996 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.997 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.998 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.998 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.702 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.718 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.394 I llama_model_loader: - type  f32:  194 tensors
0.00.024.395 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.395 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.395 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.396 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.397 I print_info: file format = GGUF V3 (latest)
0.00.024.397 I print_info: file type   = Q3_K - Medium
0.00.024.398 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.249 I load: special tokens cache size = 25
0.00.038.243 I load: token to piece cache size = 0.2984 MB
0.00.038.245 I print_info: arch             = gptneox
0.00.038.246 I print_info: vocab_only       = 0
0.00.038.246 I print_info: n_ctx_train      = 2048
0.00.038.246 I print_info: n_embd           = 2048
0.00.038.246 I print_info: n_layer          = 24
0.00.038.250 I print_info: n_head           = 16
0.00.038.250 I print_info: n_head_kv        = 16
0.00.038.251 I print_info: n_rot            = 32
0.00.038.251 I print_info: n_swa            = 0
0.00.038.251 I print_info: n_embd_head_k    = 128
0.00.038.251 I print_info: n_embd_head_v    = 128
0.00.038.252 I print_info: n_gqa            = 1
0.00.038.253 I print_info: n_embd_k_gqa     = 2048
0.00.038.253 I print_info: n_embd_v_gqa     = 2048
0.00.038.254 I print_info: f_norm_eps       = 1.0e-05
0.00.038.254 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.255 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.256 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.256 I print_info: f_logit_scale    = 0.0e+00
0.00.038.257 I print_info: n_ff             = 8192
0.00.038.257 I print_info: n_expert         = 0
0.00.038.257 I print_info: n_expert_used    = 0
0.00.038.257 I print_info: causal attn      = 1
0.00.038.257 I print_info: pooling type     = 0
0.00.038.258 I print_info: rope type        = 2
0.00.038.258 I print_info: rope scaling     = linear
0.00.038.258 I print_info: freq_base_train  = 10000.0
0.00.038.259 I print_info: freq_scale_train = 1
0.00.038.259 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.259 I print_info: rope_finetuned   = unknown
0.00.038.259 I print_info: ssm_d_conv       = 0
0.00.038.259 I print_info: ssm_d_inner      = 0
0.00.038.259 I print_info: ssm_d_state      = 0
0.00.038.260 I print_info: ssm_dt_rank      = 0
0.00.038.262 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.262 I print_info: model type       = 1.4B
0.00.038.262 I print_info: model params     = 1.41 B
0.00.038.263 I print_info: general.name     = 1.4B
0.00.038.263 I print_info: vocab type       = BPE
0.00.038.263 I print_info: n_vocab          = 50304
0.00.038.263 I print_info: n_merges         = 50009
0.00.038.264 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.264 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.264 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.264 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.268 I print_info: LF token         = 187 'Ċ'
0.00.038.268 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.268 I print_info: max token length = 1024
0.00.457.568 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.576 I load_tensors: offloading output layer to GPU
0.00.457.577 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.610 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.457.611 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.459.000 I llama_context: n_seq_max     = 1
0.00.459.004 I llama_context: n_ctx         = 128
0.00.459.004 I llama_context: n_ctx_per_seq = 128
0.00.459.005 I llama_context: n_batch       = 128
0.00.459.005 I llama_context: n_ubatch      = 128
0.00.459.005 I llama_context: flash_attn    = 0
0.00.459.007 I llama_context: freq_base     = 10000.0
0.00.459.008 I llama_context: freq_scale    = 1
0.00.459.008 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.459.014 I ggml_metal_init: allocating
0.00.459.123 I ggml_metal_init: found device: Apple M4
0.00.459.136 I ggml_metal_init: picking default device: Apple M4
0.00.461.020 I ggml_metal_init: using embedded metal library
0.00.467.197 I ggml_metal_init: GPU name:   Apple M4
0.00.467.203 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.467.204 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.467.205 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.467.206 I ggml_metal_init: simdgroup reduction   = true
0.00.467.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.467.206 I ggml_metal_init: has residency sets    = true
0.00.467.207 I ggml_metal_init: has bfloat            = true
0.00.467.207 I ggml_metal_init: use bfloat            = true
0.00.467.208 I ggml_metal_init: hasUnifiedMemory      = true
0.00.467.210 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.485.718 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.489.237 I init:      Metal KV buffer size =    24.00 MiB
0.00.489.246 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.489.277 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.492.658 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.492.660 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.492.660 I llama_context: graph nodes  = 967
0.00.492.661 I llama_context: graph splits = 2
0.00.492.680 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.492.682 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.520.258 I 
0.00.520.337 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.349 I perplexity: tokenizing the input ..
0.00.527.706 I perplexity: tokenization took 7.357 ms
0.00.527.726 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.660.750 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.662.277 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.662.301 I llama_perf_context_print:        load time =     511.20 ms
0.00.662.304 I llama_perf_context_print: prompt eval time =     132.10 ms /   128 tokens (    1.03 ms per token,   968.98 tokens per second)
0.00.662.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.662.309 I llama_perf_context_print:       total time =     142.05 ms /   129 tokens
0.00.662.838 I ggml_metal_free: deallocating

real	0m0.677s
user	0m0.080s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.768 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.899 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.906 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.908 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.908 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.909 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.909 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.909 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.910 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.911 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.911 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.918 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.918 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.765 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.609 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.611 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.612 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.612 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.612 I llama_model_loader: - type  f32:  194 tensors
0.00.025.613 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.613 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.613 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.614 I print_info: file format = GGUF V3 (latest)
0.00.025.614 I print_info: file type   = Q4_K - Medium
0.00.025.616 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.794 I load: special tokens cache size = 25
0.00.039.861 I load: token to piece cache size = 0.2984 MB
0.00.039.864 I print_info: arch             = gptneox
0.00.039.865 I print_info: vocab_only       = 0
0.00.039.865 I print_info: n_ctx_train      = 2048
0.00.039.865 I print_info: n_embd           = 2048
0.00.039.865 I print_info: n_layer          = 24
0.00.039.868 I print_info: n_head           = 16
0.00.039.869 I print_info: n_head_kv        = 16
0.00.039.869 I print_info: n_rot            = 32
0.00.039.870 I print_info: n_swa            = 0
0.00.039.870 I print_info: n_embd_head_k    = 128
0.00.039.870 I print_info: n_embd_head_v    = 128
0.00.039.871 I print_info: n_gqa            = 1
0.00.039.872 I print_info: n_embd_k_gqa     = 2048
0.00.039.872 I print_info: n_embd_v_gqa     = 2048
0.00.039.873 I print_info: f_norm_eps       = 1.0e-05
0.00.039.873 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.873 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.873 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.874 I print_info: f_logit_scale    = 0.0e+00
0.00.039.874 I print_info: n_ff             = 8192
0.00.039.875 I print_info: n_expert         = 0
0.00.039.875 I print_info: n_expert_used    = 0
0.00.039.875 I print_info: causal attn      = 1
0.00.039.875 I print_info: pooling type     = 0
0.00.039.875 I print_info: rope type        = 2
0.00.039.875 I print_info: rope scaling     = linear
0.00.039.878 I print_info: freq_base_train  = 10000.0
0.00.039.878 I print_info: freq_scale_train = 1
0.00.039.879 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.879 I print_info: rope_finetuned   = unknown
0.00.039.879 I print_info: ssm_d_conv       = 0
0.00.039.879 I print_info: ssm_d_inner      = 0
0.00.039.879 I print_info: ssm_d_state      = 0
0.00.039.880 I print_info: ssm_dt_rank      = 0
0.00.039.880 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.880 I print_info: model type       = 1.4B
0.00.039.880 I print_info: model params     = 1.41 B
0.00.039.880 I print_info: general.name     = 1.4B
0.00.039.881 I print_info: vocab type       = BPE
0.00.039.881 I print_info: n_vocab          = 50304
0.00.039.881 I print_info: n_merges         = 50009
0.00.039.881 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.881 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.882 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.882 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.883 I print_info: LF token         = 187 'Ċ'
0.00.039.887 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.887 I print_info: max token length = 1024
0.00.512.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.512.564 I load_tensors: offloading output layer to GPU
0.00.512.565 I load_tensors: offloaded 25/25 layers to GPU
0.00.512.600 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.512.602 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.514.089 I llama_context: n_seq_max     = 1
0.00.514.094 I llama_context: n_ctx         = 128
0.00.514.095 I llama_context: n_ctx_per_seq = 128
0.00.514.095 I llama_context: n_batch       = 128
0.00.514.095 I llama_context: n_ubatch      = 128
0.00.514.096 I llama_context: flash_attn    = 0
0.00.514.103 I llama_context: freq_base     = 10000.0
0.00.514.105 I llama_context: freq_scale    = 1
0.00.514.107 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.514.110 I ggml_metal_init: allocating
0.00.514.189 I ggml_metal_init: found device: Apple M4
0.00.514.202 I ggml_metal_init: picking default device: Apple M4
0.00.515.965 I ggml_metal_init: using embedded metal library
0.00.522.669 I ggml_metal_init: GPU name:   Apple M4
0.00.522.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.522.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.522.677 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.522.678 I ggml_metal_init: simdgroup reduction   = true
0.00.522.678 I ggml_metal_init: simdgroup matrix mul. = true
0.00.522.678 I ggml_metal_init: has residency sets    = true
0.00.522.678 I ggml_metal_init: has bfloat            = true
0.00.522.679 I ggml_metal_init: use bfloat            = true
0.00.522.680 I ggml_metal_init: hasUnifiedMemory      = true
0.00.522.681 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.540.889 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.544.460 I init:      Metal KV buffer size =    24.00 MiB
0.00.544.464 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.544.498 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.547.689 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.547.691 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.547.691 I llama_context: graph nodes  = 967
0.00.547.692 I llama_context: graph splits = 2
0.00.547.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.547.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.573.775 I 
0.00.573.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.573.868 I perplexity: tokenizing the input ..
0.00.580.768 I perplexity: tokenization took 6.896 ms
0.00.580.780 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.716.303 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.717.822 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.717.843 I llama_perf_context_print:        load time =     564.00 ms
0.00.717.844 I llama_perf_context_print: prompt eval time =     134.59 ms /   128 tokens (    1.05 ms per token,   951.02 tokens per second)
0.00.717.846 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.717.850 I llama_perf_context_print:       total time =     144.07 ms /   129 tokens
0.00.718.419 I ggml_metal_free: deallocating

real	0m0.735s
user	0m0.081s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.503 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.583 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.448 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.449 I llama_model_loader: - type  f32:  194 tensors
0.00.025.449 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.449 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.450 I print_info: file format = GGUF V3 (latest)
0.00.025.451 I print_info: file type   = Q5_K - Medium
0.00.025.452 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.689 I load: special tokens cache size = 25
0.00.039.820 I load: token to piece cache size = 0.2984 MB
0.00.039.823 I print_info: arch             = gptneox
0.00.039.823 I print_info: vocab_only       = 0
0.00.039.823 I print_info: n_ctx_train      = 2048
0.00.039.823 I print_info: n_embd           = 2048
0.00.039.823 I print_info: n_layer          = 24
0.00.039.827 I print_info: n_head           = 16
0.00.039.828 I print_info: n_head_kv        = 16
0.00.039.828 I print_info: n_rot            = 32
0.00.039.828 I print_info: n_swa            = 0
0.00.039.829 I print_info: n_embd_head_k    = 128
0.00.039.829 I print_info: n_embd_head_v    = 128
0.00.039.829 I print_info: n_gqa            = 1
0.00.039.830 I print_info: n_embd_k_gqa     = 2048
0.00.039.831 I print_info: n_embd_v_gqa     = 2048
0.00.039.831 I print_info: f_norm_eps       = 1.0e-05
0.00.039.832 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.832 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.832 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.833 I print_info: f_logit_scale    = 0.0e+00
0.00.039.834 I print_info: n_ff             = 8192
0.00.039.834 I print_info: n_expert         = 0
0.00.039.834 I print_info: n_expert_used    = 0
0.00.039.835 I print_info: causal attn      = 1
0.00.039.835 I print_info: pooling type     = 0
0.00.039.835 I print_info: rope type        = 2
0.00.039.835 I print_info: rope scaling     = linear
0.00.039.837 I print_info: freq_base_train  = 10000.0
0.00.039.837 I print_info: freq_scale_train = 1
0.00.039.838 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.838 I print_info: rope_finetuned   = unknown
0.00.039.838 I print_info: ssm_d_conv       = 0
0.00.039.838 I print_info: ssm_d_inner      = 0
0.00.039.838 I print_info: ssm_d_state      = 0
0.00.039.838 I print_info: ssm_dt_rank      = 0
0.00.039.838 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.839 I print_info: model type       = 1.4B
0.00.039.839 I print_info: model params     = 1.41 B
0.00.039.839 I print_info: general.name     = 1.4B
0.00.039.840 I print_info: vocab type       = BPE
0.00.039.840 I print_info: n_vocab          = 50304
0.00.039.840 I print_info: n_merges         = 50009
0.00.039.840 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.841 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.841 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.841 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.842 I print_info: LF token         = 187 'Ċ'
0.00.039.843 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.844 I print_info: max token length = 1024
0.00.597.807 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.820 I load_tensors: offloading output layer to GPU
0.00.597.821 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.852 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.853 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.599.351 I llama_context: n_seq_max     = 1
0.00.599.358 I llama_context: n_ctx         = 128
0.00.599.358 I llama_context: n_ctx_per_seq = 128
0.00.599.359 I llama_context: n_batch       = 128
0.00.599.360 I llama_context: n_ubatch      = 128
0.00.599.360 I llama_context: flash_attn    = 0
0.00.599.361 I llama_context: freq_base     = 10000.0
0.00.599.362 I llama_context: freq_scale    = 1
0.00.599.362 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.365 I ggml_metal_init: allocating
0.00.599.413 I ggml_metal_init: found device: Apple M4
0.00.599.424 I ggml_metal_init: picking default device: Apple M4
0.00.601.951 I ggml_metal_init: using embedded metal library
0.00.608.653 I ggml_metal_init: GPU name:   Apple M4
0.00.608.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.659 I ggml_metal_init: simdgroup reduction   = true
0.00.608.660 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.660 I ggml_metal_init: has residency sets    = true
0.00.608.660 I ggml_metal_init: has bfloat            = true
0.00.608.660 I ggml_metal_init: use bfloat            = true
0.00.608.661 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.662 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.795 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.473 I init:      Metal KV buffer size =    24.00 MiB
0.00.629.477 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.504 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.632.868 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.632.870 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.632.870 I llama_context: graph nodes  = 967
0.00.632.870 I llama_context: graph splits = 2
0.00.632.889 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.890 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.299 I 
0.00.670.375 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.382 I perplexity: tokenizing the input ..
0.00.677.120 I perplexity: tokenization took 6.737 ms
0.00.677.126 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.955 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.831.716 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.831.738 I llama_perf_context_print:        load time =     660.79 ms
0.00.831.740 I llama_perf_context_print: prompt eval time =     152.59 ms /   128 tokens (    1.19 ms per token,   838.86 tokens per second)
0.00.831.741 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.743 I llama_perf_context_print:       total time =     161.44 ms /   129 tokens
0.00.832.297 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.079s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.668 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.674 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.675 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.677 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.678 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.684 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.499 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.583 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.379 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.380 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.380 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.381 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.381 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.381 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.382 I llama_model_loader: - type  f32:  194 tensors
0.00.024.382 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.382 I print_info: file format = GGUF V3 (latest)
0.00.024.383 I print_info: file type   = Q6_K
0.00.024.383 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.501 I load: special tokens cache size = 25
0.00.038.615 I load: token to piece cache size = 0.2984 MB
0.00.038.619 I print_info: arch             = gptneox
0.00.038.620 I print_info: vocab_only       = 0
0.00.038.620 I print_info: n_ctx_train      = 2048
0.00.038.620 I print_info: n_embd           = 2048
0.00.038.620 I print_info: n_layer          = 24
0.00.038.625 I print_info: n_head           = 16
0.00.038.626 I print_info: n_head_kv        = 16
0.00.038.626 I print_info: n_rot            = 32
0.00.038.626 I print_info: n_swa            = 0
0.00.038.626 I print_info: n_embd_head_k    = 128
0.00.038.626 I print_info: n_embd_head_v    = 128
0.00.038.627 I print_info: n_gqa            = 1
0.00.038.629 I print_info: n_embd_k_gqa     = 2048
0.00.038.630 I print_info: n_embd_v_gqa     = 2048
0.00.038.631 I print_info: f_norm_eps       = 1.0e-05
0.00.038.631 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.631 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.631 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.632 I print_info: f_logit_scale    = 0.0e+00
0.00.038.632 I print_info: n_ff             = 8192
0.00.038.633 I print_info: n_expert         = 0
0.00.038.633 I print_info: n_expert_used    = 0
0.00.038.633 I print_info: causal attn      = 1
0.00.038.633 I print_info: pooling type     = 0
0.00.038.633 I print_info: rope type        = 2
0.00.038.633 I print_info: rope scaling     = linear
0.00.038.633 I print_info: freq_base_train  = 10000.0
0.00.038.634 I print_info: freq_scale_train = 1
0.00.038.634 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.634 I print_info: rope_finetuned   = unknown
0.00.038.634 I print_info: ssm_d_conv       = 0
0.00.038.634 I print_info: ssm_d_inner      = 0
0.00.038.636 I print_info: ssm_d_state      = 0
0.00.038.636 I print_info: ssm_dt_rank      = 0
0.00.038.636 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.637 I print_info: model type       = 1.4B
0.00.038.637 I print_info: model params     = 1.41 B
0.00.038.637 I print_info: general.name     = 1.4B
0.00.038.638 I print_info: vocab type       = BPE
0.00.038.638 I print_info: n_vocab          = 50304
0.00.038.638 I print_info: n_merges         = 50009
0.00.038.638 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.638 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.639 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.639 I print_info: LF token         = 187 'Ċ'
0.00.038.639 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.639 I print_info: max token length = 1024
0.00.459.912 I load_tensors: offloading 24 repeating layers to GPU
0.00.459.920 I load_tensors: offloading output layer to GPU
0.00.459.920 I load_tensors: offloaded 25/25 layers to GPU
0.00.459.951 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.459.952 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.460.853 I llama_context: n_seq_max     = 1
0.00.460.855 I llama_context: n_ctx         = 128
0.00.460.855 I llama_context: n_ctx_per_seq = 128
0.00.460.855 I llama_context: n_batch       = 128
0.00.460.855 I llama_context: n_ubatch      = 128
0.00.460.856 I llama_context: flash_attn    = 0
0.00.460.856 I llama_context: freq_base     = 10000.0
0.00.460.856 I llama_context: freq_scale    = 1
0.00.460.857 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.460.858 I ggml_metal_init: allocating
0.00.460.897 I ggml_metal_init: found device: Apple M4
0.00.460.913 I ggml_metal_init: picking default device: Apple M4
0.00.462.043 I ggml_metal_init: using embedded metal library
0.00.464.764 I ggml_metal_init: GPU name:   Apple M4
0.00.464.766 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.464.767 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.464.767 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.464.768 I ggml_metal_init: simdgroup reduction   = true
0.00.464.768 I ggml_metal_init: simdgroup matrix mul. = true
0.00.464.768 I ggml_metal_init: has residency sets    = true
0.00.464.768 I ggml_metal_init: has bfloat            = true
0.00.464.768 I ggml_metal_init: use bfloat            = true
0.00.464.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.464.769 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.474.735 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.441 I init:      Metal KV buffer size =    24.00 MiB
0.00.476.443 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.476.458 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.478.062 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.478.063 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.478.064 I llama_context: graph nodes  = 967
0.00.478.064 I llama_context: graph splits = 2
0.00.478.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.478.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.510.705 I 
0.00.510.741 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.510.743 I perplexity: tokenizing the input ..
0.00.514.701 I perplexity: tokenization took 3.956 ms
0.00.514.706 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.653.841 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.655.470 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.655.540 I llama_perf_context_print:        load time =     501.97 ms
0.00.655.541 I llama_perf_context_print: prompt eval time =     138.89 ms /   128 tokens (    1.09 ms per token,   921.62 tokens per second)
0.00.655.542 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.542 I llama_perf_context_print:       total time =     144.84 ms /   129 tokens
0.00.656.037 I ggml_metal_free: deallocating

real	0m0.670s
user	0m0.066s
sys	0m0.095s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.291 I build: 4678 (d26f5cff) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.907 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.841 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.846 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.848 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.849 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.849 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.849 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.856 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.857 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.857 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.035 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.061 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.320 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.320 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.321 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.321 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.322 I llama_model_loader: - type  f32:  194 tensors
0.00.043.323 I llama_model_loader: - type  f16:   98 tensors
0.00.043.324 I print_info: file format = GGUF V3 (latest)
0.00.043.325 I print_info: file type   = all F32 (guessed)
0.00.043.326 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.056.396 I load: special tokens cache size = 25
0.00.065.408 I load: token to piece cache size = 0.2984 MB
0.00.065.412 I print_info: arch             = gptneox
0.00.065.412 I print_info: vocab_only       = 0
0.00.065.413 I print_info: n_ctx_train      = 2048
0.00.065.413 I print_info: n_embd           = 2048
0.00.065.413 I print_info: n_layer          = 24
0.00.065.417 I print_info: n_head           = 16
0.00.065.418 I print_info: n_head_kv        = 16
0.00.065.418 I print_info: n_rot            = 32
0.00.065.419 I print_info: n_swa            = 0
0.00.065.419 I print_info: n_embd_head_k    = 128
0.00.065.419 I print_info: n_embd_head_v    = 128
0.00.065.420 I print_info: n_gqa            = 1
0.00.065.421 I print_info: n_embd_k_gqa     = 2048
0.00.065.422 I print_info: n_embd_v_gqa     = 2048
0.00.065.423 I print_info: f_norm_eps       = 1.0e-05
0.00.065.423 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.423 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.423 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.424 I print_info: f_logit_scale    = 0.0e+00
0.00.065.424 I print_info: n_ff             = 8192
0.00.065.424 I print_info: n_expert         = 0
0.00.065.426 I print_info: n_expert_used    = 0
0.00.065.428 I print_info: causal attn      = 1
0.00.065.428 I print_info: pooling type     = 0
0.00.065.429 I print_info: rope type        = 2
0.00.065.429 I print_info: rope scaling     = linear
0.00.065.429 I print_info: freq_base_train  = 10000.0
0.00.065.430 I print_info: freq_scale_train = 1
0.00.065.430 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.430 I print_info: rope_finetuned   = unknown
0.00.065.431 I print_info: ssm_d_conv       = 0
0.00.065.431 I print_info: ssm_d_inner      = 0
0.00.065.431 I print_info: ssm_d_state      = 0
0.00.065.431 I print_info: ssm_dt_rank      = 0
0.00.065.431 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.433 I print_info: model type       = 1.4B
0.00.065.433 I print_info: model params     = 1.41 B
0.00.065.434 I print_info: general.name     = 1.4B
0.00.065.434 I print_info: vocab type       = BPE
0.00.065.435 I print_info: n_vocab          = 50304
0.00.065.435 I print_info: n_merges         = 50009
0.00.065.435 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.436 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.436 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.436 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.441 I print_info: LF token         = 187 'Ċ'
0.00.065.442 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.442 I print_info: max token length = 1024
0.01.384.808 I load_tensors: offloading 24 repeating layers to GPU
0.01.384.812 I load_tensors: offloading output layer to GPU
0.01.384.812 I load_tensors: offloaded 25/25 layers to GPU
0.01.384.826 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.384.827 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.385.294 I llama_context: n_seq_max     = 1
0.01.385.295 I llama_context: n_ctx         = 128
0.01.385.296 I llama_context: n_ctx_per_seq = 128
0.01.385.296 I llama_context: n_batch       = 128
0.01.385.296 I llama_context: n_ubatch      = 128
0.01.385.297 I llama_context: flash_attn    = 0
0.01.385.300 I llama_context: freq_base     = 10000.0
0.01.385.300 I llama_context: freq_scale    = 1
0.01.385.300 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.385.301 I ggml_metal_init: allocating
0.01.385.337 I ggml_metal_init: found device: Apple M4
0.01.385.343 I ggml_metal_init: picking default device: Apple M4
0.01.385.947 I ggml_metal_init: using embedded metal library
0.01.392.621 I ggml_metal_init: GPU name:   Apple M4
0.01.392.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.392.624 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.392.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.392.624 I ggml_metal_init: simdgroup reduction   = true
0.01.392.624 I ggml_metal_init: simdgroup matrix mul. = true
0.01.392.625 I ggml_metal_init: has residency sets    = true
0.01.392.625 I ggml_metal_init: has bfloat            = true
0.01.392.625 I ggml_metal_init: use bfloat            = true
0.01.392.625 I ggml_metal_init: hasUnifiedMemory      = true
0.01.392.626 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.402.135 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.403.726 I init:      Metal KV buffer size =    24.00 MiB
0.01.403.729 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.403.744 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.405.330 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.405.332 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.405.332 I llama_context: graph nodes  = 967
0.01.405.332 I llama_context: graph splits = 2
0.01.405.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.405.347 I 
0.01.405.387 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.405.389 I compute_imatrix: tokenizing the input ..
0.01.409.460 I compute_imatrix: tokenization took 4.07 ms
0.01.409.462 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.691.472 I compute_imatrix: 0.28 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.694.244 I llama_perf_context_print:        load time =    1673.56 ms
0.01.694.245 I llama_perf_context_print: prompt eval time =     280.14 ms /   128 tokens (    2.19 ms per token,   456.92 tokens per second)
0.01.694.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.694.246 I llama_perf_context_print:       total time =    1676.33 ms /   129 tokens
0.01.694.986 I ggml_metal_free: deallocating

real	0m1.900s
user	0m0.133s
sys	0m0.214s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4678 (d26f5cff)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ae05260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ae085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ae08a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ae08ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ae09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ae097b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ae09c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ae0a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ae0a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ae0a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ae0ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ae0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ae0bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ae0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ae0cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ae0d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ae0dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ae0e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ae0ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ae0f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ae0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ae101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ae10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ae111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ae118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ae11b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ae11e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ae122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ae129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ae12e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ae13410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ae13920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ae13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ae14050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ae144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ae14930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ae14e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ae15390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ae15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ae15d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ae16290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ae16790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ae16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ae17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ae17690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ae17b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ae17f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ae183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ae18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ae18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ae19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ae198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ae19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ae1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ae1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ae1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ae1b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ae1b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ae1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ae1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ae1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ae1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ae1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ae1d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ae1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ae1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ae1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ae1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ae1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ae1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ae1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ae1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ae1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ae201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ae20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ae20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ae211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ae21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ae21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ae221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ae22720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ae22c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ae231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ae23710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ae23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ae241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ae24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ae24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ae251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ae256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ae25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ae26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ae266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ae26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ae27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ae276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ae27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ae186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ae28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ae28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ae28d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ae292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ae29830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ae29d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ae2a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ae2a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ae2ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ae2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ae2b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ae2bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ae2c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ae2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ae2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ae2d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ae2d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ae2db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ae2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ae2e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ae2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ae2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ae2f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ae2f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ae2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ae302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ae30560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ae30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ae30f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ae31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ae31960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ae31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ae32360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ae32860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ae32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ae33260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ae33760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ae33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ae34160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ae34660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ae34b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ae35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ae35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ae35a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ae35f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ae36460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ae36960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ae36e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ae37360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ae37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ae37d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ae38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ae38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ae38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ae39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ae39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ae39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ae3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ae3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ae3aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ae3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ae3b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ae3b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ae3be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ae3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ae3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ae3cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ae3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ae3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ae3dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ae3e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ae3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ae3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ae3f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ae3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ae3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ae3ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ae40460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ae40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ae40e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ae41360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ae41860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ae41d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ae42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ae42760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ae42c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ae43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ae43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ae43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ae44060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ae44560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ae44a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ae44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ae45460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ae45960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ae45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ae46410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ae469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ae46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ae47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ae47b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ae48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ae48750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ae48f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ae493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ae496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ae49cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ae4a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ae4aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ae4af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ae4b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ae4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ae4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ae4c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ae4cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ae4d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ae4d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ae4dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ae4e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ae4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ae4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ae4f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ae4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ae4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ae50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ae50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ae50aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ae50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ae51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ae51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ae51fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ae52530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ae52a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ae52fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ae53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ae53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ae53fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ae54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ae54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ae54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ae55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ae55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ae55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ae564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ae56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ae56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ae574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ae57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ae57f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ae584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ae58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ae58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ae594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ae59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ae59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ae5a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13af04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13af046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13af04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13af04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13af053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13af05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13af05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13af06140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13af065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13af06a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13af06e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13af07300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13af07770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13af07be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13af080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13af08550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13af089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13af08e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13af092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13af09710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13af09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13af09ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13af0a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13af0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13af0ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13af0b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13af0b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13af0ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13af0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13af0ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13af0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13af0d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13af0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13af0e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13af0e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13af0e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13af0ee10 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
0.00.737.208 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.737.211 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139f091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139f09620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139f09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139f09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139f0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139f0a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139f0ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139f0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139f0b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139f0b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139f0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139f0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139f0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139f0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139f0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139f0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139f0edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139f0f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139f0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139f10400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139f10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139f11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139f11960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139f12080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139f127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139f12a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139f12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139f13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139f13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139f13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139f13f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139f14480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139f148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139f14bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139f15020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139f15490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139f159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139f15ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139f163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139f168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139f16df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139f172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139f177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139f17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139f181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139f18660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139f18ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139f18f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139f193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139f19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139f19c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139f1a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139f1a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139f1a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139f1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139f1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139f1bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139f1bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139f1c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139f1cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139f1d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139f1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139f1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139f1de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139f1e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139f1e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139f1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139f1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139f1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139f1f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139f1fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139f20300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139f207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139f20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139f21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139f21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139f21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139f22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139f22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139f22cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139f23220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139f23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139f23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139f24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139f24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139f24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139f25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139f25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139f25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139f261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139f26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139f26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139f271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139f27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139f27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139f281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139f28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139f28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139f291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139f29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139f29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139f2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139f2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139f2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139f2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139f2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139f2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139f2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139f2d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139f2d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139f2dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139f2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139f2e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139f2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139f2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139f2f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139f2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139f2fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139f30120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139f305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139f30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139f30f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139f313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139f31840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139f31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139f32180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139f32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139f32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139f32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139f33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139f338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139f33d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139f341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139f34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139f34fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139f35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139f35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139f35da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139f36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139f366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139f36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139f37020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139f374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139f37960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139f37e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139f382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139f38740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139f38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139f39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139f39520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139f399c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139f39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139f3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139f3a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139f3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139f3b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139f3b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139f3ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139f3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139f3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139f3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139f3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139f3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139f3d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139f3da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139f3df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139f3e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139f3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139f3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139f3f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139f3f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139f3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139f3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139f40420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139f408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139f40d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139f41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139f416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139f41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139f41fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139f42480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139f42920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139f42dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139f43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139f43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139f43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139f44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139f444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139f44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139f44e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139f45370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139f458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139f45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139f46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139f46620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139f46c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139f47240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139f47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139f48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139f484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139f487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139f48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x139f493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139f49bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139f4a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139f4a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139f4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139f4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139f4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139f4bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139f4c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139f4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139f4cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139f4d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139f4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139f4dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139f4e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139f4e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139f4ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139f4f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139f4f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139f4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139f500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139f50640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139f50b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139f510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139f51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139f51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139f520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139f52620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139f52b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139f53610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139f53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139f540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139f54600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139f54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139f550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139f555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139f55b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139f56090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139f565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139f56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139f57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139f575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139f57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139f58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139f585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139f58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139f59060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139f595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139f59b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139f5a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139f5a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139f5aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139f5b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139f5bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139f5c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139f5c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139f5cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139f5d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139f5d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139f5dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x139f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x139f5e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139f5e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139f5ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139f5f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139f5f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139f5fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139f5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139f60460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139f60900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139f60da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139f61240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139f616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139f61b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139f62020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139f62570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139f62c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139f633b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139f63ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139f641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139f644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x139f64ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139f64f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139f65570 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139f65220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139f46ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139f468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139f47500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139f1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139f49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139f0c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139f08d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139f1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139f64770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139f1b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139f49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139f0c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139f65ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139f66310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139f665d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139f66890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139f66b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139f66e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139f670d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139f67390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139f67650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139f67910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139f67bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139f67e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139f68150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139f68410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139f686d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139f68990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139f68c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139f68f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139f691d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139f69490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139f69750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139f69a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139f69cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139f69f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139f6a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139f6a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139f6a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139f6aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139f6ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139f6b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139f6b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139f6b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139f6b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139f6bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139f6bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139f6c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139f6c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139f6c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139f6c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139f6cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139f6ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139f6d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139f6d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139f6d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139f6d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139f6dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139f6ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139f6e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139f6e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139f6e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139f6e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139f6ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139f6ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139f6f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139f6f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139f6f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139f6fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139f6fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139f6ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139f70290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139f70550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139f70810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139f70ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139f70d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139f71050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139f71310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139f715d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139f71890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139f71b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139f71e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139f720d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139f72390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139f72650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139f72910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139f72bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139f72e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139f73150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139f73410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139f736d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139f73990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139f73c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139f73f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139f741d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139f74490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139f74750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139f74a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139f74cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139f74f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139f75250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139f75510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139f757d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139f75a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139f75d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139f76010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139f762d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139f76590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139f76850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139f76b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139f76dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139f77090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139f77350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139f77610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139f778d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139f77b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139f77e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139f78110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139f783d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139f78690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139f78950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139f78c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139f78ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139f79190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139f79450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139f79710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139f799d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139f79c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139f79f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139f7a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139f7a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139f7a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139f7aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139f7ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139f7afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139f7b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139f7b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139f7b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139f7bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139f7bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139f7c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139f7c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139f7c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139f7c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139f7cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139f7ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139f7d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139f7d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139f7d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139f7d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139f7dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139f7de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139f7e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139f7e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139f7e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139f7e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139f7ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139f7ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139f7f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139f7f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139f7f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139f7fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139f7fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139f7ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139f80250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139f80510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139f807d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139f80a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139f80d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139f81010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139f812d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139f81590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139f81850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139f81b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139f81dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139f82090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139f82350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139f82610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139f828d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139f82b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139f82e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139f83110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139f833d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139f83690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139f83950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139f83c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139f83ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139f84190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139f84450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139f84710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139f849d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139f84c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139f84f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139f85210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139f854d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139f85790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139f85a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139f85d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139f85fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139f86290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x139f86550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x139f86810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13af0c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13af0f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13af0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13af0f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13af0fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13af0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13af10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13af10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13af10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13af10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13af110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13af11390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13af11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13af11910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13af11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13af11e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13af12150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13af12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13af126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13af12990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13af12c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13af12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13af131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13af13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13af13750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13af13a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13af13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13af13f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13af14250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13af14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13af147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13af14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13af14d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13af15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13af15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13af15ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13af16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13af16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13af16aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13af16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13af17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13af17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13af17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13af18530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13af18a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13af18fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13af19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13af19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13af19fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13af1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13af1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13af1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13af1b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13af1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13af1bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13af1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13af1ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13af1cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13af1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13af1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13af1df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13af1e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13af1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13af1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13af1f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13af1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13af1fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13af201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13af20690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13af20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13af20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13af21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13af21910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13af21db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13af22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13af226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13af22b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13af23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13af234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13af23a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13af24140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13af24860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13af24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13af256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13af25960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13af26150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13af26410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13af26a20 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.829s
user	0m0.279s
sys	0m0.328s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4678 (d26f5cff)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13370ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13370f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13370f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13370fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1337102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1337108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133710e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133711400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1337119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133711eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1337123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1337128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1337133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133713b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133714390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133714ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1337151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1337158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133716010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1337167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133716f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133717620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133717d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1337185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133718d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133718fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1337195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13371a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13371a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13371aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13371aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13371b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13371ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13371bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13371c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13371c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13371cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13371d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13371d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13371d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13371ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13371e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13371e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13371ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13371ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13371f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13371fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1337203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1337209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133720ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133721600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133721c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133722220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133722830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133723020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1337234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133723960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133723c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133724230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133724a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133724ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133725180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133725620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133725ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133725f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133726400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1337268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133726d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1337271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133727680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133727b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133727fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133728460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1337289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133728f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133729450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1337299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133729ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13372a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13372a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13372aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13372b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13372b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13372bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13372c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13372c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13372cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13372d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13372d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13372deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13372e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13372e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13372eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13372f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13372f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13372fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1337303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1337200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133730850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133731000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133731550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133731aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133731ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133732540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133732a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133732fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133733530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133733a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133733fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133734520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133734a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133734fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133735510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1337359b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133735e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1337362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133736790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133736c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1337370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133737570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133737a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133737eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133738350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1337387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133738c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133739130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1337395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133739a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133739f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13373a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13373a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13373acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13373b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13373b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13373bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13373bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13373c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13373c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13373cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13373d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13373d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13373db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13373dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13373e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13373e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13373edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13373f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13373f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13373fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133740030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1337404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133740970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133740e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1337412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133741750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133741bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133742090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133742530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1337429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133742e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133743310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1337437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133743c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1337440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133744590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133744a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133744ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133745370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133745810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133745cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133746150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1337465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133746a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133746f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1337473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133747870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133747d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1337481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133748650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133748af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133748f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133749430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1337498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133749d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13374a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13374a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13374ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13374aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13374b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13374b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13374bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13374c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13374c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13374cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13374d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13374d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13374dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13374df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13374e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13374eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13374f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13374f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13374fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133750090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1337506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133750cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1337514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133751940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133751de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133752280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133752a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133752f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1337534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133753a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133753f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1337544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133754a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133754f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1337554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133755a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133755f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1337564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1337569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133756f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133757490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1337579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133757f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133758480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1337589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133758f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133759470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1337599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133759f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13375a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13375a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13375af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13375b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13375b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13375bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13375c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13375c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13375cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13375d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13375d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13375ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13375e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13375e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13375eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13375f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13375f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13375feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133760400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133760950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133760ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1337613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133761940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133761e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1337623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133762930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133762e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1337633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133763920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133763e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1337643c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133764910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133764e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1337653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133765850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133765cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133766190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133766630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133766ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133766f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133767410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1337678b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133767d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1337681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133768690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133768b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133768fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133769470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133769910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133769e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13376a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13376aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13376b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13376bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13376bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13376c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13376c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13376ce60 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
0.00.101.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.993 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13376cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13374e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13374e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13374edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133721ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1337218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133723ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133750960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133719280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13371fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133720690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133720ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13371f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1337212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133718280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1337244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133730b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13376c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13371b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13371b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133750f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13374f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133719890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133719b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133719e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13376d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13376d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13376d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13376db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13376ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13376e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13376e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13376e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13376e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13376eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13376ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13376f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13376f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13376f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13376f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13376fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13376fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133770180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133770440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133770700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1337709c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133770c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133770f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133771200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1337714c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133771780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133771a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133771d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133771fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133772280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133772540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133772800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133772ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133772d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133773040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133773300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1337735c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133773880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133773b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133773e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1337740c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133774380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133774640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133774900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133774bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133774e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133775140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133775400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1337756c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133775980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133775c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133775f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1337761c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133776480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133776740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133776a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133776cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133776f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133777240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133777500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1337777c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133777a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133777d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133778000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1337782c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133778580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133778840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133778b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133778dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133779080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133779340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133779600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1337798c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133779b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133779e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13377a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13377a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13377a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13377a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13377ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13377aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13377b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13377b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13377b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13377b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13377bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13377bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13377c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13377c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13377c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13377ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13377cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13377cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13377d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13377d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13377d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13377dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13377dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13377e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13377e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13377e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13377e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13377eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13377ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13377f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13377f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13377f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13377f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13377fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13377fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133780140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133780400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1337806c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133780980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133780c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133780f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1337811c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133781480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133781740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133781a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133781cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133781f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133782240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133782500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1337827c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133782a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133782d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133783000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1337832c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133783580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133783840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133783b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133783dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133784080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133784340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133784600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1337848c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133784b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133784e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133785100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1337853c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133785680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133785940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133785c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133785ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133786180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133786440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133786700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1337869c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133786c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133786f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133787200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1337874c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133787780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133787a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133787d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133787fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133788280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133788540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133788800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133788ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133788d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133789040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133789300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1337895c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133789880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133789b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133789e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13378a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13378a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13378a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13378a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13378abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13378ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13378b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13378b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13378b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13378b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13378bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13378bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13378c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13378c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13378cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13378d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13378d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13378dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13378df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13378e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13378e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13378eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13378ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13378f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13378f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13378fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133790120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133790590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133790a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133790e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1337912e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133791750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133791bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133792030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1337924a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133792910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133792d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1337931f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133793660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133793ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133793f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1337943b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133794820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133794c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133795100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133795570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1337959e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133795e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1337962c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133796730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133796ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133797010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133797480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1337978f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133797d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1337981d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133798640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133798ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133798f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133799390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133799800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133799c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13379a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13379a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13379a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13379ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13379b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13379b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13379bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13379bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13379c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13379c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13379cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13379d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13379d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13379da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13379df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13379e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13379e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13379ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13379f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13379f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13379f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13379fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1337a0280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1337a06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1337a0b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1337a0fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1337a1440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1337a18b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1337a2320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1337a2a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1337a3160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1337a3880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1337a3b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1337a4330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1337a45f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1337a4c00 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1359044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135904950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135904dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135905230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1359056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135905b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135905f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1359063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135906860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135906cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135907140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135907810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135908330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135908ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1359092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135909a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13590a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13590a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13590af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13590b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13590be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13590c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13590cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13590d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13590dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13590dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13590e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13590e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13590e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13590edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13590f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13590f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13590fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13590fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1359102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135910760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135910bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135911040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1359114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135911920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135911d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135912200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135912670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135912ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135912f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1359133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135913830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135913ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135914110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135914580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1359149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135914e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1359152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135915740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135915bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135916020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135916590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135916a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135916f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135917370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1359177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135917c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1359180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135918530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1359189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135918e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135919280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1359196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135919b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x135919fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13591a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13591a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13591ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133750350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1337a48b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1337a1b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1337a3e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1337a5060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1337a5320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1337a55e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1337a58a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1337a5b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1337a5e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1337a60e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1337a63a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1337a6660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1337a6920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1337a6be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1337a6ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1337a7160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1337a7420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1337a76e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1337a79a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1337a7c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1337a7f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1337a81e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1337a84a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1337a8760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1337a8a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1337a8ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1337a8fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1337a9260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1337a9520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1337a97e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1337a9aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1337a9d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1337aa020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1337aa2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1337aa5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1337aa860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1337aab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1337aade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1337ab0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1337ab360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1337ab620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1337ab8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1337abba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1337abe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1337ac120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1337ac3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1337ac6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1337ac960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1337acc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1337acee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1337ad1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1337ad460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1337ad720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1337ad9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1337adca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1337adf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1337ae220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1337ae4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1337ae7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1337aea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1337aed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1337aefe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1337af2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1337af560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1337af820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1337afae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1337afda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1337b0060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1337b0320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1337b05e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1337b08a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1337b0b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1337b0e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1337b10e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1337b13a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1337b1660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1337b1920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1337b1be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1337b1ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1337b2160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1337b2420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1337b26e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1337b29a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1337b2c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1337b2f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1337b31e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1337b34a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1337b3760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1337b3a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1337b3ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1337b3fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1337b4260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1337b4520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1337b47e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1337b4aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1337b4d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1337b5020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1337b52e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1337b55a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1337b5860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1337b5b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1337b5de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1337b60a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1337b6360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1337b6620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1337b68e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1337b6ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1337b6e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1337b7120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1337b73e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1337b76a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1337b7960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1337b7c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1337b7ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1337b81a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1337b8460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1337b8720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1337b89e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1337b8ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1337b8f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1337b9220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1337b94e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1337b97a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1337b9a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1337b9d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1337b9fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1337ba2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1337ba560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1337ba820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1337baae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1337bada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1337bb060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1337bb320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1337bb5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1337bb8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1337bbb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1337bc130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1337bc3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1337bc6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1337bc970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1337bcc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1337bcef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1337bd1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1337bd470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1337bd730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1337bd9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1337bdcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1337bdf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1337be230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1337be4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1337be7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1337bea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1337bed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1337beff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1337bf2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1337bf570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1337bf830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1337bfaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1337bfdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1337c0070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1337c0330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1337c05f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1337c08b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1337c0b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1337c0e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1337c10f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1337c13b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1337c1670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1337c1930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1337c1bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1337c1eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1337c2170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1337c2430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1337c26f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1337c29b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1337c2c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1337c2f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1337c31f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1337c34b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1337c3770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1337c3a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1337c3cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1337c3fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1337c4270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1337c4530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1337c47f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1337c4ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1337c4d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1337c5030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1337c52f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1337c55b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1337c5870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1337c5b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1337c5df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1337c60b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1337c6370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1337c6630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1337c68f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1337c6bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1337c6e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1337c7130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1337c73f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1337c76b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1337c7970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1337c7c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1337c7ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1337c81b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1337c8470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1337c8730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1337c89f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1337c8cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1337c8f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1337c9230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1337c94f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1337c97b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1337c9a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1337c9d30 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.948s
user	0m0.230s
sys	0m0.186s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
