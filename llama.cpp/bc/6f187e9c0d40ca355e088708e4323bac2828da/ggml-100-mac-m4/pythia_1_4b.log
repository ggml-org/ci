Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.5s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.724s
user	0m0.837s
sys	0m1.283s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha1
[  6%] Built target sha256
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Linking CXX executable ../../bin/llama-gguf-hash
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Built target llava
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target test-c
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-simple
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-chat
[ 50%] Linking CXX executable ../bin/test-grammar-integration
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-sampling
[ 50%] Built target test-log
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-backend-ops
[ 64%] Built target test-autorelease
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Built target test-quantize-perf
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Built target test-rope
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-infill
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 75%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookahead
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-create
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-parallel
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-cli
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-perplexity
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-passkey
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-tts
[ 89%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-tts
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-gen-docs
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Built target llama-run
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.225s
user	0m6.592s
sys	0m10.187s

main: quantize time =  4775.60 ms
main:    total time =  4775.60 ms

main: quantize time =  4388.91 ms
main:    total time =  4388.91 ms

main: quantize time =  4020.40 ms
main:    total time =  4020.40 ms

main: quantize time =  2777.34 ms
main:    total time =  2777.34 ms

main: quantize time =  2855.81 ms
main:    total time =  2855.81 ms

main: quantize time =  5439.29 ms
main:    total time =  5439.29 ms

main: quantize time =  6126.38 ms
main:    total time =  6126.38 ms

main: quantize time =  6935.15 ms
main:    total time =  6935.15 ms

main: quantize time =  5933.86 ms
main:    total time =  5933.86 ms

main: quantize time =  4907.78 ms
main:    total time =  4907.78 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.142 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.327 I main: llama backend init
0.00.000.333 I main: load the model and apply lora adapter, if any
0.00.089.142 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.101.497 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.101.514 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.101.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.101.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.101.521 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.101.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.101.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.101.524 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.101.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.101.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.101.526 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.101.527 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.101.527 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.101.528 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.101.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.101.533 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.101.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.108.339 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.110.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.117.592 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.117.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.117.601 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.117.602 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.117.602 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.117.604 I llama_model_loader: - type  f32:  194 tensors
0.00.117.604 I llama_model_loader: - type  f16:   98 tensors
0.00.117.614 I print_info: file format = GGUF V3 (latest)
0.00.117.616 I print_info: file type   = all F32 (guessed)
0.00.117.618 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.135.485 I load: special tokens cache size = 25
0.00.145.644 I load: token to piece cache size = 0.2984 MB
0.00.145.672 I print_info: arch             = gptneox
0.00.145.672 I print_info: vocab_only       = 0
0.00.145.673 I print_info: n_ctx_train      = 2048
0.00.145.673 I print_info: n_embd           = 2048
0.00.145.673 I print_info: n_layer          = 24
0.00.145.678 I print_info: n_head           = 16
0.00.145.679 I print_info: n_head_kv        = 16
0.00.145.680 I print_info: n_rot            = 32
0.00.145.680 I print_info: n_swa            = 0
0.00.145.680 I print_info: n_embd_head_k    = 128
0.00.145.680 I print_info: n_embd_head_v    = 128
0.00.145.681 I print_info: n_gqa            = 1
0.00.145.682 I print_info: n_embd_k_gqa     = 2048
0.00.145.683 I print_info: n_embd_v_gqa     = 2048
0.00.145.684 I print_info: f_norm_eps       = 1.0e-05
0.00.145.684 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.145.684 I print_info: f_clamp_kqv      = 0.0e+00
0.00.145.685 I print_info: f_max_alibi_bias = 0.0e+00
0.00.145.685 I print_info: f_logit_scale    = 0.0e+00
0.00.145.686 I print_info: n_ff             = 8192
0.00.145.686 I print_info: n_expert         = 0
0.00.145.686 I print_info: n_expert_used    = 0
0.00.145.686 I print_info: causal attn      = 1
0.00.145.687 I print_info: pooling type     = 0
0.00.145.687 I print_info: rope type        = 2
0.00.145.687 I print_info: rope scaling     = linear
0.00.145.688 I print_info: freq_base_train  = 10000.0
0.00.145.688 I print_info: freq_scale_train = 1
0.00.145.688 I print_info: n_ctx_orig_yarn  = 2048
0.00.145.689 I print_info: rope_finetuned   = unknown
0.00.145.689 I print_info: ssm_d_conv       = 0
0.00.145.689 I print_info: ssm_d_inner      = 0
0.00.145.689 I print_info: ssm_d_state      = 0
0.00.145.689 I print_info: ssm_dt_rank      = 0
0.00.145.689 I print_info: ssm_dt_b_c_rms   = 0
0.00.145.690 I print_info: model type       = 1.4B
0.00.145.690 I print_info: model params     = 1.41 B
0.00.145.691 I print_info: general.name     = 1.4B
0.00.145.691 I print_info: vocab type       = BPE
0.00.145.691 I print_info: n_vocab          = 50304
0.00.145.692 I print_info: n_merges         = 50009
0.00.145.692 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.145.692 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.145.692 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.145.693 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.145.693 I print_info: LF token         = 187 'Ċ'
0.00.145.693 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.145.693 I print_info: max token length = 1024
0.00.145.694 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.190.547 I load_tensors: offloading 24 repeating layers to GPU
0.00.190.550 I load_tensors: offloading output layer to GPU
0.00.190.550 I load_tensors: offloaded 25/25 layers to GPU
0.00.190.573 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.190.574 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.190.925 I llama_context: n_seq_max     = 1
0.00.190.926 I llama_context: n_ctx         = 2048
0.00.190.927 I llama_context: n_ctx_per_seq = 2048
0.00.190.927 I llama_context: n_batch       = 2048
0.00.190.927 I llama_context: n_ubatch      = 512
0.00.190.927 I llama_context: flash_attn    = 0
0.00.190.928 I llama_context: freq_base     = 10000.0
0.00.190.928 I llama_context: freq_scale    = 1
0.00.190.929 I ggml_metal_init: allocating
0.00.190.943 I ggml_metal_init: found device: Apple M4
0.00.190.949 I ggml_metal_init: picking default device: Apple M4
0.00.191.566 I ggml_metal_init: using embedded metal library
0.00.201.092 I ggml_metal_init: GPU name:   Apple M4
0.00.201.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.201.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.201.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.201.095 I ggml_metal_init: simdgroup reduction   = true
0.00.201.095 I ggml_metal_init: simdgroup matrix mul. = true
0.00.201.095 I ggml_metal_init: has residency sets    = true
0.00.201.095 I ggml_metal_init: has bfloat            = true
0.00.201.096 I ggml_metal_init: use bfloat            = true
0.00.201.096 I ggml_metal_init: hasUnifiedMemory      = true
0.00.201.097 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.225.001 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.225.003 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.252.081 I init:      Metal KV buffer size =   384.00 MiB
0.00.252.087 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.255.648 I init:      Metal compute buffer size =   102.25 MiB
0.00.255.650 I init:        CPU compute buffer size =     8.01 MiB
0.00.255.650 I init: graph nodes  = 967
0.00.255.651 I init: graph splits = 2
0.00.255.654 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.255.770 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.255.771 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.323.524 I main: llama threadpool init, n_threads = 4
0.00.323.587 I 
0.00.323.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.323.606 I 
0.00.323.663 I sampler seed: 1234
0.00.323.668 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.323.692 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.323.694 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.323.694 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.164.911 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.02.164.912 I llama_perf_context_print:        load time =     233.54 ms
0.02.164.912 I llama_perf_context_print: prompt eval time =      44.15 ms /     7 tokens (    6.31 ms per token,   158.56 tokens per second)
0.02.164.913 I llama_perf_context_print:        eval time =    1794.22 ms /    63 runs   (   28.48 ms per token,    35.11 tokens per second)
0.02.164.914 I llama_perf_context_print:       total time =    1842.22 ms /    70 tokens
0.02.168.810 I ggml_metal_free: deallocating

real	0m2.460s
user	0m0.132s
sys	0m0.142s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.807 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.058 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.131 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.345 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.346 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.347 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.348 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.348 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.349 I llama_model_loader: - type  f32:  194 tensors
0.00.031.349 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.350 I print_info: file format = GGUF V3 (latest)
0.00.031.350 I print_info: file type   = Q8_0
0.00.031.353 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.039.542 I load: special tokens cache size = 25
0.00.045.767 I load: token to piece cache size = 0.2984 MB
0.00.045.783 I print_info: arch             = gptneox
0.00.045.784 I print_info: vocab_only       = 0
0.00.045.784 I print_info: n_ctx_train      = 2048
0.00.045.784 I print_info: n_embd           = 2048
0.00.045.785 I print_info: n_layer          = 24
0.00.045.791 I print_info: n_head           = 16
0.00.045.792 I print_info: n_head_kv        = 16
0.00.045.792 I print_info: n_rot            = 32
0.00.045.792 I print_info: n_swa            = 0
0.00.045.792 I print_info: n_embd_head_k    = 128
0.00.045.792 I print_info: n_embd_head_v    = 128
0.00.045.793 I print_info: n_gqa            = 1
0.00.045.794 I print_info: n_embd_k_gqa     = 2048
0.00.045.794 I print_info: n_embd_v_gqa     = 2048
0.00.045.795 I print_info: f_norm_eps       = 1.0e-05
0.00.045.795 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.795 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.796 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.796 I print_info: f_logit_scale    = 0.0e+00
0.00.045.797 I print_info: n_ff             = 8192
0.00.045.797 I print_info: n_expert         = 0
0.00.045.797 I print_info: n_expert_used    = 0
0.00.045.798 I print_info: causal attn      = 1
0.00.045.798 I print_info: pooling type     = 0
0.00.045.798 I print_info: rope type        = 2
0.00.045.798 I print_info: rope scaling     = linear
0.00.045.800 I print_info: freq_base_train  = 10000.0
0.00.045.800 I print_info: freq_scale_train = 1
0.00.045.801 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.801 I print_info: rope_finetuned   = unknown
0.00.045.801 I print_info: ssm_d_conv       = 0
0.00.045.801 I print_info: ssm_d_inner      = 0
0.00.045.801 I print_info: ssm_d_state      = 0
0.00.045.801 I print_info: ssm_dt_rank      = 0
0.00.045.801 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.802 I print_info: model type       = 1.4B
0.00.045.802 I print_info: model params     = 1.41 B
0.00.045.802 I print_info: general.name     = 1.4B
0.00.045.807 I print_info: vocab type       = BPE
0.00.045.807 I print_info: n_vocab          = 50304
0.00.045.807 I print_info: n_merges         = 50009
0.00.045.807 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.807 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.808 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.808 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.809 I print_info: LF token         = 187 'Ċ'
0.00.045.809 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.809 I print_info: max token length = 1024
0.00.045.809 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.031.924 I load_tensors: offloading 24 repeating layers to GPU
0.01.031.929 I load_tensors: offloading output layer to GPU
0.01.031.931 I load_tensors: offloaded 25/25 layers to GPU
0.01.031.955 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.031.956 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.032.840 I llama_context: n_seq_max     = 1
0.01.032.842 I llama_context: n_ctx         = 2048
0.01.032.843 I llama_context: n_ctx_per_seq = 2048
0.01.032.843 I llama_context: n_batch       = 2048
0.01.032.843 I llama_context: n_ubatch      = 512
0.01.032.844 I llama_context: flash_attn    = 0
0.01.032.844 I llama_context: freq_base     = 10000.0
0.01.032.845 I llama_context: freq_scale    = 1
0.01.032.846 I ggml_metal_init: allocating
0.01.032.856 I ggml_metal_init: found device: Apple M4
0.01.032.865 I ggml_metal_init: picking default device: Apple M4
0.01.034.185 I ggml_metal_init: using embedded metal library
0.01.040.067 I ggml_metal_init: GPU name:   Apple M4
0.01.040.071 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.040.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.040.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.040.073 I ggml_metal_init: simdgroup reduction   = true
0.01.040.073 I ggml_metal_init: simdgroup matrix mul. = true
0.01.040.074 I ggml_metal_init: has residency sets    = true
0.01.040.074 I ggml_metal_init: has bfloat            = true
0.01.040.074 I ggml_metal_init: use bfloat            = true
0.01.040.075 I ggml_metal_init: hasUnifiedMemory      = true
0.01.040.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.056.542 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.056.546 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.109.948 I init:      Metal KV buffer size =   384.00 MiB
0.01.109.956 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.114.605 I init:      Metal compute buffer size =   102.25 MiB
0.01.114.608 I init:        CPU compute buffer size =     8.01 MiB
0.01.114.609 I init: graph nodes  = 967
0.01.114.609 I init: graph splits = 2
0.01.114.615 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.114.745 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.114.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.171.098 I main: llama threadpool init, n_threads = 4
0.01.171.146 I 
0.01.171.164 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.171.164 I 
0.01.171.347 I sampler seed: 1234
0.01.171.352 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.171.363 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.171.363 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.171.363 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.266.559 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54741.71 tokens per second)
0.02.266.560 I llama_perf_context_print:        load time =    1160.58 ms
0.02.266.562 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.13 tokens per second)
0.02.266.562 I llama_perf_context_print:        eval time =    1043.36 ms /    63 runs   (   16.56 ms per token,    60.38 tokens per second)
0.02.266.563 I llama_perf_context_print:       total time =    1096.16 ms /    70 tokens
0.02.270.435 I ggml_metal_free: deallocating

real	0m2.288s
user	0m0.108s
sys	0m0.266s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.011.138 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.900 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.904 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.907 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.908 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.909 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.910 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.910 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.910 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.911 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.911 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.913 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.913 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.851 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.852 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.853 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.853 I llama_model_loader: - type  f32:  194 tensors
0.00.027.854 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.854 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.854 I print_info: file format = GGUF V3 (latest)
0.00.027.855 I print_info: file type   = Q4_0
0.00.027.856 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.910 I load: special tokens cache size = 25
0.00.041.865 I load: token to piece cache size = 0.2984 MB
0.00.041.881 I print_info: arch             = gptneox
0.00.041.882 I print_info: vocab_only       = 0
0.00.041.882 I print_info: n_ctx_train      = 2048
0.00.041.883 I print_info: n_embd           = 2048
0.00.041.883 I print_info: n_layer          = 24
0.00.041.886 I print_info: n_head           = 16
0.00.041.887 I print_info: n_head_kv        = 16
0.00.041.888 I print_info: n_rot            = 32
0.00.041.888 I print_info: n_swa            = 0
0.00.041.888 I print_info: n_embd_head_k    = 128
0.00.041.888 I print_info: n_embd_head_v    = 128
0.00.041.889 I print_info: n_gqa            = 1
0.00.041.890 I print_info: n_embd_k_gqa     = 2048
0.00.041.891 I print_info: n_embd_v_gqa     = 2048
0.00.041.891 I print_info: f_norm_eps       = 1.0e-05
0.00.041.893 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.893 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.894 I print_info: f_logit_scale    = 0.0e+00
0.00.041.895 I print_info: n_ff             = 8192
0.00.041.895 I print_info: n_expert         = 0
0.00.041.895 I print_info: n_expert_used    = 0
0.00.041.895 I print_info: causal attn      = 1
0.00.041.895 I print_info: pooling type     = 0
0.00.041.895 I print_info: rope type        = 2
0.00.041.895 I print_info: rope scaling     = linear
0.00.041.896 I print_info: freq_base_train  = 10000.0
0.00.041.896 I print_info: freq_scale_train = 1
0.00.041.896 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.896 I print_info: rope_finetuned   = unknown
0.00.041.896 I print_info: ssm_d_conv       = 0
0.00.041.896 I print_info: ssm_d_inner      = 0
0.00.041.897 I print_info: ssm_d_state      = 0
0.00.041.897 I print_info: ssm_dt_rank      = 0
0.00.041.897 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.897 I print_info: model type       = 1.4B
0.00.041.898 I print_info: model params     = 1.41 B
0.00.041.898 I print_info: general.name     = 1.4B
0.00.041.898 I print_info: vocab type       = BPE
0.00.041.899 I print_info: n_vocab          = 50304
0.00.041.899 I print_info: n_merges         = 50009
0.00.041.899 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.899 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.899 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.902 I print_info: LF token         = 187 'Ċ'
0.00.041.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.902 I print_info: max token length = 1024
0.00.041.902 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.577.031 I load_tensors: offloading 24 repeating layers to GPU
0.00.577.048 I load_tensors: offloading output layer to GPU
0.00.577.049 I load_tensors: offloaded 25/25 layers to GPU
0.00.577.091 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.577.092 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.578.753 I llama_context: n_seq_max     = 1
0.00.578.756 I llama_context: n_ctx         = 2048
0.00.578.756 I llama_context: n_ctx_per_seq = 2048
0.00.578.757 I llama_context: n_batch       = 2048
0.00.578.757 I llama_context: n_ubatch      = 512
0.00.578.758 I llama_context: flash_attn    = 0
0.00.578.760 I llama_context: freq_base     = 10000.0
0.00.578.760 I llama_context: freq_scale    = 1
0.00.578.762 I ggml_metal_init: allocating
0.00.578.842 I ggml_metal_init: found device: Apple M4
0.00.578.855 I ggml_metal_init: picking default device: Apple M4
0.00.580.697 I ggml_metal_init: using embedded metal library
0.00.586.156 I ggml_metal_init: GPU name:   Apple M4
0.00.586.161 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.586.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.586.162 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.586.163 I ggml_metal_init: simdgroup reduction   = true
0.00.586.163 I ggml_metal_init: simdgroup matrix mul. = true
0.00.586.164 I ggml_metal_init: has residency sets    = true
0.00.586.164 I ggml_metal_init: has bfloat            = true
0.00.586.164 I ggml_metal_init: use bfloat            = true
0.00.586.165 I ggml_metal_init: hasUnifiedMemory      = true
0.00.586.167 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.605.282 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.605.287 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.658.951 I init:      Metal KV buffer size =   384.00 MiB
0.00.658.960 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.663.109 I init:      Metal compute buffer size =   102.25 MiB
0.00.663.111 I init:        CPU compute buffer size =     8.01 MiB
0.00.663.112 I init: graph nodes  = 967
0.00.663.112 I init: graph splits = 2
0.00.663.118 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.663.243 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.663.243 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.749 I main: llama threadpool init, n_threads = 4
0.00.715.794 I 
0.00.715.809 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.809 I 
0.00.715.958 I sampler seed: 1234
0.00.715.963 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.715.995 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.715.999 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.715.999 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.394.640 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51263.54 tokens per second)
0.01.394.640 I llama_perf_context_print:        load time =     703.91 ms
0.01.394.641 I llama_perf_context_print: prompt eval time =      39.71 ms /     7 tokens (    5.67 ms per token,   176.27 tokens per second)
0.01.394.642 I llama_perf_context_print:        eval time =     636.07 ms /    63 runs   (   10.10 ms per token,    99.05 tokens per second)
0.01.394.643 I llama_perf_context_print:       total time =     679.59 ms /    70 tokens
0.01.398.522 I ggml_metal_free: deallocating

real	0m1.416s
user	0m0.109s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.008.782 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.048 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.050 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.058 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.058 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.059 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.061 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.062 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.793 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.385 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.386 I llama_model_loader: - type  f32:  194 tensors
0.00.025.386 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.386 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.387 I print_info: file format = GGUF V3 (latest)
0.00.025.388 I print_info: file type   = Q4_1
0.00.025.388 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.623 I load: special tokens cache size = 25
0.00.039.625 I load: token to piece cache size = 0.2984 MB
0.00.039.639 I print_info: arch             = gptneox
0.00.039.641 I print_info: vocab_only       = 0
0.00.039.641 I print_info: n_ctx_train      = 2048
0.00.039.641 I print_info: n_embd           = 2048
0.00.039.641 I print_info: n_layer          = 24
0.00.039.644 I print_info: n_head           = 16
0.00.039.645 I print_info: n_head_kv        = 16
0.00.039.645 I print_info: n_rot            = 32
0.00.039.645 I print_info: n_swa            = 0
0.00.039.645 I print_info: n_embd_head_k    = 128
0.00.039.645 I print_info: n_embd_head_v    = 128
0.00.039.646 I print_info: n_gqa            = 1
0.00.039.647 I print_info: n_embd_k_gqa     = 2048
0.00.039.648 I print_info: n_embd_v_gqa     = 2048
0.00.039.648 I print_info: f_norm_eps       = 1.0e-05
0.00.039.649 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.649 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.649 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.649 I print_info: f_logit_scale    = 0.0e+00
0.00.039.650 I print_info: n_ff             = 8192
0.00.039.650 I print_info: n_expert         = 0
0.00.039.650 I print_info: n_expert_used    = 0
0.00.039.650 I print_info: causal attn      = 1
0.00.039.650 I print_info: pooling type     = 0
0.00.039.650 I print_info: rope type        = 2
0.00.039.651 I print_info: rope scaling     = linear
0.00.039.651 I print_info: freq_base_train  = 10000.0
0.00.039.652 I print_info: freq_scale_train = 1
0.00.039.652 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.652 I print_info: rope_finetuned   = unknown
0.00.039.652 I print_info: ssm_d_conv       = 0
0.00.039.652 I print_info: ssm_d_inner      = 0
0.00.039.653 I print_info: ssm_d_state      = 0
0.00.039.653 I print_info: ssm_dt_rank      = 0
0.00.039.653 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.653 I print_info: model type       = 1.4B
0.00.039.653 I print_info: model params     = 1.41 B
0.00.039.653 I print_info: general.name     = 1.4B
0.00.039.654 I print_info: vocab type       = BPE
0.00.039.654 I print_info: n_vocab          = 50304
0.00.039.654 I print_info: n_merges         = 50009
0.00.039.654 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.655 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.655 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.655 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.656 I print_info: LF token         = 187 'Ċ'
0.00.039.657 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.657 I print_info: max token length = 1024
0.00.039.657 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.172 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.186 I load_tensors: offloading output layer to GPU
0.00.649.187 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.220 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.649.221 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.650.679 I llama_context: n_seq_max     = 1
0.00.650.681 I llama_context: n_ctx         = 2048
0.00.650.682 I llama_context: n_ctx_per_seq = 2048
0.00.650.682 I llama_context: n_batch       = 2048
0.00.650.683 I llama_context: n_ubatch      = 512
0.00.650.683 I llama_context: flash_attn    = 0
0.00.650.686 I llama_context: freq_base     = 10000.0
0.00.650.686 I llama_context: freq_scale    = 1
0.00.650.689 I ggml_metal_init: allocating
0.00.650.771 I ggml_metal_init: found device: Apple M4
0.00.650.784 I ggml_metal_init: picking default device: Apple M4
0.00.652.693 I ggml_metal_init: using embedded metal library
0.00.659.412 I ggml_metal_init: GPU name:   Apple M4
0.00.659.417 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.418 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.419 I ggml_metal_init: simdgroup reduction   = true
0.00.659.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.420 I ggml_metal_init: has residency sets    = true
0.00.659.420 I ggml_metal_init: has bfloat            = true
0.00.659.420 I ggml_metal_init: use bfloat            = true
0.00.659.421 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.422 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.513 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.677.518 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.005 I init:      Metal KV buffer size =   384.00 MiB
0.00.735.011 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.739.249 I init:      Metal compute buffer size =   102.25 MiB
0.00.739.250 I init:        CPU compute buffer size =     8.01 MiB
0.00.739.251 I init: graph nodes  = 967
0.00.739.251 I init: graph splits = 2
0.00.739.257 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.739.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.739.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.416 I main: llama threadpool init, n_threads = 4
0.00.795.465 I 
0.00.795.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.480 I 
0.00.795.634 I sampler seed: 1234
0.00.795.639 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.659 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.660 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.660 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.522.558 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.01.522.559 I llama_perf_context_print:        load time =     785.94 ms
0.01.522.560 I llama_perf_context_print: prompt eval time =      48.82 ms /     7 tokens (    6.97 ms per token,   143.40 tokens per second)
0.01.522.561 I llama_perf_context_print:        eval time =     675.38 ms /    63 runs   (   10.72 ms per token,    93.28 tokens per second)
0.01.522.562 I llama_perf_context_print:       total time =     727.84 ms /    70 tokens
0.01.525.754 I ggml_metal_free: deallocating

real	0m1.539s
user	0m0.109s
sys	0m0.214s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.735 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.311 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.311 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.312 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.313 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.313 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.314 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.314 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.208 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.289 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.120 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.121 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.122 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.122 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.122 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.122 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.123 I llama_model_loader: - type  f32:  194 tensors
0.00.026.123 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.124 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.124 I print_info: file format = GGUF V3 (latest)
0.00.026.125 I print_info: file type   = Q5_0
0.00.026.127 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.961 I load: special tokens cache size = 25
0.00.039.822 I load: token to piece cache size = 0.2984 MB
0.00.039.836 I print_info: arch             = gptneox
0.00.039.837 I print_info: vocab_only       = 0
0.00.039.838 I print_info: n_ctx_train      = 2048
0.00.039.838 I print_info: n_embd           = 2048
0.00.039.838 I print_info: n_layer          = 24
0.00.039.841 I print_info: n_head           = 16
0.00.039.841 I print_info: n_head_kv        = 16
0.00.039.842 I print_info: n_rot            = 32
0.00.039.842 I print_info: n_swa            = 0
0.00.039.842 I print_info: n_embd_head_k    = 128
0.00.039.842 I print_info: n_embd_head_v    = 128
0.00.039.843 I print_info: n_gqa            = 1
0.00.039.844 I print_info: n_embd_k_gqa     = 2048
0.00.039.844 I print_info: n_embd_v_gqa     = 2048
0.00.039.845 I print_info: f_norm_eps       = 1.0e-05
0.00.039.845 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.846 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.846 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.846 I print_info: f_logit_scale    = 0.0e+00
0.00.039.847 I print_info: n_ff             = 8192
0.00.039.847 I print_info: n_expert         = 0
0.00.039.847 I print_info: n_expert_used    = 0
0.00.039.847 I print_info: causal attn      = 1
0.00.039.847 I print_info: pooling type     = 0
0.00.039.849 I print_info: rope type        = 2
0.00.039.850 I print_info: rope scaling     = linear
0.00.039.851 I print_info: freq_base_train  = 10000.0
0.00.039.851 I print_info: freq_scale_train = 1
0.00.039.851 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.851 I print_info: rope_finetuned   = unknown
0.00.039.851 I print_info: ssm_d_conv       = 0
0.00.039.852 I print_info: ssm_d_inner      = 0
0.00.039.852 I print_info: ssm_d_state      = 0
0.00.039.852 I print_info: ssm_dt_rank      = 0
0.00.039.852 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.852 I print_info: model type       = 1.4B
0.00.039.853 I print_info: model params     = 1.41 B
0.00.039.854 I print_info: general.name     = 1.4B
0.00.039.854 I print_info: vocab type       = BPE
0.00.039.854 I print_info: n_vocab          = 50304
0.00.039.854 I print_info: n_merges         = 50009
0.00.039.855 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.855 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.855 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.855 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.855 I print_info: LF token         = 187 'Ċ'
0.00.039.856 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.856 I print_info: max token length = 1024
0.00.039.856 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.629.910 I load_tensors: offloading 24 repeating layers to GPU
0.00.629.921 I load_tensors: offloading output layer to GPU
0.00.629.922 I load_tensors: offloaded 25/25 layers to GPU
0.00.629.956 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.629.958 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.631.608 I llama_context: n_seq_max     = 1
0.00.631.610 I llama_context: n_ctx         = 2048
0.00.631.611 I llama_context: n_ctx_per_seq = 2048
0.00.631.611 I llama_context: n_batch       = 2048
0.00.631.612 I llama_context: n_ubatch      = 512
0.00.631.612 I llama_context: flash_attn    = 0
0.00.631.614 I llama_context: freq_base     = 10000.0
0.00.631.615 I llama_context: freq_scale    = 1
0.00.631.617 I ggml_metal_init: allocating
0.00.631.685 I ggml_metal_init: found device: Apple M4
0.00.631.698 I ggml_metal_init: picking default device: Apple M4
0.00.633.669 I ggml_metal_init: using embedded metal library
0.00.640.239 I ggml_metal_init: GPU name:   Apple M4
0.00.640.243 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.244 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.245 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.245 I ggml_metal_init: simdgroup reduction   = true
0.00.640.246 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.246 I ggml_metal_init: has residency sets    = true
0.00.640.246 I ggml_metal_init: has bfloat            = true
0.00.640.247 I ggml_metal_init: use bfloat            = true
0.00.640.247 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.468 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.658.473 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.477 I init:      Metal KV buffer size =   384.00 MiB
0.00.716.484 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.720.767 I init:      Metal compute buffer size =   102.25 MiB
0.00.720.769 I init:        CPU compute buffer size =     8.01 MiB
0.00.720.769 I init: graph nodes  = 967
0.00.720.769 I init: graph splits = 2
0.00.720.776 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.895 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.620 I main: llama threadpool init, n_threads = 4
0.00.777.666 I 
0.00.777.681 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.681 I 
0.00.777.843 I sampler seed: 1234
0.00.777.848 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.892 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.894 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.894 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.558.958 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.558.959 I llama_perf_context_print:        load time =     767.20 ms
0.01.558.960 I llama_perf_context_print: prompt eval time =      43.10 ms /     7 tokens (    6.16 ms per token,   162.43 tokens per second)
0.01.558.961 I llama_perf_context_print:        eval time =     735.08 ms /    63 runs   (   11.67 ms per token,    85.70 tokens per second)
0.01.558.961 I llama_perf_context_print:       total time =     782.03 ms /    70 tokens
0.01.562.845 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.109s
sys	0m0.225s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.894 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.685 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.690 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.691 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.692 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.694 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.694 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.695 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.695 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.695 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.700 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.701 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.720 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.599 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.602 I llama_model_loader: - type  f32:  194 tensors
0.00.025.602 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.603 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.603 I print_info: file format = GGUF V3 (latest)
0.00.025.604 I print_info: file type   = Q5_1
0.00.025.605 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.797 I load: special tokens cache size = 25
0.00.039.737 I load: token to piece cache size = 0.2984 MB
0.00.039.751 I print_info: arch             = gptneox
0.00.039.752 I print_info: vocab_only       = 0
0.00.039.752 I print_info: n_ctx_train      = 2048
0.00.039.753 I print_info: n_embd           = 2048
0.00.039.753 I print_info: n_layer          = 24
0.00.039.755 I print_info: n_head           = 16
0.00.039.756 I print_info: n_head_kv        = 16
0.00.039.756 I print_info: n_rot            = 32
0.00.039.756 I print_info: n_swa            = 0
0.00.039.757 I print_info: n_embd_head_k    = 128
0.00.039.757 I print_info: n_embd_head_v    = 128
0.00.039.758 I print_info: n_gqa            = 1
0.00.039.758 I print_info: n_embd_k_gqa     = 2048
0.00.039.760 I print_info: n_embd_v_gqa     = 2048
0.00.039.760 I print_info: f_norm_eps       = 1.0e-05
0.00.039.761 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.761 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.762 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.762 I print_info: f_logit_scale    = 0.0e+00
0.00.039.763 I print_info: n_ff             = 8192
0.00.039.763 I print_info: n_expert         = 0
0.00.039.763 I print_info: n_expert_used    = 0
0.00.039.763 I print_info: causal attn      = 1
0.00.039.765 I print_info: pooling type     = 0
0.00.039.766 I print_info: rope type        = 2
0.00.039.767 I print_info: rope scaling     = linear
0.00.039.767 I print_info: freq_base_train  = 10000.0
0.00.039.767 I print_info: freq_scale_train = 1
0.00.039.768 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.768 I print_info: rope_finetuned   = unknown
0.00.039.768 I print_info: ssm_d_conv       = 0
0.00.039.768 I print_info: ssm_d_inner      = 0
0.00.039.768 I print_info: ssm_d_state      = 0
0.00.039.768 I print_info: ssm_dt_rank      = 0
0.00.039.769 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.769 I print_info: model type       = 1.4B
0.00.039.770 I print_info: model params     = 1.41 B
0.00.039.770 I print_info: general.name     = 1.4B
0.00.039.770 I print_info: vocab type       = BPE
0.00.039.770 I print_info: n_vocab          = 50304
0.00.039.771 I print_info: n_merges         = 50009
0.00.039.771 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.771 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.771 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.771 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.772 I print_info: LF token         = 187 'Ċ'
0.00.039.772 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.772 I print_info: max token length = 1024
0.00.039.773 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.717.636 I load_tensors: offloading 24 repeating layers to GPU
0.00.717.650 I load_tensors: offloading output layer to GPU
0.00.717.651 I load_tensors: offloaded 25/25 layers to GPU
0.00.717.682 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.717.684 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.719.403 I llama_context: n_seq_max     = 1
0.00.719.413 I llama_context: n_ctx         = 2048
0.00.719.413 I llama_context: n_ctx_per_seq = 2048
0.00.719.414 I llama_context: n_batch       = 2048
0.00.719.414 I llama_context: n_ubatch      = 512
0.00.719.414 I llama_context: flash_attn    = 0
0.00.719.417 I llama_context: freq_base     = 10000.0
0.00.719.418 I llama_context: freq_scale    = 1
0.00.719.422 I ggml_metal_init: allocating
0.00.719.475 I ggml_metal_init: found device: Apple M4
0.00.719.491 I ggml_metal_init: picking default device: Apple M4
0.00.721.858 I ggml_metal_init: using embedded metal library
0.00.728.387 I ggml_metal_init: GPU name:   Apple M4
0.00.728.391 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.728.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.728.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.728.393 I ggml_metal_init: simdgroup reduction   = true
0.00.728.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.728.393 I ggml_metal_init: has residency sets    = true
0.00.728.393 I ggml_metal_init: has bfloat            = true
0.00.728.394 I ggml_metal_init: use bfloat            = true
0.00.728.395 I ggml_metal_init: hasUnifiedMemory      = true
0.00.728.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.747.737 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.747.742 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.803.957 I init:      Metal KV buffer size =   384.00 MiB
0.00.803.963 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.809.572 I init:      Metal compute buffer size =   102.25 MiB
0.00.809.575 I init:        CPU compute buffer size =     8.01 MiB
0.00.809.575 I init: graph nodes  = 967
0.00.809.575 I init: graph splits = 2
0.00.809.582 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.809.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.809.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.866.990 I main: llama threadpool init, n_threads = 4
0.00.867.030 I 
0.00.867.045 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.867.045 I 
0.00.867.220 I sampler seed: 1234
0.00.867.225 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.867.244 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.867.244 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.867.244 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.698.223 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48070.41 tokens per second)
0.01.698.224 I llama_perf_context_print:        load time =     857.34 ms
0.01.698.224 I llama_perf_context_print: prompt eval time =      41.89 ms /     7 tokens (    5.98 ms per token,   167.12 tokens per second)
0.01.698.225 I llama_perf_context_print:        eval time =     786.37 ms /    63 runs   (   12.48 ms per token,    80.12 tokens per second)
0.01.698.225 I llama_perf_context_print:       total time =     831.99 ms /    70 tokens
0.01.702.000 I ggml_metal_free: deallocating

real	0m1.717s
user	0m0.113s
sys	0m0.233s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.621 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.134 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.139 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.141 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.142 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.142 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.142 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.143 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.144 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.144 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.147 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.147 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.216 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.235 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.264 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.265 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.265 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.266 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.266 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.266 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.267 I llama_model_loader: - type  f32:  194 tensors
0.00.025.267 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.267 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.268 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.268 I print_info: file format = GGUF V3 (latest)
0.00.025.269 I print_info: file type   = Q2_K - Medium
0.00.025.269 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.161 I load: special tokens cache size = 25
0.00.039.216 I load: token to piece cache size = 0.2984 MB
0.00.039.230 I print_info: arch             = gptneox
0.00.039.231 I print_info: vocab_only       = 0
0.00.039.232 I print_info: n_ctx_train      = 2048
0.00.039.232 I print_info: n_embd           = 2048
0.00.039.232 I print_info: n_layer          = 24
0.00.039.234 I print_info: n_head           = 16
0.00.039.235 I print_info: n_head_kv        = 16
0.00.039.236 I print_info: n_rot            = 32
0.00.039.236 I print_info: n_swa            = 0
0.00.039.236 I print_info: n_embd_head_k    = 128
0.00.039.236 I print_info: n_embd_head_v    = 128
0.00.039.237 I print_info: n_gqa            = 1
0.00.039.238 I print_info: n_embd_k_gqa     = 2048
0.00.039.239 I print_info: n_embd_v_gqa     = 2048
0.00.039.239 I print_info: f_norm_eps       = 1.0e-05
0.00.039.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.242 I print_info: f_logit_scale    = 0.0e+00
0.00.039.242 I print_info: n_ff             = 8192
0.00.039.242 I print_info: n_expert         = 0
0.00.039.242 I print_info: n_expert_used    = 0
0.00.039.244 I print_info: causal attn      = 1
0.00.039.244 I print_info: pooling type     = 0
0.00.039.244 I print_info: rope type        = 2
0.00.039.244 I print_info: rope scaling     = linear
0.00.039.244 I print_info: freq_base_train  = 10000.0
0.00.039.245 I print_info: freq_scale_train = 1
0.00.039.245 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.245 I print_info: rope_finetuned   = unknown
0.00.039.245 I print_info: ssm_d_conv       = 0
0.00.039.245 I print_info: ssm_d_inner      = 0
0.00.039.245 I print_info: ssm_d_state      = 0
0.00.039.245 I print_info: ssm_dt_rank      = 0
0.00.039.245 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.246 I print_info: model type       = 1.4B
0.00.039.246 I print_info: model params     = 1.41 B
0.00.039.246 I print_info: general.name     = 1.4B
0.00.039.247 I print_info: vocab type       = BPE
0.00.039.247 I print_info: n_vocab          = 50304
0.00.039.247 I print_info: n_merges         = 50009
0.00.039.247 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.247 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.248 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.248 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.248 I print_info: LF token         = 187 'Ċ'
0.00.039.248 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.248 I print_info: max token length = 1024
0.00.039.249 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.396.863 I load_tensors: offloading 24 repeating layers to GPU
0.00.396.875 I load_tensors: offloading output layer to GPU
0.00.396.876 I load_tensors: offloaded 25/25 layers to GPU
0.00.396.908 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.396.909 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.398.488 I llama_context: n_seq_max     = 1
0.00.398.490 I llama_context: n_ctx         = 2048
0.00.398.491 I llama_context: n_ctx_per_seq = 2048
0.00.398.491 I llama_context: n_batch       = 2048
0.00.398.491 I llama_context: n_ubatch      = 512
0.00.398.492 I llama_context: flash_attn    = 0
0.00.398.494 I llama_context: freq_base     = 10000.0
0.00.398.495 I llama_context: freq_scale    = 1
0.00.398.497 I ggml_metal_init: allocating
0.00.398.602 I ggml_metal_init: found device: Apple M4
0.00.398.615 I ggml_metal_init: picking default device: Apple M4
0.00.400.625 I ggml_metal_init: using embedded metal library
0.00.406.089 I ggml_metal_init: GPU name:   Apple M4
0.00.406.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.406.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.406.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.406.099 I ggml_metal_init: simdgroup reduction   = true
0.00.406.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.406.100 I ggml_metal_init: has residency sets    = true
0.00.406.101 I ggml_metal_init: has bfloat            = true
0.00.406.101 I ggml_metal_init: use bfloat            = true
0.00.406.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.406.111 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.427.658 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.427.663 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.491.312 I init:      Metal KV buffer size =   384.00 MiB
0.00.491.322 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.495.829 I init:      Metal compute buffer size =   102.25 MiB
0.00.495.831 I init:        CPU compute buffer size =     8.01 MiB
0.00.495.831 I init: graph nodes  = 967
0.00.495.832 I init: graph splits = 2
0.00.495.838 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.495.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.495.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.844 I main: llama threadpool init, n_threads = 4
0.00.547.888 I 
0.00.547.903 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.547.903 I 
0.00.548.041 I sampler seed: 1234
0.00.548.046 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.548.089 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.548.092 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.548.092 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.223.777 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.223.778 I llama_perf_context_print:        load time =     537.53 ms
0.01.223.779 I llama_perf_context_print: prompt eval time =      38.27 ms /     7 tokens (    5.47 ms per token,   182.90 tokens per second)
0.01.223.780 I llama_perf_context_print:        eval time =     634.60 ms /    63 runs   (   10.07 ms per token,    99.28 tokens per second)
0.01.223.781 I llama_perf_context_print:       total time =     676.62 ms /    70 tokens
0.01.227.727 I ggml_metal_free: deallocating

real	0m1.246s
user	0m0.112s
sys	0m0.169s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.572 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.224 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.240 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.242 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.242 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.242 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.243 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.244 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.244 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.245 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.245 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.245 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.247 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.248 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.421 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.510 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.485 I llama_model_loader: - type  f32:  194 tensors
0.00.025.485 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.486 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.486 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.486 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.487 I print_info: file format = GGUF V3 (latest)
0.00.025.487 I print_info: file type   = Q3_K - Medium
0.00.025.488 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.652 I load: special tokens cache size = 25
0.00.039.740 I load: token to piece cache size = 0.2984 MB
0.00.039.755 I print_info: arch             = gptneox
0.00.039.756 I print_info: vocab_only       = 0
0.00.039.756 I print_info: n_ctx_train      = 2048
0.00.039.756 I print_info: n_embd           = 2048
0.00.039.757 I print_info: n_layer          = 24
0.00.039.759 I print_info: n_head           = 16
0.00.039.760 I print_info: n_head_kv        = 16
0.00.039.760 I print_info: n_rot            = 32
0.00.039.760 I print_info: n_swa            = 0
0.00.039.760 I print_info: n_embd_head_k    = 128
0.00.039.761 I print_info: n_embd_head_v    = 128
0.00.039.761 I print_info: n_gqa            = 1
0.00.039.762 I print_info: n_embd_k_gqa     = 2048
0.00.039.763 I print_info: n_embd_v_gqa     = 2048
0.00.039.763 I print_info: f_norm_eps       = 1.0e-05
0.00.039.764 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.769 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.771 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.771 I print_info: f_logit_scale    = 0.0e+00
0.00.039.777 I print_info: n_ff             = 8192
0.00.039.777 I print_info: n_expert         = 0
0.00.039.779 I print_info: n_expert_used    = 0
0.00.039.779 I print_info: causal attn      = 1
0.00.039.779 I print_info: pooling type     = 0
0.00.039.779 I print_info: rope type        = 2
0.00.039.779 I print_info: rope scaling     = linear
0.00.039.779 I print_info: freq_base_train  = 10000.0
0.00.039.780 I print_info: freq_scale_train = 1
0.00.039.780 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.780 I print_info: rope_finetuned   = unknown
0.00.039.780 I print_info: ssm_d_conv       = 0
0.00.039.781 I print_info: ssm_d_inner      = 0
0.00.039.781 I print_info: ssm_d_state      = 0
0.00.039.781 I print_info: ssm_dt_rank      = 0
0.00.039.781 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.781 I print_info: model type       = 1.4B
0.00.039.781 I print_info: model params     = 1.41 B
0.00.039.782 I print_info: general.name     = 1.4B
0.00.039.782 I print_info: vocab type       = BPE
0.00.039.787 I print_info: n_vocab          = 50304
0.00.039.789 I print_info: n_merges         = 50009
0.00.039.789 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.789 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.789 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: LF token         = 187 'Ċ'
0.00.039.790 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: max token length = 1024
0.00.039.791 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.437.380 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.397 I load_tensors: offloading output layer to GPU
0.00.437.397 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.429 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.431 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.438.840 I llama_context: n_seq_max     = 1
0.00.438.842 I llama_context: n_ctx         = 2048
0.00.438.843 I llama_context: n_ctx_per_seq = 2048
0.00.438.844 I llama_context: n_batch       = 2048
0.00.438.844 I llama_context: n_ubatch      = 512
0.00.438.844 I llama_context: flash_attn    = 0
0.00.438.846 I llama_context: freq_base     = 10000.0
0.00.438.847 I llama_context: freq_scale    = 1
0.00.438.856 I ggml_metal_init: allocating
0.00.438.927 I ggml_metal_init: found device: Apple M4
0.00.438.942 I ggml_metal_init: picking default device: Apple M4
0.00.440.844 I ggml_metal_init: using embedded metal library
0.00.446.343 I ggml_metal_init: GPU name:   Apple M4
0.00.446.353 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.355 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.355 I ggml_metal_init: simdgroup reduction   = true
0.00.446.355 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.356 I ggml_metal_init: has residency sets    = true
0.00.446.356 I ggml_metal_init: has bfloat            = true
0.00.446.356 I ggml_metal_init: use bfloat            = true
0.00.446.358 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.288 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.466.292 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.523.652 I init:      Metal KV buffer size =   384.00 MiB
0.00.523.659 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.528.029 I init:      Metal compute buffer size =   102.25 MiB
0.00.528.031 I init:        CPU compute buffer size =     8.01 MiB
0.00.528.031 I init: graph nodes  = 967
0.00.528.032 I init: graph splits = 2
0.00.528.039 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.528.171 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.528.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.076 I main: llama threadpool init, n_threads = 4
0.00.583.125 I 
0.00.583.140 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.141 I 
0.00.583.297 I sampler seed: 1234
0.00.583.302 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.583.321 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.583.322 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.583.322 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.331.415 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.331.416 I llama_perf_context_print:        load time =     573.82 ms
0.01.331.418 I llama_perf_context_print: prompt eval time =      47.20 ms /     7 tokens (    6.74 ms per token,   148.31 tokens per second)
0.01.331.419 I llama_perf_context_print:        eval time =     697.99 ms /    63 runs   (   11.08 ms per token,    90.26 tokens per second)
0.01.331.420 I llama_perf_context_print:       total time =     749.02 ms /    70 tokens
0.01.335.296 I ggml_metal_free: deallocating

real	0m1.351s
user	0m0.110s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.635 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.265 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.272 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.274 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.274 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.275 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.276 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.279 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.279 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.281 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.420 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.350 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.351 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.352 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.352 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.352 I llama_model_loader: - type  f32:  194 tensors
0.00.025.353 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.353 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.353 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.354 I print_info: file format = GGUF V3 (latest)
0.00.025.354 I print_info: file type   = Q4_K - Medium
0.00.025.355 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.186 I load: special tokens cache size = 25
0.00.039.184 I load: token to piece cache size = 0.2984 MB
0.00.039.198 I print_info: arch             = gptneox
0.00.039.199 I print_info: vocab_only       = 0
0.00.039.200 I print_info: n_ctx_train      = 2048
0.00.039.200 I print_info: n_embd           = 2048
0.00.039.200 I print_info: n_layer          = 24
0.00.039.203 I print_info: n_head           = 16
0.00.039.204 I print_info: n_head_kv        = 16
0.00.039.204 I print_info: n_rot            = 32
0.00.039.204 I print_info: n_swa            = 0
0.00.039.204 I print_info: n_embd_head_k    = 128
0.00.039.204 I print_info: n_embd_head_v    = 128
0.00.039.205 I print_info: n_gqa            = 1
0.00.039.206 I print_info: n_embd_k_gqa     = 2048
0.00.039.207 I print_info: n_embd_v_gqa     = 2048
0.00.039.207 I print_info: f_norm_eps       = 1.0e-05
0.00.039.208 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.208 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.208 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.208 I print_info: f_logit_scale    = 0.0e+00
0.00.039.209 I print_info: n_ff             = 8192
0.00.039.209 I print_info: n_expert         = 0
0.00.039.209 I print_info: n_expert_used    = 0
0.00.039.209 I print_info: causal attn      = 1
0.00.039.211 I print_info: pooling type     = 0
0.00.039.212 I print_info: rope type        = 2
0.00.039.212 I print_info: rope scaling     = linear
0.00.039.212 I print_info: freq_base_train  = 10000.0
0.00.039.212 I print_info: freq_scale_train = 1
0.00.039.213 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.213 I print_info: rope_finetuned   = unknown
0.00.039.213 I print_info: ssm_d_conv       = 0
0.00.039.213 I print_info: ssm_d_inner      = 0
0.00.039.213 I print_info: ssm_d_state      = 0
0.00.039.213 I print_info: ssm_dt_rank      = 0
0.00.039.213 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.214 I print_info: model type       = 1.4B
0.00.039.218 I print_info: model params     = 1.41 B
0.00.039.218 I print_info: general.name     = 1.4B
0.00.039.218 I print_info: vocab type       = BPE
0.00.039.218 I print_info: n_vocab          = 50304
0.00.039.218 I print_info: n_merges         = 50009
0.00.039.219 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.220 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.220 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.220 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.220 I print_info: LF token         = 187 'Ċ'
0.00.039.221 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.221 I print_info: max token length = 1024
0.00.039.221 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.527.454 I load_tensors: offloading 24 repeating layers to GPU
0.00.527.470 I load_tensors: offloading output layer to GPU
0.00.527.470 I load_tensors: offloaded 25/25 layers to GPU
0.00.527.506 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.527.517 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.529.132 I llama_context: n_seq_max     = 1
0.00.529.134 I llama_context: n_ctx         = 2048
0.00.529.135 I llama_context: n_ctx_per_seq = 2048
0.00.529.135 I llama_context: n_batch       = 2048
0.00.529.136 I llama_context: n_ubatch      = 512
0.00.529.136 I llama_context: flash_attn    = 0
0.00.529.139 I llama_context: freq_base     = 10000.0
0.00.529.139 I llama_context: freq_scale    = 1
0.00.529.142 I ggml_metal_init: allocating
0.00.529.205 I ggml_metal_init: found device: Apple M4
0.00.529.218 I ggml_metal_init: picking default device: Apple M4
0.00.531.221 I ggml_metal_init: using embedded metal library
0.00.537.772 I ggml_metal_init: GPU name:   Apple M4
0.00.537.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.537.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.537.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.537.778 I ggml_metal_init: simdgroup reduction   = true
0.00.537.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.537.779 I ggml_metal_init: has residency sets    = true
0.00.537.779 I ggml_metal_init: has bfloat            = true
0.00.537.779 I ggml_metal_init: use bfloat            = true
0.00.537.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.537.782 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.555.010 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.555.015 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.612.559 I init:      Metal KV buffer size =   384.00 MiB
0.00.612.572 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.616.934 I init:      Metal compute buffer size =   102.25 MiB
0.00.616.936 I init:        CPU compute buffer size =     8.01 MiB
0.00.616.936 I init: graph nodes  = 967
0.00.616.937 I init: graph splits = 2
0.00.616.942 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.617.057 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.617.058 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.301 I main: llama threadpool init, n_threads = 4
0.00.676.342 I 
0.00.676.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.357 I 
0.00.676.501 I sampler seed: 1234
0.00.676.505 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.676.516 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.676.517 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.676.517 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.439.802 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48931.77 tokens per second)
0.01.439.802 I llama_perf_context_print:        load time =     666.94 ms
0.01.439.803 I llama_perf_context_print: prompt eval time =      55.24 ms /     7 tokens (    7.89 ms per token,   126.72 tokens per second)
0.01.439.804 I llama_perf_context_print:        eval time =     705.12 ms /    63 runs   (   11.19 ms per token,    89.35 tokens per second)
0.01.439.806 I llama_perf_context_print:       total time =     764.22 ms /    70 tokens
0.01.443.916 I ggml_metal_free: deallocating

real	0m1.461s
user	0m0.109s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.992 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.971 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.976 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.978 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.978 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.980 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.980 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.983 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.983 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.984 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.986 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.986 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.987 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.929 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.752 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.753 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.753 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.754 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.754 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.755 I llama_model_loader: - type  f32:  194 tensors
0.00.026.755 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.755 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.756 I print_info: file format = GGUF V3 (latest)
0.00.026.756 I print_info: file type   = Q5_K - Medium
0.00.026.757 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.070 I load: special tokens cache size = 25
0.00.041.273 I load: token to piece cache size = 0.2984 MB
0.00.041.288 I print_info: arch             = gptneox
0.00.041.289 I print_info: vocab_only       = 0
0.00.041.289 I print_info: n_ctx_train      = 2048
0.00.041.289 I print_info: n_embd           = 2048
0.00.041.290 I print_info: n_layer          = 24
0.00.041.292 I print_info: n_head           = 16
0.00.041.293 I print_info: n_head_kv        = 16
0.00.041.293 I print_info: n_rot            = 32
0.00.041.293 I print_info: n_swa            = 0
0.00.041.294 I print_info: n_embd_head_k    = 128
0.00.041.294 I print_info: n_embd_head_v    = 128
0.00.041.294 I print_info: n_gqa            = 1
0.00.041.295 I print_info: n_embd_k_gqa     = 2048
0.00.041.296 I print_info: n_embd_v_gqa     = 2048
0.00.041.297 I print_info: f_norm_eps       = 1.0e-05
0.00.041.297 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.297 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.297 I print_info: f_logit_scale    = 0.0e+00
0.00.041.298 I print_info: n_ff             = 8192
0.00.041.298 I print_info: n_expert         = 0
0.00.041.298 I print_info: n_expert_used    = 0
0.00.041.299 I print_info: causal attn      = 1
0.00.041.299 I print_info: pooling type     = 0
0.00.041.300 I print_info: rope type        = 2
0.00.041.302 I print_info: rope scaling     = linear
0.00.041.302 I print_info: freq_base_train  = 10000.0
0.00.041.302 I print_info: freq_scale_train = 1
0.00.041.302 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.303 I print_info: rope_finetuned   = unknown
0.00.041.303 I print_info: ssm_d_conv       = 0
0.00.041.303 I print_info: ssm_d_inner      = 0
0.00.041.303 I print_info: ssm_d_state      = 0
0.00.041.303 I print_info: ssm_dt_rank      = 0
0.00.041.303 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.304 I print_info: model type       = 1.4B
0.00.041.304 I print_info: model params     = 1.41 B
0.00.041.304 I print_info: general.name     = 1.4B
0.00.041.305 I print_info: vocab type       = BPE
0.00.041.305 I print_info: n_vocab          = 50304
0.00.041.305 I print_info: n_merges         = 50009
0.00.041.305 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.305 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.305 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.306 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.306 I print_info: LF token         = 187 'Ċ'
0.00.041.307 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.307 I print_info: max token length = 1024
0.00.041.307 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.610.416 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.433 I load_tensors: offloading output layer to GPU
0.00.610.434 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.476 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.610.478 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.611.845 I llama_context: n_seq_max     = 1
0.00.611.847 I llama_context: n_ctx         = 2048
0.00.611.848 I llama_context: n_ctx_per_seq = 2048
0.00.611.848 I llama_context: n_batch       = 2048
0.00.611.849 I llama_context: n_ubatch      = 512
0.00.611.849 I llama_context: flash_attn    = 0
0.00.611.850 I llama_context: freq_base     = 10000.0
0.00.611.851 I llama_context: freq_scale    = 1
0.00.611.852 I ggml_metal_init: allocating
0.00.611.877 I ggml_metal_init: found device: Apple M4
0.00.611.887 I ggml_metal_init: picking default device: Apple M4
0.00.613.397 I ggml_metal_init: using embedded metal library
0.00.619.649 I ggml_metal_init: GPU name:   Apple M4
0.00.619.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.654 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.654 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.655 I ggml_metal_init: simdgroup reduction   = true
0.00.619.655 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.655 I ggml_metal_init: has residency sets    = true
0.00.619.656 I ggml_metal_init: has bfloat            = true
0.00.619.656 I ggml_metal_init: use bfloat            = true
0.00.619.657 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.658 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.966 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.636.970 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.106 I init:      Metal KV buffer size =   384.00 MiB
0.00.694.117 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.267 I init:      Metal compute buffer size =   102.25 MiB
0.00.699.269 I init:        CPU compute buffer size =     8.01 MiB
0.00.699.269 I init: graph nodes  = 967
0.00.699.269 I init: graph splits = 2
0.00.699.276 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.699.408 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.699.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.799 I main: llama threadpool init, n_threads = 4
0.00.759.842 I 
0.00.759.858 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.858 I 
0.00.760.000 I sampler seed: 1234
0.00.760.004 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.015 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.015 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.015 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.601.405 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.01.601.407 I llama_perf_context_print:        load time =     749.12 ms
0.01.601.407 I llama_perf_context_print: prompt eval time =      52.63 ms /     7 tokens (    7.52 ms per token,   133.01 tokens per second)
0.01.601.408 I llama_perf_context_print:        eval time =     785.92 ms /    63 runs   (   12.47 ms per token,    80.16 tokens per second)
0.01.601.409 I llama_perf_context_print:       total time =     842.30 ms /    70 tokens
0.01.605.471 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.108s
sys	0m0.230s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.547 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.088 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.092 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.096 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.098 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.098 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.099 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.101 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.102 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.102 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.146 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.165 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.164 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.165 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.166 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.166 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.166 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.166 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.167 I llama_model_loader: - type  f32:  194 tensors
0.00.025.167 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.168 I print_info: file format = GGUF V3 (latest)
0.00.025.168 I print_info: file type   = Q6_K
0.00.025.169 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.067 I load: special tokens cache size = 25
0.00.039.032 I load: token to piece cache size = 0.2984 MB
0.00.039.041 I print_info: arch             = gptneox
0.00.039.042 I print_info: vocab_only       = 0
0.00.039.043 I print_info: n_ctx_train      = 2048
0.00.039.043 I print_info: n_embd           = 2048
0.00.039.043 I print_info: n_layer          = 24
0.00.039.046 I print_info: n_head           = 16
0.00.039.047 I print_info: n_head_kv        = 16
0.00.039.047 I print_info: n_rot            = 32
0.00.039.047 I print_info: n_swa            = 0
0.00.039.047 I print_info: n_embd_head_k    = 128
0.00.039.047 I print_info: n_embd_head_v    = 128
0.00.039.048 I print_info: n_gqa            = 1
0.00.039.049 I print_info: n_embd_k_gqa     = 2048
0.00.039.049 I print_info: n_embd_v_gqa     = 2048
0.00.039.050 I print_info: f_norm_eps       = 1.0e-05
0.00.039.050 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.050 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.051 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.052 I print_info: f_logit_scale    = 0.0e+00
0.00.039.052 I print_info: n_ff             = 8192
0.00.039.052 I print_info: n_expert         = 0
0.00.039.053 I print_info: n_expert_used    = 0
0.00.039.053 I print_info: causal attn      = 1
0.00.039.053 I print_info: pooling type     = 0
0.00.039.053 I print_info: rope type        = 2
0.00.039.055 I print_info: rope scaling     = linear
0.00.039.056 I print_info: freq_base_train  = 10000.0
0.00.039.057 I print_info: freq_scale_train = 1
0.00.039.057 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.057 I print_info: rope_finetuned   = unknown
0.00.039.057 I print_info: ssm_d_conv       = 0
0.00.039.057 I print_info: ssm_d_inner      = 0
0.00.039.057 I print_info: ssm_d_state      = 0
0.00.039.058 I print_info: ssm_dt_rank      = 0
0.00.039.058 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.058 I print_info: model type       = 1.4B
0.00.039.058 I print_info: model params     = 1.41 B
0.00.039.058 I print_info: general.name     = 1.4B
0.00.039.059 I print_info: vocab type       = BPE
0.00.039.059 I print_info: n_vocab          = 50304
0.00.039.059 I print_info: n_merges         = 50009
0.00.039.059 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: LF token         = 187 'Ċ'
0.00.039.063 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.067 I print_info: max token length = 1024
0.00.039.069 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.666.302 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.308 I load_tensors: offloading output layer to GPU
0.00.666.310 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.334 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.666.336 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.667.651 I llama_context: n_seq_max     = 1
0.00.667.653 I llama_context: n_ctx         = 2048
0.00.667.653 I llama_context: n_ctx_per_seq = 2048
0.00.667.653 I llama_context: n_batch       = 2048
0.00.667.654 I llama_context: n_ubatch      = 512
0.00.667.654 I llama_context: flash_attn    = 0
0.00.667.655 I llama_context: freq_base     = 10000.0
0.00.667.655 I llama_context: freq_scale    = 1
0.00.667.657 I ggml_metal_init: allocating
0.00.667.672 I ggml_metal_init: found device: Apple M4
0.00.667.685 I ggml_metal_init: picking default device: Apple M4
0.00.669.533 I ggml_metal_init: using embedded metal library
0.00.675.478 I ggml_metal_init: GPU name:   Apple M4
0.00.675.482 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.483 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.484 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.484 I ggml_metal_init: simdgroup reduction   = true
0.00.675.485 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.485 I ggml_metal_init: has residency sets    = true
0.00.675.485 I ggml_metal_init: has bfloat            = true
0.00.675.486 I ggml_metal_init: use bfloat            = true
0.00.675.486 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.604 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.692.608 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.752.160 I init:      Metal KV buffer size =   384.00 MiB
0.00.752.177 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.757.294 I init:      Metal compute buffer size =   102.25 MiB
0.00.757.297 I init:        CPU compute buffer size =     8.01 MiB
0.00.757.297 I init: graph nodes  = 967
0.00.757.297 I init: graph splits = 2
0.00.757.303 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.757.436 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.757.436 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.970 I main: llama threadpool init, n_threads = 4
0.00.822.017 I 
0.00.822.031 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.031 I 
0.00.822.174 I sampler seed: 1234
0.00.822.178 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.189 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.189 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.189 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.694.776 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.694.777 I llama_perf_context_print:        load time =     812.70 ms
0.01.694.778 I llama_perf_context_print: prompt eval time =      57.48 ms /     7 tokens (    8.21 ms per token,   121.78 tokens per second)
0.01.694.778 I llama_perf_context_print:        eval time =     812.22 ms /    63 runs   (   12.89 ms per token,    77.57 tokens per second)
0.01.694.778 I llama_perf_context_print:       total time =     873.53 ms /    70 tokens
0.01.698.733 I ggml_metal_free: deallocating

real	0m1.714s
user	0m0.108s
sys	0m0.240s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.587 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.844 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.302 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.316 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.317 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.318 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.318 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.320 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.321 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.321 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.322 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.322 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.324 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.324 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.408 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.723 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.725 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.726 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.726 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.727 I llama_model_loader: - type  f32:  194 tensors
0.00.053.727 I llama_model_loader: - type  f16:   98 tensors
0.00.053.728 I print_info: file format = GGUF V3 (latest)
0.00.053.729 I print_info: file type   = all F32 (guessed)
0.00.053.730 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.693 I load: special tokens cache size = 25
0.00.072.199 I load: token to piece cache size = 0.2984 MB
0.00.072.215 I print_info: arch             = gptneox
0.00.072.216 I print_info: vocab_only       = 0
0.00.072.216 I print_info: n_ctx_train      = 2048
0.00.072.216 I print_info: n_embd           = 2048
0.00.072.216 I print_info: n_layer          = 24
0.00.072.220 I print_info: n_head           = 16
0.00.072.220 I print_info: n_head_kv        = 16
0.00.072.221 I print_info: n_rot            = 32
0.00.072.221 I print_info: n_swa            = 0
0.00.072.221 I print_info: n_embd_head_k    = 128
0.00.072.221 I print_info: n_embd_head_v    = 128
0.00.072.222 I print_info: n_gqa            = 1
0.00.072.223 I print_info: n_embd_k_gqa     = 2048
0.00.072.223 I print_info: n_embd_v_gqa     = 2048
0.00.072.224 I print_info: f_norm_eps       = 1.0e-05
0.00.072.224 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.224 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.225 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.225 I print_info: f_logit_scale    = 0.0e+00
0.00.072.226 I print_info: n_ff             = 8192
0.00.072.226 I print_info: n_expert         = 0
0.00.072.226 I print_info: n_expert_used    = 0
0.00.072.226 I print_info: causal attn      = 1
0.00.072.226 I print_info: pooling type     = 0
0.00.072.226 I print_info: rope type        = 2
0.00.072.227 I print_info: rope scaling     = linear
0.00.072.227 I print_info: freq_base_train  = 10000.0
0.00.072.229 I print_info: freq_scale_train = 1
0.00.072.230 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.230 I print_info: rope_finetuned   = unknown
0.00.072.230 I print_info: ssm_d_conv       = 0
0.00.072.230 I print_info: ssm_d_inner      = 0
0.00.072.230 I print_info: ssm_d_state      = 0
0.00.072.230 I print_info: ssm_dt_rank      = 0
0.00.072.230 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.231 I print_info: model type       = 1.4B
0.00.072.231 I print_info: model params     = 1.41 B
0.00.072.231 I print_info: general.name     = 1.4B
0.00.072.233 I print_info: vocab type       = BPE
0.00.072.233 I print_info: n_vocab          = 50304
0.00.072.233 I print_info: n_merges         = 50009
0.00.072.233 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.233 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.234 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.234 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.234 I print_info: LF token         = 187 'Ċ'
0.00.072.234 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.235 I print_info: max token length = 1024
0.00.072.235 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.358.230 I load_tensors: offloading 24 repeating layers to GPU
0.01.358.234 I load_tensors: offloading output layer to GPU
0.01.358.234 I load_tensors: offloaded 25/25 layers to GPU
0.01.358.266 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.358.268 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.359.111 I llama_context: n_seq_max     = 1
0.01.359.112 I llama_context: n_ctx         = 128
0.01.359.112 I llama_context: n_ctx_per_seq = 128
0.01.359.112 I llama_context: n_batch       = 128
0.01.359.113 I llama_context: n_ubatch      = 128
0.01.359.113 I llama_context: flash_attn    = 0
0.01.359.113 I llama_context: freq_base     = 10000.0
0.01.359.113 I llama_context: freq_scale    = 1
0.01.359.114 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.359.117 I ggml_metal_init: allocating
0.01.359.173 I ggml_metal_init: found device: Apple M4
0.01.359.179 I ggml_metal_init: picking default device: Apple M4
0.01.360.278 I ggml_metal_init: using embedded metal library
0.01.363.945 I ggml_metal_init: GPU name:   Apple M4
0.01.363.947 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.363.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.363.948 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.363.948 I ggml_metal_init: simdgroup reduction   = true
0.01.363.949 I ggml_metal_init: simdgroup matrix mul. = true
0.01.363.949 I ggml_metal_init: has residency sets    = true
0.01.363.949 I ggml_metal_init: has bfloat            = true
0.01.363.949 I ggml_metal_init: use bfloat            = true
0.01.363.950 I ggml_metal_init: hasUnifiedMemory      = true
0.01.363.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.375.020 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.375.022 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.376.739 I init:      Metal KV buffer size =    24.00 MiB
0.01.376.741 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.378.378 I init:      Metal compute buffer size =    25.56 MiB
0.01.378.380 I init:        CPU compute buffer size =     1.06 MiB
0.01.378.380 I init: graph nodes  = 967
0.01.378.380 I init: graph splits = 2
0.01.378.382 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.378.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.413.438 I 
0.01.413.463 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.413.467 I perplexity: tokenizing the input ..
0.01.418.761 I perplexity: tokenization took 5.293 ms
0.01.418.768 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.537.237 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.538.966 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.538.995 I llama_perf_context_print:        load time =    1391.59 ms
0.01.538.996 I llama_perf_context_print: prompt eval time =     118.20 ms /   128 tokens (    0.92 ms per token,  1082.86 tokens per second)
0.01.538.997 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.538.997 I llama_perf_context_print:       total time =     125.56 ms /   129 tokens
0.01.539.637 I ggml_metal_free: deallocating

real	0m1.737s
user	0m0.098s
sys	0m0.264s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.264 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.029 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.333 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.340 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.342 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.343 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.351 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.351 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.352 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.354 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.354 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.630 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.630 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.631 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.632 I llama_model_loader: - type  f32:  194 tensors
0.00.027.632 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.632 I print_info: file format = GGUF V3 (latest)
0.00.027.633 I print_info: file type   = Q8_0
0.00.027.636 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.655 I load: special tokens cache size = 25
0.00.042.980 I load: token to piece cache size = 0.2984 MB
0.00.043.000 I print_info: arch             = gptneox
0.00.043.001 I print_info: vocab_only       = 0
0.00.043.001 I print_info: n_ctx_train      = 2048
0.00.043.002 I print_info: n_embd           = 2048
0.00.043.002 I print_info: n_layer          = 24
0.00.043.007 I print_info: n_head           = 16
0.00.043.007 I print_info: n_head_kv        = 16
0.00.043.007 I print_info: n_rot            = 32
0.00.043.008 I print_info: n_swa            = 0
0.00.043.008 I print_info: n_embd_head_k    = 128
0.00.043.008 I print_info: n_embd_head_v    = 128
0.00.043.009 I print_info: n_gqa            = 1
0.00.043.009 I print_info: n_embd_k_gqa     = 2048
0.00.043.010 I print_info: n_embd_v_gqa     = 2048
0.00.043.010 I print_info: f_norm_eps       = 1.0e-05
0.00.043.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.011 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.011 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.011 I print_info: f_logit_scale    = 0.0e+00
0.00.043.012 I print_info: n_ff             = 8192
0.00.043.012 I print_info: n_expert         = 0
0.00.043.012 I print_info: n_expert_used    = 0
0.00.043.012 I print_info: causal attn      = 1
0.00.043.012 I print_info: pooling type     = 0
0.00.043.012 I print_info: rope type        = 2
0.00.043.012 I print_info: rope scaling     = linear
0.00.043.013 I print_info: freq_base_train  = 10000.0
0.00.043.013 I print_info: freq_scale_train = 1
0.00.043.013 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.013 I print_info: rope_finetuned   = unknown
0.00.043.013 I print_info: ssm_d_conv       = 0
0.00.043.014 I print_info: ssm_d_inner      = 0
0.00.043.014 I print_info: ssm_d_state      = 0
0.00.043.014 I print_info: ssm_dt_rank      = 0
0.00.043.014 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.014 I print_info: model type       = 1.4B
0.00.043.014 I print_info: model params     = 1.41 B
0.00.043.014 I print_info: general.name     = 1.4B
0.00.043.015 I print_info: vocab type       = BPE
0.00.043.015 I print_info: n_vocab          = 50304
0.00.043.015 I print_info: n_merges         = 50009
0.00.043.015 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.016 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.016 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.016 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.016 I print_info: LF token         = 187 'Ċ'
0.00.043.016 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.017 I print_info: max token length = 1024
0.00.043.017 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.764.455 I load_tensors: offloading 24 repeating layers to GPU
0.00.764.461 I load_tensors: offloading output layer to GPU
0.00.764.461 I load_tensors: offloaded 25/25 layers to GPU
0.00.764.495 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.764.498 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.765.910 I llama_context: n_seq_max     = 1
0.00.765.912 I llama_context: n_ctx         = 128
0.00.765.912 I llama_context: n_ctx_per_seq = 128
0.00.765.912 I llama_context: n_batch       = 128
0.00.765.913 I llama_context: n_ubatch      = 128
0.00.765.913 I llama_context: flash_attn    = 0
0.00.765.914 I llama_context: freq_base     = 10000.0
0.00.765.915 I llama_context: freq_scale    = 1
0.00.765.915 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.765.917 I ggml_metal_init: allocating
0.00.765.987 I ggml_metal_init: found device: Apple M4
0.00.765.997 I ggml_metal_init: picking default device: Apple M4
0.00.767.472 I ggml_metal_init: using embedded metal library
0.00.772.807 I ggml_metal_init: GPU name:   Apple M4
0.00.772.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.772.810 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.772.811 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.772.811 I ggml_metal_init: simdgroup reduction   = true
0.00.772.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.772.812 I ggml_metal_init: has residency sets    = true
0.00.772.812 I ggml_metal_init: has bfloat            = true
0.00.772.812 I ggml_metal_init: use bfloat            = true
0.00.772.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.772.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.788.399 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.788.403 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.791.754 I init:      Metal KV buffer size =    24.00 MiB
0.00.791.758 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.794.709 I init:      Metal compute buffer size =    25.56 MiB
0.00.794.711 I init:        CPU compute buffer size =     1.06 MiB
0.00.794.711 I init: graph nodes  = 967
0.00.794.712 I init: graph splits = 2
0.00.794.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.794.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.820.791 I 
0.00.820.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.820.849 I perplexity: tokenizing the input ..
0.00.827.650 I perplexity: tokenization took 6.797 ms
0.00.827.661 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.965.884 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.967.298 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.967.322 I llama_perf_context_print:        load time =     809.75 ms
0.00.967.322 I llama_perf_context_print: prompt eval time =     137.27 ms /   128 tokens (    1.07 ms per token,   932.49 tokens per second)
0.00.967.323 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.967.323 I llama_perf_context_print:       total time =     146.53 ms /   129 tokens
0.00.967.866 I ggml_metal_free: deallocating

real	0m0.986s
user	0m0.079s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.255 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.099 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.511 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.520 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.522 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.522 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.523 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.523 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.524 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.524 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.526 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.527 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.591 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.589 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.591 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.591 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.592 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.592 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.593 I llama_model_loader: - type  f32:  194 tensors
0.00.026.594 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.594 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.595 I print_info: file format = GGUF V3 (latest)
0.00.026.595 I print_info: file type   = Q4_0
0.00.026.596 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.105 I load: special tokens cache size = 25
0.00.041.209 I load: token to piece cache size = 0.2984 MB
0.00.041.226 I print_info: arch             = gptneox
0.00.041.227 I print_info: vocab_only       = 0
0.00.041.228 I print_info: n_ctx_train      = 2048
0.00.041.228 I print_info: n_embd           = 2048
0.00.041.228 I print_info: n_layer          = 24
0.00.041.232 I print_info: n_head           = 16
0.00.041.233 I print_info: n_head_kv        = 16
0.00.041.233 I print_info: n_rot            = 32
0.00.041.233 I print_info: n_swa            = 0
0.00.041.233 I print_info: n_embd_head_k    = 128
0.00.041.233 I print_info: n_embd_head_v    = 128
0.00.041.234 I print_info: n_gqa            = 1
0.00.041.235 I print_info: n_embd_k_gqa     = 2048
0.00.041.235 I print_info: n_embd_v_gqa     = 2048
0.00.041.236 I print_info: f_norm_eps       = 1.0e-05
0.00.041.236 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.237 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.237 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.238 I print_info: f_logit_scale    = 0.0e+00
0.00.041.238 I print_info: n_ff             = 8192
0.00.041.238 I print_info: n_expert         = 0
0.00.041.238 I print_info: n_expert_used    = 0
0.00.041.239 I print_info: causal attn      = 1
0.00.041.239 I print_info: pooling type     = 0
0.00.041.239 I print_info: rope type        = 2
0.00.041.239 I print_info: rope scaling     = linear
0.00.041.239 I print_info: freq_base_train  = 10000.0
0.00.041.240 I print_info: freq_scale_train = 1
0.00.041.240 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.240 I print_info: rope_finetuned   = unknown
0.00.041.240 I print_info: ssm_d_conv       = 0
0.00.041.242 I print_info: ssm_d_inner      = 0
0.00.041.242 I print_info: ssm_d_state      = 0
0.00.041.242 I print_info: ssm_dt_rank      = 0
0.00.041.242 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.242 I print_info: model type       = 1.4B
0.00.041.243 I print_info: model params     = 1.41 B
0.00.041.243 I print_info: general.name     = 1.4B
0.00.041.243 I print_info: vocab type       = BPE
0.00.041.243 I print_info: n_vocab          = 50304
0.00.041.244 I print_info: n_merges         = 50009
0.00.041.244 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.244 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.244 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.244 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.244 I print_info: LF token         = 187 'Ċ'
0.00.041.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.245 I print_info: max token length = 1024
0.00.041.245 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.581.691 I load_tensors: offloading 24 repeating layers to GPU
0.00.581.708 I load_tensors: offloading output layer to GPU
0.00.581.709 I load_tensors: offloaded 25/25 layers to GPU
0.00.581.744 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.581.745 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.583.345 I llama_context: n_seq_max     = 1
0.00.583.348 I llama_context: n_ctx         = 128
0.00.583.348 I llama_context: n_ctx_per_seq = 128
0.00.583.349 I llama_context: n_batch       = 128
0.00.583.349 I llama_context: n_ubatch      = 128
0.00.583.349 I llama_context: flash_attn    = 0
0.00.583.352 I llama_context: freq_base     = 10000.0
0.00.583.352 I llama_context: freq_scale    = 1
0.00.583.353 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.583.355 I ggml_metal_init: allocating
0.00.583.436 I ggml_metal_init: found device: Apple M4
0.00.583.451 I ggml_metal_init: picking default device: Apple M4
0.00.585.275 I ggml_metal_init: using embedded metal library
0.00.590.723 I ggml_metal_init: GPU name:   Apple M4
0.00.590.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.590.737 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.590.738 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.590.739 I ggml_metal_init: simdgroup reduction   = true
0.00.590.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.590.739 I ggml_metal_init: has residency sets    = true
0.00.590.740 I ggml_metal_init: has bfloat            = true
0.00.590.740 I ggml_metal_init: use bfloat            = true
0.00.590.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.590.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.610.976 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.610.981 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.619 I init:      Metal KV buffer size =    24.00 MiB
0.00.614.626 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.617.922 I init:      Metal compute buffer size =    25.56 MiB
0.00.617.924 I init:        CPU compute buffer size =     1.06 MiB
0.00.617.925 I init: graph nodes  = 967
0.00.617.925 I init: graph splits = 2
0.00.617.929 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.617.929 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.505 I 
0.00.648.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.588 I perplexity: tokenizing the input ..
0.00.654.029 I perplexity: tokenization took 5.44 ms
0.00.654.033 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.108 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.779.459 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.779.484 I llama_perf_context_print:        load time =     638.40 ms
0.00.779.486 I llama_perf_context_print: prompt eval time =     123.84 ms /   128 tokens (    0.97 ms per token,  1033.55 tokens per second)
0.00.779.486 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.487 I llama_perf_context_print:       total time =     130.98 ms /   129 tokens
0.00.780.025 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.078s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.709 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.111 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.121 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.123 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.124 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.125 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.127 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.127 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.127 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.240 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.359 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.497 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.499 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.499 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.501 I llama_model_loader: - type  f32:  194 tensors
0.00.025.501 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.502 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.502 I print_info: file format = GGUF V3 (latest)
0.00.025.503 I print_info: file type   = Q4_1
0.00.025.505 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.077 I load: special tokens cache size = 25
0.00.040.285 I load: token to piece cache size = 0.2984 MB
0.00.040.301 I print_info: arch             = gptneox
0.00.040.302 I print_info: vocab_only       = 0
0.00.040.302 I print_info: n_ctx_train      = 2048
0.00.040.302 I print_info: n_embd           = 2048
0.00.040.303 I print_info: n_layer          = 24
0.00.040.306 I print_info: n_head           = 16
0.00.040.311 I print_info: n_head_kv        = 16
0.00.040.311 I print_info: n_rot            = 32
0.00.040.312 I print_info: n_swa            = 0
0.00.040.312 I print_info: n_embd_head_k    = 128
0.00.040.312 I print_info: n_embd_head_v    = 128
0.00.040.312 I print_info: n_gqa            = 1
0.00.040.313 I print_info: n_embd_k_gqa     = 2048
0.00.040.314 I print_info: n_embd_v_gqa     = 2048
0.00.040.314 I print_info: f_norm_eps       = 1.0e-05
0.00.040.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.315 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.315 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.315 I print_info: f_logit_scale    = 0.0e+00
0.00.040.316 I print_info: n_ff             = 8192
0.00.040.316 I print_info: n_expert         = 0
0.00.040.316 I print_info: n_expert_used    = 0
0.00.040.316 I print_info: causal attn      = 1
0.00.040.316 I print_info: pooling type     = 0
0.00.040.316 I print_info: rope type        = 2
0.00.040.317 I print_info: rope scaling     = linear
0.00.040.317 I print_info: freq_base_train  = 10000.0
0.00.040.317 I print_info: freq_scale_train = 1
0.00.040.317 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.318 I print_info: rope_finetuned   = unknown
0.00.040.318 I print_info: ssm_d_conv       = 0
0.00.040.318 I print_info: ssm_d_inner      = 0
0.00.040.318 I print_info: ssm_d_state      = 0
0.00.040.318 I print_info: ssm_dt_rank      = 0
0.00.040.318 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.320 I print_info: model type       = 1.4B
0.00.040.320 I print_info: model params     = 1.41 B
0.00.040.320 I print_info: general.name     = 1.4B
0.00.040.321 I print_info: vocab type       = BPE
0.00.040.321 I print_info: n_vocab          = 50304
0.00.040.321 I print_info: n_merges         = 50009
0.00.040.321 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.321 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.322 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.322 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.322 I print_info: LF token         = 187 'Ċ'
0.00.040.322 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.323 I print_info: max token length = 1024
0.00.040.323 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.094 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.106 I load_tensors: offloading output layer to GPU
0.00.639.107 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.141 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.639.142 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.640.922 I llama_context: n_seq_max     = 1
0.00.640.925 I llama_context: n_ctx         = 128
0.00.640.926 I llama_context: n_ctx_per_seq = 128
0.00.640.927 I llama_context: n_batch       = 128
0.00.640.927 I llama_context: n_ubatch      = 128
0.00.640.927 I llama_context: flash_attn    = 0
0.00.640.929 I llama_context: freq_base     = 10000.0
0.00.640.929 I llama_context: freq_scale    = 1
0.00.640.930 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.640.932 I ggml_metal_init: allocating
0.00.641.015 I ggml_metal_init: found device: Apple M4
0.00.641.030 I ggml_metal_init: picking default device: Apple M4
0.00.642.831 I ggml_metal_init: using embedded metal library
0.00.649.367 I ggml_metal_init: GPU name:   Apple M4
0.00.649.377 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.378 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.379 I ggml_metal_init: simdgroup reduction   = true
0.00.649.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.380 I ggml_metal_init: has residency sets    = true
0.00.649.380 I ggml_metal_init: has bfloat            = true
0.00.649.381 I ggml_metal_init: use bfloat            = true
0.00.649.382 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.394 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.385 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.668.389 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.671.964 I init:      Metal KV buffer size =    24.00 MiB
0.00.671.971 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.675.205 I init:      Metal compute buffer size =    25.56 MiB
0.00.675.206 I init:        CPU compute buffer size =     1.06 MiB
0.00.675.207 I init: graph nodes  = 967
0.00.675.207 I init: graph splits = 2
0.00.675.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.675.211 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.905 I 
0.00.703.968 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.976 I perplexity: tokenizing the input ..
0.00.710.553 I perplexity: tokenization took 6.574 ms
0.00.710.558 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.842.614 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.843.943 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.843.977 I llama_perf_context_print:        load time =     695.19 ms
0.00.843.978 I llama_perf_context_print: prompt eval time =     131.34 ms /   128 tokens (    1.03 ms per token,   974.55 tokens per second)
0.00.843.979 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.843.980 I llama_perf_context_print:       total time =     140.08 ms /   129 tokens
0.00.844.495 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.080s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.951 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.835 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.840 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.844 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.845 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.845 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.846 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.852 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.950 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.106 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.269 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.271 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.271 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.271 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.272 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.272 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.273 I llama_model_loader: - type  f32:  194 tensors
0.00.026.273 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.273 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.274 I print_info: file format = GGUF V3 (latest)
0.00.026.274 I print_info: file type   = Q5_0
0.00.026.277 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.515 I load: special tokens cache size = 25
0.00.040.731 I load: token to piece cache size = 0.2984 MB
0.00.040.748 I print_info: arch             = gptneox
0.00.040.749 I print_info: vocab_only       = 0
0.00.040.749 I print_info: n_ctx_train      = 2048
0.00.040.750 I print_info: n_embd           = 2048
0.00.040.750 I print_info: n_layer          = 24
0.00.040.754 I print_info: n_head           = 16
0.00.040.755 I print_info: n_head_kv        = 16
0.00.040.755 I print_info: n_rot            = 32
0.00.040.755 I print_info: n_swa            = 0
0.00.040.755 I print_info: n_embd_head_k    = 128
0.00.040.755 I print_info: n_embd_head_v    = 128
0.00.040.756 I print_info: n_gqa            = 1
0.00.040.756 I print_info: n_embd_k_gqa     = 2048
0.00.040.757 I print_info: n_embd_v_gqa     = 2048
0.00.040.758 I print_info: f_norm_eps       = 1.0e-05
0.00.040.761 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.761 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.761 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.761 I print_info: f_logit_scale    = 0.0e+00
0.00.040.762 I print_info: n_ff             = 8192
0.00.040.762 I print_info: n_expert         = 0
0.00.040.762 I print_info: n_expert_used    = 0
0.00.040.762 I print_info: causal attn      = 1
0.00.040.762 I print_info: pooling type     = 0
0.00.040.762 I print_info: rope type        = 2
0.00.040.763 I print_info: rope scaling     = linear
0.00.040.763 I print_info: freq_base_train  = 10000.0
0.00.040.763 I print_info: freq_scale_train = 1
0.00.040.763 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.764 I print_info: rope_finetuned   = unknown
0.00.040.764 I print_info: ssm_d_conv       = 0
0.00.040.764 I print_info: ssm_d_inner      = 0
0.00.040.764 I print_info: ssm_d_state      = 0
0.00.040.764 I print_info: ssm_dt_rank      = 0
0.00.040.764 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.764 I print_info: model type       = 1.4B
0.00.040.765 I print_info: model params     = 1.41 B
0.00.040.765 I print_info: general.name     = 1.4B
0.00.040.765 I print_info: vocab type       = BPE
0.00.040.765 I print_info: n_vocab          = 50304
0.00.040.766 I print_info: n_merges         = 50009
0.00.040.766 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.766 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.766 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.766 I print_info: LF token         = 187 'Ċ'
0.00.040.767 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.767 I print_info: max token length = 1024
0.00.040.767 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.611.616 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.631 I load_tensors: offloading output layer to GPU
0.00.611.631 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.665 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.611.666 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.613.272 I llama_context: n_seq_max     = 1
0.00.613.279 I llama_context: n_ctx         = 128
0.00.613.280 I llama_context: n_ctx_per_seq = 128
0.00.613.280 I llama_context: n_batch       = 128
0.00.613.281 I llama_context: n_ubatch      = 128
0.00.613.281 I llama_context: flash_attn    = 0
0.00.613.282 I llama_context: freq_base     = 10000.0
0.00.613.283 I llama_context: freq_scale    = 1
0.00.613.283 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.613.285 I ggml_metal_init: allocating
0.00.613.342 I ggml_metal_init: found device: Apple M4
0.00.613.358 I ggml_metal_init: picking default device: Apple M4
0.00.615.494 I ggml_metal_init: using embedded metal library
0.00.622.091 I ggml_metal_init: GPU name:   Apple M4
0.00.622.095 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.096 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.097 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.098 I ggml_metal_init: simdgroup reduction   = true
0.00.622.098 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.098 I ggml_metal_init: has residency sets    = true
0.00.622.098 I ggml_metal_init: has bfloat            = true
0.00.622.099 I ggml_metal_init: use bfloat            = true
0.00.622.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.300 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.639.304 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.962 I init:      Metal KV buffer size =    24.00 MiB
0.00.642.966 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.176 I init:      Metal compute buffer size =    25.56 MiB
0.00.646.178 I init:        CPU compute buffer size =     1.06 MiB
0.00.646.179 I init: graph nodes  = 967
0.00.646.179 I init: graph splits = 2
0.00.646.183 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.185 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.826 I 
0.00.672.887 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.896 I perplexity: tokenizing the input ..
0.00.679.361 I perplexity: tokenization took 6.462 ms
0.00.679.371 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.233 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.815.698 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.815.722 I llama_perf_context_print:        load time =     662.87 ms
0.00.815.723 I llama_perf_context_print: prompt eval time =     133.89 ms /   128 tokens (    1.05 ms per token,   955.99 tokens per second)
0.00.815.723 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.724 I llama_perf_context_print:       total time =     142.90 ms /   129 tokens
0.00.816.306 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.079s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.198 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.576 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.582 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.589 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.589 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.590 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.590 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.593 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.593 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.594 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.594 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.594 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.597 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.567 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.599 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.617 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.620 I llama_model_loader: - type  f32:  194 tensors
0.00.025.620 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.620 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.621 I print_info: file format = GGUF V3 (latest)
0.00.025.621 I print_info: file type   = Q5_1
0.00.025.623 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.769 I load: special tokens cache size = 25
0.00.039.589 I load: token to piece cache size = 0.2984 MB
0.00.039.602 I print_info: arch             = gptneox
0.00.039.603 I print_info: vocab_only       = 0
0.00.039.603 I print_info: n_ctx_train      = 2048
0.00.039.603 I print_info: n_embd           = 2048
0.00.039.604 I print_info: n_layer          = 24
0.00.039.608 I print_info: n_head           = 16
0.00.039.608 I print_info: n_head_kv        = 16
0.00.039.609 I print_info: n_rot            = 32
0.00.039.609 I print_info: n_swa            = 0
0.00.039.609 I print_info: n_embd_head_k    = 128
0.00.039.609 I print_info: n_embd_head_v    = 128
0.00.039.610 I print_info: n_gqa            = 1
0.00.039.610 I print_info: n_embd_k_gqa     = 2048
0.00.039.611 I print_info: n_embd_v_gqa     = 2048
0.00.039.611 I print_info: f_norm_eps       = 1.0e-05
0.00.039.612 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.612 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.612 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.612 I print_info: f_logit_scale    = 0.0e+00
0.00.039.615 I print_info: n_ff             = 8192
0.00.039.615 I print_info: n_expert         = 0
0.00.039.615 I print_info: n_expert_used    = 0
0.00.039.615 I print_info: causal attn      = 1
0.00.039.615 I print_info: pooling type     = 0
0.00.039.615 I print_info: rope type        = 2
0.00.039.616 I print_info: rope scaling     = linear
0.00.039.616 I print_info: freq_base_train  = 10000.0
0.00.039.616 I print_info: freq_scale_train = 1
0.00.039.616 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.617 I print_info: rope_finetuned   = unknown
0.00.039.617 I print_info: ssm_d_conv       = 0
0.00.039.617 I print_info: ssm_d_inner      = 0
0.00.039.617 I print_info: ssm_d_state      = 0
0.00.039.617 I print_info: ssm_dt_rank      = 0
0.00.039.617 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.617 I print_info: model type       = 1.4B
0.00.039.618 I print_info: model params     = 1.41 B
0.00.039.618 I print_info: general.name     = 1.4B
0.00.039.618 I print_info: vocab type       = BPE
0.00.039.619 I print_info: n_vocab          = 50304
0.00.039.619 I print_info: n_merges         = 50009
0.00.039.619 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.619 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: LF token         = 187 'Ċ'
0.00.039.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.620 I print_info: max token length = 1024
0.00.039.620 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.684.428 I load_tensors: offloading 24 repeating layers to GPU
0.00.684.439 I load_tensors: offloading output layer to GPU
0.00.684.439 I load_tensors: offloaded 25/25 layers to GPU
0.00.684.494 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.684.498 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.686.098 I llama_context: n_seq_max     = 1
0.00.686.101 I llama_context: n_ctx         = 128
0.00.686.101 I llama_context: n_ctx_per_seq = 128
0.00.686.102 I llama_context: n_batch       = 128
0.00.686.102 I llama_context: n_ubatch      = 128
0.00.686.103 I llama_context: flash_attn    = 0
0.00.686.105 I llama_context: freq_base     = 10000.0
0.00.686.105 I llama_context: freq_scale    = 1
0.00.686.106 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.686.108 I ggml_metal_init: allocating
0.00.686.244 I ggml_metal_init: found device: Apple M4
0.00.686.257 I ggml_metal_init: picking default device: Apple M4
0.00.688.300 I ggml_metal_init: using embedded metal library
0.00.694.745 I ggml_metal_init: GPU name:   Apple M4
0.00.694.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.694.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.694.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.694.753 I ggml_metal_init: simdgroup reduction   = true
0.00.694.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.694.754 I ggml_metal_init: has residency sets    = true
0.00.694.754 I ggml_metal_init: has bfloat            = true
0.00.694.755 I ggml_metal_init: use bfloat            = true
0.00.694.756 I ggml_metal_init: hasUnifiedMemory      = true
0.00.694.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.282 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.712.286 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.846 I init:      Metal KV buffer size =    24.00 MiB
0.00.715.853 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.719.049 I init:      Metal compute buffer size =    25.56 MiB
0.00.719.051 I init:        CPU compute buffer size =     1.06 MiB
0.00.719.051 I init: graph nodes  = 967
0.00.719.052 I init: graph splits = 2
0.00.719.056 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.719.058 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.123 I 
0.00.751.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.191 I perplexity: tokenizing the input ..
0.00.758.587 I perplexity: tokenization took 7.393 ms
0.00.758.594 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.906.156 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.907.503 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.907.524 I llama_perf_context_print:        load time =     741.92 ms
0.00.907.525 I llama_perf_context_print: prompt eval time =     146.61 ms /   128 tokens (    1.15 ms per token,   873.05 tokens per second)
0.00.907.526 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.907.526 I llama_perf_context_print:       total time =     156.41 ms /   129 tokens
0.00.908.116 I ggml_metal_free: deallocating

real	0m0.922s
user	0m0.080s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.001 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.009 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.010 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.010 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.013 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.014 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.014 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.015 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.017 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.017 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.018 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.205 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.463 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.464 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.465 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.465 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.465 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.466 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.467 I llama_model_loader: - type  f32:  194 tensors
0.00.026.467 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.467 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.467 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.468 I print_info: file format = GGUF V3 (latest)
0.00.026.469 I print_info: file type   = Q2_K - Medium
0.00.026.470 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.023 I load: special tokens cache size = 25
0.00.041.358 I load: token to piece cache size = 0.2984 MB
0.00.041.376 I print_info: arch             = gptneox
0.00.041.376 I print_info: vocab_only       = 0
0.00.041.377 I print_info: n_ctx_train      = 2048
0.00.041.377 I print_info: n_embd           = 2048
0.00.041.377 I print_info: n_layer          = 24
0.00.041.380 I print_info: n_head           = 16
0.00.041.381 I print_info: n_head_kv        = 16
0.00.041.381 I print_info: n_rot            = 32
0.00.041.381 I print_info: n_swa            = 0
0.00.041.381 I print_info: n_embd_head_k    = 128
0.00.041.381 I print_info: n_embd_head_v    = 128
0.00.041.382 I print_info: n_gqa            = 1
0.00.041.383 I print_info: n_embd_k_gqa     = 2048
0.00.041.383 I print_info: n_embd_v_gqa     = 2048
0.00.041.384 I print_info: f_norm_eps       = 1.0e-05
0.00.041.384 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.385 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.385 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.385 I print_info: f_logit_scale    = 0.0e+00
0.00.041.388 I print_info: n_ff             = 8192
0.00.041.388 I print_info: n_expert         = 0
0.00.041.388 I print_info: n_expert_used    = 0
0.00.041.389 I print_info: causal attn      = 1
0.00.041.389 I print_info: pooling type     = 0
0.00.041.389 I print_info: rope type        = 2
0.00.041.389 I print_info: rope scaling     = linear
0.00.041.389 I print_info: freq_base_train  = 10000.0
0.00.041.390 I print_info: freq_scale_train = 1
0.00.041.391 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.391 I print_info: rope_finetuned   = unknown
0.00.041.391 I print_info: ssm_d_conv       = 0
0.00.041.391 I print_info: ssm_d_inner      = 0
0.00.041.391 I print_info: ssm_d_state      = 0
0.00.041.392 I print_info: ssm_dt_rank      = 0
0.00.041.392 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.392 I print_info: model type       = 1.4B
0.00.041.392 I print_info: model params     = 1.41 B
0.00.041.392 I print_info: general.name     = 1.4B
0.00.041.393 I print_info: vocab type       = BPE
0.00.041.393 I print_info: n_vocab          = 50304
0.00.041.393 I print_info: n_merges         = 50009
0.00.041.393 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.394 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.394 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.394 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.394 I print_info: LF token         = 187 'Ċ'
0.00.041.394 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.394 I print_info: max token length = 1024
0.00.041.395 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.396.194 I load_tensors: offloading 24 repeating layers to GPU
0.00.396.209 I load_tensors: offloading output layer to GPU
0.00.396.209 I load_tensors: offloaded 25/25 layers to GPU
0.00.396.245 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.396.247 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.397.314 I llama_context: n_seq_max     = 1
0.00.397.317 I llama_context: n_ctx         = 128
0.00.397.317 I llama_context: n_ctx_per_seq = 128
0.00.397.318 I llama_context: n_batch       = 128
0.00.397.318 I llama_context: n_ubatch      = 128
0.00.397.318 I llama_context: flash_attn    = 0
0.00.397.321 I llama_context: freq_base     = 10000.0
0.00.397.321 I llama_context: freq_scale    = 1
0.00.397.322 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.397.324 I ggml_metal_init: allocating
0.00.397.417 I ggml_metal_init: found device: Apple M4
0.00.397.432 I ggml_metal_init: picking default device: Apple M4
0.00.399.408 I ggml_metal_init: using embedded metal library
0.00.405.040 I ggml_metal_init: GPU name:   Apple M4
0.00.405.055 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.405.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.405.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.405.058 I ggml_metal_init: simdgroup reduction   = true
0.00.405.058 I ggml_metal_init: simdgroup matrix mul. = true
0.00.405.059 I ggml_metal_init: has residency sets    = true
0.00.405.059 I ggml_metal_init: has bfloat            = true
0.00.405.059 I ggml_metal_init: use bfloat            = true
0.00.405.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.405.066 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.426.331 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.426.336 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.430.100 I init:      Metal KV buffer size =    24.00 MiB
0.00.430.106 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.433.289 I init:      Metal compute buffer size =    25.56 MiB
0.00.433.291 I init:        CPU compute buffer size =     1.06 MiB
0.00.433.292 I init: graph nodes  = 967
0.00.433.292 I init: graph splits = 2
0.00.433.296 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.433.296 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.463.975 I 
0.00.464.032 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.464.038 I perplexity: tokenizing the input ..
0.00.471.304 I perplexity: tokenization took 7.262 ms
0.00.471.312 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.616.207 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.617.565 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.617.584 I llama_perf_context_print:        load time =     454.02 ms
0.00.617.585 I llama_perf_context_print: prompt eval time =     144.02 ms /   128 tokens (    1.13 ms per token,   888.74 tokens per second)
0.00.617.586 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.617.586 I llama_perf_context_print:       total time =     153.61 ms /   129 tokens
0.00.618.122 I ggml_metal_free: deallocating

real	0m0.632s
user	0m0.082s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.018 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.025 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.027 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.027 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.031 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.032 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.034 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.035 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.040 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.041 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.041 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.144 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.147 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.148 I llama_model_loader: - type  f32:  194 tensors
0.00.025.148 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.148 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.149 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.149 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.150 I print_info: file format = GGUF V3 (latest)
0.00.025.151 I print_info: file type   = Q3_K - Medium
0.00.025.152 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.249 I load: special tokens cache size = 25
0.00.039.241 I load: token to piece cache size = 0.2984 MB
0.00.039.257 I print_info: arch             = gptneox
0.00.039.258 I print_info: vocab_only       = 0
0.00.039.258 I print_info: n_ctx_train      = 2048
0.00.039.258 I print_info: n_embd           = 2048
0.00.039.259 I print_info: n_layer          = 24
0.00.039.263 I print_info: n_head           = 16
0.00.039.264 I print_info: n_head_kv        = 16
0.00.039.264 I print_info: n_rot            = 32
0.00.039.264 I print_info: n_swa            = 0
0.00.039.265 I print_info: n_embd_head_k    = 128
0.00.039.265 I print_info: n_embd_head_v    = 128
0.00.039.265 I print_info: n_gqa            = 1
0.00.039.266 I print_info: n_embd_k_gqa     = 2048
0.00.039.266 I print_info: n_embd_v_gqa     = 2048
0.00.039.267 I print_info: f_norm_eps       = 1.0e-05
0.00.039.270 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.270 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.270 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.270 I print_info: f_logit_scale    = 0.0e+00
0.00.039.271 I print_info: n_ff             = 8192
0.00.039.271 I print_info: n_expert         = 0
0.00.039.271 I print_info: n_expert_used    = 0
0.00.039.271 I print_info: causal attn      = 1
0.00.039.272 I print_info: pooling type     = 0
0.00.039.274 I print_info: rope type        = 2
0.00.039.276 I print_info: rope scaling     = linear
0.00.039.276 I print_info: freq_base_train  = 10000.0
0.00.039.276 I print_info: freq_scale_train = 1
0.00.039.276 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.277 I print_info: rope_finetuned   = unknown
0.00.039.277 I print_info: ssm_d_conv       = 0
0.00.039.277 I print_info: ssm_d_inner      = 0
0.00.039.277 I print_info: ssm_d_state      = 0
0.00.039.277 I print_info: ssm_dt_rank      = 0
0.00.039.277 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.277 I print_info: model type       = 1.4B
0.00.039.278 I print_info: model params     = 1.41 B
0.00.039.278 I print_info: general.name     = 1.4B
0.00.039.278 I print_info: vocab type       = BPE
0.00.039.278 I print_info: n_vocab          = 50304
0.00.039.278 I print_info: n_merges         = 50009
0.00.039.279 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: LF token         = 187 'Ċ'
0.00.039.280 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.280 I print_info: max token length = 1024
0.00.039.280 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.886 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.895 I load_tensors: offloading output layer to GPU
0.00.445.895 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.927 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.931 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.447.593 I llama_context: n_seq_max     = 1
0.00.447.598 I llama_context: n_ctx         = 128
0.00.447.598 I llama_context: n_ctx_per_seq = 128
0.00.447.599 I llama_context: n_batch       = 128
0.00.447.599 I llama_context: n_ubatch      = 128
0.00.447.600 I llama_context: flash_attn    = 0
0.00.447.601 I llama_context: freq_base     = 10000.0
0.00.447.601 I llama_context: freq_scale    = 1
0.00.447.602 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.447.604 I ggml_metal_init: allocating
0.00.447.657 I ggml_metal_init: found device: Apple M4
0.00.447.670 I ggml_metal_init: picking default device: Apple M4
0.00.449.835 I ggml_metal_init: using embedded metal library
0.00.455.848 I ggml_metal_init: GPU name:   Apple M4
0.00.455.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.455.854 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.455.855 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.455.856 I ggml_metal_init: simdgroup reduction   = true
0.00.455.856 I ggml_metal_init: simdgroup matrix mul. = true
0.00.455.856 I ggml_metal_init: has residency sets    = true
0.00.455.857 I ggml_metal_init: has bfloat            = true
0.00.455.857 I ggml_metal_init: use bfloat            = true
0.00.455.858 I ggml_metal_init: hasUnifiedMemory      = true
0.00.455.860 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.129 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.475.134 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.478.737 I init:      Metal KV buffer size =    24.00 MiB
0.00.478.745 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.482.081 I init:      Metal compute buffer size =    25.56 MiB
0.00.482.083 I init:        CPU compute buffer size =     1.06 MiB
0.00.482.084 I init: graph nodes  = 967
0.00.482.084 I init: graph splits = 2
0.00.482.088 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.482.088 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.512.440 I 
0.00.512.501 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.512.508 I perplexity: tokenizing the input ..
0.00.519.707 I perplexity: tokenization took 7.198 ms
0.00.519.718 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.657.857 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.659.362 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.659.389 I llama_perf_context_print:        load time =     503.59 ms
0.00.659.390 I llama_perf_context_print: prompt eval time =     137.81 ms /   128 tokens (    1.08 ms per token,   928.82 tokens per second)
0.00.659.391 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.659.391 I llama_perf_context_print:       total time =     146.95 ms /   129 tokens
0.00.659.933 I ggml_metal_free: deallocating

real	0m0.673s
user	0m0.078s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.713 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.715 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.715 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.716 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.716 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.716 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.717 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.718 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.718 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.719 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.721 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.722 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.737 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.848 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.852 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.853 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.853 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.854 I llama_model_loader: - type  f32:  194 tensors
0.00.024.855 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.855 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.855 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.856 I print_info: file format = GGUF V3 (latest)
0.00.024.856 I print_info: file type   = Q4_K - Medium
0.00.024.857 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.859 I load: special tokens cache size = 25
0.00.038.972 I load: token to piece cache size = 0.2984 MB
0.00.038.989 I print_info: arch             = gptneox
0.00.038.990 I print_info: vocab_only       = 0
0.00.038.990 I print_info: n_ctx_train      = 2048
0.00.038.990 I print_info: n_embd           = 2048
0.00.038.990 I print_info: n_layer          = 24
0.00.038.995 I print_info: n_head           = 16
0.00.038.996 I print_info: n_head_kv        = 16
0.00.038.996 I print_info: n_rot            = 32
0.00.038.996 I print_info: n_swa            = 0
0.00.038.996 I print_info: n_embd_head_k    = 128
0.00.038.996 I print_info: n_embd_head_v    = 128
0.00.038.997 I print_info: n_gqa            = 1
0.00.038.997 I print_info: n_embd_k_gqa     = 2048
0.00.038.999 I print_info: n_embd_v_gqa     = 2048
0.00.039.000 I print_info: f_norm_eps       = 1.0e-05
0.00.039.000 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.000 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.001 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.001 I print_info: f_logit_scale    = 0.0e+00
0.00.039.001 I print_info: n_ff             = 8192
0.00.039.001 I print_info: n_expert         = 0
0.00.039.002 I print_info: n_expert_used    = 0
0.00.039.002 I print_info: causal attn      = 1
0.00.039.002 I print_info: pooling type     = 0
0.00.039.002 I print_info: rope type        = 2
0.00.039.002 I print_info: rope scaling     = linear
0.00.039.004 I print_info: freq_base_train  = 10000.0
0.00.039.005 I print_info: freq_scale_train = 1
0.00.039.005 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.005 I print_info: rope_finetuned   = unknown
0.00.039.005 I print_info: ssm_d_conv       = 0
0.00.039.005 I print_info: ssm_d_inner      = 0
0.00.039.005 I print_info: ssm_d_state      = 0
0.00.039.006 I print_info: ssm_dt_rank      = 0
0.00.039.006 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.006 I print_info: model type       = 1.4B
0.00.039.006 I print_info: model params     = 1.41 B
0.00.039.006 I print_info: general.name     = 1.4B
0.00.039.007 I print_info: vocab type       = BPE
0.00.039.007 I print_info: n_vocab          = 50304
0.00.039.007 I print_info: n_merges         = 50009
0.00.039.007 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.007 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.008 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.008 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.008 I print_info: LF token         = 187 'Ċ'
0.00.039.008 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.009 I print_info: max token length = 1024
0.00.039.009 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.076 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.091 I load_tensors: offloading output layer to GPU
0.00.528.092 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.128 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.129 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.529.901 I llama_context: n_seq_max     = 1
0.00.529.903 I llama_context: n_ctx         = 128
0.00.529.904 I llama_context: n_ctx_per_seq = 128
0.00.529.905 I llama_context: n_batch       = 128
0.00.529.905 I llama_context: n_ubatch      = 128
0.00.529.905 I llama_context: flash_attn    = 0
0.00.529.908 I llama_context: freq_base     = 10000.0
0.00.529.908 I llama_context: freq_scale    = 1
0.00.529.909 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.529.911 I ggml_metal_init: allocating
0.00.530.007 I ggml_metal_init: found device: Apple M4
0.00.530.021 I ggml_metal_init: picking default device: Apple M4
0.00.531.939 I ggml_metal_init: using embedded metal library
0.00.538.209 I ggml_metal_init: GPU name:   Apple M4
0.00.538.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.218 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.220 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.220 I ggml_metal_init: simdgroup reduction   = true
0.00.538.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.221 I ggml_metal_init: has residency sets    = true
0.00.538.221 I ggml_metal_init: has bfloat            = true
0.00.538.221 I ggml_metal_init: use bfloat            = true
0.00.538.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.226 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.993 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.556.998 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.560.597 I init:      Metal KV buffer size =    24.00 MiB
0.00.560.601 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.563.866 I init:      Metal compute buffer size =    25.56 MiB
0.00.563.868 I init:        CPU compute buffer size =     1.06 MiB
0.00.563.868 I init: graph nodes  = 967
0.00.563.869 I init: graph splits = 2
0.00.563.873 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.563.874 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.235 I 
0.00.591.298 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.304 I perplexity: tokenizing the input ..
0.00.597.962 I perplexity: tokenization took 6.655 ms
0.00.597.967 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.739.018 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.740.358 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.740.387 I llama_perf_context_print:        load time =     582.30 ms
0.00.740.388 I llama_perf_context_print: prompt eval time =     140.48 ms /   128 tokens (    1.10 ms per token,   911.16 tokens per second)
0.00.740.389 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.740.389 I llama_perf_context_print:       total time =     149.16 ms /   129 tokens
0.00.740.930 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.079s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.916 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.991 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.997 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.999 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.999 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.000 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.000 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.000 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.001 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.002 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.002 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.003 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.003 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.003 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.004 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.006 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.006 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.006 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.098 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.146 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.178 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.180 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.180 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.181 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.181 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.182 I llama_model_loader: - type  f32:  194 tensors
0.00.026.182 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.182 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.183 I print_info: file format = GGUF V3 (latest)
0.00.026.184 I print_info: file type   = Q5_K - Medium
0.00.026.185 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.217 I load: special tokens cache size = 25
0.00.040.176 I load: token to piece cache size = 0.2984 MB
0.00.040.193 I print_info: arch             = gptneox
0.00.040.194 I print_info: vocab_only       = 0
0.00.040.194 I print_info: n_ctx_train      = 2048
0.00.040.194 I print_info: n_embd           = 2048
0.00.040.194 I print_info: n_layer          = 24
0.00.040.198 I print_info: n_head           = 16
0.00.040.199 I print_info: n_head_kv        = 16
0.00.040.199 I print_info: n_rot            = 32
0.00.040.199 I print_info: n_swa            = 0
0.00.040.199 I print_info: n_embd_head_k    = 128
0.00.040.199 I print_info: n_embd_head_v    = 128
0.00.040.200 I print_info: n_gqa            = 1
0.00.040.201 I print_info: n_embd_k_gqa     = 2048
0.00.040.201 I print_info: n_embd_v_gqa     = 2048
0.00.040.202 I print_info: f_norm_eps       = 1.0e-05
0.00.040.202 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.202 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.202 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.203 I print_info: f_logit_scale    = 0.0e+00
0.00.040.203 I print_info: n_ff             = 8192
0.00.040.203 I print_info: n_expert         = 0
0.00.040.203 I print_info: n_expert_used    = 0
0.00.040.204 I print_info: causal attn      = 1
0.00.040.204 I print_info: pooling type     = 0
0.00.040.204 I print_info: rope type        = 2
0.00.040.204 I print_info: rope scaling     = linear
0.00.040.208 I print_info: freq_base_train  = 10000.0
0.00.040.208 I print_info: freq_scale_train = 1
0.00.040.208 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.209 I print_info: rope_finetuned   = unknown
0.00.040.209 I print_info: ssm_d_conv       = 0
0.00.040.209 I print_info: ssm_d_inner      = 0
0.00.040.209 I print_info: ssm_d_state      = 0
0.00.040.209 I print_info: ssm_dt_rank      = 0
0.00.040.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.209 I print_info: model type       = 1.4B
0.00.040.210 I print_info: model params     = 1.41 B
0.00.040.210 I print_info: general.name     = 1.4B
0.00.040.210 I print_info: vocab type       = BPE
0.00.040.211 I print_info: n_vocab          = 50304
0.00.040.211 I print_info: n_merges         = 50009
0.00.040.211 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.211 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.211 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.211 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.212 I print_info: LF token         = 187 'Ċ'
0.00.040.213 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.213 I print_info: max token length = 1024
0.00.040.214 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.597.928 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.932 I load_tensors: offloading output layer to GPU
0.00.597.934 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.958 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.960 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.460 I llama_context: n_seq_max     = 1
0.00.599.462 I llama_context: n_ctx         = 128
0.00.599.462 I llama_context: n_ctx_per_seq = 128
0.00.599.463 I llama_context: n_batch       = 128
0.00.599.463 I llama_context: n_ubatch      = 128
0.00.599.464 I llama_context: flash_attn    = 0
0.00.599.464 I llama_context: freq_base     = 10000.0
0.00.599.465 I llama_context: freq_scale    = 1
0.00.599.466 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.467 I ggml_metal_init: allocating
0.00.599.480 I ggml_metal_init: found device: Apple M4
0.00.599.488 I ggml_metal_init: picking default device: Apple M4
0.00.600.926 I ggml_metal_init: using embedded metal library
0.00.607.060 I ggml_metal_init: GPU name:   Apple M4
0.00.607.063 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.064 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.066 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.070 I ggml_metal_init: simdgroup reduction   = true
0.00.607.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.070 I ggml_metal_init: has residency sets    = true
0.00.607.070 I ggml_metal_init: has bfloat            = true
0.00.607.071 I ggml_metal_init: use bfloat            = true
0.00.607.072 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.083 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.071 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.624.075 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.468 I init:      Metal KV buffer size =    24.00 MiB
0.00.627.471 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.580 I init:      Metal compute buffer size =    25.56 MiB
0.00.630.581 I init:        CPU compute buffer size =     1.06 MiB
0.00.630.582 I init: graph nodes  = 967
0.00.630.582 I init: graph splits = 2
0.00.630.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.630.586 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.731 I 
0.00.666.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.804 I perplexity: tokenizing the input ..
0.00.673.684 I perplexity: tokenization took 6.878 ms
0.00.673.691 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.407 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.824.764 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.824.782 I llama_perf_context_print:        load time =     656.81 ms
0.00.824.783 I llama_perf_context_print: prompt eval time =     148.77 ms /   128 tokens (    1.16 ms per token,   860.39 tokens per second)
0.00.824.784 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.784 I llama_perf_context_print:       total time =     158.06 ms /   129 tokens
0.00.825.377 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.078s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.972 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.981 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.982 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.982 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.982 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.983 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.984 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.984 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.984 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.985 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.985 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.987 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.988 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.989 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.989 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.160 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.162 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.163 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.164 I llama_model_loader: - type  f32:  194 tensors
0.00.025.164 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.165 I print_info: file format = GGUF V3 (latest)
0.00.025.166 I print_info: file type   = Q6_K
0.00.025.167 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.677 I load: special tokens cache size = 25
0.00.039.707 I load: token to piece cache size = 0.2984 MB
0.00.039.723 I print_info: arch             = gptneox
0.00.039.724 I print_info: vocab_only       = 0
0.00.039.725 I print_info: n_ctx_train      = 2048
0.00.039.725 I print_info: n_embd           = 2048
0.00.039.725 I print_info: n_layer          = 24
0.00.039.729 I print_info: n_head           = 16
0.00.039.730 I print_info: n_head_kv        = 16
0.00.039.730 I print_info: n_rot            = 32
0.00.039.730 I print_info: n_swa            = 0
0.00.039.730 I print_info: n_embd_head_k    = 128
0.00.039.733 I print_info: n_embd_head_v    = 128
0.00.039.734 I print_info: n_gqa            = 1
0.00.039.735 I print_info: n_embd_k_gqa     = 2048
0.00.039.735 I print_info: n_embd_v_gqa     = 2048
0.00.039.736 I print_info: f_norm_eps       = 1.0e-05
0.00.039.737 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.737 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.737 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.737 I print_info: f_logit_scale    = 0.0e+00
0.00.039.738 I print_info: n_ff             = 8192
0.00.039.738 I print_info: n_expert         = 0
0.00.039.738 I print_info: n_expert_used    = 0
0.00.039.738 I print_info: causal attn      = 1
0.00.039.739 I print_info: pooling type     = 0
0.00.039.739 I print_info: rope type        = 2
0.00.039.739 I print_info: rope scaling     = linear
0.00.039.740 I print_info: freq_base_train  = 10000.0
0.00.039.740 I print_info: freq_scale_train = 1
0.00.039.741 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.741 I print_info: rope_finetuned   = unknown
0.00.039.741 I print_info: ssm_d_conv       = 0
0.00.039.742 I print_info: ssm_d_inner      = 0
0.00.039.742 I print_info: ssm_d_state      = 0
0.00.039.742 I print_info: ssm_dt_rank      = 0
0.00.039.742 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.743 I print_info: model type       = 1.4B
0.00.039.743 I print_info: model params     = 1.41 B
0.00.039.744 I print_info: general.name     = 1.4B
0.00.039.744 I print_info: vocab type       = BPE
0.00.039.744 I print_info: n_vocab          = 50304
0.00.039.745 I print_info: n_merges         = 50009
0.00.039.745 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.745 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.745 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.745 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.746 I print_info: LF token         = 187 'Ċ'
0.00.039.746 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: max token length = 1024
0.00.039.747 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.558.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.558.147 I load_tensors: offloading output layer to GPU
0.00.558.148 I load_tensors: offloaded 25/25 layers to GPU
0.00.558.182 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.558.183 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.559.769 I llama_context: n_seq_max     = 1
0.00.559.774 I llama_context: n_ctx         = 128
0.00.559.774 I llama_context: n_ctx_per_seq = 128
0.00.559.775 I llama_context: n_batch       = 128
0.00.559.776 I llama_context: n_ubatch      = 128
0.00.559.776 I llama_context: flash_attn    = 0
0.00.559.779 I llama_context: freq_base     = 10000.0
0.00.559.779 I llama_context: freq_scale    = 1
0.00.559.780 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.559.785 I ggml_metal_init: allocating
0.00.559.877 I ggml_metal_init: found device: Apple M4
0.00.559.891 I ggml_metal_init: picking default device: Apple M4
0.00.561.768 I ggml_metal_init: using embedded metal library
0.00.568.136 I ggml_metal_init: GPU name:   Apple M4
0.00.568.140 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.568.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.568.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.568.142 I ggml_metal_init: simdgroup reduction   = true
0.00.568.142 I ggml_metal_init: simdgroup matrix mul. = true
0.00.568.142 I ggml_metal_init: has residency sets    = true
0.00.568.143 I ggml_metal_init: has bfloat            = true
0.00.568.143 I ggml_metal_init: use bfloat            = true
0.00.568.144 I ggml_metal_init: hasUnifiedMemory      = true
0.00.568.146 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.585.218 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.585.223 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.588.868 I init:      Metal KV buffer size =    24.00 MiB
0.00.588.871 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.592.000 I init:      Metal compute buffer size =    25.56 MiB
0.00.592.002 I init:        CPU compute buffer size =     1.06 MiB
0.00.592.003 I init: graph nodes  = 967
0.00.592.003 I init: graph splits = 2
0.00.592.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.592.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.071 I 
0.00.630.134 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.146 I perplexity: tokenizing the input ..
0.00.636.693 I perplexity: tokenization took 6.549 ms
0.00.636.699 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.767.325 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.768.654 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.768.679 I llama_perf_context_print:        load time =     621.17 ms
0.00.768.680 I llama_perf_context_print: prompt eval time =     130.40 ms /   128 tokens (    1.02 ms per token,   981.61 tokens per second)
0.00.768.680 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.768.681 I llama_perf_context_print:       total time =     138.61 ms /   129 tokens
0.00.769.277 I ggml_metal_free: deallocating

real	0m0.783s
user	0m0.079s
sys	0m0.144s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4793 (bc6f187e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.474 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.369 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.378 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.387 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.388 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.390 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.390 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.392 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.393 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.393 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.397 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.397 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.398 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.399 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.402 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.583 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.329 I llama_model_loader: - type  f32:  194 tensors
0.00.055.329 I llama_model_loader: - type  f16:   98 tensors
0.00.055.330 I print_info: file format = GGUF V3 (latest)
0.00.055.331 I print_info: file type   = all F32 (guessed)
0.00.055.332 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.914 I load: special tokens cache size = 25
0.00.074.592 I load: token to piece cache size = 0.2984 MB
0.00.074.607 I print_info: arch             = gptneox
0.00.074.608 I print_info: vocab_only       = 0
0.00.074.609 I print_info: n_ctx_train      = 2048
0.00.074.609 I print_info: n_embd           = 2048
0.00.074.609 I print_info: n_layer          = 24
0.00.074.612 I print_info: n_head           = 16
0.00.074.612 I print_info: n_head_kv        = 16
0.00.074.613 I print_info: n_rot            = 32
0.00.074.613 I print_info: n_swa            = 0
0.00.074.615 I print_info: n_embd_head_k    = 128
0.00.074.616 I print_info: n_embd_head_v    = 128
0.00.074.616 I print_info: n_gqa            = 1
0.00.074.619 I print_info: n_embd_k_gqa     = 2048
0.00.074.619 I print_info: n_embd_v_gqa     = 2048
0.00.074.620 I print_info: f_norm_eps       = 1.0e-05
0.00.074.620 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.620 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.621 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.622 I print_info: f_logit_scale    = 0.0e+00
0.00.074.623 I print_info: n_ff             = 8192
0.00.074.623 I print_info: n_expert         = 0
0.00.074.623 I print_info: n_expert_used    = 0
0.00.074.623 I print_info: causal attn      = 1
0.00.074.624 I print_info: pooling type     = 0
0.00.074.624 I print_info: rope type        = 2
0.00.074.624 I print_info: rope scaling     = linear
0.00.074.625 I print_info: freq_base_train  = 10000.0
0.00.074.626 I print_info: freq_scale_train = 1
0.00.074.626 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.626 I print_info: rope_finetuned   = unknown
0.00.074.627 I print_info: ssm_d_conv       = 0
0.00.074.627 I print_info: ssm_d_inner      = 0
0.00.074.627 I print_info: ssm_d_state      = 0
0.00.074.628 I print_info: ssm_dt_rank      = 0
0.00.074.628 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.628 I print_info: model type       = 1.4B
0.00.074.628 I print_info: model params     = 1.41 B
0.00.074.629 I print_info: general.name     = 1.4B
0.00.074.629 I print_info: vocab type       = BPE
0.00.074.629 I print_info: n_vocab          = 50304
0.00.074.630 I print_info: n_merges         = 50009
0.00.074.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.632 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.632 I print_info: LF token         = 187 'Ċ'
0.00.074.632 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.632 I print_info: max token length = 1024
0.00.074.633 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.342.812 I load_tensors: offloading 24 repeating layers to GPU
0.01.342.818 I load_tensors: offloading output layer to GPU
0.01.342.818 I load_tensors: offloaded 25/25 layers to GPU
0.01.342.843 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.342.844 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.343.773 I llama_context: n_seq_max     = 1
0.01.343.774 I llama_context: n_ctx         = 128
0.01.343.774 I llama_context: n_ctx_per_seq = 128
0.01.343.774 I llama_context: n_batch       = 128
0.01.343.775 I llama_context: n_ubatch      = 128
0.01.343.775 I llama_context: flash_attn    = 0
0.01.343.776 I llama_context: freq_base     = 10000.0
0.01.343.776 I llama_context: freq_scale    = 1
0.01.343.777 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.343.777 I ggml_metal_init: allocating
0.01.343.816 I ggml_metal_init: found device: Apple M4
0.01.343.822 I ggml_metal_init: picking default device: Apple M4
0.01.344.916 I ggml_metal_init: using embedded metal library
0.01.349.158 I ggml_metal_init: GPU name:   Apple M4
0.01.349.161 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.349.161 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.349.162 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.349.162 I ggml_metal_init: simdgroup reduction   = true
0.01.349.162 I ggml_metal_init: simdgroup matrix mul. = true
0.01.349.162 I ggml_metal_init: has residency sets    = true
0.01.349.162 I ggml_metal_init: has bfloat            = true
0.01.349.163 I ggml_metal_init: use bfloat            = true
0.01.349.163 I ggml_metal_init: hasUnifiedMemory      = true
0.01.349.164 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.360.405 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.360.407 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.362.209 I init:      Metal KV buffer size =    24.00 MiB
0.01.362.212 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.363.866 I init:      Metal compute buffer size =    25.56 MiB
0.01.363.868 I init:        CPU compute buffer size =     1.06 MiB
0.01.363.868 I init: graph nodes  = 967
0.01.363.868 I init: graph splits = 2
0.01.363.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.363.870 I 
0.01.363.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.363.898 I compute_imatrix: tokenizing the input ..
0.01.368.154 I compute_imatrix: tokenization took 4.255 ms
0.01.368.156 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.631.014 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.634.525 I llama_perf_context_print:        load time =    1607.53 ms
0.01.634.526 I llama_perf_context_print: prompt eval time =     261.10 ms /   128 tokens (    2.04 ms per token,   490.23 tokens per second)
0.01.634.526 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.634.527 I llama_perf_context_print:       total time =    1611.04 ms /   129 tokens
0.01.635.345 I ggml_metal_free: deallocating

real	0m1.821s
user	0m0.126s
sys	0m0.266s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4793 (bc6f187e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142c05f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142c06680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142c06c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142c071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142c07790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142c07d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142c082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142c088a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142c08e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142c09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142c09850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142c09d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142c0a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142c0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142c0b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142c0bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142c0c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142c0cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142c0d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142c0dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142c0e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142c0eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142c0f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142c0fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142c101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142c10460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142c10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142c116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142c11c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142c11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142c12380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142c12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142c12ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142c13410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142c136d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142c13b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142c14010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142c144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142c14950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142c14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142c15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142c15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142c15bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142c16070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142c16330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142c16940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142c16f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142c17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142c17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142c18490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142c18aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142c190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142c196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142c19cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142c1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142c1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142c1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142c1b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142c1b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142c1bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142c1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142c1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142c1cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142c1cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142c1d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142c1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142c1dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142c1e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142c1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142c1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142c1efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142c1f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142c1f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142c1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142c203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142c208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142c20e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142c21390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142c218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142c21e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142c22380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142c228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142c22e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142c23370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142c238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142c23e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142c24360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142c248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142c24e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142c25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142c258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142c25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142c26340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142c26890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142c26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142c27330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142c27880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142c17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142c27cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142c284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142c289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142c28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142c29490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142c299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142c29f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142c2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142c2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142c2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142c2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142c2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142c2bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142c2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142c2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142c2ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142c2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142c2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142c2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142c2e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142c2e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142c2ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142c2eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142c2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142c2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142c2fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142c30130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142c305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142c30a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142c30f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142c313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142c31850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142c31cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142c32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142c32630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142c32ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142c32f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142c33410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142c338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142c33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142c341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142c34690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142c34b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142c34fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142c35470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142c35910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142c35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142c36250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142c366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142c36b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142c37030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142c374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142c37970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142c37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142c382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142c38750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142c38bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142c39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142c39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142c399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142c39e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142c3a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142c3a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142c3ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142c3b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142c3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142c3ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142c3bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142c3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142c3c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142c3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142c3d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142c3d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142c3da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142c3df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142c3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142c3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142c3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142c3f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142c3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142c3faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142c3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142c40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142c408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142c40d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142c41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142c416b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142c41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142c41ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142c42490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142c42930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142c42dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142c43270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142c43710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142c43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142c44100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142c44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142c44ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142c450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142c453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142c459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142c45fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142c465e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142c46dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x142c47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142c47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142c47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142c48150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142c48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142c48de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142c49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142c49720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142c49ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142c4a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142c4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142c4aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142c4b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142c4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142c4beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142c4c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142c4c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142c4cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142c4d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142c4d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142c4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142c4e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142c4e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142c4ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142c4f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142c4f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142c4fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142c503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142c50910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142c50e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142c513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142c51900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142c51e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142c523a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142c528f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142c52e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142c53390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142c538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142c53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142c54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142c548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142c54e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142c55370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142c558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142c55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142c56360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142c568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142c56e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142c57350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142c578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142c57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142c58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142c58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142c58de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142c59330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142c59880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142c59dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142c5a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142c5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142c5adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142c5b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142c5b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142c5bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142c5c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142c5c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142c5ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142c5d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142c5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142c5dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142c5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142c5e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142c5e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142c5ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142c5f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142c5f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142c5fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142c5ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142c60470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142c60910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142c60db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142c61300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142c61a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142c62140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142c62860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142c62f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142c63240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142c63a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142c63cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142c64300 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
0.00.704.891 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x113e089a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x113e08e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x113e09280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x113e096f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x113e09b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x113e09fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x113e0a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x113e0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x113e0ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x113e0b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x113e0b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x113e0bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x113e0c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x113e0d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x113e0d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x113e0df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x113e0e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x113e0ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x113e0f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x113e0fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x113e102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x113e10a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x113e11120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x113e11840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x113e11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x113e12220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x113e124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x113e12950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x113e12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x113e13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x113e13730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x113e13c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x113e140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x113e14370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x113e147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x113e14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x113e151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x113e156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x113e15bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x113e160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x113e165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x113e16ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x113e16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x113e174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x113e179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x113e17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x113e18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x113e18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x113e18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x113e18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x113e19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x113e198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x113e19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x113e1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x113e1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x113e1ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x113e1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x113e1b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x113e1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x113e1c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x113e1c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x113e1cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x113e1d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x113e1d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x113e1da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x113e1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x113e1e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x113e1e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x113e1ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x113e1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x113e1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x113e1fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x113e1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x113e204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x113e20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x113e20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x113e214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x113e219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x113e21f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x113e22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x113e229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x113e22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x113e23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x113e239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x113e23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x113e24470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x113e249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x113e24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x113e25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x113e259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x113e25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x113e26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x113e269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x113e26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x113e27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x113e27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x113e27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x113e28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x113e28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x113e28ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x113e29420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x113e29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x113e29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x113e2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x113e2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x113e2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x113e2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x113e2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x113e2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x113e2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x113e2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x113e2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x113e2d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x113e2d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x113e2dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x113e2e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x113e2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x113e2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x113e2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x113e2f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x113e2f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x113e2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x113e30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x113e306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x113e30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x113e31000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x113e314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x113e31940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x113e31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x113e32280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x113e32720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x113e32bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x113e33060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x113e33500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x113e339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x113e33e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x113e342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x113e34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x113e34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x113e350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x113e35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x113e35a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x113e35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x113e36340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x113e367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x113e36c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x113e37120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x113e375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x113e37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x113e37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x113e383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x113e38840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x113e38ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x113e39180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x113e39620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x113e39ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x113e39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x113e3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x113e3a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x113e3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x113e3b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x113e3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x113e3bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x113e3bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x113e3c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x113e3c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x113e3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x113e3d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x113e3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x113e3db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x113e3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x113e3e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x113e3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x113e3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x113e3f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x113e3f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x113e3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x113e40080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x113e40520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x113e409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x113e40e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x113e41300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x113e417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x113e41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x113e420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x113e42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x113e42a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x113e42ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x113e43360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x113e43800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x113e43ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x113e44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x113e445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x113e44b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x113e45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x113e455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x113e45b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x113e45de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x113e463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x113e46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x113e47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x113e47800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x113e47ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x113e47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x113e48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x113e48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x113e49370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x113e49810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x113e49cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x113e4a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x113e4a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x113e4ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x113e4b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x113e4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x113e4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x113e4c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x113e4c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x113e4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x113e4d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x113e4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x113e4de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x113e4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x113e4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x113e4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x113e4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x113e4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x113e4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x113e50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x113e508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x113e50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x113e51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x113e51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x113e51de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x113e52330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x113e52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x113e52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x113e53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x113e53870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x113e53dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x113e54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x113e54860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x113e54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x113e55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x113e55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x113e55da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x113e562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x113e56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x113e56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x113e572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x113e57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x113e57d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x113e582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x113e58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x113e58d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x113e592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x113e59810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x113e59d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x113e5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x113e5a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x113e5ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x113e5b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x113e5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x113e5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x113e5c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x113e5c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x113e5cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x113e5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x113e5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x113e5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x113e5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x113e5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x113e5e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x113e5ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x113e5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x113e5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x113e5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x113e600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x113e60560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x113e60a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x113e60ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x113e61340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x113e617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x113e61d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x113e62450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x113e62b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x113e63290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x113e639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x113e63c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x113e64460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x113e64720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x113e64d30 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142b081f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142b08660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142b08ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142b08f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142b093b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142b09820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142b09c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142b0a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142b0a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142b0aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142b0af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142b0b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142b0c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142b0c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142b0d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142b0d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142b0dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142b0e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142b0ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142b0f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142b0fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142b102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142b10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142b11130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142b11850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142b11b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142b11dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142b12240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142b126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142b12b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142b12f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142b134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142b13930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142b13bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142b14060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142b144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142b14940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142b14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142b15220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142b15690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142b15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142b15f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142b163e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142b16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142b16cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142b17130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142b175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142b17a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142b17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142b182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142b18760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142b18bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142b19040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142b194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142b19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142b19d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142b1a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142b1a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142b1ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142b1b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142b1b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142b1b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142b1be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142b1c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142b1c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142b1cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142b1cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142b1d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142b1d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142b1dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142b1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142b1e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142b1ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142b1ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142b1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142b1f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142b1fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142b200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142b20530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142b209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142b20e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142b21280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142b216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142b21b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142b21fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142b22440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142b228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142b22d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142b23190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142b23600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142b23a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142b23ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142b24350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142b247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142b24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142b250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142b25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142b25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142b25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142b26260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142b266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142b26b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142b26fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142b27420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142b27d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142b28050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142b284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142b28930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142b28da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142b29210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142b29680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142b29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142b29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142b2a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142b2a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142b2acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142b2b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142b2b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142b2ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142b2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142b2c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142b2c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142b2cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142b2d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142b2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142b2d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142b2dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142b2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142b2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142b2ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142b2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142b2f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142b2f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142b2fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142b30100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142b30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142b309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142b30e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142b312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142b31730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142b31ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142b32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142b32480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142b328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142b32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142b331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142b33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142b33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142b33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142b34390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142b34800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142b34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142b350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142b35550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142b359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142b35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142b362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142b36710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142b36b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142b36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142b37460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142b378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142b37d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142b381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142b38620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142b38a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142b38f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142b39370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142b397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142b39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142b3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142b3a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142b3a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142b3ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142b3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142b3b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142b3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142b3bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142b3c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142b3c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142b3cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142b3d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142b3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142b3da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142b3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142b3e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142b3e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142b3ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142b3f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142b3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142b3f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142b3fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142b40260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142b406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142b40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142b40fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142b41420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142b41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142b41d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142b42170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142b425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142b42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x142b42ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142b43330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142b437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142b43c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142b44080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142b44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142b44a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142b44ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142b45a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142b45d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142b45fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142b46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142b468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142b46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142b47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142b475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142b47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142b47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142b48340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142b487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142b48c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142b49090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142b49500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142b49970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142b49de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142b4a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142b4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142b4ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142b4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142b4b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142b4b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142b4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142b4c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142b4c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142b4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142b4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142b4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142b4d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142b4dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142b4e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142b4e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142b4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142b4edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142b4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142b4f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142b4fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142b4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142b503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142b50860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142b50cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142b51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142b515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142b51a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142b51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142b52300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142b52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142b52be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142b53050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142b534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142b53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142b53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142b54210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142b54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142b54af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142b54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142b553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142b55840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142b55cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142b56120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142b56590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142b56a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142b56e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142b572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142b57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142b57bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142b58030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142b584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142b58910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142b58d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142b591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142b59660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142b5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142b5a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142b5af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142b5b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142b5b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142b5bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142b5c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142b5c970 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.761s
user	0m0.279s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4793 (bc6f187e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156007950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156008090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156008640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156008bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1560091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156009750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156009d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15600a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15600a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15600ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15600b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15600b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15600c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15600ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15600d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15600d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15600e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15600e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15600eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15600f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15600fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1560104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156010bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156011490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156011bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156011e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156012480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1560130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156013630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1560138f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156013d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156014050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1560148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156014e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1560150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156015580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156015a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156015ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156016360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156016800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156016ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156017140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1560175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156017a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156017d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156018350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156018960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156019ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15601a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15601aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15601b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15601b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15601bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15601c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15601c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15601cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15601d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15601d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15601db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15601e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15601e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15601e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15601ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15601f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15601f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15601fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156020090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156020530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1560209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156020e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156021310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156021860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156022300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156022850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156022da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1560232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156023840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156023d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1560242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156024830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156024d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1560252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156025820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156025d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1560262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156026810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156026d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1560272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156027800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156027d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1560282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1560287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156028d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156029290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156018f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156029700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156029eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15602a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15602a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15602aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15602b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15602b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15602be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15602c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15602c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15602ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15602d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15602d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15602de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15602e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15602e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15602ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15602f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15602f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15602fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15602ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156030420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1560308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156030d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156031200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1560316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156031b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156032480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156032920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156032dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156033260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156033700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156033ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156034040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1560344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156034980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156034e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1560352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156035760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156035c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1560360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156036540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1560369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156036e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156037320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1560377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156037c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156038100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1560385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156038a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156038ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156039380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156039820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156039cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15603a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15603a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15603aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15603af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15603b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15603b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15603bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15603c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15603c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15603cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15603cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15603d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15603d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15603dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15603e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15603e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15603eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15603f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15603f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15603f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15603fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156040280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156040720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156041060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156041500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1560419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156041e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1560422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156042780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156042c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1560430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156043560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156043a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156043ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156044340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1560447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156044c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156045120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1560455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156045b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156046060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1560465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156046b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156046dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1560473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1560479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156047ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1560487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156048c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156048f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156049550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156049b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15604a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15604a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15604ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15604b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15604b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15604be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154607800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154607c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1546080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154608550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1546089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154608e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1546092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154609710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154609ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15460a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15460a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15460adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15460b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15460b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15460bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15460c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15460ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15460d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15460d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15460db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15460e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15460e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15460ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15460f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15460f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15460fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154610340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1546108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154610ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154611450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154611a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154611fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154612560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154612b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1546130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154613670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154613c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1546141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154614780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154614d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1546152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154615890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154615e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1546163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1546169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154616f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154617500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154617ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154618060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154618610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154618bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154619170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154619720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154619cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15461a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15461a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15461ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15461b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15461b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15461bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15461c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15461c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15461cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15461d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15461d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15461da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15461df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15461e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15461e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15461ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15461f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15461fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154620460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154620b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1546212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154621560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x154621d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x154622010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154622620 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
0.00.102.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.708 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156008350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156047690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156049200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156047080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156047ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15601ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15601a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15601cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156049810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156012130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156018c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156019540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156018610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15601b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15601a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156011130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15601b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15601d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1560299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156014310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1560145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156049e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1560482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156012740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156012a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156012cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15604c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15604c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15604c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15604c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15604cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15604ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15604d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15604d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15604d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15604d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15604dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15604df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15604e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15604e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15604e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15604ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15604ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15604efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15604f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15604f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15604f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15604fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15604fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156050030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1560502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1560505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156050870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156050b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156050df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1560510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156051370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156051630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1560518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156051bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156051e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156052130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1560523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1560526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156052970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156052c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156052ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1560531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156053470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156053730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1560539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156053cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156053f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156054230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1560544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1560547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156054a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156054d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156054ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1560552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156055570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156055830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156055af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156055db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156056070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156056330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1560565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1560568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156056b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156056e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1560570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1560573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156057670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156057930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156057bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156057eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156058170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156058430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1560586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1560589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156058c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156058f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1560591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1560594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156059770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156059a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156059cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156059fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15605a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15605a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15605a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15605aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15605ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15605b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15605b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15605b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15605b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15605bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15605bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15605c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15605c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15605c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15605c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15605cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15605ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15605d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15605d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15605d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15605d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15605dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15605def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15605e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15605e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15605e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15605e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15605ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15605ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15605f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15605f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15605f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15605fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15605fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15605fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1560602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156060570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156060830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156060af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156060db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156061070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156061330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1560615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1560618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156061b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156061e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1560620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1560623b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156062670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156062930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156062bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156062eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156063170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156063430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1560636f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1560639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156063c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156063f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156064330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1560645f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1560648b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156064d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156065190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156065600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156065a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156065ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156066350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1560667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156066c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1560670a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156067510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156067980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156067df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156068260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1560686d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156068b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156068fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156069420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156069890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156069d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15606a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15606a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15606aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15606aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15606b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15606b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15606bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15606c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15606c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15606c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15606cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15606d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15606d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15606db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15606e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15606e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15606ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15606ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15606f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15606f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15606fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156070180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156070cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156070fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156071570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156071b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1560720f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1560726b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156072c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156073230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1560737f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156073db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156074370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156074930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156074ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1560754b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156075a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156076030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1560765f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156076bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156077170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156077730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156077cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1560782b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156078870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156078e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1560793f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1560799b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156079f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15607a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15607aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15607b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15607b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15607bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15607c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15607c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15607cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15607d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15607d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15607deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15607e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15607ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15607eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15607f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15607fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156080130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1560806f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156080cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156081270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156081830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156081df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1560823b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156082970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156082f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1560834f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156083ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156084070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156084630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156084bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1560851b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1560856b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156085bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1560860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1560865b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156086ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156086fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1560874b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1560879b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156087eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1560883b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1560888b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156088db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1560892b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1560897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156089cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15608a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15608ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15608b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15608bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15608bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15608c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15608c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15608cfa0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15460a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15460de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154610050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15460ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154612dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15460d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154613ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x154611160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15460b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154619430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154611cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15460ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15460faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15460b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154609fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154611710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1546188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1546177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154623860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154623f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1546246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154624dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1546254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154625c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154625ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1546264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154626ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1546270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1546278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154627d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154628040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1546288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154628e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1546290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154629570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154629a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154629eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15462a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15462a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15462ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15462b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15462b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15462ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15462bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15462c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15462c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15462cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15462d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15462db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15462e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15462e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15462edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15462f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15462fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154630050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1546304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1546307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x154630dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1546315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154631a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x154631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x154632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x154632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1546343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x154634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1546351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154635720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x154635c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1546361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x154636c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1546371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154637700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x154637c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1546381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1546386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x154638c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154639190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1546396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154639c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15463a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15463a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15463ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15463b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15463b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15463bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15463c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15463c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15463cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15463d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15463d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15463dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15463e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15463e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15463ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15463f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15463f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15463fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x154640120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x154640670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154640bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x154641110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154641660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x154641bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x154642650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x154642af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154642f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154643430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1546438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x154643d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154644210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1546446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154644b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154644ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154645490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154645930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154645dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154646270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154646710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154646bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154647050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1546474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154647990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154647e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1546482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154648770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154648c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1546490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154649550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1546499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154649e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15464a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15464a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15464ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15464b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15464b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15464ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15464bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15464c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15464c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15464ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15464d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15464d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15464dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15464df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15464e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15464e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15464ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15464f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15464f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15464fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15464ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x154650450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1546508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x154650d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x154651230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1546516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x154651b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x154652010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1546524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x154652950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154652df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154653290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154653730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154653bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154654070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154654510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1546549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154654e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1546552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154655790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154655c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1546560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154656570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154656a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154656eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154657350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1546577f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154657c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154658130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1546585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154658a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154658f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1546593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154659850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154659da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15465a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15465a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15465ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15465b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15465b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15465bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15465c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15465ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15465cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15465d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15465d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15465ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15465e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15465ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15465ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15465f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15465fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1546600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154660610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154660b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1546610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154661600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154661b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1546620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1546625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154662b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154663090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1546635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154663b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154664080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1546645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154664b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154665070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1546655c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154665b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154666060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1546665b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154666b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154667050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1546675a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154667af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154668040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154668590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154668ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154669030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154669580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154669ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15466a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15466a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15466aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15466b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15466b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15466bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15466c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15466c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15466caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15466cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15466d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15466da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15466dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15466e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15466ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15466efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15466f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15466fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15466ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154670510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154670a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154670fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154671500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154671a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154671fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1546724f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154672990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154672e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1546732d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154673770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154673c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1546740b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154674550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1546749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154674e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154675330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1546757d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154675c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154676110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1546765b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154676a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154676fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1546776c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154677de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154678500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154678c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154678ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1546796d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x154679990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154679fa0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.979s
user	0m0.229s
sys	0m0.185s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
