++ nproc
+ make -j8
[  0%] Generating build details from Git
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target sha1
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target xxhash
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  7%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  7%] Linking CXX shared library libggml-base.so
[  7%] Built target build_info
[  7%] Built target ggml-base
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 13%] Linking CXX shared library libggml.so
[ 13%] Built target ggml
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Linking CXX executable ../../bin/llama-gguf
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf-hash
[ 24%] Built target llama-gguf
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_get_data_internal(llama_context*, llama_data_write&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1150:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1150 |         .write = [&](const void * src, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1153:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1153 |         .write_tensor_data = [&](const struct ggml_tensor * tensor, size_t offset, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1156:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1156 |         .read    = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1157:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1157 |         .read_to = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_set_data_internal(llama_context*, llama_data_read&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1198:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1198 |         .write = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1199:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1199 |         .write_tensor_data = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1200:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1200 |         .read = [&](size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1203:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1203 |         .read_to = [&](void * dst, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_seq_get_data_internal(llama_context*, llama_data_write&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1305:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1305 |         .write = [&](const void * src, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1308:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1308 |         .write_tensor_data = [&](const struct ggml_tensor * tensor, size_t offset, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1311:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1311 |         .read = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1312:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1312 |         .read_to = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_seq_set_data_internal(llama_context*, llama_data_read&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1339:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1339 |         .write = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1340:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1340 |         .write_tensor_data = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1341:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1341 |         .read = [&](size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1344:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1344 |         .read_to = [&](void * dst, size_t size) {
      |         ^
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1767: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m2.048s
user	0m3.484s
sys	0m0.977s
