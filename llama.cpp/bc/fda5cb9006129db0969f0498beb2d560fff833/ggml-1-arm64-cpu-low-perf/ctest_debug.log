+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=ares+crypto+noprofile+dotprod+noi8mm+nosve 
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m2.063s
user	0m1.291s
sys	0m0.571s
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  3%] Built target sha256
[  3%] Built target xxhash
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target sha1
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  7%] Built target build_info
[  7%] Linking CXX shared library libggml-base.so
[  7%] Built target ggml-base
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 13%] Linking CXX shared library libggml.so
[ 13%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Linking CXX executable ../../bin/llama-gguf
[ 16%] Linking CXX executable ../../bin/llama-gguf-hash
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Built target llama-gguf
[ 20%] Built target llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_get_data_internal(llama_context*, llama_data_write&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1150:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1150 |         .write = [&](const void * src, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1153:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1153 |         .write_tensor_data = [&](const struct ggml_tensor * tensor, size_t offset, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1156:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1156 |         .read    = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1157:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1157 |         .read_to = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_set_data_internal(llama_context*, llama_data_read&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1198:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1198 |         .write = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1199:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1199 |         .write_tensor_data = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1200:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1200 |         .read = [&](size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1203:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1203 |         .read_to = [&](void * dst, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_seq_get_data_internal(llama_context*, llama_data_write&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1305:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1305 |         .write = [&](const void * src, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1308:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1308 |         .write_tensor_data = [&](const struct ggml_tensor * tensor, size_t offset, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1311:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1311 |         .read = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1312:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1312 |         .read_to = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_seq_set_data_internal(llama_context*, llama_data_read&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1339:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1339 |         .write = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1340:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1340 |         .write_tensor_data = nullptr,
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1341:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1341 |         .read = [&](size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1344:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1344 |         .read_to = [&](void * dst, size_t size) {
      |         ^
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1767: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m2.212s
user	0m3.718s
sys	0m0.867s
