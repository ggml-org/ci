Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.550s
user	0m0.898s
sys	0m1.215s
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Built target sha1
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 23%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 24%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Built target llava
[ 31%] Linking CXX static library libcommon.a
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target llama-simple-chat
[ 32%] Built target llama-simple
[ 32%] Built target llama-quantize-stats
[ 32%] Built target test-c
[ 32%] Built target llava_static
[ 32%] Built target common
[ 32%] Built target llava_shared
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-0
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 44%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-json-schema-to-grammar
[ 46%] Built target test-grammar-parser
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-grammar-integration
[ 46%] Built target test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Built target test-log
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Built target test-arg-parser
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-gguf
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Linking CXX executable ../bin/test-rope
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Built target test-gguf
[ 60%] Built target test-chat-template
[ 60%] Built target test-model-load-cancel
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Built target test-autorelease
[ 61%] Built target test-backend-ops
[ 61%] Built target test-rope
[ 61%] Built target test-quantize-perf
[ 61%] Built target test-quantize-fns
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Built target llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Built target llama-embedding
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-batched
[ 71%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Built target llama-infill
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Built target llama-bench
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 74%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-lookup
[ 81%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-parallel
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-passkey
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Built target llama-cli
[ 82%] Built target llama-lookup-create
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-quantize
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.934s
user	0m5.741s
sys	0m8.780s

main: quantize time =  2368.14 ms
main:    total time =  2368.14 ms

main: quantize time =  1435.52 ms
main:    total time =  1435.52 ms

main: quantize time =  1400.62 ms
main:    total time =  1400.62 ms

main: quantize time =  1396.84 ms
main:    total time =  1396.85 ms

main: quantize time =  2523.60 ms
main:    total time =  2523.60 ms

main: quantize time =  5060.32 ms
main:    total time =  5060.32 ms

main: quantize time =  5646.29 ms
main:    total time =  5646.29 ms

main: quantize time =  6887.24 ms
main:    total time =  6887.24 ms

main: quantize time =  6013.22 ms
main:    total time =  6013.22 ms

main: quantize time =  4588.61 ms
main:    total time =  4588.61 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.103 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.226 I main: llama backend init
0.00.000.232 I main: load the model and apply lora adapter, if any
0.00.046.734 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.057.853 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.869 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.886 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.886 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.887 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.888 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.889 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.889 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.890 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.895 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.896 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.064.793 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.284 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.785 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.076.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.790 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.792 I llama_model_loader: - type  f32:  194 tensors
0.00.076.792 I llama_model_loader: - type  f16:   98 tensors
0.00.111.231 I llm_load_vocab: special tokens cache size = 25
0.00.118.465 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.118.468 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.118.469 I llm_load_print_meta: arch             = gptneox
0.00.118.469 I llm_load_print_meta: vocab type       = BPE
0.00.118.469 I llm_load_print_meta: n_vocab          = 50304
0.00.118.469 I llm_load_print_meta: n_merges         = 50009
0.00.118.469 I llm_load_print_meta: vocab_only       = 0
0.00.118.470 I llm_load_print_meta: n_ctx_train      = 2048
0.00.118.470 I llm_load_print_meta: n_embd           = 2048
0.00.118.470 I llm_load_print_meta: n_layer          = 24
0.00.118.473 I llm_load_print_meta: n_head           = 16
0.00.118.474 I llm_load_print_meta: n_head_kv        = 16
0.00.118.474 I llm_load_print_meta: n_rot            = 32
0.00.118.474 I llm_load_print_meta: n_swa            = 0
0.00.118.475 I llm_load_print_meta: n_embd_head_k    = 128
0.00.118.476 I llm_load_print_meta: n_embd_head_v    = 128
0.00.118.477 I llm_load_print_meta: n_gqa            = 1
0.00.118.477 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.118.478 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.118.479 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.118.479 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.118.479 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.118.480 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.118.480 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.118.480 I llm_load_print_meta: n_ff             = 8192
0.00.118.480 I llm_load_print_meta: n_expert         = 0
0.00.118.481 I llm_load_print_meta: n_expert_used    = 0
0.00.118.481 I llm_load_print_meta: causal attn      = 1
0.00.118.481 I llm_load_print_meta: pooling type     = 0
0.00.118.481 I llm_load_print_meta: rope type        = 2
0.00.118.481 I llm_load_print_meta: rope scaling     = linear
0.00.118.482 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.118.482 I llm_load_print_meta: freq_scale_train = 1
0.00.118.482 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.118.482 I llm_load_print_meta: rope_finetuned   = unknown
0.00.118.482 I llm_load_print_meta: ssm_d_conv       = 0
0.00.118.483 I llm_load_print_meta: ssm_d_inner      = 0
0.00.118.483 I llm_load_print_meta: ssm_d_state      = 0
0.00.118.483 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.118.483 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.118.483 I llm_load_print_meta: model type       = 1.4B
0.00.118.484 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.118.484 I llm_load_print_meta: model params     = 1.41 B
0.00.118.485 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.118.485 I llm_load_print_meta: general.name     = 1.4B
0.00.118.485 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.118.485 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.118.485 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.118.486 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.118.486 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.118.486 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.118.486 I llm_load_print_meta: max token length = 1024
0.00.121.122 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.121.123 I llm_load_tensors: offloading output layer to GPU
0.00.121.123 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.121.142 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.121.143 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.122.138 I llama_new_context_with_model: n_seq_max     = 1
0.00.122.139 I llama_new_context_with_model: n_ctx         = 2048
0.00.122.139 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.122.140 I llama_new_context_with_model: n_batch       = 2048
0.00.122.140 I llama_new_context_with_model: n_ubatch      = 512
0.00.122.140 I llama_new_context_with_model: flash_attn    = 0
0.00.122.140 I llama_new_context_with_model: freq_base     = 10000.0
0.00.122.141 I llama_new_context_with_model: freq_scale    = 1
0.00.122.141 I ggml_metal_init: allocating
0.00.122.144 I ggml_metal_init: found device: Apple M4
0.00.122.146 I ggml_metal_init: picking default device: Apple M4
0.00.122.825 I ggml_metal_init: using embedded metal library
0.00.204.058 I ggml_metal_init: GPU name:   Apple M4
0.00.204.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.204.064 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.204.064 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.204.065 I ggml_metal_init: simdgroup reduction   = true
0.00.204.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.204.065 I ggml_metal_init: has bfloat            = true
0.00.204.065 I ggml_metal_init: use bfloat            = true
0.00.204.066 I ggml_metal_init: hasUnifiedMemory      = true
0.00.204.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.264.833 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.285.024 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.285.030 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.285.056 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.286.102 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.286.104 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.286.105 I llama_new_context_with_model: graph nodes  = 967
0.00.286.105 I llama_new_context_with_model: graph splits = 2
0.00.286.130 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.286.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.286.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.644 I main: llama threadpool init, n_threads = 4
0.00.365.674 I 
0.00.365.693 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.693 I 
0.00.365.778 I sampler seed: 1234
0.00.365.783 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.365.808 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.365.811 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.365.811 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.264.814 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.02.264.814 I llama_perf_context_print:        load time =     318.90 ms
0.02.264.815 I llama_perf_context_print: prompt eval time =      44.04 ms /     7 tokens (    6.29 ms per token,   158.95 tokens per second)
0.02.264.817 I llama_perf_context_print:        eval time =    1852.04 ms /    63 runs   (   29.40 ms per token,    34.02 tokens per second)
0.02.264.817 I llama_perf_context_print:       total time =    1899.17 ms /    70 tokens
0.02.264.985 I ggml_metal_free: deallocating

real	0m2.564s
user	0m0.152s
sys	0m0.101s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.040 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.274 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.277 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.277 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.277 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.278 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.279 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.279 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.279 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.280 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.280 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.280 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.281 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.284 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.321 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.094 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.097 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.097 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.097 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.098 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.098 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.099 I llama_model_loader: - type  f32:  194 tensors
0.00.038.099 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.843 I llm_load_vocab: special tokens cache size = 25
0.00.072.517 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.521 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.521 I llm_load_print_meta: arch             = gptneox
0.00.072.522 I llm_load_print_meta: vocab type       = BPE
0.00.072.522 I llm_load_print_meta: n_vocab          = 50304
0.00.072.522 I llm_load_print_meta: n_merges         = 50009
0.00.072.522 I llm_load_print_meta: vocab_only       = 0
0.00.072.523 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.523 I llm_load_print_meta: n_embd           = 2048
0.00.072.523 I llm_load_print_meta: n_layer          = 24
0.00.072.529 I llm_load_print_meta: n_head           = 16
0.00.072.530 I llm_load_print_meta: n_head_kv        = 16
0.00.072.530 I llm_load_print_meta: n_rot            = 32
0.00.072.530 I llm_load_print_meta: n_swa            = 0
0.00.072.531 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.531 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.532 I llm_load_print_meta: n_gqa            = 1
0.00.072.533 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.540 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.541 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.542 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.542 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.542 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.543 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.544 I llm_load_print_meta: n_ff             = 8192
0.00.072.544 I llm_load_print_meta: n_expert         = 0
0.00.072.544 I llm_load_print_meta: n_expert_used    = 0
0.00.072.544 I llm_load_print_meta: causal attn      = 1
0.00.072.544 I llm_load_print_meta: pooling type     = 0
0.00.072.545 I llm_load_print_meta: rope type        = 2
0.00.072.545 I llm_load_print_meta: rope scaling     = linear
0.00.072.546 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.546 I llm_load_print_meta: freq_scale_train = 1
0.00.072.546 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.547 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.547 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.548 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.548 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.549 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.549 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.549 I llm_load_print_meta: model type       = 1.4B
0.00.072.550 I llm_load_print_meta: model ftype      = Q8_0
0.00.072.550 I llm_load_print_meta: model params     = 1.41 B
0.00.072.551 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.072.551 I llm_load_print_meta: general.name     = 1.4B
0.00.072.551 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.552 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.552 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.552 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.552 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.553 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.553 I llm_load_print_meta: max token length = 1024
0.00.075.189 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.189 I llm_load_tensors: offloading output layer to GPU
0.00.075.190 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.202 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.204 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.076.698 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.699 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.699 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.700 I llama_new_context_with_model: n_batch       = 2048
0.00.076.700 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.700 I llama_new_context_with_model: flash_attn    = 0
0.00.076.701 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.701 I llama_new_context_with_model: freq_scale    = 1
0.00.076.702 I ggml_metal_init: allocating
0.00.076.707 I ggml_metal_init: found device: Apple M4
0.00.076.710 I ggml_metal_init: picking default device: Apple M4
0.00.077.741 I ggml_metal_init: using embedded metal library
0.00.081.643 I ggml_metal_init: GPU name:   Apple M4
0.00.081.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.647 I ggml_metal_init: simdgroup reduction   = true
0.00.081.647 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.648 I ggml_metal_init: has bfloat            = true
0.00.081.648 I ggml_metal_init: use bfloat            = true
0.00.081.648 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.784 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.119.686 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.695 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.722 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.759 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.761 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.761 I llama_new_context_with_model: graph nodes  = 967
0.00.120.761 I llama_new_context_with_model: graph splits = 2
0.00.120.782 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.911 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.912 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.445.333 I main: llama threadpool init, n_threads = 4
0.01.445.422 I 
0.01.445.486 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.445.489 I 
0.01.445.826 I sampler seed: 1234
0.01.445.837 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.445.896 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.445.904 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.445.904 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.586.548 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49203.05 tokens per second)
0.02.586.549 I llama_perf_context_print:        load time =    1435.28 ms
0.02.586.550 I llama_perf_context_print: prompt eval time =      50.01 ms /     7 tokens (    7.14 ms per token,   139.98 tokens per second)
0.02.586.550 I llama_perf_context_print:        eval time =    1087.29 ms /    63 runs   (   17.26 ms per token,    57.94 tokens per second)
0.02.586.551 I llama_perf_context_print:       total time =    1141.23 ms /    70 tokens
0.02.586.743 I ggml_metal_free: deallocating

real	0m2.615s
user	0m0.139s
sys	0m0.252s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.016.685 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.681 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.687 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.694 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.699 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.699 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.699 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.704 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.704 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.883 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.939 I llama_model_loader: - type  f32:  194 tensors
0.00.038.939 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.939 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.463 I llm_load_vocab: special tokens cache size = 25
0.00.071.655 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.659 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.659 I llm_load_print_meta: arch             = gptneox
0.00.071.660 I llm_load_print_meta: vocab type       = BPE
0.00.071.660 I llm_load_print_meta: n_vocab          = 50304
0.00.071.660 I llm_load_print_meta: n_merges         = 50009
0.00.071.660 I llm_load_print_meta: vocab_only       = 0
0.00.071.660 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.660 I llm_load_print_meta: n_embd           = 2048
0.00.071.661 I llm_load_print_meta: n_layer          = 24
0.00.071.665 I llm_load_print_meta: n_head           = 16
0.00.071.666 I llm_load_print_meta: n_head_kv        = 16
0.00.071.666 I llm_load_print_meta: n_rot            = 32
0.00.071.666 I llm_load_print_meta: n_swa            = 0
0.00.071.666 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.667 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.667 I llm_load_print_meta: n_gqa            = 1
0.00.071.668 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.670 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.671 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.671 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.672 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.672 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.672 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.673 I llm_load_print_meta: n_ff             = 8192
0.00.071.673 I llm_load_print_meta: n_expert         = 0
0.00.071.673 I llm_load_print_meta: n_expert_used    = 0
0.00.071.673 I llm_load_print_meta: causal attn      = 1
0.00.071.673 I llm_load_print_meta: pooling type     = 0
0.00.071.673 I llm_load_print_meta: rope type        = 2
0.00.071.674 I llm_load_print_meta: rope scaling     = linear
0.00.071.680 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.683 I llm_load_print_meta: freq_scale_train = 1
0.00.071.683 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.684 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.684 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.684 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.684 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.684 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.685 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.685 I llm_load_print_meta: model type       = 1.4B
0.00.071.685 I llm_load_print_meta: model ftype      = Q4_0
0.00.071.686 I llm_load_print_meta: model params     = 1.41 B
0.00.071.686 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.071.686 I llm_load_print_meta: general.name     = 1.4B
0.00.071.687 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.687 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.688 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.688 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.689 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.071.689 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.690 I llm_load_print_meta: max token length = 1024
0.00.074.323 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.323 I llm_load_tensors: offloading output layer to GPU
0.00.074.324 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.335 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.074.336 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.075.459 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.460 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.460 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.460 I llama_new_context_with_model: n_batch       = 2048
0.00.075.461 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.461 I llama_new_context_with_model: flash_attn    = 0
0.00.075.461 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.462 I llama_new_context_with_model: freq_scale    = 1
0.00.075.462 I ggml_metal_init: allocating
0.00.075.466 I ggml_metal_init: found device: Apple M4
0.00.075.468 I ggml_metal_init: picking default device: Apple M4
0.00.076.315 I ggml_metal_init: using embedded metal library
0.00.079.951 I ggml_metal_init: GPU name:   Apple M4
0.00.079.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.955 I ggml_metal_init: simdgroup reduction   = true
0.00.079.955 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.955 I ggml_metal_init: has bfloat            = true
0.00.079.956 I ggml_metal_init: use bfloat            = true
0.00.079.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.575 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.125.090 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.098 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.131 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.338 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.340 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.341 I llama_new_context_with_model: graph nodes  = 967
0.00.126.341 I llama_new_context_with_model: graph splits = 2
0.00.126.359 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.126.490 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.126.490 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.553 I main: llama threadpool init, n_threads = 4
0.00.766.595 I 
0.00.766.615 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.615 I 
0.00.766.850 I sampler seed: 1234
0.00.766.855 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.866 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.866 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.866 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.450.077 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.01.450.077 I llama_perf_context_print:        load time =     749.86 ms
0.01.450.078 I llama_perf_context_print: prompt eval time =      39.80 ms /     7 tokens (    5.69 ms per token,   175.87 tokens per second)
0.01.450.079 I llama_perf_context_print:        eval time =     640.27 ms /    63 runs   (   10.16 ms per token,    98.40 tokens per second)
0.01.450.079 I llama_perf_context_print:       total time =     683.53 ms /    70 tokens
0.01.450.278 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.124s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.647 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.589 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.595 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.600 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.601 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.601 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.602 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.602 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.602 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.605 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.605 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.605 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.715 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.995 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.996 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.997 I llama_model_loader: - type  f32:  194 tensors
0.00.037.997 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.997 I llama_model_loader: - type q6_K:    1 tensors
0.00.063.320 I llm_load_vocab: special tokens cache size = 25
0.00.072.001 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.004 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.005 I llm_load_print_meta: arch             = gptneox
0.00.072.005 I llm_load_print_meta: vocab type       = BPE
0.00.072.005 I llm_load_print_meta: n_vocab          = 50304
0.00.072.005 I llm_load_print_meta: n_merges         = 50009
0.00.072.006 I llm_load_print_meta: vocab_only       = 0
0.00.072.006 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.006 I llm_load_print_meta: n_embd           = 2048
0.00.072.006 I llm_load_print_meta: n_layer          = 24
0.00.072.009 I llm_load_print_meta: n_head           = 16
0.00.072.011 I llm_load_print_meta: n_head_kv        = 16
0.00.072.011 I llm_load_print_meta: n_rot            = 32
0.00.072.011 I llm_load_print_meta: n_swa            = 0
0.00.072.011 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.011 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.012 I llm_load_print_meta: n_gqa            = 1
0.00.072.014 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.014 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.015 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.016 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.016 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.016 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.016 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.017 I llm_load_print_meta: n_ff             = 8192
0.00.072.017 I llm_load_print_meta: n_expert         = 0
0.00.072.017 I llm_load_print_meta: n_expert_used    = 0
0.00.072.019 I llm_load_print_meta: causal attn      = 1
0.00.072.021 I llm_load_print_meta: pooling type     = 0
0.00.072.021 I llm_load_print_meta: rope type        = 2
0.00.072.021 I llm_load_print_meta: rope scaling     = linear
0.00.072.022 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.022 I llm_load_print_meta: freq_scale_train = 1
0.00.072.023 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.023 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.023 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.023 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.023 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.023 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.024 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.024 I llm_load_print_meta: model type       = 1.4B
0.00.072.024 I llm_load_print_meta: model ftype      = Q4_1
0.00.072.025 I llm_load_print_meta: model params     = 1.41 B
0.00.072.030 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.072.030 I llm_load_print_meta: general.name     = 1.4B
0.00.072.031 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.031 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.031 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.031 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.032 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.034 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.034 I llm_load_print_meta: max token length = 1024
0.00.074.645 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.646 I llm_load_tensors: offloading output layer to GPU
0.00.074.646 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.657 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.074.659 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.076.034 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.035 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.035 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.036 I llama_new_context_with_model: n_batch       = 2048
0.00.076.036 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.036 I llama_new_context_with_model: flash_attn    = 0
0.00.076.037 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.037 I llama_new_context_with_model: freq_scale    = 1
0.00.076.038 I ggml_metal_init: allocating
0.00.076.046 I ggml_metal_init: found device: Apple M4
0.00.076.049 I ggml_metal_init: picking default device: Apple M4
0.00.076.883 I ggml_metal_init: using embedded metal library
0.00.080.759 I ggml_metal_init: GPU name:   Apple M4
0.00.080.762 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.763 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.763 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.763 I ggml_metal_init: simdgroup reduction   = true
0.00.080.764 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.764 I ggml_metal_init: has bfloat            = true
0.00.080.766 I ggml_metal_init: use bfloat            = true
0.00.080.766 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.705 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.118.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.118.010 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.118.030 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.065 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.119.066 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.119.067 I llama_new_context_with_model: graph nodes  = 967
0.00.119.067 I llama_new_context_with_model: graph splits = 2
0.00.119.084 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.119.232 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.119.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.572 I main: llama threadpool init, n_threads = 4
0.00.760.609 I 
0.00.760.628 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.628 I 
0.00.760.862 I sampler seed: 1234
0.00.760.866 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.877 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.878 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.878 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.487.346 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63223.51 tokens per second)
0.01.487.346 I llama_perf_context_print:        load time =     751.92 ms
0.01.487.347 I llama_perf_context_print: prompt eval time =      44.52 ms /     7 tokens (    6.36 ms per token,   157.23 tokens per second)
0.01.487.348 I llama_perf_context_print:        eval time =     678.90 ms /    63 runs   (   10.78 ms per token,    92.80 tokens per second)
0.01.487.348 I llama_perf_context_print:       total time =     726.78 ms /    70 tokens
0.01.487.493 I ggml_metal_free: deallocating

real	0m1.511s
user	0m0.127s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.011.229 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.266 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.272 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.274 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.275 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.276 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.277 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.282 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.283 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.161 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.193 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.035 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.037 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.037 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.038 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.038 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.039 I llama_model_loader: - type  f32:  194 tensors
0.00.026.039 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.039 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.146 I llm_load_vocab: special tokens cache size = 25
0.00.053.126 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.129 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.129 I llm_load_print_meta: arch             = gptneox
0.00.053.130 I llm_load_print_meta: vocab type       = BPE
0.00.053.130 I llm_load_print_meta: n_vocab          = 50304
0.00.053.130 I llm_load_print_meta: n_merges         = 50009
0.00.053.130 I llm_load_print_meta: vocab_only       = 0
0.00.053.130 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.131 I llm_load_print_meta: n_embd           = 2048
0.00.053.131 I llm_load_print_meta: n_layer          = 24
0.00.053.133 I llm_load_print_meta: n_head           = 16
0.00.053.136 I llm_load_print_meta: n_head_kv        = 16
0.00.053.136 I llm_load_print_meta: n_rot            = 32
0.00.053.136 I llm_load_print_meta: n_swa            = 0
0.00.053.137 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.137 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.137 I llm_load_print_meta: n_gqa            = 1
0.00.053.138 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.139 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.139 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.140 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.140 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.140 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.140 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.141 I llm_load_print_meta: n_ff             = 8192
0.00.053.141 I llm_load_print_meta: n_expert         = 0
0.00.053.141 I llm_load_print_meta: n_expert_used    = 0
0.00.053.142 I llm_load_print_meta: causal attn      = 1
0.00.053.142 I llm_load_print_meta: pooling type     = 0
0.00.053.142 I llm_load_print_meta: rope type        = 2
0.00.053.142 I llm_load_print_meta: rope scaling     = linear
0.00.053.144 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.144 I llm_load_print_meta: freq_scale_train = 1
0.00.053.144 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.145 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.145 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.145 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.145 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.145 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.145 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.145 I llm_load_print_meta: model type       = 1.4B
0.00.053.146 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.146 I llm_load_print_meta: model params     = 1.41 B
0.00.053.147 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.148 I llm_load_print_meta: general.name     = 1.4B
0.00.053.148 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.149 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.149 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.149 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.149 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.151 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.151 I llm_load_print_meta: max token length = 1024
0.00.055.186 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.186 I llm_load_tensors: offloading output layer to GPU
0.00.055.187 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.197 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.199 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.094 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.094 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.095 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.095 I llama_new_context_with_model: n_batch       = 2048
0.00.056.095 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.095 I llama_new_context_with_model: flash_attn    = 0
0.00.056.096 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.096 I llama_new_context_with_model: freq_scale    = 1
0.00.056.096 I ggml_metal_init: allocating
0.00.056.101 I ggml_metal_init: found device: Apple M4
0.00.056.103 I ggml_metal_init: picking default device: Apple M4
0.00.056.694 I ggml_metal_init: using embedded metal library
0.00.059.015 I ggml_metal_init: GPU name:   Apple M4
0.00.059.017 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.018 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.018 I ggml_metal_init: simdgroup reduction   = true
0.00.059.018 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.018 I ggml_metal_init: has bfloat            = true
0.00.059.018 I ggml_metal_init: use bfloat            = true
0.00.059.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.020 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.519 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.489 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.500 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.524 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.583 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.585 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.585 I llama_new_context_with_model: graph nodes  = 967
0.00.088.585 I llama_new_context_with_model: graph splits = 2
0.00.088.599 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.893 I main: llama threadpool init, n_threads = 4
0.00.742.933 I 
0.00.742.970 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.970 I 
0.00.743.210 I sampler seed: 1234
0.00.743.214 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.273 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.277 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.277 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.528.322 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.528.322 I llama_perf_context_print:        load time =     731.66 ms
0.01.528.324 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.28 tokens per second)
0.01.528.325 I llama_perf_context_print:        eval time =     738.99 ms /    63 runs   (   11.73 ms per token,    85.25 tokens per second)
0.01.528.325 I llama_perf_context_print:       total time =     785.43 ms /    70 tokens
0.01.528.498 I ggml_metal_free: deallocating

real	0m1.547s
user	0m0.110s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.131 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.189 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.193 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.198 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.199 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.199 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.201 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.202 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.202 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.202 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.203 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.203 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.206 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.207 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.208 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.209 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.140 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.186 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.076 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.077 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.078 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.078 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.078 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.079 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.079 I llama_model_loader: - type  f32:  194 tensors
0.00.024.079 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.079 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.381 I llm_load_vocab: special tokens cache size = 25
0.00.050.304 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.307 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.307 I llm_load_print_meta: arch             = gptneox
0.00.050.308 I llm_load_print_meta: vocab type       = BPE
0.00.050.308 I llm_load_print_meta: n_vocab          = 50304
0.00.050.308 I llm_load_print_meta: n_merges         = 50009
0.00.050.308 I llm_load_print_meta: vocab_only       = 0
0.00.050.308 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.309 I llm_load_print_meta: n_embd           = 2048
0.00.050.309 I llm_load_print_meta: n_layer          = 24
0.00.050.311 I llm_load_print_meta: n_head           = 16
0.00.050.312 I llm_load_print_meta: n_head_kv        = 16
0.00.050.312 I llm_load_print_meta: n_rot            = 32
0.00.050.314 I llm_load_print_meta: n_swa            = 0
0.00.050.314 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.314 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.315 I llm_load_print_meta: n_gqa            = 1
0.00.050.316 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.317 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.317 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.317 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.318 I llm_load_print_meta: n_ff             = 8192
0.00.050.319 I llm_load_print_meta: n_expert         = 0
0.00.050.319 I llm_load_print_meta: n_expert_used    = 0
0.00.050.319 I llm_load_print_meta: causal attn      = 1
0.00.050.319 I llm_load_print_meta: pooling type     = 0
0.00.050.319 I llm_load_print_meta: rope type        = 2
0.00.050.320 I llm_load_print_meta: rope scaling     = linear
0.00.050.320 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.320 I llm_load_print_meta: freq_scale_train = 1
0.00.050.320 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.321 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.321 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.321 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.321 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.321 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.321 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.322 I llm_load_print_meta: model type       = 1.4B
0.00.050.322 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.322 I llm_load_print_meta: model params     = 1.41 B
0.00.050.323 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.323 I llm_load_print_meta: general.name     = 1.4B
0.00.050.323 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.323 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.324 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.324 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.324 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.324 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.325 I llm_load_print_meta: max token length = 1024
0.00.052.371 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.371 I llm_load_tensors: offloading output layer to GPU
0.00.052.371 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.382 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.383 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.300 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.301 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.301 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.301 I llama_new_context_with_model: n_batch       = 2048
0.00.053.302 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.302 I llama_new_context_with_model: flash_attn    = 0
0.00.053.302 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.302 I llama_new_context_with_model: freq_scale    = 1
0.00.053.303 I ggml_metal_init: allocating
0.00.053.306 I ggml_metal_init: found device: Apple M4
0.00.053.308 I ggml_metal_init: picking default device: Apple M4
0.00.053.877 I ggml_metal_init: using embedded metal library
0.00.056.228 I ggml_metal_init: GPU name:   Apple M4
0.00.056.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.230 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.230 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.231 I ggml_metal_init: simdgroup reduction   = true
0.00.056.231 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.231 I ggml_metal_init: has bfloat            = true
0.00.056.231 I ggml_metal_init: use bfloat            = true
0.00.056.231 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.979 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.178 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.183 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.202 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.329 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.331 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.331 I llama_new_context_with_model: graph nodes  = 967
0.00.086.331 I llama_new_context_with_model: graph splits = 2
0.00.086.349 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.411 I main: llama threadpool init, n_threads = 4
0.00.699.450 I 
0.00.699.496 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.497 I 
0.00.699.729 I sampler seed: 1234
0.00.699.734 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.776 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.778 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.778 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.544.288 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.544.289 I llama_perf_context_print:        load time =     690.27 ms
0.01.544.290 I llama_perf_context_print: prompt eval time =      46.22 ms /     7 tokens (    6.60 ms per token,   151.46 tokens per second)
0.01.544.290 I llama_perf_context_print:        eval time =     795.28 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.544.291 I llama_perf_context_print:       total time =     844.88 ms /    70 tokens
0.01.544.461 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.348 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.853 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.857 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.859 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.860 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.860 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.860 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.862 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.862 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.862 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.863 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.865 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.867 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.867 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.868 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.782 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.822 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.654 I llama_model_loader: - type  f32:  194 tensors
0.00.023.654 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.654 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.655 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.015 I llm_load_vocab: special tokens cache size = 25
0.00.049.674 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.677 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.677 I llm_load_print_meta: arch             = gptneox
0.00.049.678 I llm_load_print_meta: vocab type       = BPE
0.00.049.678 I llm_load_print_meta: n_vocab          = 50304
0.00.049.678 I llm_load_print_meta: n_merges         = 50009
0.00.049.678 I llm_load_print_meta: vocab_only       = 0
0.00.049.678 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.679 I llm_load_print_meta: n_embd           = 2048
0.00.049.679 I llm_load_print_meta: n_layer          = 24
0.00.049.682 I llm_load_print_meta: n_head           = 16
0.00.049.682 I llm_load_print_meta: n_head_kv        = 16
0.00.049.683 I llm_load_print_meta: n_rot            = 32
0.00.049.683 I llm_load_print_meta: n_swa            = 0
0.00.049.683 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.683 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.684 I llm_load_print_meta: n_gqa            = 1
0.00.049.685 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.685 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.686 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.686 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.687 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.687 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.687 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.688 I llm_load_print_meta: n_ff             = 8192
0.00.049.688 I llm_load_print_meta: n_expert         = 0
0.00.049.688 I llm_load_print_meta: n_expert_used    = 0
0.00.049.688 I llm_load_print_meta: causal attn      = 1
0.00.049.688 I llm_load_print_meta: pooling type     = 0
0.00.049.689 I llm_load_print_meta: rope type        = 2
0.00.049.689 I llm_load_print_meta: rope scaling     = linear
0.00.049.689 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.689 I llm_load_print_meta: freq_scale_train = 1
0.00.049.690 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.690 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.690 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.690 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.690 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.692 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.692 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.693 I llm_load_print_meta: model type       = 1.4B
0.00.049.693 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.693 I llm_load_print_meta: model params     = 1.41 B
0.00.049.694 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.694 I llm_load_print_meta: general.name     = 1.4B
0.00.049.694 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.694 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.695 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.695 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.695 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.695 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.695 I llm_load_print_meta: max token length = 1024
0.00.051.558 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.558 I llm_load_tensors: offloading output layer to GPU
0.00.051.558 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.569 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.570 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.464 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.464 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.465 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.465 I llama_new_context_with_model: n_batch       = 2048
0.00.052.465 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.465 I llama_new_context_with_model: flash_attn    = 0
0.00.052.466 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.466 I llama_new_context_with_model: freq_scale    = 1
0.00.052.466 I ggml_metal_init: allocating
0.00.052.469 I ggml_metal_init: found device: Apple M4
0.00.052.472 I ggml_metal_init: picking default device: Apple M4
0.00.053.059 I ggml_metal_init: using embedded metal library
0.00.055.369 I ggml_metal_init: GPU name:   Apple M4
0.00.055.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.372 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.372 I ggml_metal_init: simdgroup reduction   = true
0.00.055.372 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.372 I ggml_metal_init: has bfloat            = true
0.00.055.372 I ggml_metal_init: use bfloat            = true
0.00.055.373 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.373 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.149 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.028 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.036 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.055 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.106 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.107 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.108 I llama_new_context_with_model: graph nodes  = 967
0.00.086.108 I llama_new_context_with_model: graph splits = 2
0.00.086.123 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.265 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.756 I main: llama threadpool init, n_threads = 4
0.00.437.797 I 
0.00.437.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.823 I 
0.00.438.062 I sampler seed: 1234
0.00.438.068 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.438.110 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.438.112 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.438.112 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.117.630 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.117.630 I llama_perf_context_print:        load time =     428.40 ms
0.01.117.631 I llama_perf_context_print: prompt eval time =      35.82 ms /     7 tokens (    5.12 ms per token,   195.41 tokens per second)
0.01.117.632 I llama_perf_context_print:        eval time =     640.67 ms /    63 runs   (   10.17 ms per token,    98.33 tokens per second)
0.01.117.632 I llama_perf_context_print:       total time =     679.88 ms /    70 tokens
0.01.117.802 I ggml_metal_free: deallocating

real	0m1.137s
user	0m0.109s
sys	0m0.107s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.226 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.767 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.772 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.773 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.774 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.775 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.776 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.777 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.778 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.781 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.735 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.736 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.736 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.737 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.738 I llama_model_loader: - type  f32:  194 tensors
0.00.024.738 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.738 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.739 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.739 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.981 I llm_load_vocab: special tokens cache size = 25
0.00.052.008 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.011 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.011 I llm_load_print_meta: arch             = gptneox
0.00.052.011 I llm_load_print_meta: vocab type       = BPE
0.00.052.012 I llm_load_print_meta: n_vocab          = 50304
0.00.052.012 I llm_load_print_meta: n_merges         = 50009
0.00.052.012 I llm_load_print_meta: vocab_only       = 0
0.00.052.012 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.012 I llm_load_print_meta: n_embd           = 2048
0.00.052.012 I llm_load_print_meta: n_layer          = 24
0.00.052.016 I llm_load_print_meta: n_head           = 16
0.00.052.016 I llm_load_print_meta: n_head_kv        = 16
0.00.052.017 I llm_load_print_meta: n_rot            = 32
0.00.052.017 I llm_load_print_meta: n_swa            = 0
0.00.052.017 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.017 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.018 I llm_load_print_meta: n_gqa            = 1
0.00.052.019 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.019 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.020 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.020 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.020 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.021 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.021 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.022 I llm_load_print_meta: n_ff             = 8192
0.00.052.023 I llm_load_print_meta: n_expert         = 0
0.00.052.025 I llm_load_print_meta: n_expert_used    = 0
0.00.052.025 I llm_load_print_meta: causal attn      = 1
0.00.052.026 I llm_load_print_meta: pooling type     = 0
0.00.052.026 I llm_load_print_meta: rope type        = 2
0.00.052.026 I llm_load_print_meta: rope scaling     = linear
0.00.052.026 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.027 I llm_load_print_meta: freq_scale_train = 1
0.00.052.027 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.027 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.027 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.027 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.027 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.027 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.028 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.028 I llm_load_print_meta: model type       = 1.4B
0.00.052.028 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.029 I llm_load_print_meta: model params     = 1.41 B
0.00.052.029 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.029 I llm_load_print_meta: general.name     = 1.4B
0.00.052.036 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.037 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.037 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.037 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.037 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.037 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.037 I llm_load_print_meta: max token length = 1024
0.00.054.027 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.027 I llm_load_tensors: offloading output layer to GPU
0.00.054.027 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.038 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.040 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.952 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.953 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.953 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.953 I llama_new_context_with_model: n_batch       = 2048
0.00.054.953 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.953 I llama_new_context_with_model: flash_attn    = 0
0.00.054.954 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.954 I llama_new_context_with_model: freq_scale    = 1
0.00.054.955 I ggml_metal_init: allocating
0.00.054.958 I ggml_metal_init: found device: Apple M4
0.00.054.960 I ggml_metal_init: picking default device: Apple M4
0.00.055.567 I ggml_metal_init: using embedded metal library
0.00.057.915 I ggml_metal_init: GPU name:   Apple M4
0.00.057.918 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.919 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.919 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.919 I ggml_metal_init: simdgroup reduction   = true
0.00.057.919 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.919 I ggml_metal_init: has bfloat            = true
0.00.057.920 I ggml_metal_init: use bfloat            = true
0.00.057.920 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.925 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.850 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.708 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.715 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.737 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.713 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.714 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.714 I llama_new_context_with_model: graph nodes  = 967
0.00.087.714 I llama_new_context_with_model: graph splits = 2
0.00.087.729 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.977 I main: llama threadpool init, n_threads = 4
0.00.584.034 I 
0.00.584.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.065 I 
0.00.584.308 I sampler seed: 1234
0.00.584.312 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.584.361 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.584.364 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.584.364 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.320.641 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48005.41 tokens per second)
0.01.320.641 I llama_perf_context_print:        load time =     574.74 ms
0.01.320.643 I llama_perf_context_print: prompt eval time =      40.55 ms /     7 tokens (    5.79 ms per token,   172.61 tokens per second)
0.01.320.644 I llama_perf_context_print:        eval time =     693.20 ms /    63 runs   (   11.00 ms per token,    90.88 tokens per second)
0.01.320.645 I llama_perf_context_print:       total time =     736.67 ms /    70 tokens
0.01.320.841 I ggml_metal_free: deallocating

real	0m1.338s
user	0m0.110s
sys	0m0.136s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.831 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.294 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.026.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.301 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.302 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.302 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.307 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.307 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.307 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.308 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.308 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.309 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.312 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.312 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.312 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.130 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.166 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.071 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.072 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.073 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.073 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.073 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.035.074 I llama_model_loader: - type  f32:  194 tensors
0.00.035.074 I llama_model_loader: - type q4_K:   61 tensors
0.00.035.074 I llama_model_loader: - type q5_K:   24 tensors
0.00.035.074 I llama_model_loader: - type q6_K:   13 tensors
0.00.057.792 I llm_load_vocab: special tokens cache size = 25
0.00.063.959 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.962 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.962 I llm_load_print_meta: arch             = gptneox
0.00.063.963 I llm_load_print_meta: vocab type       = BPE
0.00.063.963 I llm_load_print_meta: n_vocab          = 50304
0.00.063.963 I llm_load_print_meta: n_merges         = 50009
0.00.063.963 I llm_load_print_meta: vocab_only       = 0
0.00.063.963 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.963 I llm_load_print_meta: n_embd           = 2048
0.00.063.964 I llm_load_print_meta: n_layer          = 24
0.00.063.966 I llm_load_print_meta: n_head           = 16
0.00.063.967 I llm_load_print_meta: n_head_kv        = 16
0.00.063.967 I llm_load_print_meta: n_rot            = 32
0.00.063.967 I llm_load_print_meta: n_swa            = 0
0.00.063.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.969 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.970 I llm_load_print_meta: n_gqa            = 1
0.00.063.971 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.971 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.972 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.972 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.972 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.972 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.972 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.973 I llm_load_print_meta: n_ff             = 8192
0.00.063.973 I llm_load_print_meta: n_expert         = 0
0.00.063.975 I llm_load_print_meta: n_expert_used    = 0
0.00.063.976 I llm_load_print_meta: causal attn      = 1
0.00.063.976 I llm_load_print_meta: pooling type     = 0
0.00.063.976 I llm_load_print_meta: rope type        = 2
0.00.063.976 I llm_load_print_meta: rope scaling     = linear
0.00.063.977 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.977 I llm_load_print_meta: freq_scale_train = 1
0.00.063.977 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.977 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.977 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.977 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.978 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.978 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.978 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.978 I llm_load_print_meta: model type       = 1.4B
0.00.063.978 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.063.979 I llm_load_print_meta: model params     = 1.41 B
0.00.063.979 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.063.979 I llm_load_print_meta: general.name     = 1.4B
0.00.063.980 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.980 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.983 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.983 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.983 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.989 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.991 I llm_load_print_meta: max token length = 1024
0.00.066.119 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.120 I llm_load_tensors: offloading output layer to GPU
0.00.066.120 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.131 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.066.132 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.067.059 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.060 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.060 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.060 I llama_new_context_with_model: n_batch       = 2048
0.00.067.060 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.060 I llama_new_context_with_model: flash_attn    = 0
0.00.067.061 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.061 I llama_new_context_with_model: freq_scale    = 1
0.00.067.061 I ggml_metal_init: allocating
0.00.067.064 I ggml_metal_init: found device: Apple M4
0.00.067.066 I ggml_metal_init: picking default device: Apple M4
0.00.067.682 I ggml_metal_init: using embedded metal library
0.00.070.272 I ggml_metal_init: GPU name:   Apple M4
0.00.070.273 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.274 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.274 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.276 I ggml_metal_init: simdgroup reduction   = true
0.00.070.276 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.276 I ggml_metal_init: has bfloat            = true
0.00.070.276 I ggml_metal_init: use bfloat            = true
0.00.070.277 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.277 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.188 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.101.346 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.358 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.376 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.358 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.360 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.360 I llama_new_context_with_model: graph nodes  = 967
0.00.102.360 I llama_new_context_with_model: graph splits = 2
0.00.102.375 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.510 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.858 I main: llama threadpool init, n_threads = 4
0.00.735.897 I 
0.00.735.919 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.919 I 
0.00.736.152 I sampler seed: 1234
0.00.736.157 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.177 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.178 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.178 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.499.380 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.499.381 I llama_perf_context_print:        load time =     726.02 ms
0.01.499.381 I llama_perf_context_print: prompt eval time =      47.16 ms /     7 tokens (    6.74 ms per token,   148.44 tokens per second)
0.01.499.382 I llama_perf_context_print:        eval time =     713.14 ms /    63 runs   (   11.32 ms per token,    88.34 tokens per second)
0.01.499.382 I llama_perf_context_print:       total time =     763.53 ms /    70 tokens
0.01.499.573 I ggml_metal_free: deallocating

real	0m1.518s
user	0m0.112s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.705 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.026.175 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.177 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.177 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.178 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.179 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.180 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.180 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.180 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.181 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.181 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.182 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.184 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.184 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.185 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.807 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.048 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.594 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.036.595 I llama_model_loader: - type  f32:  194 tensors
0.00.036.595 I llama_model_loader: - type q5_K:   61 tensors
0.00.036.595 I llama_model_loader: - type q6_K:   37 tensors
0.00.067.132 I llm_load_vocab: special tokens cache size = 25
0.00.077.356 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.360 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.361 I llm_load_print_meta: arch             = gptneox
0.00.077.361 I llm_load_print_meta: vocab type       = BPE
0.00.077.362 I llm_load_print_meta: n_vocab          = 50304
0.00.077.362 I llm_load_print_meta: n_merges         = 50009
0.00.077.362 I llm_load_print_meta: vocab_only       = 0
0.00.077.362 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.363 I llm_load_print_meta: n_embd           = 2048
0.00.077.363 I llm_load_print_meta: n_layer          = 24
0.00.077.367 I llm_load_print_meta: n_head           = 16
0.00.077.368 I llm_load_print_meta: n_head_kv        = 16
0.00.077.370 I llm_load_print_meta: n_rot            = 32
0.00.077.370 I llm_load_print_meta: n_swa            = 0
0.00.077.371 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.373 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.374 I llm_load_print_meta: n_gqa            = 1
0.00.077.375 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.376 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.377 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.377 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.378 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.378 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.378 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.379 I llm_load_print_meta: n_ff             = 8192
0.00.077.379 I llm_load_print_meta: n_expert         = 0
0.00.077.380 I llm_load_print_meta: n_expert_used    = 0
0.00.077.382 I llm_load_print_meta: causal attn      = 1
0.00.077.384 I llm_load_print_meta: pooling type     = 0
0.00.077.384 I llm_load_print_meta: rope type        = 2
0.00.077.384 I llm_load_print_meta: rope scaling     = linear
0.00.077.385 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.385 I llm_load_print_meta: freq_scale_train = 1
0.00.077.386 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.386 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.388 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.389 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.389 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.389 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.389 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.389 I llm_load_print_meta: model type       = 1.4B
0.00.077.390 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.077.391 I llm_load_print_meta: model params     = 1.41 B
0.00.077.391 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.077.392 I llm_load_print_meta: general.name     = 1.4B
0.00.077.392 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.392 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.394 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.394 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.395 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.077.395 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.395 I llm_load_print_meta: max token length = 1024
0.00.080.331 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.332 I llm_load_tensors: offloading output layer to GPU
0.00.080.332 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.344 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.080.345 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.081.852 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.853 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.853 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.853 I llama_new_context_with_model: n_batch       = 2048
0.00.081.854 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.854 I llama_new_context_with_model: flash_attn    = 0
0.00.081.855 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.855 I llama_new_context_with_model: freq_scale    = 1
0.00.081.856 I ggml_metal_init: allocating
0.00.081.863 I ggml_metal_init: found device: Apple M4
0.00.081.867 I ggml_metal_init: picking default device: Apple M4
0.00.082.793 I ggml_metal_init: using embedded metal library
0.00.086.654 I ggml_metal_init: GPU name:   Apple M4
0.00.086.657 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.659 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.660 I ggml_metal_init: simdgroup reduction   = true
0.00.086.660 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.660 I ggml_metal_init: has bfloat            = true
0.00.086.660 I ggml_metal_init: use bfloat            = true
0.00.086.661 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.662 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.718 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.122.232 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.238 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.258 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.123.410 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.123.411 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.123.412 I llama_new_context_with_model: graph nodes  = 967
0.00.123.412 I llama_new_context_with_model: graph splits = 2
0.00.123.429 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.123.570 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.123.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.321 I main: llama threadpool init, n_threads = 4
0.00.847.365 I 
0.00.847.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.388 I 
0.00.847.699 I sampler seed: 1234
0.00.847.704 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.847.726 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.847.728 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.847.728 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.698.256 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62280.70 tokens per second)
0.01.698.257 I llama_perf_context_print:        load time =     838.61 ms
0.01.698.257 I llama_perf_context_print: prompt eval time =      51.77 ms /     7 tokens (    7.40 ms per token,   135.21 tokens per second)
0.01.698.258 I llama_perf_context_print:        eval time =     795.86 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.698.258 I llama_perf_context_print:       total time =     850.94 ms /    70 tokens
0.01.698.455 I ggml_metal_free: deallocating

real	0m1.731s
user	0m0.135s
sys	0m0.175s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.373 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.897 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.902 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.903 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.903 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.904 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.906 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.906 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.907 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.907 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.907 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.908 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.908 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.909 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.909 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.913 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.914 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.928 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.991 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.872 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.873 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.874 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.874 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.875 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.875 I llama_model_loader: - type  f32:  194 tensors
0.00.024.876 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.170 I llm_load_vocab: special tokens cache size = 25
0.00.052.277 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.280 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.280 I llm_load_print_meta: arch             = gptneox
0.00.052.280 I llm_load_print_meta: vocab type       = BPE
0.00.052.281 I llm_load_print_meta: n_vocab          = 50304
0.00.052.281 I llm_load_print_meta: n_merges         = 50009
0.00.052.281 I llm_load_print_meta: vocab_only       = 0
0.00.052.281 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.281 I llm_load_print_meta: n_embd           = 2048
0.00.052.282 I llm_load_print_meta: n_layer          = 24
0.00.052.284 I llm_load_print_meta: n_head           = 16
0.00.052.287 I llm_load_print_meta: n_head_kv        = 16
0.00.052.287 I llm_load_print_meta: n_rot            = 32
0.00.052.287 I llm_load_print_meta: n_swa            = 0
0.00.052.288 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.288 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.288 I llm_load_print_meta: n_gqa            = 1
0.00.052.294 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.295 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.296 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.296 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.296 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.297 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.297 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.299 I llm_load_print_meta: n_ff             = 8192
0.00.052.299 I llm_load_print_meta: n_expert         = 0
0.00.052.299 I llm_load_print_meta: n_expert_used    = 0
0.00.052.299 I llm_load_print_meta: causal attn      = 1
0.00.052.301 I llm_load_print_meta: pooling type     = 0
0.00.052.302 I llm_load_print_meta: rope type        = 2
0.00.052.302 I llm_load_print_meta: rope scaling     = linear
0.00.052.302 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.302 I llm_load_print_meta: freq_scale_train = 1
0.00.052.308 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.308 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.308 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.308 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.308 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.308 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.308 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.309 I llm_load_print_meta: model type       = 1.4B
0.00.052.310 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.310 I llm_load_print_meta: model params     = 1.41 B
0.00.052.310 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.311 I llm_load_print_meta: general.name     = 1.4B
0.00.052.311 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.311 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.311 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.311 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.311 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.312 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.312 I llm_load_print_meta: max token length = 1024
0.00.054.425 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.425 I llm_load_tensors: offloading output layer to GPU
0.00.054.425 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.436 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.437 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.360 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.361 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.361 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.361 I llama_new_context_with_model: n_batch       = 2048
0.00.055.362 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.362 I llama_new_context_with_model: flash_attn    = 0
0.00.055.362 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.362 I llama_new_context_with_model: freq_scale    = 1
0.00.055.363 I ggml_metal_init: allocating
0.00.055.366 I ggml_metal_init: found device: Apple M4
0.00.055.368 I ggml_metal_init: picking default device: Apple M4
0.00.055.969 I ggml_metal_init: using embedded metal library
0.00.058.350 I ggml_metal_init: GPU name:   Apple M4
0.00.058.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.352 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.352 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.352 I ggml_metal_init: simdgroup reduction   = true
0.00.058.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.354 I ggml_metal_init: has bfloat            = true
0.00.058.354 I ggml_metal_init: use bfloat            = true
0.00.058.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.482 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.203 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.209 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.226 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.259 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.260 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.261 I llama_new_context_with_model: graph nodes  = 967
0.00.090.261 I llama_new_context_with_model: graph splits = 2
0.00.090.276 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.417 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.418 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.840 I main: llama threadpool init, n_threads = 4
0.00.792.881 I 
0.00.792.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.918 I 
0.00.793.151 I sampler seed: 1234
0.00.793.156 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.188 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.188 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.188 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.674.768 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.01.674.768 I llama_perf_context_print:        load time =     783.46 ms
0.01.674.769 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.47 tokens per second)
0.01.674.770 I llama_perf_context_print:        eval time =     824.17 ms /    63 runs   (   13.08 ms per token,    76.44 tokens per second)
0.01.674.770 I llama_perf_context_print:       total time =     881.93 ms /    70 tokens
0.01.674.941 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.112s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.670 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.823 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.742 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.747 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.151 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.152 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.152 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.153 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.153 I llama_model_loader: - type  f32:  194 tensors
0.00.053.154 I llama_model_loader: - type  f16:   98 tensors
0.00.083.501 I llm_load_vocab: special tokens cache size = 25
0.00.089.923 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.926 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.926 I llm_load_print_meta: arch             = gptneox
0.00.089.927 I llm_load_print_meta: vocab type       = BPE
0.00.089.927 I llm_load_print_meta: n_vocab          = 50304
0.00.089.927 I llm_load_print_meta: n_merges         = 50009
0.00.089.927 I llm_load_print_meta: vocab_only       = 0
0.00.089.927 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.927 I llm_load_print_meta: n_embd           = 2048
0.00.089.928 I llm_load_print_meta: n_layer          = 24
0.00.089.931 I llm_load_print_meta: n_head           = 16
0.00.089.932 I llm_load_print_meta: n_head_kv        = 16
0.00.089.932 I llm_load_print_meta: n_rot            = 32
0.00.089.932 I llm_load_print_meta: n_swa            = 0
0.00.089.932 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.932 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.933 I llm_load_print_meta: n_gqa            = 1
0.00.089.934 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.934 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.935 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.935 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.935 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.935 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.935 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.936 I llm_load_print_meta: n_ff             = 8192
0.00.089.936 I llm_load_print_meta: n_expert         = 0
0.00.089.936 I llm_load_print_meta: n_expert_used    = 0
0.00.089.937 I llm_load_print_meta: causal attn      = 1
0.00.089.937 I llm_load_print_meta: pooling type     = 0
0.00.089.937 I llm_load_print_meta: rope type        = 2
0.00.089.937 I llm_load_print_meta: rope scaling     = linear
0.00.089.937 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.938 I llm_load_print_meta: freq_scale_train = 1
0.00.089.938 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.938 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.940 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.941 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.941 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.941 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.941 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.941 I llm_load_print_meta: model type       = 1.4B
0.00.089.941 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.942 I llm_load_print_meta: model params     = 1.41 B
0.00.089.942 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.942 I llm_load_print_meta: general.name     = 1.4B
0.00.089.946 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.946 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.946 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.948 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.948 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.949 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.949 I llm_load_print_meta: max token length = 1024
0.00.092.643 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.643 I llm_load_tensors: offloading output layer to GPU
0.00.092.643 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.654 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.655 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.585 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.586 I llama_new_context_with_model: n_ctx         = 128
0.00.093.586 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.586 I llama_new_context_with_model: n_batch       = 128
0.00.093.586 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.586 I llama_new_context_with_model: flash_attn    = 0
0.00.093.587 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.587 I llama_new_context_with_model: freq_scale    = 1
0.00.093.587 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.588 I ggml_metal_init: allocating
0.00.093.590 I ggml_metal_init: found device: Apple M4
0.00.093.592 I ggml_metal_init: picking default device: Apple M4
0.00.094.188 I ggml_metal_init: using embedded metal library
0.00.096.742 I ggml_metal_init: GPU name:   Apple M4
0.00.096.743 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.744 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.744 I ggml_metal_init: simdgroup reduction   = true
0.00.096.744 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.744 I ggml_metal_init: has bfloat            = true
0.00.096.744 I ggml_metal_init: use bfloat            = true
0.00.096.745 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.745 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.917 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.108.317 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.319 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.333 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.212 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.213 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.214 I llama_new_context_with_model: graph nodes  = 967
0.00.109.214 I llama_new_context_with_model: graph splits = 2
0.00.109.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.009.571 I 
0.01.009.628 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.009.697 I perplexity: tokenizing the input ..
0.01.024.432 I perplexity: tokenization took 14.729 ms
0.01.024.440 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.145.544 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.147.331 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.147.354 I llama_perf_context_print:        load time =     985.73 ms
0.01.147.355 I llama_perf_context_print: prompt eval time =     120.08 ms /   128 tokens (    0.94 ms per token,  1065.94 tokens per second)
0.01.147.356 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.147.357 I llama_perf_context_print:       total time =     137.79 ms /   129 tokens
0.01.147.988 I ggml_metal_free: deallocating

real	0m1.337s
user	0m0.125s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.937 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.572 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.580 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.580 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.582 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.585 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.587 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.755 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.153 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.345 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.346 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.346 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.347 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.347 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.347 I llama_model_loader: - type  f32:  194 tensors
0.00.030.348 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.541 I llm_load_vocab: special tokens cache size = 25
0.00.061.669 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.672 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.672 I llm_load_print_meta: arch             = gptneox
0.00.061.673 I llm_load_print_meta: vocab type       = BPE
0.00.061.673 I llm_load_print_meta: n_vocab          = 50304
0.00.061.673 I llm_load_print_meta: n_merges         = 50009
0.00.061.673 I llm_load_print_meta: vocab_only       = 0
0.00.061.673 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.673 I llm_load_print_meta: n_embd           = 2048
0.00.061.673 I llm_load_print_meta: n_layer          = 24
0.00.061.677 I llm_load_print_meta: n_head           = 16
0.00.061.677 I llm_load_print_meta: n_head_kv        = 16
0.00.061.678 I llm_load_print_meta: n_rot            = 32
0.00.061.680 I llm_load_print_meta: n_swa            = 0
0.00.061.680 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.680 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.681 I llm_load_print_meta: n_gqa            = 1
0.00.061.682 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.683 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.684 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.685 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.685 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.685 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.686 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.686 I llm_load_print_meta: n_ff             = 8192
0.00.061.686 I llm_load_print_meta: n_expert         = 0
0.00.061.686 I llm_load_print_meta: n_expert_used    = 0
0.00.061.687 I llm_load_print_meta: causal attn      = 1
0.00.061.687 I llm_load_print_meta: pooling type     = 0
0.00.061.687 I llm_load_print_meta: rope type        = 2
0.00.061.687 I llm_load_print_meta: rope scaling     = linear
0.00.061.687 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.688 I llm_load_print_meta: freq_scale_train = 1
0.00.061.688 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.692 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.692 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.692 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.693 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.693 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.693 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.694 I llm_load_print_meta: model type       = 1.4B
0.00.061.694 I llm_load_print_meta: model ftype      = Q8_0
0.00.061.694 I llm_load_print_meta: model params     = 1.41 B
0.00.061.698 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.061.698 I llm_load_print_meta: general.name     = 1.4B
0.00.061.698 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.698 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.698 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.699 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.700 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.061.701 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.701 I llm_load_print_meta: max token length = 1024
0.00.063.639 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.639 I llm_load_tensors: offloading output layer to GPU
0.00.063.639 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.645 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.645 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.593 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.594 I llama_new_context_with_model: n_ctx         = 128
0.00.064.595 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.064.595 I llama_new_context_with_model: n_batch       = 128
0.00.064.595 I llama_new_context_with_model: n_ubatch      = 128
0.00.064.595 I llama_new_context_with_model: flash_attn    = 0
0.00.064.596 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.596 I llama_new_context_with_model: freq_scale    = 1
0.00.064.596 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.597 I ggml_metal_init: allocating
0.00.064.602 I ggml_metal_init: found device: Apple M4
0.00.064.604 I ggml_metal_init: picking default device: Apple M4
0.00.065.202 I ggml_metal_init: using embedded metal library
0.00.067.706 I ggml_metal_init: GPU name:   Apple M4
0.00.067.707 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.708 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.708 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.708 I ggml_metal_init: simdgroup reduction   = true
0.00.067.708 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.709 I ggml_metal_init: has bfloat            = true
0.00.067.709 I ggml_metal_init: use bfloat            = true
0.00.067.709 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.736 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.079.227 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.232 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.257 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.080.219 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.080.220 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.080.220 I llama_new_context_with_model: graph nodes  = 967
0.00.080.220 I llama_new_context_with_model: graph splits = 2
0.00.080.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.810 I 
0.00.654.832 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.878 I perplexity: tokenizing the input ..
0.00.663.067 I perplexity: tokenization took 8.188 ms
0.00.663.071 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.208 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.788.410 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.788.430 I llama_perf_context_print:        load time =     643.87 ms
0.00.788.431 I llama_perf_context_print: prompt eval time =     123.91 ms /   128 tokens (    0.97 ms per token,  1032.99 tokens per second)
0.00.788.432 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.432 I llama_perf_context_print:       total time =     133.62 ms /   129 tokens
0.00.788.950 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.091s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.599 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.694 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.705 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.706 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.706 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.707 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.708 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.708 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.709 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.709 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.709 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.711 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.712 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.655 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.457 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.458 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.459 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.459 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.459 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.460 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.460 I llama_model_loader: - type  f32:  194 tensors
0.00.024.461 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.461 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.466 I llm_load_vocab: special tokens cache size = 25
0.00.051.301 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.304 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.304 I llm_load_print_meta: arch             = gptneox
0.00.051.304 I llm_load_print_meta: vocab type       = BPE
0.00.051.304 I llm_load_print_meta: n_vocab          = 50304
0.00.051.305 I llm_load_print_meta: n_merges         = 50009
0.00.051.305 I llm_load_print_meta: vocab_only       = 0
0.00.051.305 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.305 I llm_load_print_meta: n_embd           = 2048
0.00.051.305 I llm_load_print_meta: n_layer          = 24
0.00.051.308 I llm_load_print_meta: n_head           = 16
0.00.051.309 I llm_load_print_meta: n_head_kv        = 16
0.00.051.309 I llm_load_print_meta: n_rot            = 32
0.00.051.309 I llm_load_print_meta: n_swa            = 0
0.00.051.309 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.309 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.310 I llm_load_print_meta: n_gqa            = 1
0.00.051.311 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.312 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.312 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.312 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.314 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.315 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.315 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.315 I llm_load_print_meta: n_ff             = 8192
0.00.051.315 I llm_load_print_meta: n_expert         = 0
0.00.051.317 I llm_load_print_meta: n_expert_used    = 0
0.00.051.317 I llm_load_print_meta: causal attn      = 1
0.00.051.317 I llm_load_print_meta: pooling type     = 0
0.00.051.317 I llm_load_print_meta: rope type        = 2
0.00.051.317 I llm_load_print_meta: rope scaling     = linear
0.00.051.318 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.318 I llm_load_print_meta: freq_scale_train = 1
0.00.051.318 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.319 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.319 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.319 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.319 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.319 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.320 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.321 I llm_load_print_meta: model type       = 1.4B
0.00.051.321 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.321 I llm_load_print_meta: model params     = 1.41 B
0.00.051.322 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.323 I llm_load_print_meta: general.name     = 1.4B
0.00.051.323 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.324 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.324 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.324 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.324 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.325 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.326 I llm_load_print_meta: max token length = 1024
0.00.053.215 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.215 I llm_load_tensors: offloading output layer to GPU
0.00.053.215 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.226 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.227 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.166 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.167 I llama_new_context_with_model: n_ctx         = 128
0.00.054.167 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.167 I llama_new_context_with_model: n_batch       = 128
0.00.054.167 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.167 I llama_new_context_with_model: flash_attn    = 0
0.00.054.168 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.168 I llama_new_context_with_model: freq_scale    = 1
0.00.054.168 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.169 I ggml_metal_init: allocating
0.00.054.174 I ggml_metal_init: found device: Apple M4
0.00.054.177 I ggml_metal_init: picking default device: Apple M4
0.00.054.755 I ggml_metal_init: using embedded metal library
0.00.057.122 I ggml_metal_init: GPU name:   Apple M4
0.00.057.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.124 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.125 I ggml_metal_init: simdgroup reduction   = true
0.00.057.125 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.125 I ggml_metal_init: has bfloat            = true
0.00.057.125 I ggml_metal_init: use bfloat            = true
0.00.057.126 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.128 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.612 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.819 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.821 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.835 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.705 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.706 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.707 I llama_new_context_with_model: graph nodes  = 967
0.00.068.707 I llama_new_context_with_model: graph splits = 2
0.00.068.719 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.720 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.809 I 
0.00.639.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.851 I perplexity: tokenizing the input ..
0.00.647.630 I perplexity: tokenization took 7.778 ms
0.00.647.638 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.770.264 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.771.435 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.771.448 I llama_perf_context_print:        load time =     630.21 ms
0.00.771.449 I llama_perf_context_print: prompt eval time =     122.40 ms /   128 tokens (    0.96 ms per token,  1045.77 tokens per second)
0.00.771.450 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.771.450 I llama_perf_context_print:       total time =     131.64 ms /   129 tokens
0.00.771.871 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.078s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.605 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.535 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.539 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.541 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.541 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.542 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.542 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.543 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.546 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.546 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.546 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.547 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.547 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.549 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.549 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.321 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.321 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.321 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.321 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.322 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.322 I llama_model_loader: - type  f32:  194 tensors
0.00.023.323 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.323 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.064 I llm_load_vocab: special tokens cache size = 25
0.00.050.104 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.106 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.106 I llm_load_print_meta: arch             = gptneox
0.00.050.107 I llm_load_print_meta: vocab type       = BPE
0.00.050.107 I llm_load_print_meta: n_vocab          = 50304
0.00.050.107 I llm_load_print_meta: n_merges         = 50009
0.00.050.107 I llm_load_print_meta: vocab_only       = 0
0.00.050.108 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.108 I llm_load_print_meta: n_embd           = 2048
0.00.050.108 I llm_load_print_meta: n_layer          = 24
0.00.050.110 I llm_load_print_meta: n_head           = 16
0.00.050.111 I llm_load_print_meta: n_head_kv        = 16
0.00.050.111 I llm_load_print_meta: n_rot            = 32
0.00.050.111 I llm_load_print_meta: n_swa            = 0
0.00.050.113 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.113 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.114 I llm_load_print_meta: n_gqa            = 1
0.00.050.115 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.116 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.116 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.117 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.117 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.117 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.117 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.118 I llm_load_print_meta: n_ff             = 8192
0.00.050.118 I llm_load_print_meta: n_expert         = 0
0.00.050.118 I llm_load_print_meta: n_expert_used    = 0
0.00.050.119 I llm_load_print_meta: causal attn      = 1
0.00.050.119 I llm_load_print_meta: pooling type     = 0
0.00.050.119 I llm_load_print_meta: rope type        = 2
0.00.050.119 I llm_load_print_meta: rope scaling     = linear
0.00.050.120 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.120 I llm_load_print_meta: freq_scale_train = 1
0.00.050.120 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.120 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.121 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.121 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.121 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.121 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.121 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.121 I llm_load_print_meta: model type       = 1.4B
0.00.050.122 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.122 I llm_load_print_meta: model params     = 1.41 B
0.00.050.123 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.123 I llm_load_print_meta: general.name     = 1.4B
0.00.050.123 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.124 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.124 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.124 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.126 I llm_load_print_meta: max token length = 1024
0.00.052.052 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.053 I llm_load_tensors: offloading output layer to GPU
0.00.052.053 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.063 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.064 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.994 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.995 I llama_new_context_with_model: n_ctx         = 128
0.00.052.995 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.995 I llama_new_context_with_model: n_batch       = 128
0.00.052.996 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.996 I llama_new_context_with_model: flash_attn    = 0
0.00.052.996 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.996 I llama_new_context_with_model: freq_scale    = 1
0.00.052.997 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.997 I ggml_metal_init: allocating
0.00.053.003 I ggml_metal_init: found device: Apple M4
0.00.053.005 I ggml_metal_init: picking default device: Apple M4
0.00.053.577 I ggml_metal_init: using embedded metal library
0.00.055.924 I ggml_metal_init: GPU name:   Apple M4
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.927 I ggml_metal_init: simdgroup reduction   = true
0.00.055.927 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.927 I ggml_metal_init: has bfloat            = true
0.00.055.927 I ggml_metal_init: use bfloat            = true
0.00.055.928 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.537 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.770 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.778 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.792 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.711 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.712 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.712 I llama_new_context_with_model: graph nodes  = 967
0.00.067.712 I llama_new_context_with_model: graph splits = 2
0.00.067.725 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.053 I 
0.00.728.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.128 I perplexity: tokenizing the input ..
0.00.736.462 I perplexity: tokenization took 8.331 ms
0.00.736.470 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.173 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.860.430 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.860.442 I llama_perf_context_print:        load time =     719.44 ms
0.00.860.443 I llama_perf_context_print: prompt eval time =     122.48 ms /   128 tokens (    0.96 ms per token,  1045.10 tokens per second)
0.00.860.444 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.860.444 I llama_perf_context_print:       total time =     132.39 ms /   129 tokens
0.00.860.907 I ggml_metal_free: deallocating

real	0m0.875s
user	0m0.078s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.045 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.652 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.653 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.655 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.655 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.656 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.660 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.661 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.661 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.664 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.558 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.328 I llama_model_loader: - type  f32:  194 tensors
0.00.024.328 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.329 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.650 I llm_load_vocab: special tokens cache size = 25
0.00.050.588 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.591 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.591 I llm_load_print_meta: arch             = gptneox
0.00.050.592 I llm_load_print_meta: vocab type       = BPE
0.00.050.592 I llm_load_print_meta: n_vocab          = 50304
0.00.050.592 I llm_load_print_meta: n_merges         = 50009
0.00.050.592 I llm_load_print_meta: vocab_only       = 0
0.00.050.593 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.593 I llm_load_print_meta: n_embd           = 2048
0.00.050.593 I llm_load_print_meta: n_layer          = 24
0.00.050.596 I llm_load_print_meta: n_head           = 16
0.00.050.598 I llm_load_print_meta: n_head_kv        = 16
0.00.050.598 I llm_load_print_meta: n_rot            = 32
0.00.050.599 I llm_load_print_meta: n_swa            = 0
0.00.050.599 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.599 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.600 I llm_load_print_meta: n_gqa            = 1
0.00.050.600 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.601 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.602 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.602 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.602 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.602 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.602 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.603 I llm_load_print_meta: n_ff             = 8192
0.00.050.603 I llm_load_print_meta: n_expert         = 0
0.00.050.604 I llm_load_print_meta: n_expert_used    = 0
0.00.050.606 I llm_load_print_meta: causal attn      = 1
0.00.050.606 I llm_load_print_meta: pooling type     = 0
0.00.050.606 I llm_load_print_meta: rope type        = 2
0.00.050.606 I llm_load_print_meta: rope scaling     = linear
0.00.050.606 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.607 I llm_load_print_meta: freq_scale_train = 1
0.00.050.607 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.607 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.608 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.608 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.608 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.608 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.608 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.608 I llm_load_print_meta: model type       = 1.4B
0.00.050.609 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.609 I llm_load_print_meta: model params     = 1.41 B
0.00.050.610 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.610 I llm_load_print_meta: general.name     = 1.4B
0.00.050.610 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.610 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.611 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.611 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.611 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.611 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.612 I llm_load_print_meta: max token length = 1024
0.00.052.605 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.605 I llm_load_tensors: offloading output layer to GPU
0.00.052.605 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.617 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.618 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.542 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.543 I llama_new_context_with_model: n_ctx         = 128
0.00.053.543 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.543 I llama_new_context_with_model: n_batch       = 128
0.00.053.543 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.543 I llama_new_context_with_model: flash_attn    = 0
0.00.053.544 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.544 I llama_new_context_with_model: freq_scale    = 1
0.00.053.544 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.545 I ggml_metal_init: allocating
0.00.053.548 I ggml_metal_init: found device: Apple M4
0.00.053.550 I ggml_metal_init: picking default device: Apple M4
0.00.054.098 I ggml_metal_init: using embedded metal library
0.00.056.421 I ggml_metal_init: GPU name:   Apple M4
0.00.056.422 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.423 I ggml_metal_init: simdgroup reduction   = true
0.00.056.423 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.423 I ggml_metal_init: has bfloat            = true
0.00.056.424 I ggml_metal_init: use bfloat            = true
0.00.056.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.424 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.220 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.499 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.503 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.519 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.411 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.412 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.413 I llama_new_context_with_model: graph nodes  = 967
0.00.068.413 I llama_new_context_with_model: graph splits = 2
0.00.068.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.894 I 
0.00.768.931 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.945 I perplexity: tokenizing the input ..
0.00.776.914 I perplexity: tokenization took 7.968 ms
0.00.776.918 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.911.590 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.912.742 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.912.759 I llama_perf_context_print:        load time =     758.84 ms
0.00.912.760 I llama_perf_context_print: prompt eval time =     134.45 ms /   128 tokens (    1.05 ms per token,   952.05 tokens per second)
0.00.912.760 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.912.761 I llama_perf_context_print:       total time =     143.87 ms /   129 tokens
0.00.913.225 I ggml_metal_free: deallocating

real	0m0.928s
user	0m0.078s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.828 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.429 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.435 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.437 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.437 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.363 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.398 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.283 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.284 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.286 I llama_model_loader: - type  f32:  194 tensors
0.00.023.286 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.286 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.265 I llm_load_vocab: special tokens cache size = 25
0.00.050.185 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.187 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.188 I llm_load_print_meta: arch             = gptneox
0.00.050.188 I llm_load_print_meta: vocab type       = BPE
0.00.050.188 I llm_load_print_meta: n_vocab          = 50304
0.00.050.188 I llm_load_print_meta: n_merges         = 50009
0.00.050.189 I llm_load_print_meta: vocab_only       = 0
0.00.050.189 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.189 I llm_load_print_meta: n_embd           = 2048
0.00.050.189 I llm_load_print_meta: n_layer          = 24
0.00.050.192 I llm_load_print_meta: n_head           = 16
0.00.050.193 I llm_load_print_meta: n_head_kv        = 16
0.00.050.193 I llm_load_print_meta: n_rot            = 32
0.00.050.193 I llm_load_print_meta: n_swa            = 0
0.00.050.194 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.194 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.195 I llm_load_print_meta: n_gqa            = 1
0.00.050.197 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.197 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.198 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.198 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.198 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.199 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.199 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.199 I llm_load_print_meta: n_ff             = 8192
0.00.050.200 I llm_load_print_meta: n_expert         = 0
0.00.050.200 I llm_load_print_meta: n_expert_used    = 0
0.00.050.200 I llm_load_print_meta: causal attn      = 1
0.00.050.200 I llm_load_print_meta: pooling type     = 0
0.00.050.200 I llm_load_print_meta: rope type        = 2
0.00.050.200 I llm_load_print_meta: rope scaling     = linear
0.00.050.203 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.203 I llm_load_print_meta: freq_scale_train = 1
0.00.050.203 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.203 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.204 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.204 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.204 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.204 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.204 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.204 I llm_load_print_meta: model type       = 1.4B
0.00.050.205 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.205 I llm_load_print_meta: model params     = 1.41 B
0.00.050.206 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.206 I llm_load_print_meta: general.name     = 1.4B
0.00.050.206 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.206 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.207 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.207 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.207 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.207 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.207 I llm_load_print_meta: max token length = 1024
0.00.052.265 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.266 I llm_load_tensors: offloading output layer to GPU
0.00.052.266 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.276 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.278 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.207 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.208 I llama_new_context_with_model: n_ctx         = 128
0.00.053.208 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.208 I llama_new_context_with_model: n_batch       = 128
0.00.053.209 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.209 I llama_new_context_with_model: flash_attn    = 0
0.00.053.209 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.209 I llama_new_context_with_model: freq_scale    = 1
0.00.053.210 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.210 I ggml_metal_init: allocating
0.00.053.214 I ggml_metal_init: found device: Apple M4
0.00.053.216 I ggml_metal_init: picking default device: Apple M4
0.00.053.776 I ggml_metal_init: using embedded metal library
0.00.056.098 I ggml_metal_init: GPU name:   Apple M4
0.00.056.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.100 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.100 I ggml_metal_init: simdgroup reduction   = true
0.00.056.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.101 I ggml_metal_init: has bfloat            = true
0.00.056.101 I ggml_metal_init: use bfloat            = true
0.00.056.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.957 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.228 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.230 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.254 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.200 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.201 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.201 I llama_new_context_with_model: graph nodes  = 967
0.00.068.201 I llama_new_context_with_model: graph splits = 2
0.00.068.213 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.968 I 
0.00.659.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.043 I perplexity: tokenizing the input ..
0.00.667.304 I perplexity: tokenization took 8.26 ms
0.00.667.312 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.802.340 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.803.557 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.803.571 I llama_perf_context_print:        load time =     650.13 ms
0.00.803.572 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.69 tokens per second)
0.00.803.573 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.573 I llama_perf_context_print:       total time =     144.61 ms /   129 tokens
0.00.804.012 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.079s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.369 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.910 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.912 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.912 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.913 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.914 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.914 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.916 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.918 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.708 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.764 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.560 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.561 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.561 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.562 I llama_model_loader: - type  f32:  194 tensors
0.00.023.562 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.562 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.563 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.890 I llm_load_vocab: special tokens cache size = 25
0.00.049.738 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.741 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.741 I llm_load_print_meta: arch             = gptneox
0.00.049.742 I llm_load_print_meta: vocab type       = BPE
0.00.049.742 I llm_load_print_meta: n_vocab          = 50304
0.00.049.742 I llm_load_print_meta: n_merges         = 50009
0.00.049.742 I llm_load_print_meta: vocab_only       = 0
0.00.049.743 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.743 I llm_load_print_meta: n_embd           = 2048
0.00.049.743 I llm_load_print_meta: n_layer          = 24
0.00.049.746 I llm_load_print_meta: n_head           = 16
0.00.049.746 I llm_load_print_meta: n_head_kv        = 16
0.00.049.747 I llm_load_print_meta: n_rot            = 32
0.00.049.747 I llm_load_print_meta: n_swa            = 0
0.00.049.747 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.748 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.749 I llm_load_print_meta: n_gqa            = 1
0.00.049.750 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.751 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.751 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.752 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.752 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.752 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.752 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.753 I llm_load_print_meta: n_ff             = 8192
0.00.049.753 I llm_load_print_meta: n_expert         = 0
0.00.049.753 I llm_load_print_meta: n_expert_used    = 0
0.00.049.753 I llm_load_print_meta: causal attn      = 1
0.00.049.753 I llm_load_print_meta: pooling type     = 0
0.00.049.754 I llm_load_print_meta: rope type        = 2
0.00.049.754 I llm_load_print_meta: rope scaling     = linear
0.00.049.754 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.756 I llm_load_print_meta: freq_scale_train = 1
0.00.049.756 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.757 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.757 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.757 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.757 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.757 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.757 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.757 I llm_load_print_meta: model type       = 1.4B
0.00.049.758 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.758 I llm_load_print_meta: model params     = 1.41 B
0.00.049.759 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.759 I llm_load_print_meta: general.name     = 1.4B
0.00.049.763 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.763 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.763 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.763 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.764 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.764 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.764 I llm_load_print_meta: max token length = 1024
0.00.051.620 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.620 I llm_load_tensors: offloading output layer to GPU
0.00.051.620 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.631 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.632 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.535 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.536 I llama_new_context_with_model: n_ctx         = 128
0.00.052.536 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.536 I llama_new_context_with_model: n_batch       = 128
0.00.052.536 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.537 I llama_new_context_with_model: flash_attn    = 0
0.00.052.537 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.537 I llama_new_context_with_model: freq_scale    = 1
0.00.052.538 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.538 I ggml_metal_init: allocating
0.00.052.541 I ggml_metal_init: found device: Apple M4
0.00.052.543 I ggml_metal_init: picking default device: Apple M4
0.00.053.099 I ggml_metal_init: using embedded metal library
0.00.055.415 I ggml_metal_init: GPU name:   Apple M4
0.00.055.417 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.418 I ggml_metal_init: simdgroup reduction   = true
0.00.055.418 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.418 I ggml_metal_init: has bfloat            = true
0.00.055.418 I ggml_metal_init: use bfloat            = true
0.00.055.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.941 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.197 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.201 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.217 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.148 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.149 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.149 I llama_new_context_with_model: graph nodes  = 967
0.00.067.149 I llama_new_context_with_model: graph splits = 2
0.00.067.162 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.162 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.405.833 I 
0.00.405.857 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.405.871 I perplexity: tokenizing the input ..
0.00.413.087 I perplexity: tokenization took 7.213 ms
0.00.413.091 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.546.019 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.547.211 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.547.236 I llama_perf_context_print:        load time =     396.46 ms
0.00.547.239 I llama_perf_context_print: prompt eval time =     132.63 ms /   128 tokens (    1.04 ms per token,   965.08 tokens per second)
0.00.547.240 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.547.241 I llama_perf_context_print:       total time =     141.40 ms /   129 tokens
0.00.547.712 I ggml_metal_free: deallocating

real	0m0.562s
user	0m0.078s
sys	0m0.066s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.575 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.283 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.289 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.289 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.290 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.290 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.292 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.298 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.059 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.060 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.060 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.061 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.061 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.062 I llama_model_loader: - type  f32:  194 tensors
0.00.023.062 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.062 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.063 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.063 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.209 I llm_load_vocab: special tokens cache size = 25
0.00.049.046 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.048 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.049 I llm_load_print_meta: arch             = gptneox
0.00.049.049 I llm_load_print_meta: vocab type       = BPE
0.00.049.049 I llm_load_print_meta: n_vocab          = 50304
0.00.049.050 I llm_load_print_meta: n_merges         = 50009
0.00.049.050 I llm_load_print_meta: vocab_only       = 0
0.00.049.050 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.050 I llm_load_print_meta: n_embd           = 2048
0.00.049.050 I llm_load_print_meta: n_layer          = 24
0.00.049.053 I llm_load_print_meta: n_head           = 16
0.00.049.054 I llm_load_print_meta: n_head_kv        = 16
0.00.049.054 I llm_load_print_meta: n_rot            = 32
0.00.049.054 I llm_load_print_meta: n_swa            = 0
0.00.049.054 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.054 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.055 I llm_load_print_meta: n_gqa            = 1
0.00.049.056 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.057 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.057 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.057 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.058 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.058 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.058 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.059 I llm_load_print_meta: n_ff             = 8192
0.00.049.059 I llm_load_print_meta: n_expert         = 0
0.00.049.059 I llm_load_print_meta: n_expert_used    = 0
0.00.049.059 I llm_load_print_meta: causal attn      = 1
0.00.049.059 I llm_load_print_meta: pooling type     = 0
0.00.049.059 I llm_load_print_meta: rope type        = 2
0.00.049.060 I llm_load_print_meta: rope scaling     = linear
0.00.049.060 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.060 I llm_load_print_meta: freq_scale_train = 1
0.00.049.061 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.061 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.061 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.061 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.061 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.061 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.062 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.062 I llm_load_print_meta: model type       = 1.4B
0.00.049.062 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.063 I llm_load_print_meta: model params     = 1.41 B
0.00.049.063 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.064 I llm_load_print_meta: general.name     = 1.4B
0.00.049.064 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.064 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.064 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.064 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.065 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.065 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.065 I llm_load_print_meta: max token length = 1024
0.00.050.611 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.611 I llm_load_tensors: offloading output layer to GPU
0.00.050.612 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.622 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.623 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.456 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.456 I llama_new_context_with_model: n_ctx         = 128
0.00.051.457 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.457 I llama_new_context_with_model: n_batch       = 128
0.00.051.457 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.457 I llama_new_context_with_model: flash_attn    = 0
0.00.051.458 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.458 I llama_new_context_with_model: freq_scale    = 1
0.00.051.458 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.459 I ggml_metal_init: allocating
0.00.051.465 I ggml_metal_init: found device: Apple M4
0.00.051.467 I ggml_metal_init: picking default device: Apple M4
0.00.052.062 I ggml_metal_init: using embedded metal library
0.00.054.420 I ggml_metal_init: GPU name:   Apple M4
0.00.054.422 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.422 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.423 I ggml_metal_init: simdgroup reduction   = true
0.00.054.423 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.423 I ggml_metal_init: has bfloat            = true
0.00.054.423 I ggml_metal_init: use bfloat            = true
0.00.054.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.424 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.030 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.349 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.351 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.366 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.232 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.233 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.234 I llama_new_context_with_model: graph nodes  = 967
0.00.066.234 I llama_new_context_with_model: graph splits = 2
0.00.066.246 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.247 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.724 I 
0.00.487.765 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.779 I perplexity: tokenizing the input ..
0.00.495.589 I perplexity: tokenization took 7.807 ms
0.00.495.593 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.627.675 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.628.846 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.628.861 I llama_perf_context_print:        load time =     479.14 ms
0.00.628.862 I llama_perf_context_print: prompt eval time =     131.86 ms /   128 tokens (    1.03 ms per token,   970.76 tokens per second)
0.00.628.863 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.628.863 I llama_perf_context_print:       total time =     141.14 ms /   129 tokens
0.00.629.256 I ggml_metal_free: deallocating

real	0m0.643s
user	0m0.078s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.857 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.858 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.863 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.865 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.865 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.866 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.866 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.866 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.867 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.867 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.868 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.868 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.869 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.869 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.872 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.872 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.873 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.684 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.722 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.480 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.482 I llama_model_loader: - type  f32:  194 tensors
0.00.024.482 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.483 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.483 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.748 I llm_load_vocab: special tokens cache size = 25
0.00.050.656 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.659 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.659 I llm_load_print_meta: arch             = gptneox
0.00.050.659 I llm_load_print_meta: vocab type       = BPE
0.00.050.660 I llm_load_print_meta: n_vocab          = 50304
0.00.050.660 I llm_load_print_meta: n_merges         = 50009
0.00.050.660 I llm_load_print_meta: vocab_only       = 0
0.00.050.660 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.660 I llm_load_print_meta: n_embd           = 2048
0.00.050.660 I llm_load_print_meta: n_layer          = 24
0.00.050.663 I llm_load_print_meta: n_head           = 16
0.00.050.665 I llm_load_print_meta: n_head_kv        = 16
0.00.050.665 I llm_load_print_meta: n_rot            = 32
0.00.050.665 I llm_load_print_meta: n_swa            = 0
0.00.050.665 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.665 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.666 I llm_load_print_meta: n_gqa            = 1
0.00.050.667 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.668 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.668 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.668 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.669 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.670 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.670 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.671 I llm_load_print_meta: n_ff             = 8192
0.00.050.671 I llm_load_print_meta: n_expert         = 0
0.00.050.671 I llm_load_print_meta: n_expert_used    = 0
0.00.050.671 I llm_load_print_meta: causal attn      = 1
0.00.050.672 I llm_load_print_meta: pooling type     = 0
0.00.050.672 I llm_load_print_meta: rope type        = 2
0.00.050.672 I llm_load_print_meta: rope scaling     = linear
0.00.050.672 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.673 I llm_load_print_meta: freq_scale_train = 1
0.00.050.673 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.673 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.673 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.673 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.673 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.675 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.675 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.676 I llm_load_print_meta: model type       = 1.4B
0.00.050.676 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.677 I llm_load_print_meta: model params     = 1.41 B
0.00.050.677 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.677 I llm_load_print_meta: general.name     = 1.4B
0.00.050.678 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.678 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.678 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.678 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.682 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.682 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.682 I llm_load_print_meta: max token length = 1024
0.00.052.644 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.644 I llm_load_tensors: offloading output layer to GPU
0.00.052.644 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.655 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.656 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.561 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.562 I llama_new_context_with_model: n_ctx         = 128
0.00.053.562 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.562 I llama_new_context_with_model: n_batch       = 128
0.00.053.562 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.563 I llama_new_context_with_model: flash_attn    = 0
0.00.053.563 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.563 I llama_new_context_with_model: freq_scale    = 1
0.00.053.564 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.564 I ggml_metal_init: allocating
0.00.053.567 I ggml_metal_init: found device: Apple M4
0.00.053.569 I ggml_metal_init: picking default device: Apple M4
0.00.054.122 I ggml_metal_init: using embedded metal library
0.00.056.456 I ggml_metal_init: GPU name:   Apple M4
0.00.056.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.458 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.458 I ggml_metal_init: simdgroup reduction   = true
0.00.056.458 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.458 I ggml_metal_init: has bfloat            = true
0.00.056.458 I ggml_metal_init: use bfloat            = true
0.00.056.459 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.459 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.023 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.278 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.280 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.293 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.240 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.241 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.242 I llama_new_context_with_model: graph nodes  = 967
0.00.068.242 I llama_new_context_with_model: graph splits = 2
0.00.068.254 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.255 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.544.357 I 
0.00.544.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.544.421 I perplexity: tokenizing the input ..
0.00.552.148 I perplexity: tokenization took 7.725 ms
0.00.552.156 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.686.578 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.687.842 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.687.858 I llama_perf_context_print:        load time =     534.49 ms
0.00.687.859 I llama_perf_context_print: prompt eval time =     134.20 ms /   128 tokens (    1.05 ms per token,   953.82 tokens per second)
0.00.687.860 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.687.863 I llama_perf_context_print:       total time =     143.50 ms /   129 tokens
0.00.688.346 I ggml_metal_free: deallocating

real	0m0.704s
user	0m0.077s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.907 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.797 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.797 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.798 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.676 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.577 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.577 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.577 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.577 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.578 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.578 I llama_model_loader: - type  f32:  194 tensors
0.00.023.579 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.579 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.795 I llm_load_vocab: special tokens cache size = 25
0.00.050.745 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.748 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.748 I llm_load_print_meta: arch             = gptneox
0.00.050.748 I llm_load_print_meta: vocab type       = BPE
0.00.050.749 I llm_load_print_meta: n_vocab          = 50304
0.00.050.749 I llm_load_print_meta: n_merges         = 50009
0.00.050.749 I llm_load_print_meta: vocab_only       = 0
0.00.050.749 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.749 I llm_load_print_meta: n_embd           = 2048
0.00.050.749 I llm_load_print_meta: n_layer          = 24
0.00.050.753 I llm_load_print_meta: n_head           = 16
0.00.050.753 I llm_load_print_meta: n_head_kv        = 16
0.00.050.756 I llm_load_print_meta: n_rot            = 32
0.00.050.756 I llm_load_print_meta: n_swa            = 0
0.00.050.756 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.756 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.757 I llm_load_print_meta: n_gqa            = 1
0.00.050.758 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.758 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.760 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.760 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.760 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.761 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.761 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.761 I llm_load_print_meta: n_ff             = 8192
0.00.050.762 I llm_load_print_meta: n_expert         = 0
0.00.050.762 I llm_load_print_meta: n_expert_used    = 0
0.00.050.762 I llm_load_print_meta: causal attn      = 1
0.00.050.762 I llm_load_print_meta: pooling type     = 0
0.00.050.762 I llm_load_print_meta: rope type        = 2
0.00.050.762 I llm_load_print_meta: rope scaling     = linear
0.00.050.764 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.765 I llm_load_print_meta: freq_scale_train = 1
0.00.050.766 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.766 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.766 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.766 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.766 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.766 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.766 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.767 I llm_load_print_meta: model type       = 1.4B
0.00.050.771 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.771 I llm_load_print_meta: model params     = 1.41 B
0.00.050.772 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.772 I llm_load_print_meta: general.name     = 1.4B
0.00.050.772 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.772 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.772 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.772 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.777 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.780 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.781 I llm_load_print_meta: max token length = 1024
0.00.052.843 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.843 I llm_load_tensors: offloading output layer to GPU
0.00.052.843 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.854 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.855 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.792 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.793 I llama_new_context_with_model: n_ctx         = 128
0.00.053.793 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.793 I llama_new_context_with_model: n_batch       = 128
0.00.053.793 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.794 I llama_new_context_with_model: flash_attn    = 0
0.00.053.794 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.794 I llama_new_context_with_model: freq_scale    = 1
0.00.053.795 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.795 I ggml_metal_init: allocating
0.00.053.798 I ggml_metal_init: found device: Apple M4
0.00.053.800 I ggml_metal_init: picking default device: Apple M4
0.00.054.382 I ggml_metal_init: using embedded metal library
0.00.056.688 I ggml_metal_init: GPU name:   Apple M4
0.00.056.689 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.690 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.690 I ggml_metal_init: simdgroup reduction   = true
0.00.056.692 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.692 I ggml_metal_init: has bfloat            = true
0.00.056.692 I ggml_metal_init: use bfloat            = true
0.00.056.693 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.248 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.544 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.550 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.563 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.454 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.455 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.456 I llama_new_context_with_model: graph nodes  = 967
0.00.068.456 I llama_new_context_with_model: graph splits = 2
0.00.068.468 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.333 I 
0.00.631.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.395 I perplexity: tokenizing the input ..
0.00.639.522 I perplexity: tokenization took 8.125 ms
0.00.639.525 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.998 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.781.203 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.781.219 I llama_perf_context_print:        load time =     622.42 ms
0.00.781.220 I llama_perf_context_print: prompt eval time =     140.24 ms /   128 tokens (    1.10 ms per token,   912.72 tokens per second)
0.00.781.224 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.225 I llama_perf_context_print:       total time =     149.89 ms /   129 tokens
0.00.781.643 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.079s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.978 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.664 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.668 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.674 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.676 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.677 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.678 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.678 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.678 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.679 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.680 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.681 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.681 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.553 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.628 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.527 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.528 I llama_model_loader: - type  f32:  194 tensors
0.00.024.528 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.682 I llm_load_vocab: special tokens cache size = 25
0.00.050.382 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.384 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.385 I llm_load_print_meta: arch             = gptneox
0.00.050.385 I llm_load_print_meta: vocab type       = BPE
0.00.050.385 I llm_load_print_meta: n_vocab          = 50304
0.00.050.385 I llm_load_print_meta: n_merges         = 50009
0.00.050.386 I llm_load_print_meta: vocab_only       = 0
0.00.050.386 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.386 I llm_load_print_meta: n_embd           = 2048
0.00.050.386 I llm_load_print_meta: n_layer          = 24
0.00.050.388 I llm_load_print_meta: n_head           = 16
0.00.050.389 I llm_load_print_meta: n_head_kv        = 16
0.00.050.389 I llm_load_print_meta: n_rot            = 32
0.00.050.390 I llm_load_print_meta: n_swa            = 0
0.00.050.390 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.390 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.391 I llm_load_print_meta: n_gqa            = 1
0.00.050.391 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.392 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.392 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.393 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.395 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.395 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.395 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.396 I llm_load_print_meta: n_ff             = 8192
0.00.050.396 I llm_load_print_meta: n_expert         = 0
0.00.050.396 I llm_load_print_meta: n_expert_used    = 0
0.00.050.398 I llm_load_print_meta: causal attn      = 1
0.00.050.398 I llm_load_print_meta: pooling type     = 0
0.00.050.398 I llm_load_print_meta: rope type        = 2
0.00.050.398 I llm_load_print_meta: rope scaling     = linear
0.00.050.398 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.399 I llm_load_print_meta: freq_scale_train = 1
0.00.050.399 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.399 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.399 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.399 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.400 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.400 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.400 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.403 I llm_load_print_meta: model type       = 1.4B
0.00.050.404 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.407 I llm_load_print_meta: model params     = 1.41 B
0.00.050.408 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.408 I llm_load_print_meta: general.name     = 1.4B
0.00.050.408 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.408 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.409 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.410 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.410 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.410 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.410 I llm_load_print_meta: max token length = 1024
0.00.052.402 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.402 I llm_load_tensors: offloading output layer to GPU
0.00.052.402 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.413 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.414 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.340 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.341 I llama_new_context_with_model: n_ctx         = 128
0.00.053.341 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.341 I llama_new_context_with_model: n_batch       = 128
0.00.053.342 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.342 I llama_new_context_with_model: flash_attn    = 0
0.00.053.342 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.342 I llama_new_context_with_model: freq_scale    = 1
0.00.053.343 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.343 I ggml_metal_init: allocating
0.00.053.346 I ggml_metal_init: found device: Apple M4
0.00.053.348 I ggml_metal_init: picking default device: Apple M4
0.00.053.897 I ggml_metal_init: using embedded metal library
0.00.056.204 I ggml_metal_init: GPU name:   Apple M4
0.00.056.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.206 I ggml_metal_init: simdgroup reduction   = true
0.00.056.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.207 I ggml_metal_init: has bfloat            = true
0.00.056.207 I ggml_metal_init: use bfloat            = true
0.00.056.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.693 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.901 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.905 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.918 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.794 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.795 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.796 I llama_new_context_with_model: graph nodes  = 967
0.00.067.796 I llama_new_context_with_model: graph splits = 2
0.00.067.808 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.809 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.224.090 I 
0.00.224.116 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.224.131 I perplexity: tokenizing the input ..
0.00.231.288 I perplexity: tokenization took 7.156 ms
0.00.231.292 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.371.728 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.372.984 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.373.003 I llama_perf_context_print:        load time =     214.11 ms
0.00.373.004 I llama_perf_context_print: prompt eval time =     140.21 ms /   128 tokens (    1.10 ms per token,   912.89 tokens per second)
0.00.373.006 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.373.006 I llama_perf_context_print:       total time =     148.91 ms /   129 tokens
0.00.373.437 I ggml_metal_free: deallocating

real	0m0.388s
user	0m0.077s
sys	0m0.046s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.351 I build: 4401 (bc7b1f86) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.610 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.310 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.315 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.317 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.325 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.325 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.326 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.326 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.327 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.327 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.327 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.328 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.331 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.333 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.334 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.171 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.481 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.483 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.484 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.485 I llama_model_loader: - type  f32:  194 tensors
0.00.050.485 I llama_model_loader: - type  f16:   98 tensors
0.00.078.744 I llm_load_vocab: special tokens cache size = 25
0.00.085.163 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.167 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.168 I llm_load_print_meta: arch             = gptneox
0.00.085.168 I llm_load_print_meta: vocab type       = BPE
0.00.085.168 I llm_load_print_meta: n_vocab          = 50304
0.00.085.168 I llm_load_print_meta: n_merges         = 50009
0.00.085.170 I llm_load_print_meta: vocab_only       = 0
0.00.085.170 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.170 I llm_load_print_meta: n_embd           = 2048
0.00.085.171 I llm_load_print_meta: n_layer          = 24
0.00.085.173 I llm_load_print_meta: n_head           = 16
0.00.085.174 I llm_load_print_meta: n_head_kv        = 16
0.00.085.174 I llm_load_print_meta: n_rot            = 32
0.00.085.174 I llm_load_print_meta: n_swa            = 0
0.00.085.174 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.175 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.175 I llm_load_print_meta: n_gqa            = 1
0.00.085.176 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.177 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.177 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.177 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.178 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.178 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.178 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.179 I llm_load_print_meta: n_ff             = 8192
0.00.085.179 I llm_load_print_meta: n_expert         = 0
0.00.085.179 I llm_load_print_meta: n_expert_used    = 0
0.00.085.179 I llm_load_print_meta: causal attn      = 1
0.00.085.179 I llm_load_print_meta: pooling type     = 0
0.00.085.180 I llm_load_print_meta: rope type        = 2
0.00.085.180 I llm_load_print_meta: rope scaling     = linear
0.00.085.180 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.180 I llm_load_print_meta: freq_scale_train = 1
0.00.085.181 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.181 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.181 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.181 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.181 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.181 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.181 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.183 I llm_load_print_meta: model type       = 1.4B
0.00.085.183 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.183 I llm_load_print_meta: model params     = 1.41 B
0.00.085.184 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.184 I llm_load_print_meta: general.name     = 1.4B
0.00.085.184 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.184 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.185 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.185 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.185 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.185 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.185 I llm_load_print_meta: max token length = 1024
0.00.087.712 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.713 I llm_load_tensors: offloading output layer to GPU
0.00.087.713 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.724 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.725 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.726 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.727 I llama_new_context_with_model: n_ctx         = 128
0.00.088.727 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.727 I llama_new_context_with_model: n_batch       = 128
0.00.088.728 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.728 I llama_new_context_with_model: flash_attn    = 0
0.00.088.728 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.728 I llama_new_context_with_model: freq_scale    = 1
0.00.088.729 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.729 I ggml_metal_init: allocating
0.00.088.732 I ggml_metal_init: found device: Apple M4
0.00.088.734 I ggml_metal_init: picking default device: Apple M4
0.00.089.386 I ggml_metal_init: using embedded metal library
0.00.092.148 I ggml_metal_init: GPU name:   Apple M4
0.00.092.150 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.151 I ggml_metal_init: simdgroup reduction   = true
0.00.092.151 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.151 I ggml_metal_init: has bfloat            = true
0.00.092.151 I ggml_metal_init: use bfloat            = true
0.00.092.152 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.152 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.286 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.105.656 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.660 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.673 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.593 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.594 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.594 I llama_new_context_with_model: graph nodes  = 967
0.00.106.594 I llama_new_context_with_model: graph splits = 2
0.00.106.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.608 I 
0.00.106.634 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.636 I compute_imatrix: tokenizing the input ..
0.00.114.132 I compute_imatrix: tokenization took 7.495 ms
0.00.114.135 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.374.766 I compute_imatrix: 1.26 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.377.889 I llama_perf_context_print:        load time =    1351.16 ms
0.01.377.890 I llama_perf_context_print: prompt eval time =    1259.95 ms /   128 tokens (    9.84 ms per token,   101.59 tokens per second)
0.01.377.891 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.377.891 I llama_perf_context_print:       total time =    1354.27 ms /   129 tokens
0.01.378.789 I ggml_metal_free: deallocating

real	0m1.564s
user	0m0.177s
sys	0m0.198s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4401 (bc7b1f86)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f60a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f60a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f60af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f60b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f60baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f60c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f60c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f60cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f60d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f60d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f60dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f60e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f60ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f60f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f60fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f6102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f6109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f6110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f611810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f611fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f612700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f612e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f613540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f613de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f614500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f6147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f614dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f615a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f615f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f616240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f6166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f6169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f617230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f617770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f617a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f617ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f618370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f618810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f618cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f619150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f6195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f619a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f619f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f61a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f61a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f61aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f61b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f61bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f61c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f61c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f61ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f61d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f61da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f61e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f61e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f61ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f61f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f61f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f61fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f620220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f6204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f620980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f620e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f6212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f621760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f621c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f6220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f622540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f6229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f622e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f623320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f6237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f623c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f6241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f624700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f624c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f6251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f6256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f625c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f626190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f6266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f626c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f6276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f627c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f628170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f6286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f628c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f629160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f6296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f629c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f62a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f62a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f62abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f62b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f62b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f62bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f61b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f62c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f62c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f62cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f62d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f62d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f62dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f62e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f62e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f62ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f62f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f62f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f62fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f630270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f6307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f630d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f6311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f631650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f631af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f631f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f632430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f6328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f632d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f633210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f6336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f633b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f633ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f634490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f634930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f635270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f635710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f635bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f6364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f636990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f636e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f6372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f637770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f637c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f6380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f638550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f6389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f638e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f639330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f6397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f639c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f63a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f63a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f63aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f63aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f63b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f63b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f63bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f63c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f63c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f63cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f63cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f63d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f63d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f63dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f63e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f63e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f63eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f63efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f63f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f63f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f63fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f640230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f6406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f640b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f641010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f6414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f641950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f641df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f642290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f642730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f642bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f643070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f643510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f6439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f643e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f6442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f644790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f644c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f6450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f645570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f645a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f645eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f646350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f6467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f646c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f647130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f6475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f647a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f647f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f648460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f6489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f648f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f649450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f649710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f649d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f64a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f64a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f64b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f64b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f64b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f64bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f64c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f64cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f64d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f64d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f64da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f64e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f64e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f64ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f64f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f64f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f64fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f650210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f650760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f650cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f651200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f651750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f651ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f6521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f652740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f652c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f6531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f653730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f653c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f6541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f654720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f654c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f6551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f655710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f655c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f6561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f656700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f656c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f6571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f6576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f657c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f658190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f6586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f658c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f659180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f6596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f659c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f65a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f65a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f65ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f65b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f65b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f65bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f65c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f65c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f65cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f65d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f65d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f65dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f65e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f65e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f65ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f65f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f65f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f65fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f660110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f660660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f660bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f661050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f6614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f661990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f661e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f6622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f662770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f662c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f6630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f663550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f6639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f663e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f664330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f6647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f664c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f665110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f665660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f665d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f6664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f666bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f6672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f6675a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f667d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f668050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f668660 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f625420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f625d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f626170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f6265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f626a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f627330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f6277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f627c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f628080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f628660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f628f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f6296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f629eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f62a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f62ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f62b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f62ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f62c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f62cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f62d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f62d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f62dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f62e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f62eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f62ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f62f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f62f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f62fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f630140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f6305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f630ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f631150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f6315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f631ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f632310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f632780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f632bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f633060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f6334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f633940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f633db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f634220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f634690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f634b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f634f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f6353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f635850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f636130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f6365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f636a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f636e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f6372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f637760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f637bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f638040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f6384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f638920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f638d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f639200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f639670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f639ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f639f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f63a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f63a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f63aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f63b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f63b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f63b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f63be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f63c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f63c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f63cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f63d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f63d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f63d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f63dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f63e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f63e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f63eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f63ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f63f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f63f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f63fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f6400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f640560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f6409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f640e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f6412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f641720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f641b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f642000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f642470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f6428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f642d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f6431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f643630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f643aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f643f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f644380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f6447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f644c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f6450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f6459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f645e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f646700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f646b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f647450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f6478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f647d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f6481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f648610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f648a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f648ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f649360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f6497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f649c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f64a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f64a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f64a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f64ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f64b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f64b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f64bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f64bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f64c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f64c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f64cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f64d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f64d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f64da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f64ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f64e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f64e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f64ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f64f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f64f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f64f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f64fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f650250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f6506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f650fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f651410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f651880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f651cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f652160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f6525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f652a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f652eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f653c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f654070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f6544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f654950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f654dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f655230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f6556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f655b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f655f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f6563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f656860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f656cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f657140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f6575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f657a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f657e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f658300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f658770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f658be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f659050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f6594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f659da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f65a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f65a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f65aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f65af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f65b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f65b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f65bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f65c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f65c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f65ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f65ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f65d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f65d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f65dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f65e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f65e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f65e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f65ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f65f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f65f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f65fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b604080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b6044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b604960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b604dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b605240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b6056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b605b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b6066d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b606990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b606c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b6070c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b607530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b6079a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b607e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b608280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b6086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b608b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b608fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b609440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b6098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b609d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b60a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b60a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b60aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b60aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b60b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b60b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b60bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b60c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b60c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b60c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b60cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b60d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b60d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b60db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b60dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b60e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b60e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b60ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b60f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b60f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b60fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b60fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b610330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b6107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b610c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b611080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b6114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b611960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b611dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b612240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b6126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b612b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b612f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b613400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b613870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b613ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b614150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b6145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b614a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b614ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b615310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b615780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b615bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b616060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b6164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b616940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b616db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b617220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b617f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b6183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b618850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b618cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b619130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b6195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b619a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b619e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b61a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b61ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b61b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b61bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b61c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b61c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b61c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b61cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b61d600 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f625420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f625d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f626170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f6265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f626a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f627330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f6277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f627c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f628080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f628660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f628f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f6296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f629eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f62a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f62ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f62b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f62ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f62c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f62cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f62d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f62d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f62dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f62e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f62eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f62ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f62f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f62f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f62fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f630140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f6305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f630ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f631150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f6315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f631ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f632310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f632780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f632bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f633060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f6334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f633940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f633db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f634220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f634690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f634b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f634f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f6353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f635850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f635cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f636130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f6365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f636a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f636e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f6372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f637760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f637bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f638040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f6384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f638920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f638d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f639200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f639670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f639ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f639f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f63a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f63a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f63aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f63b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f63b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f63b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f63be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f63c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f63c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f63cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f63d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f63d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f63d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f63dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f63e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f63e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f63eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f63ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f63f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f63f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f63fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f6400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f640560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f6409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f640e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f6412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f641720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f641b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f642000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f642470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f6428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f642d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f6431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f643630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f643aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f643f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f644380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f6447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f644c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f6450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f6459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f645e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f646700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f646b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f647450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f6478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f647d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f6481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f648610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f648a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f648ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f649360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f6497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f649c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f64a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f64a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f64a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f64ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f64b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f64b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f64bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f64bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f64c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f64c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f64cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f64d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f64d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f64da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f64ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f64e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f64e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f64ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f64f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f64f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f64f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f64fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f650250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f6506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f650fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f651410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f651880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f651cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f652160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f6525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f652a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f652eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f653c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f654070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f6544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f654950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f654dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f655230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f6556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f655b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f655f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f6563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f656860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f656cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f657140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f6575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f657a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f657e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f658300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f658770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f658be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f659050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f6594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f659da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f65a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f65a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f65aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f65af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f65b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f65b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f65bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f65c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f65c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f65ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f65ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f65d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f65d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f65dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f65e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f65e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f65e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f65ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f65f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f65f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f65fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f65ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f6603b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12f660820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f660c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f661100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f661570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f6619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f662160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f6625d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f662a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f662eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f663320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f663790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f663c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f664070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f6644e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f664950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f664dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f665230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f6656a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f665b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f665f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f6663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f666860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f666cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f667140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f6675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f667a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f667e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f668300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f668770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f60b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f60af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f60a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f617760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f617a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f617e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f618300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f618770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f618be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f619050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f6194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f619930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f619da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f61a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f61a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f61aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f61af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f61b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f61b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f61bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f61c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f61c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f61ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f61ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f61d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f61d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f61dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f61e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f61e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f61e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f61ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f61f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f61f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12f61fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12f61ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f6203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f620820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f620c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f621100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f621570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f6219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f621e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f6222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f622730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f622ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f623010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f623480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f6238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f623d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f6245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f6163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f616a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f617180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f60d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f60dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f60e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f60e570 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.805s
user	0m0.291s
sys	0m0.308s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4401 (bc7b1f86)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f80a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f80a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f80ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f80b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f80b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f80bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f80c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f80cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f80d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f80d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f80dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f80dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f80ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f80f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f80fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f8101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f8108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f810ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f811710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f811ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f812600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f812d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f813440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f813ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f814400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f8146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f814cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f815940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f815e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f816140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f8165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f8168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f817130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f817670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f817930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f817dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f818270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f818710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f818bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f819050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f8194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f819990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f819e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f81a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f81a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f81aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f81b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f81bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f81c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f81c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f81cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f81d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f81d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f81df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f81e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f81ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f81f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f81f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f81f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f820120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f8203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f820880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f820d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f8211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f821660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f821b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f821fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f822440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f8228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f822d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f823220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f8236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f823b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f8240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f824600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f824b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f8250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f8255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f825b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f826090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f8265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f826b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f827080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f8275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f827b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f828070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f8285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f828b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f829060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f8295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f829b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f82a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f82a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f82aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f82b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f82b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f82bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f81b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f82bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f82c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f82cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f82d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f82d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f82dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f82e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f82e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f82ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f82f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f82f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f82fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f830170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f8306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f830c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f8310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f831550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f8319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f831e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f832330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f8327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f832c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f833110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f8335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f833a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f834390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f834830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f834cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f835170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f835610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f835ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f835f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f8363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f836890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f836d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f8371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f837670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f837b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f837fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f838450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f8388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f838d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f839230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f8396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f839b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f83a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f83a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f83a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f83adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f83b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f83b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f83bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f83c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f83c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f83c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f83ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f83d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f83d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f83dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f83e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f83e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f83ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f83eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f83f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f83f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f83fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f840130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f8405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f840a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f840f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f8413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f841850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f841cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f842190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f842630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f842ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f842f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f8438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f843d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f8441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f844690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f844fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f845470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f845910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f845db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f8466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f846b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f847030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f8474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f847970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f847e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f848360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f8488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f848e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f849350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f849610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f849c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f84a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f84a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f84b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f84b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f84b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f84bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f84c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f84cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f84d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f84d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f84d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f84e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f84e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f84ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f84f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f84f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f84fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f850110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f850660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f851100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f851650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f851ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f8520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f852640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f852b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f8530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f853630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f853b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f8540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f854620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f854b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f8550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f855610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f855b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f8560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f856600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f856b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f8570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f8575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f857b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f858090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f8585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f858b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f859080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f8595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f859b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f85a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f85a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f85ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f85b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f85b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f85bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f85c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f85c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f85caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f85d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f85d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f85dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f85e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f85e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f85ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f85f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f85f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f85fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f860010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f860560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f860ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f860f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f8613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f861890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f861d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f8621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f862670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f862b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f862fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f863450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f8638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f863d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f864230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f8646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f864b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f865010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f865560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f865c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f8663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f866ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f8671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f8674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f867c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f867f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f868560 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.103.903 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11b504dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11b505240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11b5056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11b505b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11b505f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11b506400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11b506870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11b506ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11b507150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11b5075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11b507a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11b508120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11b508c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11b5093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11b509c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11b50a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11b50aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11b50b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11b50b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11b50bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11b50c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11b50cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11b50d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11b50dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11b50e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11b50e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11b50e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11b50ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11b50f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11b50f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11b50fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11b50ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11b510430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11b5106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11b510b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11b510fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11b511440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11b5118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11b511d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11b512190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11b512600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11b512a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11b512ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11b513350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11b5137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11b513c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11b5140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11b514510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11b514980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11b514df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11b515260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11b5156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11b515b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11b515fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11b516420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11b516890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11b516e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11b517300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11b517770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11b517be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11b518050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11b5184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11b518930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11b518da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11b519210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11b519680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11b519af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11b519f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11b51a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11b51a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11b51acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11b51b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11b51b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11b51ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11b51be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11b51c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11b51c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11b51cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11b51d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11b51d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11b51d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11b51dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11b51e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11b51e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11b51ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11b51ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11b51f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11b51f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11b51fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11b520100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11b520570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11b5209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11b520e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11b5212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11b521730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11b521ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11b522010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11b522480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11b5228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11b522d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11b5231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11b523640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11b523ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11b523f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11b524390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11b524800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11b524c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11b5250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11b525550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11b5259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11b525e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11b5262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11b526710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11b526b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11b526ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11b527460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11b5278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11b527d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11b5281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11b528620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11b528a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11b528f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11b529370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11b5297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11b529c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11b52a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11b52a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11b52a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11b52ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11b52b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11b52b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11b52bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11b52bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11b52c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11b52c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11b52cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11b52d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11b52d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11b52da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11b52dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11b52e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11b52e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11b52ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11b52f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11b52f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11b52f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11b52fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11b530260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11b5306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11b530b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11b530fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11b531420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11b531890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11b531d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11b532170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11b5325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11b532a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11b532ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11b533330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11b5337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11b533c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11b534080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11b5344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11b534960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11b534dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11b535240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11b5356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11b535b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11b535f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11b536400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11b536870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11b536ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11b537150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11b5375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11b537a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11b537ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11b538310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11b538780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11b538bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11b539060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11b5394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11b539940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11b539db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11b53a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11b53a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11b53ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11b53af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11b53b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11b53b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11b53bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11b53c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11b53c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11b53ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11b53ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11b53d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11b53d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11b53dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11b53e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11b53e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11b53e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11b53ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11b53f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11b53f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11b53fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11b53ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11b5403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11b540830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11b540dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11b541230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11b5416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11b5421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11b5424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11b542770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11b542be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11b543050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11b5434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11b543930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11b543da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11b544210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11b544680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11b544af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11b544f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11b5453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11b545840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11b545cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11b546120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11b546590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11b546a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11b546e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11b5472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11b547750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11b547bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11b548030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11b5484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11b548910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11b548d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11b5491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11b549660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11b549ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11b549f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11b54a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11b54a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11b54ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11b54b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11b54b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11b54b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11b54be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11b54c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11b54c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11b54cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11b54d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11b54d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11b54d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11b54dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11b54e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11b54e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11b54eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11b54ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11b54f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11b54f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11b54fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11b5500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11b550550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11b5509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11b550e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11b5512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11b551710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11b551b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11b551ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11b552460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11b5528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11b552d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11b5531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11b553620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11b553a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11b553f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11b554370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11b5547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11b554c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11b5550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11b555530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11b5559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11b555e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11b556880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11b556fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11b5576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11b557de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11b5580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11b558510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11b558b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11b559120 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f824c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f8250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f825550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f8259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f825e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f8262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f826710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f826b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f826ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f827460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f8278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f827eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f8287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f828f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f829700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f829df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f82a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f82abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f82b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f82bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f82c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f82ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f82d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f82d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f82def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f82e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f82e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f82ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f82f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f82f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f82f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f82fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f830270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f830530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f8309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f830e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f831280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f8316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f831b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f831fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f832440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f8328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f832d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f833190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f833600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f833a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f833ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f834350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f8347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f834c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f8350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f835510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f835980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f835df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f836260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f8366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f836b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f836fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f837420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f837890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f837d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f838170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f8385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f838a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f838ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f839330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f8397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f839c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f83a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f83a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f83a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f83add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f83b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f83b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f83bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f83bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f83c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f83c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f83cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f83d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f83d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f83da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f83dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f83e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f83e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f83ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f83f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f83f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f83f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f83fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f840220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f840690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f840b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f840f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f8413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f841850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f841cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f842130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f8425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f842a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f842e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f8432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f843760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f843bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f844040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f8444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f844920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f844d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f845200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f845670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f845ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f845f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f8463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f846830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f846ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f847110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f847580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f8479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f847e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f8482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f848740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f848bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f849020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f849490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f849900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f849d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f84a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f84a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f84aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f84af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f84b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f84b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f84bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f84c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f84c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f84c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f84ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f84d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f84d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f84db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f84e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f84e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f84e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f84ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f84f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f84f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f84faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f84ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f850380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f8507f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f850c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f8510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f851540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f8519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f851e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f852290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f852700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f852b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f852fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f853450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f8538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f853d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f8541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f854610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f854a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f854ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f855360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f8557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f855c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f8560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f856520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f856990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f856e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f857270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f8576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f857b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f857fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f858430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f8588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f858d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f859180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f8595f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f859a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f859ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f85a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f85a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f85ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f85b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f85b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f85b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f85bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f85c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f85c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f85cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f85cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f85d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f85d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f85dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f85e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f85e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f85ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f85eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f85f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f85f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f85fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f860070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f8604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f860950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f860dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f861230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f8619b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f861e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f862290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f862700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f862b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f862fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f863450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f8638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f863d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f8641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f864610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f864a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f864ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f865360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f8657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f865c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f8660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f866520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f866990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f866e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f867270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f8676e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f867b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f867fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f868430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f80b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f80ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f809800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f80a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f817810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f817c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f8180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f818560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f8189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f818e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f8192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f819720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f819b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f81a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f81a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f81a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f81ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f81b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f81b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f81baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f81bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f81c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f81c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f81cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f81d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f81d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f81d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f81de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f81e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f81e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f81eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f81efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f81f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f81f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f81fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f8201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f820610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f820a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f820ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f821360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f8217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f821c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f8220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f822520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f822990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f822e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f823270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f8236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f823dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f8244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f8162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f816990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f816e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f80d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f80d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f80de50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.959s
user	0m0.243s
sys	0m0.159s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
