### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.30 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.63 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.63 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.40 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.28 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.91 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.24 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.24 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.28 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.21 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.06 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.01 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.20 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.32 sec*proc (28 tests)

Total Test time (real) = 222.37 sec

real	3m42.452s
user	7m38.153s
sys	0m6.389s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.16 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.47 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.32 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.39 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.13 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.24 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.59 sec*proc (28 tests)

Total Test time (real) =  51.60 sec

real	0m51.610s
user	1m12.066s
sys	0m5.473s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.116 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.268 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.997 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.010 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.011 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.011 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.012 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.013 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.014 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.015 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.015 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.016 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.016 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.019 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.020 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.021 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.021 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.022 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.022 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.023 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.226 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.556 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.560 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.561 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.561 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.562 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.562 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.563 I llama_model_loader: - type  f32:  124 tensors
0.00.027.564 I llama_model_loader: - type  f16:   73 tensors
0.00.027.565 I print_info: file format = GGUF V3 (latest)
0.00.027.565 I print_info: file type   = F16
0.00.027.568 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.032.483 I load: special tokens cache size = 5
0.00.034.410 I load: token to piece cache size = 0.2032 MB
0.00.034.430 I print_info: arch             = bert
0.00.034.431 I print_info: vocab_only       = 0
0.00.034.431 I print_info: n_ctx_train      = 512
0.00.034.431 I print_info: n_embd           = 384
0.00.034.431 I print_info: n_layer          = 12
0.00.034.438 I print_info: n_head           = 12
0.00.034.438 I print_info: n_head_kv        = 12
0.00.034.438 I print_info: n_rot            = 32
0.00.034.439 I print_info: n_swa            = 0
0.00.034.441 I print_info: n_embd_head_k    = 32
0.00.034.442 I print_info: n_embd_head_v    = 32
0.00.034.442 I print_info: n_gqa            = 1
0.00.034.443 I print_info: n_embd_k_gqa     = 384
0.00.034.443 I print_info: n_embd_v_gqa     = 384
0.00.034.444 I print_info: f_norm_eps       = 1.0e-12
0.00.034.444 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.444 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.444 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.445 I print_info: f_logit_scale    = 0.0e+00
0.00.034.446 I print_info: n_ff             = 1536
0.00.034.446 I print_info: n_expert         = 0
0.00.034.446 I print_info: n_expert_used    = 0
0.00.034.446 I print_info: causal attn      = 0
0.00.034.446 I print_info: pooling type     = 2
0.00.034.447 I print_info: rope type        = 2
0.00.034.448 I print_info: rope scaling     = linear
0.00.034.448 I print_info: freq_base_train  = 10000.0
0.00.034.448 I print_info: freq_scale_train = 1
0.00.034.448 I print_info: n_ctx_orig_yarn  = 512
0.00.034.448 I print_info: rope_finetuned   = unknown
0.00.034.449 I print_info: ssm_d_conv       = 0
0.00.034.449 I print_info: ssm_d_inner      = 0
0.00.034.449 I print_info: ssm_d_state      = 0
0.00.034.449 I print_info: ssm_dt_rank      = 0
0.00.034.449 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.449 I print_info: model type       = 33M
0.00.034.450 I print_info: model params     = 33.21 M
0.00.034.450 I print_info: general.name     = Bge Small
0.00.034.450 I print_info: vocab type       = WPM
0.00.034.451 I print_info: n_vocab          = 30522
0.00.034.451 I print_info: n_merges         = 0
0.00.034.451 I print_info: BOS token        = 101 '[CLS]'
0.00.034.451 I print_info: UNK token        = 100 '[UNK]'
0.00.034.451 I print_info: SEP token        = 102 '[SEP]'
0.00.034.451 I print_info: PAD token        = 0 '[PAD]'
0.00.034.452 I print_info: MASK token       = 103 '[MASK]'
0.00.034.452 I print_info: LF token         = 0 '[PAD]'
0.00.034.452 I print_info: max token length = 21
0.00.035.704 I load_tensors: offloading 12 repeating layers to GPU
0.00.035.704 I load_tensors: offloading output layer to GPU
0.00.035.705 I load_tensors: offloaded 13/13 layers to GPU
0.00.035.725 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.726 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.035.902 I llama_init_from_model: n_seq_max     = 1
0.00.035.903 I llama_init_from_model: n_ctx         = 512
0.00.035.903 I llama_init_from_model: n_ctx_per_seq = 512
0.00.035.903 I llama_init_from_model: n_batch       = 2048
0.00.035.903 I llama_init_from_model: n_ubatch      = 2048
0.00.035.903 I llama_init_from_model: flash_attn    = 0
0.00.035.904 I llama_init_from_model: freq_base     = 10000.0
0.00.035.904 I llama_init_from_model: freq_scale    = 1
0.00.035.904 I ggml_metal_init: allocating
0.00.035.908 I ggml_metal_init: found device: Apple M4
0.00.035.910 I ggml_metal_init: picking default device: Apple M4
0.00.036.582 I ggml_metal_init: using embedded metal library
0.00.039.177 I ggml_metal_init: GPU name:   Apple M4
0.00.039.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.179 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.180 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.180 I ggml_metal_init: simdgroup reduction   = true
0.00.039.180 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.180 I ggml_metal_init: has bfloat            = true
0.00.039.180 I ggml_metal_init: use bfloat            = true
0.00.039.181 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.182 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.049.381 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.049.873 I init:      Metal KV buffer size =     9.00 MiB
0.00.049.875 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.049.877 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.050.534 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.050.535 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.050.535 I llama_init_from_model: graph nodes  = 429
0.00.050.536 I llama_init_from_model: graph splits = 2
0.00.050.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.050.537 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.055.618 I 
0.00.055.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.056.172 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.060.495 I llama_perf_context_print:        load time =      38.34 ms
0.00.060.496 I llama_perf_context_print: prompt eval time =       4.20 ms /     9 tokens (    0.47 ms per token,  2143.88 tokens per second)
0.00.060.497 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.060.497 I llama_perf_context_print:       total time =       4.88 ms /    10 tokens
0.00.060.657 I ggml_metal_free: deallocating

real	0m0.277s
user	0m0.043s
sys	0m0.024s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.776 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.897 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.900 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.902 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.902 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.903 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.905 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.906 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.907 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.907 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.907 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.908 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.908 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.910 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.911 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.911 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.911 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.912 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.913 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.047 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.646 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.647 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.648 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.648 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.648 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.649 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.649 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.649 I llama_model_loader: - type  f32:  124 tensors
0.00.014.650 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.650 I print_info: file format = GGUF V3 (latest)
0.00.014.651 I print_info: file type   = Q8_0
0.00.014.652 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.891 I load: special tokens cache size = 5
0.00.018.093 I load: token to piece cache size = 0.2032 MB
0.00.018.095 I print_info: arch             = bert
0.00.018.095 I print_info: vocab_only       = 0
0.00.018.095 I print_info: n_ctx_train      = 512
0.00.018.095 I print_info: n_embd           = 384
0.00.018.096 I print_info: n_layer          = 12
0.00.018.098 I print_info: n_head           = 12
0.00.018.098 I print_info: n_head_kv        = 12
0.00.018.099 I print_info: n_rot            = 32
0.00.018.099 I print_info: n_swa            = 0
0.00.018.099 I print_info: n_embd_head_k    = 32
0.00.018.099 I print_info: n_embd_head_v    = 32
0.00.018.100 I print_info: n_gqa            = 1
0.00.018.100 I print_info: n_embd_k_gqa     = 384
0.00.018.101 I print_info: n_embd_v_gqa     = 384
0.00.018.101 I print_info: f_norm_eps       = 1.0e-12
0.00.018.103 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.103 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.103 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.103 I print_info: f_logit_scale    = 0.0e+00
0.00.018.104 I print_info: n_ff             = 1536
0.00.018.104 I print_info: n_expert         = 0
0.00.018.104 I print_info: n_expert_used    = 0
0.00.018.104 I print_info: causal attn      = 0
0.00.018.105 I print_info: pooling type     = 2
0.00.018.105 I print_info: rope type        = 2
0.00.018.105 I print_info: rope scaling     = linear
0.00.018.105 I print_info: freq_base_train  = 10000.0
0.00.018.110 I print_info: freq_scale_train = 1
0.00.018.110 I print_info: n_ctx_orig_yarn  = 512
0.00.018.110 I print_info: rope_finetuned   = unknown
0.00.018.110 I print_info: ssm_d_conv       = 0
0.00.018.110 I print_info: ssm_d_inner      = 0
0.00.018.111 I print_info: ssm_d_state      = 0
0.00.018.111 I print_info: ssm_dt_rank      = 0
0.00.018.111 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.111 I print_info: model type       = 33M
0.00.018.112 I print_info: model params     = 33.21 M
0.00.018.112 I print_info: general.name     = Bge Small
0.00.018.112 I print_info: vocab type       = WPM
0.00.018.113 I print_info: n_vocab          = 30522
0.00.018.113 I print_info: n_merges         = 0
0.00.018.113 I print_info: BOS token        = 101 '[CLS]'
0.00.018.113 I print_info: UNK token        = 100 '[UNK]'
0.00.018.113 I print_info: SEP token        = 102 '[SEP]'
0.00.018.115 I print_info: PAD token        = 0 '[PAD]'
0.00.018.115 I print_info: MASK token       = 103 '[MASK]'
0.00.018.115 I print_info: LF token         = 0 '[PAD]'
0.00.018.116 I print_info: max token length = 21
0.00.019.333 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.333 I load_tensors: offloading output layer to GPU
0.00.019.334 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.341 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.343 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.019.476 I llama_init_from_model: n_seq_max     = 1
0.00.019.477 I llama_init_from_model: n_ctx         = 512
0.00.019.477 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.477 I llama_init_from_model: n_batch       = 2048
0.00.019.478 I llama_init_from_model: n_ubatch      = 2048
0.00.019.478 I llama_init_from_model: flash_attn    = 0
0.00.019.478 I llama_init_from_model: freq_base     = 10000.0
0.00.019.479 I llama_init_from_model: freq_scale    = 1
0.00.019.479 I ggml_metal_init: allocating
0.00.019.482 I ggml_metal_init: found device: Apple M4
0.00.019.484 I ggml_metal_init: picking default device: Apple M4
0.00.020.036 I ggml_metal_init: using embedded metal library
0.00.022.388 I ggml_metal_init: GPU name:   Apple M4
0.00.022.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.391 I ggml_metal_init: simdgroup reduction   = true
0.00.022.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.391 I ggml_metal_init: has bfloat            = true
0.00.022.391 I ggml_metal_init: use bfloat            = true
0.00.022.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.760 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.251 I init:      Metal KV buffer size =     9.00 MiB
0.00.033.253 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.254 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.033.845 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.033.846 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.033.846 I llama_init_from_model: graph nodes  = 429
0.00.033.846 I llama_init_from_model: graph splits = 2
0.00.033.847 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.286 I 
0.00.038.302 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.822 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.222 I llama_perf_context_print:        load time =      29.51 ms
0.00.043.223 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2101.33 tokens per second)
0.00.043.224 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.224 I llama_perf_context_print:       total time =       4.93 ms /    10 tokens
0.00.043.335 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.185 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.687 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.328 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.335 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.338 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.339 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.340 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.341 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.342 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.343 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.343 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.344 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.344 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.348 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.348 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.349 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.349 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.350 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.036 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.632 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.640 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.649 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.651 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.652 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.652 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.653 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.653 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.653 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.654 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.654 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.655 I llama_model_loader: - type  f32:   40 tensors
0.00.051.655 I llama_model_loader: - type  f16:   30 tensors
0.00.051.660 I print_info: file format = GGUF V3 (latest)
0.00.051.661 I print_info: file type   = F16
0.00.051.663 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.068.688 W load: empty token at index 5
0.00.073.284 W load: model vocab missing newline token, using special_pad_id instead
0.00.074.659 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.690 I load: special tokens cache size = 5
0.00.335.814 I load: token to piece cache size = 1.5060 MB
0.00.335.821 I print_info: arch             = jina-bert-v2
0.00.335.821 I print_info: vocab_only       = 0
0.00.335.821 I print_info: n_ctx_train      = 8192
0.00.335.822 I print_info: n_embd           = 384
0.00.335.824 I print_info: n_layer          = 4
0.00.335.830 I print_info: n_head           = 12
0.00.335.831 I print_info: n_head_kv        = 12
0.00.335.831 I print_info: n_rot            = 32
0.00.335.831 I print_info: n_swa            = 0
0.00.335.832 I print_info: n_embd_head_k    = 32
0.00.335.832 I print_info: n_embd_head_v    = 32
0.00.335.832 I print_info: n_gqa            = 1
0.00.335.833 I print_info: n_embd_k_gqa     = 384
0.00.335.833 I print_info: n_embd_v_gqa     = 384
0.00.335.834 I print_info: f_norm_eps       = 1.0e-12
0.00.335.834 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.335.835 I print_info: f_clamp_kqv      = 0.0e+00
0.00.335.835 I print_info: f_max_alibi_bias = 8.0e+00
0.00.335.835 I print_info: f_logit_scale    = 0.0e+00
0.00.335.836 I print_info: n_ff             = 1536
0.00.335.836 I print_info: n_expert         = 0
0.00.335.836 I print_info: n_expert_used    = 0
0.00.335.836 I print_info: causal attn      = 0
0.00.335.837 I print_info: pooling type     = -1
0.00.335.837 I print_info: rope type        = -1
0.00.335.837 I print_info: rope scaling     = linear
0.00.335.837 I print_info: freq_base_train  = 10000.0
0.00.335.838 I print_info: freq_scale_train = 1
0.00.335.838 I print_info: n_ctx_orig_yarn  = 8192
0.00.335.838 I print_info: rope_finetuned   = unknown
0.00.335.838 I print_info: ssm_d_conv       = 0
0.00.335.838 I print_info: ssm_d_inner      = 0
0.00.335.838 I print_info: ssm_d_state      = 0
0.00.335.838 I print_info: ssm_dt_rank      = 0
0.00.335.839 I print_info: ssm_dt_b_c_rms   = 0
0.00.335.839 I print_info: model type       = 33M
0.00.335.840 I print_info: model params     = 32.90 M
0.00.335.840 I print_info: general.name     = Jina Bert Implementation
0.00.335.841 I print_info: vocab type       = BPE
0.00.335.843 I print_info: n_vocab          = 61056
0.00.335.843 I print_info: n_merges         = 39382
0.00.335.843 I print_info: BOS token        = 0 '<s>'
0.00.335.843 I print_info: EOS token        = 2 '</s>'
0.00.335.844 I print_info: UNK token        = 3 '<unk>'
0.00.335.844 I print_info: SEP token        = 2 '</s>'
0.00.335.844 I print_info: PAD token        = 1 '<pad>'
0.00.335.844 I print_info: MASK token       = 4 '<mask>'
0.00.335.845 I print_info: EOG token        = 2 '</s>'
0.00.335.845 I print_info: max token length = 45
0.00.337.150 I load_tensors: offloading 4 repeating layers to GPU
0.00.337.150 I load_tensors: offloading output layer to GPU
0.00.337.150 I load_tensors: offloaded 5/5 layers to GPU
0.00.337.171 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.337.173 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.337.397 I llama_init_from_model: n_seq_max     = 1
0.00.337.397 I llama_init_from_model: n_ctx         = 8192
0.00.337.398 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.337.398 I llama_init_from_model: n_batch       = 2048
0.00.337.398 I llama_init_from_model: n_ubatch      = 2048
0.00.337.398 I llama_init_from_model: flash_attn    = 0
0.00.337.398 I llama_init_from_model: freq_base     = 10000.0
0.00.337.399 I llama_init_from_model: freq_scale    = 1
0.00.337.399 I ggml_metal_init: allocating
0.00.337.402 I ggml_metal_init: found device: Apple M4
0.00.337.404 I ggml_metal_init: picking default device: Apple M4
0.00.338.202 I ggml_metal_init: using embedded metal library
0.00.341.009 I ggml_metal_init: GPU name:   Apple M4
0.00.341.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.341.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.341.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.341.012 I ggml_metal_init: simdgroup reduction   = true
0.00.341.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.341.012 I ggml_metal_init: has bfloat            = true
0.00.341.013 I ggml_metal_init: use bfloat            = true
0.00.341.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.341.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.350.555 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.352.967 I init:      Metal KV buffer size =    48.00 MiB
0.00.352.969 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.352.972 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.353.504 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.353.505 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.353.505 I llama_init_from_model: graph nodes  = 154
0.00.353.505 I llama_init_from_model: graph splits = 2
0.00.353.507 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.367.036 I 
0.00.367.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.367.301 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.367.301 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.367.312 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.367.312 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.367.318 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.367.319 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.367.867 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.371.585 I llama_perf_context_print:        load time =     343.34 ms
0.00.371.586 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16720.60 tokens per second)
0.00.371.587 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.371.588 I llama_perf_context_print:       total time =       4.55 ms /    63 tokens
0.00.371.799 I ggml_metal_free: deallocating

real	0m1.091s
user	0m0.341s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.196 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.307 I main: llama backend init
0.00.000.320 I main: load the model and apply lora adapter, if any
0.00.041.913 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.057.696 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.712 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.723 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.723 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.724 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.728 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.729 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.729 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.730 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.731 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.736 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.736 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.065.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.713 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.074.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.074.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.074.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.074.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.074.655 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.074.656 I llama_model_loader: - type  f32:  194 tensors
0.00.074.656 I llama_model_loader: - type  f16:   98 tensors
0.00.074.658 I print_info: file format = GGUF V3 (latest)
0.00.074.658 I print_info: file type   = all F32 (guessed)
0.00.074.659 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.100.947 I load: special tokens cache size = 25
0.00.107.599 I load: token to piece cache size = 0.2984 MB
0.00.107.601 I print_info: arch             = gptneox
0.00.107.602 I print_info: vocab_only       = 0
0.00.107.602 I print_info: n_ctx_train      = 2048
0.00.107.602 I print_info: n_embd           = 2048
0.00.107.602 I print_info: n_layer          = 24
0.00.107.605 I print_info: n_head           = 16
0.00.107.606 I print_info: n_head_kv        = 16
0.00.107.606 I print_info: n_rot            = 32
0.00.107.606 I print_info: n_swa            = 0
0.00.107.606 I print_info: n_embd_head_k    = 128
0.00.107.608 I print_info: n_embd_head_v    = 128
0.00.107.609 I print_info: n_gqa            = 1
0.00.107.609 I print_info: n_embd_k_gqa     = 2048
0.00.107.610 I print_info: n_embd_v_gqa     = 2048
0.00.107.610 I print_info: f_norm_eps       = 1.0e-05
0.00.107.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.107.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.107.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.107.614 I print_info: f_logit_scale    = 0.0e+00
0.00.107.616 I print_info: n_ff             = 8192
0.00.107.617 I print_info: n_expert         = 0
0.00.107.617 I print_info: n_expert_used    = 0
0.00.107.617 I print_info: causal attn      = 1
0.00.107.617 I print_info: pooling type     = 0
0.00.107.617 I print_info: rope type        = 2
0.00.107.617 I print_info: rope scaling     = linear
0.00.107.618 I print_info: freq_base_train  = 10000.0
0.00.107.618 I print_info: freq_scale_train = 1
0.00.107.618 I print_info: n_ctx_orig_yarn  = 2048
0.00.107.618 I print_info: rope_finetuned   = unknown
0.00.107.618 I print_info: ssm_d_conv       = 0
0.00.107.619 I print_info: ssm_d_inner      = 0
0.00.107.619 I print_info: ssm_d_state      = 0
0.00.107.619 I print_info: ssm_dt_rank      = 0
0.00.107.622 I print_info: ssm_dt_b_c_rms   = 0
0.00.107.622 I print_info: model type       = 1.4B
0.00.107.622 I print_info: model params     = 1.41 B
0.00.107.622 I print_info: general.name     = 1.4B
0.00.107.623 I print_info: vocab type       = BPE
0.00.107.623 I print_info: n_vocab          = 50304
0.00.107.623 I print_info: n_merges         = 50009
0.00.107.623 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.107.624 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.107.624 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.107.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.107.624 I print_info: LF token         = 128 'Ä'
0.00.107.625 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.107.625 I print_info: max token length = 1024
0.00.110.099 I load_tensors: offloading 24 repeating layers to GPU
0.00.110.099 I load_tensors: offloading output layer to GPU
0.00.110.100 I load_tensors: offloaded 25/25 layers to GPU
0.00.110.117 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.110.118 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.110.395 I llama_init_from_model: n_seq_max     = 1
0.00.110.396 I llama_init_from_model: n_ctx         = 2048
0.00.110.396 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.110.396 I llama_init_from_model: n_batch       = 2048
0.00.110.396 I llama_init_from_model: n_ubatch      = 512
0.00.110.397 I llama_init_from_model: flash_attn    = 0
0.00.110.397 I llama_init_from_model: freq_base     = 10000.0
0.00.110.397 I llama_init_from_model: freq_scale    = 1
0.00.110.398 I ggml_metal_init: allocating
0.00.110.400 I ggml_metal_init: found device: Apple M4
0.00.110.402 I ggml_metal_init: picking default device: Apple M4
0.00.111.055 I ggml_metal_init: using embedded metal library
0.00.122.004 I ggml_metal_init: GPU name:   Apple M4
0.00.122.006 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.122.006 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.122.007 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.122.007 I ggml_metal_init: simdgroup reduction   = true
0.00.122.007 I ggml_metal_init: simdgroup matrix mul. = true
0.00.122.007 I ggml_metal_init: has bfloat            = true
0.00.122.007 I ggml_metal_init: use bfloat            = true
0.00.122.008 I ggml_metal_init: hasUnifiedMemory      = true
0.00.122.008 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.145.650 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.166.322 I init:      Metal KV buffer size =   384.00 MiB
0.00.166.328 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.166.348 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.167.311 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.167.313 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.167.313 I llama_init_from_model: graph nodes  = 967
0.00.167.314 I llama_init_from_model: graph splits = 2
0.00.167.317 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.167.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.167.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.250.308 I main: llama threadpool init, n_threads = 4
0.00.250.358 I 
0.00.250.379 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.250.380 I 
0.00.250.446 I sampler seed: 1234
0.00.250.451 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.250.477 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.250.479 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.250.479 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.078.118 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.02.078.119 I llama_perf_context_print:        load time =     208.38 ms
0.02.078.120 I llama_perf_context_print: prompt eval time =      43.58 ms /     7 tokens (    6.23 ms per token,   160.62 tokens per second)
0.02.078.121 I llama_perf_context_print:        eval time =    1781.25 ms /    63 runs   (   28.27 ms per token,    35.37 tokens per second)
0.02.078.121 I llama_perf_context_print:       total time =    1827.81 ms /    70 tokens
0.02.078.344 I ggml_metal_free: deallocating

real	0m2.392s
user	0m0.141s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.621 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.539 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.276 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.289 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.290 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.292 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.293 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.293 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.294 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.295 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.296 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.296 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.843 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.771 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.170 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.172 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.172 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.173 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.173 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.174 I llama_model_loader: - type  f32:  194 tensors
0.00.053.174 I llama_model_loader: - type  f16:   98 tensors
0.00.053.174 I print_info: file format = GGUF V3 (latest)
0.00.053.175 I print_info: file type   = all F32 (guessed)
0.00.053.181 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.966 I load: special tokens cache size = 25
0.00.085.609 I load: token to piece cache size = 0.2984 MB
0.00.085.612 I print_info: arch             = gptneox
0.00.085.612 I print_info: vocab_only       = 0
0.00.085.612 I print_info: n_ctx_train      = 2048
0.00.085.613 I print_info: n_embd           = 2048
0.00.085.613 I print_info: n_layer          = 24
0.00.085.615 I print_info: n_head           = 16
0.00.085.616 I print_info: n_head_kv        = 16
0.00.085.616 I print_info: n_rot            = 32
0.00.085.617 I print_info: n_swa            = 0
0.00.085.617 I print_info: n_embd_head_k    = 128
0.00.085.617 I print_info: n_embd_head_v    = 128
0.00.085.618 I print_info: n_gqa            = 1
0.00.085.618 I print_info: n_embd_k_gqa     = 2048
0.00.085.619 I print_info: n_embd_v_gqa     = 2048
0.00.085.619 I print_info: f_norm_eps       = 1.0e-05
0.00.085.623 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.623 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.624 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.625 I print_info: f_logit_scale    = 0.0e+00
0.00.085.626 I print_info: n_ff             = 8192
0.00.085.626 I print_info: n_expert         = 0
0.00.085.626 I print_info: n_expert_used    = 0
0.00.085.626 I print_info: causal attn      = 1
0.00.085.626 I print_info: pooling type     = 0
0.00.085.626 I print_info: rope type        = 2
0.00.085.627 I print_info: rope scaling     = linear
0.00.085.627 I print_info: freq_base_train  = 10000.0
0.00.085.628 I print_info: freq_scale_train = 1
0.00.085.628 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.628 I print_info: rope_finetuned   = unknown
0.00.085.628 I print_info: ssm_d_conv       = 0
0.00.085.629 I print_info: ssm_d_inner      = 0
0.00.085.629 I print_info: ssm_d_state      = 0
0.00.085.629 I print_info: ssm_dt_rank      = 0
0.00.085.629 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.629 I print_info: model type       = 1.4B
0.00.085.630 I print_info: model params     = 1.41 B
0.00.085.630 I print_info: general.name     = 1.4B
0.00.085.630 I print_info: vocab type       = BPE
0.00.085.630 I print_info: n_vocab          = 50304
0.00.085.631 I print_info: n_merges         = 50009
0.00.085.632 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.632 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.633 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.633 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.633 I print_info: LF token         = 128 'Ä'
0.00.085.633 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.633 I print_info: max token length = 1024
0.00.088.213 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.214 I load_tensors: offloading output layer to GPU
0.00.088.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.224 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.226 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.608 I llama_init_from_model: n_seq_max     = 1
0.00.088.609 I llama_init_from_model: n_ctx         = 128
0.00.088.609 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.610 I llama_init_from_model: n_batch       = 128
0.00.088.610 I llama_init_from_model: n_ubatch      = 128
0.00.088.610 I llama_init_from_model: flash_attn    = 0
0.00.088.610 I llama_init_from_model: freq_base     = 10000.0
0.00.088.611 I llama_init_from_model: freq_scale    = 1
0.00.088.611 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.611 I ggml_metal_init: allocating
0.00.088.614 I ggml_metal_init: found device: Apple M4
0.00.088.616 I ggml_metal_init: picking default device: Apple M4
0.00.089.233 I ggml_metal_init: using embedded metal library
0.00.091.777 I ggml_metal_init: GPU name:   Apple M4
0.00.091.778 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.778 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.779 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.779 I ggml_metal_init: simdgroup reduction   = true
0.00.091.779 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.779 I ggml_metal_init: has bfloat            = true
0.00.091.780 I ggml_metal_init: use bfloat            = true
0.00.091.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.046 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.427 I init:      Metal KV buffer size =    24.00 MiB
0.00.102.429 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.444 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.344 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.103.345 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.103.345 I llama_init_from_model: graph nodes  = 967
0.00.103.345 I llama_init_from_model: graph splits = 2
0.00.103.347 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.347 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.108.380 I 
0.01.108.414 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.108.448 I perplexity: tokenizing the input ..
0.01.121.909 I perplexity: tokenization took 13.458 ms
0.01.121.915 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.244.262 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.245.891 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.245.948 I llama_perf_context_print:        load time =    1085.83 ms
0.01.245.949 I llama_perf_context_print: prompt eval time =     121.38 ms /   128 tokens (    0.95 ms per token,  1054.55 tokens per second)
0.01.245.950 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.245.951 I llama_perf_context_print:       total time =     137.57 ms /   129 tokens
0.01.246.456 I ggml_metal_free: deallocating

real	0m1.450s
user	0m0.120s
sys	0m0.209s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.614 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.844 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.850 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.853 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.856 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.857 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.858 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.860 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.860 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.711 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.713 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.713 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.714 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.714 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.714 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.715 I llama_model_loader: - type  f32:  194 tensors
0.00.032.716 I llama_model_loader: - type q8_0:   98 tensors
0.00.032.717 I print_info: file format = GGUF V3 (latest)
0.00.032.717 I print_info: file type   = Q8_0
0.00.032.718 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.053.110 I load: special tokens cache size = 25
0.00.059.260 I load: token to piece cache size = 0.2984 MB
0.00.059.264 I print_info: arch             = gptneox
0.00.059.265 I print_info: vocab_only       = 0
0.00.059.265 I print_info: n_ctx_train      = 2048
0.00.059.265 I print_info: n_embd           = 2048
0.00.059.265 I print_info: n_layer          = 24
0.00.059.269 I print_info: n_head           = 16
0.00.059.270 I print_info: n_head_kv        = 16
0.00.059.271 I print_info: n_rot            = 32
0.00.059.271 I print_info: n_swa            = 0
0.00.059.271 I print_info: n_embd_head_k    = 128
0.00.059.271 I print_info: n_embd_head_v    = 128
0.00.059.274 I print_info: n_gqa            = 1
0.00.059.274 I print_info: n_embd_k_gqa     = 2048
0.00.059.275 I print_info: n_embd_v_gqa     = 2048
0.00.059.276 I print_info: f_norm_eps       = 1.0e-05
0.00.059.277 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.277 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.277 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.277 I print_info: f_logit_scale    = 0.0e+00
0.00.059.278 I print_info: n_ff             = 8192
0.00.059.278 I print_info: n_expert         = 0
0.00.059.278 I print_info: n_expert_used    = 0
0.00.059.278 I print_info: causal attn      = 1
0.00.059.279 I print_info: pooling type     = 0
0.00.059.281 I print_info: rope type        = 2
0.00.059.281 I print_info: rope scaling     = linear
0.00.059.282 I print_info: freq_base_train  = 10000.0
0.00.059.282 I print_info: freq_scale_train = 1
0.00.059.282 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.282 I print_info: rope_finetuned   = unknown
0.00.059.282 I print_info: ssm_d_conv       = 0
0.00.059.283 I print_info: ssm_d_inner      = 0
0.00.059.283 I print_info: ssm_d_state      = 0
0.00.059.283 I print_info: ssm_dt_rank      = 0
0.00.059.283 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.283 I print_info: model type       = 1.4B
0.00.059.284 I print_info: model params     = 1.41 B
0.00.059.284 I print_info: general.name     = 1.4B
0.00.059.285 I print_info: vocab type       = BPE
0.00.059.286 I print_info: n_vocab          = 50304
0.00.059.286 I print_info: n_merges         = 50009
0.00.059.287 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.287 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.287 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.288 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.288 I print_info: LF token         = 128 'Ä'
0.00.059.288 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.288 I print_info: max token length = 1024
0.00.061.748 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.748 I load_tensors: offloading output layer to GPU
0.00.061.748 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.760 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.761 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.062.119 I llama_init_from_model: n_seq_max     = 1
0.00.062.119 I llama_init_from_model: n_ctx         = 2048
0.00.062.119 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.120 I llama_init_from_model: n_batch       = 2048
0.00.062.120 I llama_init_from_model: n_ubatch      = 512
0.00.062.120 I llama_init_from_model: flash_attn    = 0
0.00.062.120 I llama_init_from_model: freq_base     = 10000.0
0.00.062.121 I llama_init_from_model: freq_scale    = 1
0.00.062.121 I ggml_metal_init: allocating
0.00.062.124 I ggml_metal_init: found device: Apple M4
0.00.062.126 I ggml_metal_init: picking default device: Apple M4
0.00.062.901 I ggml_metal_init: using embedded metal library
0.00.065.510 I ggml_metal_init: GPU name:   Apple M4
0.00.065.512 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.512 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.513 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.513 I ggml_metal_init: simdgroup reduction   = true
0.00.065.513 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.513 I ggml_metal_init: has bfloat            = true
0.00.065.513 I ggml_metal_init: use bfloat            = true
0.00.065.514 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.515 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.997 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.972 I init:      Metal KV buffer size =   384.00 MiB
0.00.101.986 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.011 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.246 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.103.248 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.103.248 I llama_init_from_model: graph nodes  = 967
0.00.103.249 I llama_init_from_model: graph splits = 2
0.00.103.253 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.384 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.423.353 I main: llama threadpool init, n_threads = 4
0.01.423.421 I 
0.01.423.467 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.423.469 I 
0.01.423.955 I sampler seed: 1234
0.01.423.967 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.424.047 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.424.050 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.424.050 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.524.794 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50569.80 tokens per second)
0.02.524.795 I llama_perf_context_print:        load time =    1413.73 ms
0.02.524.796 I llama_perf_context_print: prompt eval time =      49.66 ms /     7 tokens (    7.09 ms per token,   140.96 tokens per second)
0.02.524.796 I llama_perf_context_print:        eval time =    1048.19 ms /    63 runs   (   16.64 ms per token,    60.10 tokens per second)
0.02.524.797 I llama_perf_context_print:       total time =    1101.44 ms /    70 tokens
0.02.525.028 I ggml_metal_free: deallocating

real	0m2.544s
user	0m0.121s
sys	0m0.238s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.128 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.941 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.977 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.986 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.989 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.990 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.990 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.991 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.992 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.992 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.997 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.998 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.998 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.215 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.727 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.728 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.728 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.728 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.730 I llama_model_loader: - type  f32:  194 tensors
0.00.039.730 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.731 I print_info: file format = GGUF V3 (latest)
0.00.039.734 I print_info: file type   = Q8_0
0.00.039.735 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.065.842 I load: special tokens cache size = 25
0.00.073.050 I load: token to piece cache size = 0.2984 MB
0.00.073.053 I print_info: arch             = gptneox
0.00.073.053 I print_info: vocab_only       = 0
0.00.073.053 I print_info: n_ctx_train      = 2048
0.00.073.054 I print_info: n_embd           = 2048
0.00.073.054 I print_info: n_layer          = 24
0.00.073.058 I print_info: n_head           = 16
0.00.073.058 I print_info: n_head_kv        = 16
0.00.073.059 I print_info: n_rot            = 32
0.00.073.061 I print_info: n_swa            = 0
0.00.073.061 I print_info: n_embd_head_k    = 128
0.00.073.061 I print_info: n_embd_head_v    = 128
0.00.073.062 I print_info: n_gqa            = 1
0.00.073.063 I print_info: n_embd_k_gqa     = 2048
0.00.073.063 I print_info: n_embd_v_gqa     = 2048
0.00.073.064 I print_info: f_norm_eps       = 1.0e-05
0.00.073.065 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.065 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.066 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.066 I print_info: f_logit_scale    = 0.0e+00
0.00.073.067 I print_info: n_ff             = 8192
0.00.073.067 I print_info: n_expert         = 0
0.00.073.067 I print_info: n_expert_used    = 0
0.00.073.067 I print_info: causal attn      = 1
0.00.073.067 I print_info: pooling type     = 0
0.00.073.067 I print_info: rope type        = 2
0.00.073.068 I print_info: rope scaling     = linear
0.00.073.068 I print_info: freq_base_train  = 10000.0
0.00.073.068 I print_info: freq_scale_train = 1
0.00.073.069 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.069 I print_info: rope_finetuned   = unknown
0.00.073.072 I print_info: ssm_d_conv       = 0
0.00.073.073 I print_info: ssm_d_inner      = 0
0.00.073.073 I print_info: ssm_d_state      = 0
0.00.073.073 I print_info: ssm_dt_rank      = 0
0.00.073.073 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.075 I print_info: model type       = 1.4B
0.00.073.075 I print_info: model params     = 1.41 B
0.00.073.075 I print_info: general.name     = 1.4B
0.00.073.076 I print_info: vocab type       = BPE
0.00.073.080 I print_info: n_vocab          = 50304
0.00.073.080 I print_info: n_merges         = 50009
0.00.073.080 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.080 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.080 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.082 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.082 I print_info: LF token         = 128 'Ä'
0.00.073.082 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.083 I print_info: max token length = 1024
0.00.075.581 I load_tensors: offloading 24 repeating layers to GPU
0.00.075.582 I load_tensors: offloading output layer to GPU
0.00.075.582 I load_tensors: offloaded 25/25 layers to GPU
0.00.075.593 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.594 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.075.932 I llama_init_from_model: n_seq_max     = 1
0.00.075.933 I llama_init_from_model: n_ctx         = 128
0.00.075.933 I llama_init_from_model: n_ctx_per_seq = 128
0.00.075.933 I llama_init_from_model: n_batch       = 128
0.00.075.933 I llama_init_from_model: n_ubatch      = 128
0.00.075.933 I llama_init_from_model: flash_attn    = 0
0.00.075.934 I llama_init_from_model: freq_base     = 10000.0
0.00.075.934 I llama_init_from_model: freq_scale    = 1
0.00.075.934 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.075.935 I ggml_metal_init: allocating
0.00.075.938 I ggml_metal_init: found device: Apple M4
0.00.075.940 I ggml_metal_init: picking default device: Apple M4
0.00.076.662 I ggml_metal_init: using embedded metal library
0.00.079.419 I ggml_metal_init: GPU name:   Apple M4
0.00.079.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.421 I ggml_metal_init: simdgroup reduction   = true
0.00.079.421 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.422 I ggml_metal_init: has bfloat            = true
0.00.079.422 I ggml_metal_init: use bfloat            = true
0.00.079.422 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.423 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.117 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.512 I init:      Metal KV buffer size =    24.00 MiB
0.00.090.517 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.090.535 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.454 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.091.455 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.091.455 I llama_init_from_model: graph nodes  = 967
0.00.091.455 I llama_init_from_model: graph splits = 2
0.00.091.457 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.091.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.957.585 I 
0.00.957.630 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.957.647 I perplexity: tokenizing the input ..
0.00.965.518 I perplexity: tokenization took 7.868 ms
0.00.965.523 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.088.941 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.090.331 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.090.362 I llama_perf_context_print:        load time =     944.63 ms
0.01.090.363 I llama_perf_context_print: prompt eval time =     123.17 ms /   128 tokens (    0.96 ms per token,  1039.22 tokens per second)
0.01.090.364 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.090.364 I llama_perf_context_print:       total time =     132.78 ms /   129 tokens
0.01.090.717 I ggml_metal_free: deallocating

real	0m1.112s
user	0m0.101s
sys	0m0.150s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.012.381 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.022.166 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.168 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.168 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.169 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.169 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.170 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.171 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.175 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.177 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.040 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.867 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.868 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.868 I llama_model_loader: - type  f32:  194 tensors
0.00.030.868 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.868 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.869 I print_info: file format = GGUF V3 (latest)
0.00.030.869 I print_info: file type   = Q4_0
0.00.030.870 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.049.799 I load: special tokens cache size = 25
0.00.055.733 I load: token to piece cache size = 0.2984 MB
0.00.055.736 I print_info: arch             = gptneox
0.00.055.737 I print_info: vocab_only       = 0
0.00.055.737 I print_info: n_ctx_train      = 2048
0.00.055.737 I print_info: n_embd           = 2048
0.00.055.737 I print_info: n_layer          = 24
0.00.055.742 I print_info: n_head           = 16
0.00.055.743 I print_info: n_head_kv        = 16
0.00.055.743 I print_info: n_rot            = 32
0.00.055.743 I print_info: n_swa            = 0
0.00.055.744 I print_info: n_embd_head_k    = 128
0.00.055.744 I print_info: n_embd_head_v    = 128
0.00.055.745 I print_info: n_gqa            = 1
0.00.055.745 I print_info: n_embd_k_gqa     = 2048
0.00.055.746 I print_info: n_embd_v_gqa     = 2048
0.00.055.748 I print_info: f_norm_eps       = 1.0e-05
0.00.055.748 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.748 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.749 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.749 I print_info: f_logit_scale    = 0.0e+00
0.00.055.750 I print_info: n_ff             = 8192
0.00.055.750 I print_info: n_expert         = 0
0.00.055.750 I print_info: n_expert_used    = 0
0.00.055.750 I print_info: causal attn      = 1
0.00.055.752 I print_info: pooling type     = 0
0.00.055.752 I print_info: rope type        = 2
0.00.055.752 I print_info: rope scaling     = linear
0.00.055.753 I print_info: freq_base_train  = 10000.0
0.00.055.753 I print_info: freq_scale_train = 1
0.00.055.753 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.753 I print_info: rope_finetuned   = unknown
0.00.055.754 I print_info: ssm_d_conv       = 0
0.00.055.754 I print_info: ssm_d_inner      = 0
0.00.055.754 I print_info: ssm_d_state      = 0
0.00.055.754 I print_info: ssm_dt_rank      = 0
0.00.055.754 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.754 I print_info: model type       = 1.4B
0.00.055.755 I print_info: model params     = 1.41 B
0.00.055.755 I print_info: general.name     = 1.4B
0.00.055.756 I print_info: vocab type       = BPE
0.00.055.756 I print_info: n_vocab          = 50304
0.00.055.756 I print_info: n_merges         = 50009
0.00.055.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.756 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.757 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.757 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.757 I print_info: LF token         = 128 'Ä'
0.00.055.757 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.758 I print_info: max token length = 1024
0.00.058.060 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.061 I load_tensors: offloading output layer to GPU
0.00.058.061 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.072 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.058.074 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.058.418 I llama_init_from_model: n_seq_max     = 1
0.00.058.418 I llama_init_from_model: n_ctx         = 2048
0.00.058.419 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.419 I llama_init_from_model: n_batch       = 2048
0.00.058.419 I llama_init_from_model: n_ubatch      = 512
0.00.058.419 I llama_init_from_model: flash_attn    = 0
0.00.058.419 I llama_init_from_model: freq_base     = 10000.0
0.00.058.420 I llama_init_from_model: freq_scale    = 1
0.00.058.420 I ggml_metal_init: allocating
0.00.058.423 I ggml_metal_init: found device: Apple M4
0.00.058.425 I ggml_metal_init: picking default device: Apple M4
0.00.059.189 I ggml_metal_init: using embedded metal library
0.00.061.798 I ggml_metal_init: GPU name:   Apple M4
0.00.061.800 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.800 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.801 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.801 I ggml_metal_init: simdgroup reduction   = true
0.00.061.801 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.801 I ggml_metal_init: has bfloat            = true
0.00.061.801 I ggml_metal_init: use bfloat            = true
0.00.061.802 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.803 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.138 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.099.225 I init:      Metal KV buffer size =   384.00 MiB
0.00.099.234 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.258 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.100.574 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.100.577 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.100.577 I llama_init_from_model: graph nodes  = 967
0.00.100.578 I llama_init_from_model: graph splits = 2
0.00.100.582 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.717 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.717 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.742 I main: llama threadpool init, n_threads = 4
0.00.685.793 I 
0.00.685.818 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.821 I 
0.00.686.057 I sampler seed: 1234
0.00.686.063 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.075 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.075 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.372.108 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.372.108 I llama_perf_context_print:        load time =     673.36 ms
0.01.372.109 I llama_perf_context_print: prompt eval time =      43.66 ms /     7 tokens (    6.24 ms per token,   160.32 tokens per second)
0.01.372.110 I llama_perf_context_print:        eval time =     639.17 ms /    63 runs   (   10.15 ms per token,    98.57 tokens per second)
0.01.372.111 I llama_perf_context_print:       total time =     686.37 ms /    70 tokens
0.01.372.354 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.109s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.624 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.929 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.935 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.937 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.938 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.939 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.940 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.943 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.943 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.944 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.944 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.946 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.946 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.840 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.855 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.829 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.830 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.831 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.831 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.832 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.832 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.832 I llama_model_loader: - type  f32:  194 tensors
0.00.025.833 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.833 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.834 I print_info: file format = GGUF V3 (latest)
0.00.025.834 I print_info: file type   = Q4_0
0.00.025.835 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.380 I load: special tokens cache size = 25
0.00.052.552 I load: token to piece cache size = 0.2984 MB
0.00.052.559 I print_info: arch             = gptneox
0.00.052.559 I print_info: vocab_only       = 0
0.00.052.560 I print_info: n_ctx_train      = 2048
0.00.052.560 I print_info: n_embd           = 2048
0.00.052.560 I print_info: n_layer          = 24
0.00.052.565 I print_info: n_head           = 16
0.00.052.565 I print_info: n_head_kv        = 16
0.00.052.565 I print_info: n_rot            = 32
0.00.052.565 I print_info: n_swa            = 0
0.00.052.566 I print_info: n_embd_head_k    = 128
0.00.052.566 I print_info: n_embd_head_v    = 128
0.00.052.574 I print_info: n_gqa            = 1
0.00.052.575 I print_info: n_embd_k_gqa     = 2048
0.00.052.576 I print_info: n_embd_v_gqa     = 2048
0.00.052.576 I print_info: f_norm_eps       = 1.0e-05
0.00.052.576 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.577 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.577 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.577 I print_info: f_logit_scale    = 0.0e+00
0.00.052.577 I print_info: n_ff             = 8192
0.00.052.578 I print_info: n_expert         = 0
0.00.052.578 I print_info: n_expert_used    = 0
0.00.052.578 I print_info: causal attn      = 1
0.00.052.578 I print_info: pooling type     = 0
0.00.052.578 I print_info: rope type        = 2
0.00.052.578 I print_info: rope scaling     = linear
0.00.052.578 I print_info: freq_base_train  = 10000.0
0.00.052.579 I print_info: freq_scale_train = 1
0.00.052.579 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.579 I print_info: rope_finetuned   = unknown
0.00.052.579 I print_info: ssm_d_conv       = 0
0.00.052.579 I print_info: ssm_d_inner      = 0
0.00.052.579 I print_info: ssm_d_state      = 0
0.00.052.580 I print_info: ssm_dt_rank      = 0
0.00.052.580 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.580 I print_info: model type       = 1.4B
0.00.052.580 I print_info: model params     = 1.41 B
0.00.052.580 I print_info: general.name     = 1.4B
0.00.052.582 I print_info: vocab type       = BPE
0.00.052.583 I print_info: n_vocab          = 50304
0.00.052.583 I print_info: n_merges         = 50009
0.00.052.583 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.583 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.583 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.586 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.587 I print_info: LF token         = 128 'Ä'
0.00.052.587 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.587 I print_info: max token length = 1024
0.00.054.525 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.525 I load_tensors: offloading output layer to GPU
0.00.054.525 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.537 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.538 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.954 I llama_init_from_model: n_seq_max     = 1
0.00.054.955 I llama_init_from_model: n_ctx         = 128
0.00.054.955 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.955 I llama_init_from_model: n_batch       = 128
0.00.054.955 I llama_init_from_model: n_ubatch      = 128
0.00.054.955 I llama_init_from_model: flash_attn    = 0
0.00.054.956 I llama_init_from_model: freq_base     = 10000.0
0.00.054.957 I llama_init_from_model: freq_scale    = 1
0.00.054.957 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.958 I ggml_metal_init: allocating
0.00.054.963 I ggml_metal_init: found device: Apple M4
0.00.054.964 I ggml_metal_init: picking default device: Apple M4
0.00.055.586 I ggml_metal_init: using embedded metal library
0.00.058.307 I ggml_metal_init: GPU name:   Apple M4
0.00.058.309 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.310 I ggml_metal_init: simdgroup reduction   = true
0.00.058.312 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.312 I ggml_metal_init: has bfloat            = true
0.00.058.312 I ggml_metal_init: use bfloat            = true
0.00.058.312 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.313 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.838 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.290 I init:      Metal KV buffer size =    24.00 MiB
0.00.070.294 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.310 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.260 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.261 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.261 I llama_init_from_model: graph nodes  = 967
0.00.071.262 I llama_init_from_model: graph splits = 2
0.00.071.263 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.805 I 
0.00.614.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.847 I perplexity: tokenizing the input ..
0.00.622.467 I perplexity: tokenization took 7.618 ms
0.00.622.470 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.745.150 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.746.309 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.746.341 I llama_perf_context_print:        load time =     605.17 ms
0.00.746.342 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.29 tokens per second)
0.00.746.343 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.746.343 I llama_perf_context_print:       total time =     131.54 ms /   129 tokens
0.00.746.771 I ggml_metal_free: deallocating

real	0m0.764s
user	0m0.079s
sys	0m0.103s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.751 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.460 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.479 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.479 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.481 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.167 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.883 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.884 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.884 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.884 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.885 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.885 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.885 I llama_model_loader: - type  f32:  194 tensors
0.00.024.886 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.886 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.887 I print_info: file format = GGUF V3 (latest)
0.00.024.887 I print_info: file type   = Q4_1
0.00.024.888 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.601 I load: special tokens cache size = 25
0.00.049.555 I load: token to piece cache size = 0.2984 MB
0.00.049.557 I print_info: arch             = gptneox
0.00.049.558 I print_info: vocab_only       = 0
0.00.049.558 I print_info: n_ctx_train      = 2048
0.00.049.558 I print_info: n_embd           = 2048
0.00.049.558 I print_info: n_layer          = 24
0.00.049.561 I print_info: n_head           = 16
0.00.049.562 I print_info: n_head_kv        = 16
0.00.049.562 I print_info: n_rot            = 32
0.00.049.564 I print_info: n_swa            = 0
0.00.049.564 I print_info: n_embd_head_k    = 128
0.00.049.565 I print_info: n_embd_head_v    = 128
0.00.049.565 I print_info: n_gqa            = 1
0.00.049.566 I print_info: n_embd_k_gqa     = 2048
0.00.049.571 I print_info: n_embd_v_gqa     = 2048
0.00.049.572 I print_info: f_norm_eps       = 1.0e-05
0.00.049.572 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.572 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.574 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.574 I print_info: f_logit_scale    = 0.0e+00
0.00.049.575 I print_info: n_ff             = 8192
0.00.049.575 I print_info: n_expert         = 0
0.00.049.575 I print_info: n_expert_used    = 0
0.00.049.575 I print_info: causal attn      = 1
0.00.049.577 I print_info: pooling type     = 0
0.00.049.577 I print_info: rope type        = 2
0.00.049.577 I print_info: rope scaling     = linear
0.00.049.578 I print_info: freq_base_train  = 10000.0
0.00.049.578 I print_info: freq_scale_train = 1
0.00.049.578 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.579 I print_info: rope_finetuned   = unknown
0.00.049.579 I print_info: ssm_d_conv       = 0
0.00.049.579 I print_info: ssm_d_inner      = 0
0.00.049.579 I print_info: ssm_d_state      = 0
0.00.049.579 I print_info: ssm_dt_rank      = 0
0.00.049.579 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.580 I print_info: model type       = 1.4B
0.00.049.585 I print_info: model params     = 1.41 B
0.00.049.586 I print_info: general.name     = 1.4B
0.00.049.587 I print_info: vocab type       = BPE
0.00.049.587 I print_info: n_vocab          = 50304
0.00.049.587 I print_info: n_merges         = 50009
0.00.049.587 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.588 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.588 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.588 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.589 I print_info: LF token         = 128 'Ä'
0.00.049.591 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.591 I print_info: max token length = 1024
0.00.051.512 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.513 I load_tensors: offloading output layer to GPU
0.00.051.513 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.523 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.525 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.816 I llama_init_from_model: n_seq_max     = 1
0.00.051.817 I llama_init_from_model: n_ctx         = 2048
0.00.051.817 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.817 I llama_init_from_model: n_batch       = 2048
0.00.051.817 I llama_init_from_model: n_ubatch      = 512
0.00.051.817 I llama_init_from_model: flash_attn    = 0
0.00.051.818 I llama_init_from_model: freq_base     = 10000.0
0.00.051.818 I llama_init_from_model: freq_scale    = 1
0.00.051.818 I ggml_metal_init: allocating
0.00.051.821 I ggml_metal_init: found device: Apple M4
0.00.051.823 I ggml_metal_init: picking default device: Apple M4
0.00.052.410 I ggml_metal_init: using embedded metal library
0.00.054.794 I ggml_metal_init: GPU name:   Apple M4
0.00.054.796 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.796 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.797 I ggml_metal_init: simdgroup reduction   = true
0.00.054.797 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.797 I ggml_metal_init: has bfloat            = true
0.00.054.797 I ggml_metal_init: use bfloat            = true
0.00.054.798 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.798 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.396 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.044 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.051 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.072 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.041 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.042 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.043 I llama_init_from_model: graph nodes  = 967
0.00.086.043 I llama_init_from_model: graph splits = 2
0.00.086.046 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.175 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.176 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.974 I main: llama threadpool init, n_threads = 4
0.00.702.018 I 
0.00.702.044 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.046 I 
0.00.702.275 I sampler seed: 1234
0.00.702.280 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.325 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.325 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.325 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.433.474 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61206.90 tokens per second)
0.01.433.474 I llama_perf_context_print:        load time =     693.22 ms
0.01.433.475 I llama_perf_context_print: prompt eval time =      43.55 ms /     7 tokens (    6.22 ms per token,   160.73 tokens per second)
0.01.433.476 I llama_perf_context_print:        eval time =     684.68 ms /    63 runs   (   10.87 ms per token,    92.01 tokens per second)
0.01.433.476 I llama_perf_context_print:       total time =     731.50 ms /    70 tokens
0.01.433.800 I ggml_metal_free: deallocating

real	0m1.452s
user	0m0.108s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.842 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.688 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.693 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.699 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.701 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.702 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.702 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.706 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.707 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.707 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.417 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.432 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.204 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.205 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.206 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.206 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.207 I llama_model_loader: - type  f32:  194 tensors
0.00.024.207 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.208 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.208 I print_info: file format = GGUF V3 (latest)
0.00.024.209 I print_info: file type   = Q4_1
0.00.024.209 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.563 I load: special tokens cache size = 25
0.00.049.216 I load: token to piece cache size = 0.2984 MB
0.00.049.219 I print_info: arch             = gptneox
0.00.049.219 I print_info: vocab_only       = 0
0.00.049.219 I print_info: n_ctx_train      = 2048
0.00.049.219 I print_info: n_embd           = 2048
0.00.049.220 I print_info: n_layer          = 24
0.00.049.222 I print_info: n_head           = 16
0.00.049.223 I print_info: n_head_kv        = 16
0.00.049.223 I print_info: n_rot            = 32
0.00.049.223 I print_info: n_swa            = 0
0.00.049.224 I print_info: n_embd_head_k    = 128
0.00.049.224 I print_info: n_embd_head_v    = 128
0.00.049.225 I print_info: n_gqa            = 1
0.00.049.226 I print_info: n_embd_k_gqa     = 2048
0.00.049.227 I print_info: n_embd_v_gqa     = 2048
0.00.049.227 I print_info: f_norm_eps       = 1.0e-05
0.00.049.227 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.228 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.228 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.228 I print_info: f_logit_scale    = 0.0e+00
0.00.049.229 I print_info: n_ff             = 8192
0.00.049.229 I print_info: n_expert         = 0
0.00.049.229 I print_info: n_expert_used    = 0
0.00.049.229 I print_info: causal attn      = 1
0.00.049.229 I print_info: pooling type     = 0
0.00.049.230 I print_info: rope type        = 2
0.00.049.230 I print_info: rope scaling     = linear
0.00.049.232 I print_info: freq_base_train  = 10000.0
0.00.049.233 I print_info: freq_scale_train = 1
0.00.049.233 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.233 I print_info: rope_finetuned   = unknown
0.00.049.233 I print_info: ssm_d_conv       = 0
0.00.049.233 I print_info: ssm_d_inner      = 0
0.00.049.234 I print_info: ssm_d_state      = 0
0.00.049.234 I print_info: ssm_dt_rank      = 0
0.00.049.234 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.234 I print_info: model type       = 1.4B
0.00.049.234 I print_info: model params     = 1.41 B
0.00.049.235 I print_info: general.name     = 1.4B
0.00.049.235 I print_info: vocab type       = BPE
0.00.049.235 I print_info: n_vocab          = 50304
0.00.049.240 I print_info: n_merges         = 50009
0.00.049.240 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.240 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.240 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.241 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.241 I print_info: LF token         = 128 'Ä'
0.00.049.241 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.242 I print_info: max token length = 1024
0.00.050.889 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.890 I load_tensors: offloading output layer to GPU
0.00.050.890 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.900 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.901 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.169 I llama_init_from_model: n_seq_max     = 1
0.00.051.170 I llama_init_from_model: n_ctx         = 128
0.00.051.170 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.170 I llama_init_from_model: n_batch       = 128
0.00.051.170 I llama_init_from_model: n_ubatch      = 128
0.00.051.171 I llama_init_from_model: flash_attn    = 0
0.00.051.171 I llama_init_from_model: freq_base     = 10000.0
0.00.051.171 I llama_init_from_model: freq_scale    = 1
0.00.051.171 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.172 I ggml_metal_init: allocating
0.00.051.175 I ggml_metal_init: found device: Apple M4
0.00.051.177 I ggml_metal_init: picking default device: Apple M4
0.00.051.736 I ggml_metal_init: using embedded metal library
0.00.054.118 I ggml_metal_init: GPU name:   Apple M4
0.00.054.120 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.120 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.120 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.121 I ggml_metal_init: simdgroup reduction   = true
0.00.054.121 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.121 I ggml_metal_init: has bfloat            = true
0.00.054.121 I ggml_metal_init: use bfloat            = true
0.00.054.121 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.122 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.009 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.320 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.324 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.347 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.190 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.191 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.191 I llama_init_from_model: graph nodes  = 967
0.00.066.192 I llama_init_from_model: graph splits = 2
0.00.066.193 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.193 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.950 I 
0.00.667.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.983 I perplexity: tokenizing the input ..
0.00.675.552 I perplexity: tokenization took 7.568 ms
0.00.675.556 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.364 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.799.537 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.799.572 I llama_perf_context_print:        load time =     659.11 ms
0.00.799.577 I llama_perf_context_print: prompt eval time =     122.58 ms /   128 tokens (    0.96 ms per token,  1044.24 tokens per second)
0.00.799.578 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.578 I llama_perf_context_print:       total time =     131.62 ms /   129 tokens
0.00.800.012 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.077s
sys	0m0.107s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.833 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.488 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.494 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.496 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.496 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.497 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.504 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.429 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.262 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.263 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.263 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.264 I llama_model_loader: - type  f32:  194 tensors
0.00.026.264 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.264 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.265 I print_info: file format = GGUF V3 (latest)
0.00.026.265 I print_info: file type   = Q5_0
0.00.026.266 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.485 I load: special tokens cache size = 25
0.00.052.207 I load: token to piece cache size = 0.2984 MB
0.00.052.210 I print_info: arch             = gptneox
0.00.052.210 I print_info: vocab_only       = 0
0.00.052.211 I print_info: n_ctx_train      = 2048
0.00.052.211 I print_info: n_embd           = 2048
0.00.052.211 I print_info: n_layer          = 24
0.00.052.214 I print_info: n_head           = 16
0.00.052.215 I print_info: n_head_kv        = 16
0.00.052.215 I print_info: n_rot            = 32
0.00.052.216 I print_info: n_swa            = 0
0.00.052.216 I print_info: n_embd_head_k    = 128
0.00.052.218 I print_info: n_embd_head_v    = 128
0.00.052.219 I print_info: n_gqa            = 1
0.00.052.220 I print_info: n_embd_k_gqa     = 2048
0.00.052.222 I print_info: n_embd_v_gqa     = 2048
0.00.052.222 I print_info: f_norm_eps       = 1.0e-05
0.00.052.224 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.225 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.225 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.226 I print_info: f_logit_scale    = 0.0e+00
0.00.052.226 I print_info: n_ff             = 8192
0.00.052.226 I print_info: n_expert         = 0
0.00.052.227 I print_info: n_expert_used    = 0
0.00.052.227 I print_info: causal attn      = 1
0.00.052.227 I print_info: pooling type     = 0
0.00.052.227 I print_info: rope type        = 2
0.00.052.227 I print_info: rope scaling     = linear
0.00.052.228 I print_info: freq_base_train  = 10000.0
0.00.052.228 I print_info: freq_scale_train = 1
0.00.052.228 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.228 I print_info: rope_finetuned   = unknown
0.00.052.228 I print_info: ssm_d_conv       = 0
0.00.052.228 I print_info: ssm_d_inner      = 0
0.00.052.229 I print_info: ssm_d_state      = 0
0.00.052.232 I print_info: ssm_dt_rank      = 0
0.00.052.232 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.233 I print_info: model type       = 1.4B
0.00.052.234 I print_info: model params     = 1.41 B
0.00.052.234 I print_info: general.name     = 1.4B
0.00.052.235 I print_info: vocab type       = BPE
0.00.052.235 I print_info: n_vocab          = 50304
0.00.052.235 I print_info: n_merges         = 50009
0.00.052.235 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.235 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.236 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.236 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.237 I print_info: LF token         = 128 'Ä'
0.00.052.237 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.237 I print_info: max token length = 1024
0.00.054.282 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.283 I load_tensors: offloading output layer to GPU
0.00.054.283 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.293 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.295 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.630 I llama_init_from_model: n_seq_max     = 1
0.00.054.631 I llama_init_from_model: n_ctx         = 2048
0.00.054.631 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.631 I llama_init_from_model: n_batch       = 2048
0.00.054.631 I llama_init_from_model: n_ubatch      = 512
0.00.054.631 I llama_init_from_model: flash_attn    = 0
0.00.054.632 I llama_init_from_model: freq_base     = 10000.0
0.00.054.632 I llama_init_from_model: freq_scale    = 1
0.00.054.632 I ggml_metal_init: allocating
0.00.054.636 I ggml_metal_init: found device: Apple M4
0.00.054.638 I ggml_metal_init: picking default device: Apple M4
0.00.055.267 I ggml_metal_init: using embedded metal library
0.00.057.668 I ggml_metal_init: GPU name:   Apple M4
0.00.057.669 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.670 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.671 I ggml_metal_init: simdgroup reduction   = true
0.00.057.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.671 I ggml_metal_init: has bfloat            = true
0.00.057.671 I ggml_metal_init: use bfloat            = true
0.00.057.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.989 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.280 I init:      Metal KV buffer size =   384.00 MiB
0.00.088.286 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.305 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.382 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.385 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.386 I llama_init_from_model: graph nodes  = 967
0.00.089.386 I llama_init_from_model: graph splits = 2
0.00.089.389 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.521 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.522 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.707 I main: llama threadpool init, n_threads = 4
0.00.776.749 I 
0.00.776.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.770 I 
0.00.776.997 I sampler seed: 1234
0.00.777.001 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.012 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.012 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.012 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.554.859 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.554.859 I llama_perf_context_print:        load time =     766.87 ms
0.01.554.860 I llama_perf_context_print: prompt eval time =      43.17 ms /     7 tokens (    6.17 ms per token,   162.15 tokens per second)
0.01.554.861 I llama_perf_context_print:        eval time =     731.75 ms /    63 runs   (   11.62 ms per token,    86.09 tokens per second)
0.01.554.861 I llama_perf_context_print:       total time =     778.15 ms /    70 tokens
0.01.555.092 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.111s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.968 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.757 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.762 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.764 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.767 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.768 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.769 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.769 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.770 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.772 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.617 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.608 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.414 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.415 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.416 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.416 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.416 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.417 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.417 I llama_model_loader: - type  f32:  194 tensors
0.00.025.417 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.418 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.418 I print_info: file format = GGUF V3 (latest)
0.00.025.419 I print_info: file type   = Q5_0
0.00.025.421 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.750 I load: special tokens cache size = 25
0.00.050.768 I load: token to piece cache size = 0.2984 MB
0.00.050.771 I print_info: arch             = gptneox
0.00.050.771 I print_info: vocab_only       = 0
0.00.050.771 I print_info: n_ctx_train      = 2048
0.00.050.771 I print_info: n_embd           = 2048
0.00.050.772 I print_info: n_layer          = 24
0.00.050.774 I print_info: n_head           = 16
0.00.050.785 I print_info: n_head_kv        = 16
0.00.050.787 I print_info: n_rot            = 32
0.00.050.787 I print_info: n_swa            = 0
0.00.050.787 I print_info: n_embd_head_k    = 128
0.00.050.787 I print_info: n_embd_head_v    = 128
0.00.050.790 I print_info: n_gqa            = 1
0.00.050.790 I print_info: n_embd_k_gqa     = 2048
0.00.050.791 I print_info: n_embd_v_gqa     = 2048
0.00.050.792 I print_info: f_norm_eps       = 1.0e-05
0.00.050.792 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.793 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.793 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.794 I print_info: f_logit_scale    = 0.0e+00
0.00.050.794 I print_info: n_ff             = 8192
0.00.050.794 I print_info: n_expert         = 0
0.00.050.795 I print_info: n_expert_used    = 0
0.00.050.795 I print_info: causal attn      = 1
0.00.050.796 I print_info: pooling type     = 0
0.00.050.796 I print_info: rope type        = 2
0.00.050.797 I print_info: rope scaling     = linear
0.00.050.797 I print_info: freq_base_train  = 10000.0
0.00.050.797 I print_info: freq_scale_train = 1
0.00.050.797 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.799 I print_info: rope_finetuned   = unknown
0.00.050.799 I print_info: ssm_d_conv       = 0
0.00.050.799 I print_info: ssm_d_inner      = 0
0.00.050.799 I print_info: ssm_d_state      = 0
0.00.050.799 I print_info: ssm_dt_rank      = 0
0.00.050.799 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.800 I print_info: model type       = 1.4B
0.00.050.802 I print_info: model params     = 1.41 B
0.00.050.803 I print_info: general.name     = 1.4B
0.00.050.803 I print_info: vocab type       = BPE
0.00.050.804 I print_info: n_vocab          = 50304
0.00.050.804 I print_info: n_merges         = 50009
0.00.050.804 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.804 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.804 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.804 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.805 I print_info: LF token         = 128 'Ä'
0.00.050.806 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.806 I print_info: max token length = 1024
0.00.052.745 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.745 I load_tensors: offloading output layer to GPU
0.00.052.745 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.756 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.757 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.033 I llama_init_from_model: n_seq_max     = 1
0.00.053.034 I llama_init_from_model: n_ctx         = 128
0.00.053.034 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.034 I llama_init_from_model: n_batch       = 128
0.00.053.034 I llama_init_from_model: n_ubatch      = 128
0.00.053.034 I llama_init_from_model: flash_attn    = 0
0.00.053.035 I llama_init_from_model: freq_base     = 10000.0
0.00.053.035 I llama_init_from_model: freq_scale    = 1
0.00.053.035 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.036 I ggml_metal_init: allocating
0.00.053.039 I ggml_metal_init: found device: Apple M4
0.00.053.040 I ggml_metal_init: picking default device: Apple M4
0.00.053.613 I ggml_metal_init: using embedded metal library
0.00.055.988 I ggml_metal_init: GPU name:   Apple M4
0.00.055.990 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.990 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.990 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.991 I ggml_metal_init: simdgroup reduction   = true
0.00.055.991 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.991 I ggml_metal_init: has bfloat            = true
0.00.055.991 I ggml_metal_init: use bfloat            = true
0.00.055.991 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.438 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.698 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.701 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.716 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.613 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.614 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.614 I llama_init_from_model: graph nodes  = 967
0.00.067.614 I llama_init_from_model: graph splits = 2
0.00.067.615 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.615 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.399 I 
0.00.690.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.448 I perplexity: tokenizing the input ..
0.00.698.066 I perplexity: tokenization took 7.617 ms
0.00.698.071 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.483 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.834.703 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.834.731 I llama_perf_context_print:        load time =     680.42 ms
0.00.834.732 I llama_perf_context_print: prompt eval time =     135.17 ms /   128 tokens (    1.06 ms per token,   946.96 tokens per second)
0.00.834.732 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.734 I llama_perf_context_print:       total time =     144.34 ms /   129 tokens
0.00.835.263 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.077s
sys	0m0.106s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.699 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.396 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.411 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.027 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.027 I llama_model_loader: - type  f32:  194 tensors
0.00.025.027 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.028 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.028 I print_info: file format = GGUF V3 (latest)
0.00.025.029 I print_info: file type   = Q5_1
0.00.025.029 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.414 I load: special tokens cache size = 25
0.00.050.447 I load: token to piece cache size = 0.2984 MB
0.00.050.450 I print_info: arch             = gptneox
0.00.050.450 I print_info: vocab_only       = 0
0.00.050.450 I print_info: n_ctx_train      = 2048
0.00.050.450 I print_info: n_embd           = 2048
0.00.050.450 I print_info: n_layer          = 24
0.00.050.454 I print_info: n_head           = 16
0.00.050.454 I print_info: n_head_kv        = 16
0.00.050.455 I print_info: n_rot            = 32
0.00.050.455 I print_info: n_swa            = 0
0.00.050.455 I print_info: n_embd_head_k    = 128
0.00.050.455 I print_info: n_embd_head_v    = 128
0.00.050.456 I print_info: n_gqa            = 1
0.00.050.456 I print_info: n_embd_k_gqa     = 2048
0.00.050.457 I print_info: n_embd_v_gqa     = 2048
0.00.050.458 I print_info: f_norm_eps       = 1.0e-05
0.00.050.458 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.458 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.460 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.460 I print_info: f_logit_scale    = 0.0e+00
0.00.050.461 I print_info: n_ff             = 8192
0.00.050.461 I print_info: n_expert         = 0
0.00.050.461 I print_info: n_expert_used    = 0
0.00.050.461 I print_info: causal attn      = 1
0.00.050.463 I print_info: pooling type     = 0
0.00.050.463 I print_info: rope type        = 2
0.00.050.463 I print_info: rope scaling     = linear
0.00.050.464 I print_info: freq_base_train  = 10000.0
0.00.050.464 I print_info: freq_scale_train = 1
0.00.050.464 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.465 I print_info: rope_finetuned   = unknown
0.00.050.465 I print_info: ssm_d_conv       = 0
0.00.050.465 I print_info: ssm_d_inner      = 0
0.00.050.465 I print_info: ssm_d_state      = 0
0.00.050.465 I print_info: ssm_dt_rank      = 0
0.00.050.465 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.465 I print_info: model type       = 1.4B
0.00.050.466 I print_info: model params     = 1.41 B
0.00.050.466 I print_info: general.name     = 1.4B
0.00.050.467 I print_info: vocab type       = BPE
0.00.050.467 I print_info: n_vocab          = 50304
0.00.050.467 I print_info: n_merges         = 50009
0.00.050.467 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.467 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.471 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.472 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.472 I print_info: LF token         = 128 'Ä'
0.00.050.472 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.472 I print_info: max token length = 1024
0.00.052.493 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.494 I load_tensors: offloading output layer to GPU
0.00.052.494 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.504 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.505 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.793 I llama_init_from_model: n_seq_max     = 1
0.00.052.794 I llama_init_from_model: n_ctx         = 2048
0.00.052.794 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.794 I llama_init_from_model: n_batch       = 2048
0.00.052.795 I llama_init_from_model: n_ubatch      = 512
0.00.052.795 I llama_init_from_model: flash_attn    = 0
0.00.052.795 I llama_init_from_model: freq_base     = 10000.0
0.00.052.795 I llama_init_from_model: freq_scale    = 1
0.00.052.796 I ggml_metal_init: allocating
0.00.052.799 I ggml_metal_init: found device: Apple M4
0.00.052.801 I ggml_metal_init: picking default device: Apple M4
0.00.053.406 I ggml_metal_init: using embedded metal library
0.00.055.747 I ggml_metal_init: GPU name:   Apple M4
0.00.055.749 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.749 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.749 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.750 I ggml_metal_init: simdgroup reduction   = true
0.00.055.750 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.750 I ggml_metal_init: has bfloat            = true
0.00.055.750 I ggml_metal_init: use bfloat            = true
0.00.055.750 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.632 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.026 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.032 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.053 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.991 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.993 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.993 I llama_init_from_model: graph nodes  = 967
0.00.085.993 I llama_init_from_model: graph splits = 2
0.00.085.996 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.127 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.127 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.387 I main: llama threadpool init, n_threads = 4
0.00.722.427 I 
0.00.722.449 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.468 I 
0.00.722.701 I sampler seed: 1234
0.00.722.707 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.722.744 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.722.745 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.722.745 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.555.303 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.555.304 I llama_perf_context_print:        load time =     713.68 ms
0.01.555.305 I llama_perf_context_print: prompt eval time =      42.17 ms /     7 tokens (    6.02 ms per token,   165.98 tokens per second)
0.01.555.305 I llama_perf_context_print:        eval time =     787.40 ms /    63 runs   (   12.50 ms per token,    80.01 tokens per second)
0.01.555.309 I llama_perf_context_print:       total time =     832.92 ms /    70 tokens
0.01.555.513 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.109s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.783 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.156 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.162 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.163 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.163 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.164 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.165 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.165 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.165 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.166 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.166 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.930 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.985 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.779 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.780 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.780 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.781 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.781 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.781 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.782 I llama_model_loader: - type  f32:  194 tensors
0.00.023.782 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.782 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.783 I print_info: file format = GGUF V3 (latest)
0.00.023.783 I print_info: file type   = Q5_1
0.00.023.784 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.318 I load: special tokens cache size = 25
0.00.048.343 I load: token to piece cache size = 0.2984 MB
0.00.048.347 I print_info: arch             = gptneox
0.00.048.347 I print_info: vocab_only       = 0
0.00.048.347 I print_info: n_ctx_train      = 2048
0.00.048.348 I print_info: n_embd           = 2048
0.00.048.348 I print_info: n_layer          = 24
0.00.048.351 I print_info: n_head           = 16
0.00.048.351 I print_info: n_head_kv        = 16
0.00.048.351 I print_info: n_rot            = 32
0.00.048.354 I print_info: n_swa            = 0
0.00.048.354 I print_info: n_embd_head_k    = 128
0.00.048.354 I print_info: n_embd_head_v    = 128
0.00.048.355 I print_info: n_gqa            = 1
0.00.048.356 I print_info: n_embd_k_gqa     = 2048
0.00.048.357 I print_info: n_embd_v_gqa     = 2048
0.00.048.357 I print_info: f_norm_eps       = 1.0e-05
0.00.048.357 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.358 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.358 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.358 I print_info: f_logit_scale    = 0.0e+00
0.00.048.359 I print_info: n_ff             = 8192
0.00.048.359 I print_info: n_expert         = 0
0.00.048.360 I print_info: n_expert_used    = 0
0.00.048.360 I print_info: causal attn      = 1
0.00.048.361 I print_info: pooling type     = 0
0.00.048.361 I print_info: rope type        = 2
0.00.048.361 I print_info: rope scaling     = linear
0.00.048.361 I print_info: freq_base_train  = 10000.0
0.00.048.362 I print_info: freq_scale_train = 1
0.00.048.362 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.362 I print_info: rope_finetuned   = unknown
0.00.048.362 I print_info: ssm_d_conv       = 0
0.00.048.362 I print_info: ssm_d_inner      = 0
0.00.048.362 I print_info: ssm_d_state      = 0
0.00.048.363 I print_info: ssm_dt_rank      = 0
0.00.048.363 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.367 I print_info: model type       = 1.4B
0.00.048.367 I print_info: model params     = 1.41 B
0.00.048.367 I print_info: general.name     = 1.4B
0.00.048.368 I print_info: vocab type       = BPE
0.00.048.368 I print_info: n_vocab          = 50304
0.00.048.368 I print_info: n_merges         = 50009
0.00.048.369 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.370 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.371 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.371 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.371 I print_info: LF token         = 128 'Ä'
0.00.048.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.371 I print_info: max token length = 1024
0.00.050.162 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.162 I load_tensors: offloading output layer to GPU
0.00.050.163 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.168 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.168 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.050.529 I llama_init_from_model: n_seq_max     = 1
0.00.050.530 I llama_init_from_model: n_ctx         = 128
0.00.050.530 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.530 I llama_init_from_model: n_batch       = 128
0.00.050.530 I llama_init_from_model: n_ubatch      = 128
0.00.050.530 I llama_init_from_model: flash_attn    = 0
0.00.050.531 I llama_init_from_model: freq_base     = 10000.0
0.00.050.531 I llama_init_from_model: freq_scale    = 1
0.00.050.531 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.532 I ggml_metal_init: allocating
0.00.050.535 I ggml_metal_init: found device: Apple M4
0.00.050.537 I ggml_metal_init: picking default device: Apple M4
0.00.051.096 I ggml_metal_init: using embedded metal library
0.00.053.411 I ggml_metal_init: GPU name:   Apple M4
0.00.053.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.414 I ggml_metal_init: simdgroup reduction   = true
0.00.053.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.414 I ggml_metal_init: has bfloat            = true
0.00.053.414 I ggml_metal_init: use bfloat            = true
0.00.053.415 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.415 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.904 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.192 I init:      Metal KV buffer size =    24.00 MiB
0.00.064.195 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.212 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.110 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.111 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.112 I llama_init_from_model: graph nodes  = 967
0.00.065.112 I llama_init_from_model: graph splits = 2
0.00.065.113 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.113 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.185 I 
0.00.649.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.223 I perplexity: tokenizing the input ..
0.00.656.745 I perplexity: tokenization took 7.519 ms
0.00.656.749 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.968 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.793.132 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.793.160 I llama_perf_context_print:        load time =     640.40 ms
0.00.793.162 I llama_perf_context_print: prompt eval time =     135.00 ms /   128 tokens (    1.05 ms per token,   948.18 tokens per second)
0.00.793.163 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.164 I llama_perf_context_print:       total time =     143.98 ms /   129 tokens
0.00.793.647 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.076s
sys	0m0.120s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.833 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.585 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.598 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.600 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.601 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.403 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.238 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.238 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.238 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.239 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.239 I llama_model_loader: - type  f32:  194 tensors
0.00.025.239 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.240 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.240 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.240 I print_info: file format = GGUF V3 (latest)
0.00.025.241 I print_info: file type   = Q2_K - Medium
0.00.025.241 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.173 I load: special tokens cache size = 25
0.00.051.226 I load: token to piece cache size = 0.2984 MB
0.00.051.229 I print_info: arch             = gptneox
0.00.051.229 I print_info: vocab_only       = 0
0.00.051.229 I print_info: n_ctx_train      = 2048
0.00.051.229 I print_info: n_embd           = 2048
0.00.051.229 I print_info: n_layer          = 24
0.00.051.232 I print_info: n_head           = 16
0.00.051.232 I print_info: n_head_kv        = 16
0.00.051.233 I print_info: n_rot            = 32
0.00.051.233 I print_info: n_swa            = 0
0.00.051.233 I print_info: n_embd_head_k    = 128
0.00.051.233 I print_info: n_embd_head_v    = 128
0.00.051.234 I print_info: n_gqa            = 1
0.00.051.235 I print_info: n_embd_k_gqa     = 2048
0.00.051.235 I print_info: n_embd_v_gqa     = 2048
0.00.051.236 I print_info: f_norm_eps       = 1.0e-05
0.00.051.236 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.236 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.236 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.237 I print_info: f_logit_scale    = 0.0e+00
0.00.051.237 I print_info: n_ff             = 8192
0.00.051.237 I print_info: n_expert         = 0
0.00.051.237 I print_info: n_expert_used    = 0
0.00.051.238 I print_info: causal attn      = 1
0.00.051.238 I print_info: pooling type     = 0
0.00.051.238 I print_info: rope type        = 2
0.00.051.238 I print_info: rope scaling     = linear
0.00.051.238 I print_info: freq_base_train  = 10000.0
0.00.051.239 I print_info: freq_scale_train = 1
0.00.051.239 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.239 I print_info: rope_finetuned   = unknown
0.00.051.240 I print_info: ssm_d_conv       = 0
0.00.051.240 I print_info: ssm_d_inner      = 0
0.00.051.240 I print_info: ssm_d_state      = 0
0.00.051.241 I print_info: ssm_dt_rank      = 0
0.00.051.241 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.241 I print_info: model type       = 1.4B
0.00.051.241 I print_info: model params     = 1.41 B
0.00.051.241 I print_info: general.name     = 1.4B
0.00.051.242 I print_info: vocab type       = BPE
0.00.051.242 I print_info: n_vocab          = 50304
0.00.051.242 I print_info: n_merges         = 50009
0.00.051.243 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.243 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.243 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.245 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.245 I print_info: LF token         = 128 'Ä'
0.00.051.246 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.246 I print_info: max token length = 1024
0.00.053.131 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.131 I load_tensors: offloading output layer to GPU
0.00.053.132 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.142 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.144 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.423 I llama_init_from_model: n_seq_max     = 1
0.00.053.424 I llama_init_from_model: n_ctx         = 2048
0.00.053.424 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.424 I llama_init_from_model: n_batch       = 2048
0.00.053.425 I llama_init_from_model: n_ubatch      = 512
0.00.053.425 I llama_init_from_model: flash_attn    = 0
0.00.053.425 I llama_init_from_model: freq_base     = 10000.0
0.00.053.425 I llama_init_from_model: freq_scale    = 1
0.00.053.426 I ggml_metal_init: allocating
0.00.053.429 I ggml_metal_init: found device: Apple M4
0.00.053.431 I ggml_metal_init: picking default device: Apple M4
0.00.054.024 I ggml_metal_init: using embedded metal library
0.00.056.354 I ggml_metal_init: GPU name:   Apple M4
0.00.056.355 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.356 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.356 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.356 I ggml_metal_init: simdgroup reduction   = true
0.00.056.357 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.357 I ggml_metal_init: has bfloat            = true
0.00.056.357 I ggml_metal_init: use bfloat            = true
0.00.056.357 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.358 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.080 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.864 I init:      Metal KV buffer size =   384.00 MiB
0.00.084.869 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.886 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.976 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.977 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.978 I llama_init_from_model: graph nodes  = 967
0.00.085.978 I llama_init_from_model: graph splits = 2
0.00.085.981 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.098 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.099 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.432.524 I main: llama threadpool init, n_threads = 4
0.00.432.557 I 
0.00.432.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.432.583 I 
0.00.432.816 I sampler seed: 1234
0.00.432.823 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.432.865 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.432.868 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.432.868 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.095.590 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.01.095.591 I llama_perf_context_print:        load time =     422.68 ms
0.01.095.592 I llama_perf_context_print: prompt eval time =      35.54 ms /     7 tokens (    5.08 ms per token,   196.94 tokens per second)
0.01.095.593 I llama_perf_context_print:        eval time =     624.80 ms /    63 runs   (    9.92 ms per token,   100.83 tokens per second)
0.01.095.594 I llama_perf_context_print:       total time =     663.07 ms /    70 tokens
0.01.095.861 I ggml_metal_free: deallocating

real	0m1.115s
user	0m0.109s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.841 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.454 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.455 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.455 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.455 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.456 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.458 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.458 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.459 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.459 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.459 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.460 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.460 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.461 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.462 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.462 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.272 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.296 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.038 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.039 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.039 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.040 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.040 I llama_model_loader: - type  f32:  194 tensors
0.00.025.040 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.041 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.041 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.041 I print_info: file format = GGUF V3 (latest)
0.00.025.042 I print_info: file type   = Q2_K - Medium
0.00.025.042 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.617 I load: special tokens cache size = 25
0.00.049.666 I load: token to piece cache size = 0.2984 MB
0.00.049.669 I print_info: arch             = gptneox
0.00.049.669 I print_info: vocab_only       = 0
0.00.049.670 I print_info: n_ctx_train      = 2048
0.00.049.670 I print_info: n_embd           = 2048
0.00.049.670 I print_info: n_layer          = 24
0.00.049.673 I print_info: n_head           = 16
0.00.049.674 I print_info: n_head_kv        = 16
0.00.049.674 I print_info: n_rot            = 32
0.00.049.674 I print_info: n_swa            = 0
0.00.049.674 I print_info: n_embd_head_k    = 128
0.00.049.675 I print_info: n_embd_head_v    = 128
0.00.049.675 I print_info: n_gqa            = 1
0.00.049.676 I print_info: n_embd_k_gqa     = 2048
0.00.049.678 I print_info: n_embd_v_gqa     = 2048
0.00.049.679 I print_info: f_norm_eps       = 1.0e-05
0.00.049.679 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.681 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.681 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.681 I print_info: f_logit_scale    = 0.0e+00
0.00.049.682 I print_info: n_ff             = 8192
0.00.049.682 I print_info: n_expert         = 0
0.00.049.682 I print_info: n_expert_used    = 0
0.00.049.682 I print_info: causal attn      = 1
0.00.049.683 I print_info: pooling type     = 0
0.00.049.683 I print_info: rope type        = 2
0.00.049.683 I print_info: rope scaling     = linear
0.00.049.683 I print_info: freq_base_train  = 10000.0
0.00.049.684 I print_info: freq_scale_train = 1
0.00.049.684 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.684 I print_info: rope_finetuned   = unknown
0.00.049.684 I print_info: ssm_d_conv       = 0
0.00.049.684 I print_info: ssm_d_inner      = 0
0.00.049.684 I print_info: ssm_d_state      = 0
0.00.049.684 I print_info: ssm_dt_rank      = 0
0.00.049.685 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.685 I print_info: model type       = 1.4B
0.00.049.685 I print_info: model params     = 1.41 B
0.00.049.685 I print_info: general.name     = 1.4B
0.00.049.690 I print_info: vocab type       = BPE
0.00.049.690 I print_info: n_vocab          = 50304
0.00.049.690 I print_info: n_merges         = 50009
0.00.049.691 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.691 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.691 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.691 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.691 I print_info: LF token         = 128 'Ä'
0.00.049.692 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.692 I print_info: max token length = 1024
0.00.051.573 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.574 I load_tensors: offloading output layer to GPU
0.00.051.574 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.584 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.585 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.868 I llama_init_from_model: n_seq_max     = 1
0.00.051.869 I llama_init_from_model: n_ctx         = 128
0.00.051.869 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.869 I llama_init_from_model: n_batch       = 128
0.00.051.870 I llama_init_from_model: n_ubatch      = 128
0.00.051.870 I llama_init_from_model: flash_attn    = 0
0.00.051.870 I llama_init_from_model: freq_base     = 10000.0
0.00.051.870 I llama_init_from_model: freq_scale    = 1
0.00.051.871 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.871 I ggml_metal_init: allocating
0.00.051.875 I ggml_metal_init: found device: Apple M4
0.00.051.876 I ggml_metal_init: picking default device: Apple M4
0.00.052.444 I ggml_metal_init: using embedded metal library
0.00.054.747 I ggml_metal_init: GPU name:   Apple M4
0.00.054.748 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.749 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.749 I ggml_metal_init: simdgroup reduction   = true
0.00.054.749 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.749 I ggml_metal_init: has bfloat            = true
0.00.054.749 I ggml_metal_init: use bfloat            = true
0.00.054.750 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.237 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.621 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.623 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.637 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.563 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.564 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.564 I llama_init_from_model: graph nodes  = 967
0.00.066.564 I llama_init_from_model: graph splits = 2
0.00.066.566 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.566 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.372.639 I 
0.00.372.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.372.684 I perplexity: tokenizing the input ..
0.00.380.249 I perplexity: tokenization took 7.561 ms
0.00.380.252 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.513.039 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.514.317 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.514.354 I llama_perf_context_print:        load time =     362.79 ms
0.00.514.355 I llama_perf_context_print: prompt eval time =     132.56 ms /   128 tokens (    1.04 ms per token,   965.58 tokens per second)
0.00.514.356 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.514.356 I llama_perf_context_print:       total time =     141.72 ms /   129 tokens
0.00.514.882 I ggml_metal_free: deallocating

real	0m0.530s
user	0m0.076s
sys	0m0.065s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.870 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.388 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.401 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.402 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.403 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.403 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.403 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.403 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.405 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.405 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.405 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.405 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.406 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.406 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.406 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.408 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.408 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.409 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.345 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.291 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.292 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.292 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.293 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.293 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.294 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.294 I llama_model_loader: - type  f32:  194 tensors
0.00.026.295 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.295 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.295 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.295 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.296 I print_info: file format = GGUF V3 (latest)
0.00.026.297 I print_info: file type   = Q3_K - Medium
0.00.026.298 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.046.225 I load: special tokens cache size = 25
0.00.052.526 I load: token to piece cache size = 0.2984 MB
0.00.052.534 I print_info: arch             = gptneox
0.00.052.535 I print_info: vocab_only       = 0
0.00.052.535 I print_info: n_ctx_train      = 2048
0.00.052.535 I print_info: n_embd           = 2048
0.00.052.535 I print_info: n_layer          = 24
0.00.052.539 I print_info: n_head           = 16
0.00.052.539 I print_info: n_head_kv        = 16
0.00.052.540 I print_info: n_rot            = 32
0.00.052.540 I print_info: n_swa            = 0
0.00.052.540 I print_info: n_embd_head_k    = 128
0.00.052.540 I print_info: n_embd_head_v    = 128
0.00.052.541 I print_info: n_gqa            = 1
0.00.052.541 I print_info: n_embd_k_gqa     = 2048
0.00.052.542 I print_info: n_embd_v_gqa     = 2048
0.00.052.542 I print_info: f_norm_eps       = 1.0e-05
0.00.052.543 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.544 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.544 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.544 I print_info: f_logit_scale    = 0.0e+00
0.00.052.544 I print_info: n_ff             = 8192
0.00.052.545 I print_info: n_expert         = 0
0.00.052.545 I print_info: n_expert_used    = 0
0.00.052.545 I print_info: causal attn      = 1
0.00.052.545 I print_info: pooling type     = 0
0.00.052.545 I print_info: rope type        = 2
0.00.052.545 I print_info: rope scaling     = linear
0.00.052.546 I print_info: freq_base_train  = 10000.0
0.00.052.546 I print_info: freq_scale_train = 1
0.00.052.546 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.546 I print_info: rope_finetuned   = unknown
0.00.052.546 I print_info: ssm_d_conv       = 0
0.00.052.546 I print_info: ssm_d_inner      = 0
0.00.052.547 I print_info: ssm_d_state      = 0
0.00.052.547 I print_info: ssm_dt_rank      = 0
0.00.052.547 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.547 I print_info: model type       = 1.4B
0.00.052.547 I print_info: model params     = 1.41 B
0.00.052.547 I print_info: general.name     = 1.4B
0.00.052.548 I print_info: vocab type       = BPE
0.00.052.548 I print_info: n_vocab          = 50304
0.00.052.548 I print_info: n_merges         = 50009
0.00.052.549 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.549 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.549 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.549 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.549 I print_info: LF token         = 128 'Ä'
0.00.052.550 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.550 I print_info: max token length = 1024
0.00.054.647 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.648 I load_tensors: offloading output layer to GPU
0.00.054.652 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.663 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.664 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.055.071 I llama_init_from_model: n_seq_max     = 1
0.00.055.072 I llama_init_from_model: n_ctx         = 2048
0.00.055.072 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.072 I llama_init_from_model: n_batch       = 2048
0.00.055.072 I llama_init_from_model: n_ubatch      = 512
0.00.055.073 I llama_init_from_model: flash_attn    = 0
0.00.055.073 I llama_init_from_model: freq_base     = 10000.0
0.00.055.073 I llama_init_from_model: freq_scale    = 1
0.00.055.074 I ggml_metal_init: allocating
0.00.055.077 I ggml_metal_init: found device: Apple M4
0.00.055.079 I ggml_metal_init: picking default device: Apple M4
0.00.055.721 I ggml_metal_init: using embedded metal library
0.00.059.112 I ggml_metal_init: GPU name:   Apple M4
0.00.059.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.114 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.116 I ggml_metal_init: simdgroup reduction   = true
0.00.059.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.116 I ggml_metal_init: has bfloat            = true
0.00.059.116 I ggml_metal_init: use bfloat            = true
0.00.059.116 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.326 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.492 I init:      Metal KV buffer size =   384.00 MiB
0.00.087.507 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.527 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.487 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.488 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.488 I llama_init_from_model: graph nodes  = 967
0.00.088.489 I llama_init_from_model: graph splits = 2
0.00.088.492 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.621 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.574 I main: llama threadpool init, n_threads = 4
0.00.545.613 I 
0.00.545.638 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.639 I 
0.00.545.854 I sampler seed: 1234
0.00.545.858 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.545.894 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.545.896 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.545.896 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.292.647 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.01.292.647 I llama_perf_context_print:        load time =     536.70 ms
0.01.292.648 I llama_perf_context_print: prompt eval time =      44.35 ms /     7 tokens (    6.34 ms per token,   157.82 tokens per second)
0.01.292.649 I llama_perf_context_print:        eval time =     699.45 ms /    63 runs   (   11.10 ms per token,    90.07 tokens per second)
0.01.292.651 I llama_perf_context_print:       total time =     747.07 ms /    70 tokens
0.01.292.899 I ggml_metal_free: deallocating

real	0m1.310s
user	0m0.110s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.907 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.564 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.570 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.572 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.572 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.573 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.574 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.578 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.578 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.579 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.581 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.368 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.208 I llama_model_loader: - type  f32:  194 tensors
0.00.024.209 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.209 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.209 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.209 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.210 I print_info: file format = GGUF V3 (latest)
0.00.024.211 I print_info: file type   = Q3_K - Medium
0.00.024.211 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.169 I load: special tokens cache size = 25
0.00.049.109 I load: token to piece cache size = 0.2984 MB
0.00.049.112 I print_info: arch             = gptneox
0.00.049.112 I print_info: vocab_only       = 0
0.00.049.113 I print_info: n_ctx_train      = 2048
0.00.049.113 I print_info: n_embd           = 2048
0.00.049.113 I print_info: n_layer          = 24
0.00.049.116 I print_info: n_head           = 16
0.00.049.116 I print_info: n_head_kv        = 16
0.00.049.117 I print_info: n_rot            = 32
0.00.049.117 I print_info: n_swa            = 0
0.00.049.117 I print_info: n_embd_head_k    = 128
0.00.049.119 I print_info: n_embd_head_v    = 128
0.00.049.120 I print_info: n_gqa            = 1
0.00.049.121 I print_info: n_embd_k_gqa     = 2048
0.00.049.121 I print_info: n_embd_v_gqa     = 2048
0.00.049.122 I print_info: f_norm_eps       = 1.0e-05
0.00.049.122 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.122 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.123 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.123 I print_info: f_logit_scale    = 0.0e+00
0.00.049.123 I print_info: n_ff             = 8192
0.00.049.124 I print_info: n_expert         = 0
0.00.049.124 I print_info: n_expert_used    = 0
0.00.049.124 I print_info: causal attn      = 1
0.00.049.124 I print_info: pooling type     = 0
0.00.049.125 I print_info: rope type        = 2
0.00.049.126 I print_info: rope scaling     = linear
0.00.049.127 I print_info: freq_base_train  = 10000.0
0.00.049.127 I print_info: freq_scale_train = 1
0.00.049.127 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.128 I print_info: rope_finetuned   = unknown
0.00.049.128 I print_info: ssm_d_conv       = 0
0.00.049.129 I print_info: ssm_d_inner      = 0
0.00.049.129 I print_info: ssm_d_state      = 0
0.00.049.129 I print_info: ssm_dt_rank      = 0
0.00.049.129 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.130 I print_info: model type       = 1.4B
0.00.049.130 I print_info: model params     = 1.41 B
0.00.049.130 I print_info: general.name     = 1.4B
0.00.049.131 I print_info: vocab type       = BPE
0.00.049.131 I print_info: n_vocab          = 50304
0.00.049.132 I print_info: n_merges         = 50009
0.00.049.133 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.133 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.133 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.133 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.133 I print_info: LF token         = 128 'Ä'
0.00.049.134 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.134 I print_info: max token length = 1024
0.00.050.913 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.914 I load_tensors: offloading output layer to GPU
0.00.050.914 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.919 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.920 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.340 I llama_init_from_model: n_seq_max     = 1
0.00.051.340 I llama_init_from_model: n_ctx         = 128
0.00.051.340 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.341 I llama_init_from_model: n_batch       = 128
0.00.051.341 I llama_init_from_model: n_ubatch      = 128
0.00.051.341 I llama_init_from_model: flash_attn    = 0
0.00.051.341 I llama_init_from_model: freq_base     = 10000.0
0.00.051.341 I llama_init_from_model: freq_scale    = 1
0.00.051.342 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.342 I ggml_metal_init: allocating
0.00.051.345 I ggml_metal_init: found device: Apple M4
0.00.051.347 I ggml_metal_init: picking default device: Apple M4
0.00.051.898 I ggml_metal_init: using embedded metal library
0.00.054.225 I ggml_metal_init: GPU name:   Apple M4
0.00.054.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.227 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.227 I ggml_metal_init: simdgroup reduction   = true
0.00.054.227 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.227 I ggml_metal_init: has bfloat            = true
0.00.054.228 I ggml_metal_init: use bfloat            = true
0.00.054.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.866 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.124 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.126 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.140 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.988 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.989 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.990 I llama_init_from_model: graph nodes  = 967
0.00.065.990 I llama_init_from_model: graph splits = 2
0.00.065.991 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.992 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.148 I 
0.00.474.183 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.196 I perplexity: tokenizing the input ..
0.00.482.347 I perplexity: tokenization took 8.149 ms
0.00.482.351 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.614.632 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.615.802 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.615.834 I llama_perf_context_print:        load time =     465.24 ms
0.00.615.835 I llama_perf_context_print: prompt eval time =     132.05 ms /   128 tokens (    1.03 ms per token,   969.30 tokens per second)
0.00.615.836 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.837 I llama_perf_context_print:       total time =     141.69 ms /   129 tokens
0.00.616.327 I ggml_metal_free: deallocating

real	0m0.630s
user	0m0.078s
sys	0m0.084s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.530 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.817 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.822 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.825 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.825 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.825 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.826 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.830 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.831 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.627 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.672 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.454 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.454 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.454 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.455 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.455 I llama_model_loader: - type  f32:  194 tensors
0.00.027.456 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.456 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.456 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.457 I print_info: file format = GGUF V3 (latest)
0.00.027.457 I print_info: file type   = Q4_K - Medium
0.00.027.458 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.263 I load: special tokens cache size = 25
0.00.052.237 I load: token to piece cache size = 0.2984 MB
0.00.052.239 I print_info: arch             = gptneox
0.00.052.240 I print_info: vocab_only       = 0
0.00.052.240 I print_info: n_ctx_train      = 2048
0.00.052.240 I print_info: n_embd           = 2048
0.00.052.240 I print_info: n_layer          = 24
0.00.052.243 I print_info: n_head           = 16
0.00.052.244 I print_info: n_head_kv        = 16
0.00.052.244 I print_info: n_rot            = 32
0.00.052.245 I print_info: n_swa            = 0
0.00.052.245 I print_info: n_embd_head_k    = 128
0.00.052.245 I print_info: n_embd_head_v    = 128
0.00.052.246 I print_info: n_gqa            = 1
0.00.052.246 I print_info: n_embd_k_gqa     = 2048
0.00.052.247 I print_info: n_embd_v_gqa     = 2048
0.00.052.248 I print_info: f_norm_eps       = 1.0e-05
0.00.052.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.248 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.249 I print_info: f_logit_scale    = 0.0e+00
0.00.052.249 I print_info: n_ff             = 8192
0.00.052.249 I print_info: n_expert         = 0
0.00.052.251 I print_info: n_expert_used    = 0
0.00.052.253 I print_info: causal attn      = 1
0.00.052.253 I print_info: pooling type     = 0
0.00.052.253 I print_info: rope type        = 2
0.00.052.253 I print_info: rope scaling     = linear
0.00.052.254 I print_info: freq_base_train  = 10000.0
0.00.052.254 I print_info: freq_scale_train = 1
0.00.052.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.255 I print_info: rope_finetuned   = unknown
0.00.052.255 I print_info: ssm_d_conv       = 0
0.00.052.255 I print_info: ssm_d_inner      = 0
0.00.052.255 I print_info: ssm_d_state      = 0
0.00.052.255 I print_info: ssm_dt_rank      = 0
0.00.052.255 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.255 I print_info: model type       = 1.4B
0.00.052.257 I print_info: model params     = 1.41 B
0.00.052.257 I print_info: general.name     = 1.4B
0.00.052.258 I print_info: vocab type       = BPE
0.00.052.258 I print_info: n_vocab          = 50304
0.00.052.258 I print_info: n_merges         = 50009
0.00.052.259 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.259 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.259 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.259 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.259 I print_info: LF token         = 128 'Ä'
0.00.052.263 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.263 I print_info: max token length = 1024
0.00.054.022 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.022 I load_tensors: offloading output layer to GPU
0.00.054.023 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.028 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.028 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.304 I llama_init_from_model: n_seq_max     = 1
0.00.054.305 I llama_init_from_model: n_ctx         = 2048
0.00.054.305 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.305 I llama_init_from_model: n_batch       = 2048
0.00.054.305 I llama_init_from_model: n_ubatch      = 512
0.00.054.305 I llama_init_from_model: flash_attn    = 0
0.00.054.306 I llama_init_from_model: freq_base     = 10000.0
0.00.054.306 I llama_init_from_model: freq_scale    = 1
0.00.054.307 I ggml_metal_init: allocating
0.00.054.310 I ggml_metal_init: found device: Apple M4
0.00.054.312 I ggml_metal_init: picking default device: Apple M4
0.00.054.930 I ggml_metal_init: using embedded metal library
0.00.057.254 I ggml_metal_init: GPU name:   Apple M4
0.00.057.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.256 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.257 I ggml_metal_init: simdgroup reduction   = true
0.00.057.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.257 I ggml_metal_init: has bfloat            = true
0.00.057.257 I ggml_metal_init: use bfloat            = true
0.00.057.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.848 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.815 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.821 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.840 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.889 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.890 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.891 I llama_init_from_model: graph nodes  = 967
0.00.086.891 I llama_init_from_model: graph splits = 2
0.00.086.893 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.029 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.134 I main: llama threadpool init, n_threads = 4
0.00.611.177 I 
0.00.611.197 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.197 I 
0.00.611.418 I sampler seed: 1234
0.00.611.423 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.611.448 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.611.449 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.611.449 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.095 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.376.095 I llama_perf_context_print:        load time =     599.60 ms
0.01.376.096 I llama_perf_context_print: prompt eval time =      51.05 ms /     7 tokens (    7.29 ms per token,   137.13 tokens per second)
0.01.376.097 I llama_perf_context_print:        eval time =     710.56 ms /    63 runs   (   11.28 ms per token,    88.66 tokens per second)
0.01.376.097 I llama_perf_context_print:       total time =     764.97 ms /    70 tokens
0.01.376.281 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.109s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.884 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.020 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.020 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.021 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.022 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.022 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.023 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.023 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.023 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.024 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.024 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.027 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.027 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.028 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.702 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.702 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.703 I llama_model_loader: - type  f32:  194 tensors
0.00.024.703 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.703 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.704 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.704 I print_info: file format = GGUF V3 (latest)
0.00.024.705 I print_info: file type   = Q4_K - Medium
0.00.024.706 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.375 I load: special tokens cache size = 25
0.00.050.380 I load: token to piece cache size = 0.2984 MB
0.00.050.383 I print_info: arch             = gptneox
0.00.050.383 I print_info: vocab_only       = 0
0.00.050.383 I print_info: n_ctx_train      = 2048
0.00.050.383 I print_info: n_embd           = 2048
0.00.050.383 I print_info: n_layer          = 24
0.00.050.387 I print_info: n_head           = 16
0.00.050.387 I print_info: n_head_kv        = 16
0.00.050.388 I print_info: n_rot            = 32
0.00.050.388 I print_info: n_swa            = 0
0.00.050.388 I print_info: n_embd_head_k    = 128
0.00.050.388 I print_info: n_embd_head_v    = 128
0.00.050.389 I print_info: n_gqa            = 1
0.00.050.390 I print_info: n_embd_k_gqa     = 2048
0.00.050.390 I print_info: n_embd_v_gqa     = 2048
0.00.050.391 I print_info: f_norm_eps       = 1.0e-05
0.00.050.391 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.392 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.392 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.392 I print_info: f_logit_scale    = 0.0e+00
0.00.050.392 I print_info: n_ff             = 8192
0.00.050.393 I print_info: n_expert         = 0
0.00.050.393 I print_info: n_expert_used    = 0
0.00.050.393 I print_info: causal attn      = 1
0.00.050.393 I print_info: pooling type     = 0
0.00.050.393 I print_info: rope type        = 2
0.00.050.393 I print_info: rope scaling     = linear
0.00.050.397 I print_info: freq_base_train  = 10000.0
0.00.050.397 I print_info: freq_scale_train = 1
0.00.050.397 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.397 I print_info: rope_finetuned   = unknown
0.00.050.397 I print_info: ssm_d_conv       = 0
0.00.050.397 I print_info: ssm_d_inner      = 0
0.00.050.398 I print_info: ssm_d_state      = 0
0.00.050.398 I print_info: ssm_dt_rank      = 0
0.00.050.398 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.398 I print_info: model type       = 1.4B
0.00.050.398 I print_info: model params     = 1.41 B
0.00.050.399 I print_info: general.name     = 1.4B
0.00.050.399 I print_info: vocab type       = BPE
0.00.050.399 I print_info: n_vocab          = 50304
0.00.050.400 I print_info: n_merges         = 50009
0.00.050.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.404 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.405 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.406 I print_info: LF token         = 128 'Ä'
0.00.050.406 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.406 I print_info: max token length = 1024
0.00.052.368 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.368 I load_tensors: offloading output layer to GPU
0.00.052.368 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.378 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.379 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.660 I llama_init_from_model: n_seq_max     = 1
0.00.052.661 I llama_init_from_model: n_ctx         = 128
0.00.052.661 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.662 I llama_init_from_model: n_batch       = 128
0.00.052.662 I llama_init_from_model: n_ubatch      = 128
0.00.052.662 I llama_init_from_model: flash_attn    = 0
0.00.052.662 I llama_init_from_model: freq_base     = 10000.0
0.00.052.663 I llama_init_from_model: freq_scale    = 1
0.00.052.663 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.664 I ggml_metal_init: allocating
0.00.052.667 I ggml_metal_init: found device: Apple M4
0.00.052.669 I ggml_metal_init: picking default device: Apple M4
0.00.053.274 I ggml_metal_init: using embedded metal library
0.00.055.624 I ggml_metal_init: GPU name:   Apple M4
0.00.055.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.627 I ggml_metal_init: simdgroup reduction   = true
0.00.055.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.627 I ggml_metal_init: has bfloat            = true
0.00.055.628 I ggml_metal_init: use bfloat            = true
0.00.055.628 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.629 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.414 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.709 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.711 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.737 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.683 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.684 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.684 I llama_init_from_model: graph nodes  = 967
0.00.067.684 I llama_init_from_model: graph splits = 2
0.00.067.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.546.757 I 
0.00.546.794 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.546.814 I perplexity: tokenizing the input ..
0.00.555.093 I perplexity: tokenization took 8.276 ms
0.00.555.096 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.689.504 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.690.756 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.690.793 I llama_perf_context_print:        load time =     537.87 ms
0.00.690.793 I llama_perf_context_print: prompt eval time =     134.18 ms /   128 tokens (    1.05 ms per token,   953.92 tokens per second)
0.00.690.794 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.690.795 I llama_perf_context_print:       total time =     144.04 ms /   129 tokens
0.00.691.331 I ggml_metal_free: deallocating

real	0m0.705s
user	0m0.079s
sys	0m0.095s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.829 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.590 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.595 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.597 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.598 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.599 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.599 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.600 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.600 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.600 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.601 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.603 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.603 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.232 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.233 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.233 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.234 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.235 I llama_model_loader: - type  f32:  194 tensors
0.00.024.235 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.235 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.236 I print_info: file format = GGUF V3 (latest)
0.00.024.236 I print_info: file type   = Q5_K - Medium
0.00.024.237 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.875 I load: special tokens cache size = 25
0.00.049.837 I load: token to piece cache size = 0.2984 MB
0.00.049.840 I print_info: arch             = gptneox
0.00.049.840 I print_info: vocab_only       = 0
0.00.049.840 I print_info: n_ctx_train      = 2048
0.00.049.840 I print_info: n_embd           = 2048
0.00.049.841 I print_info: n_layer          = 24
0.00.049.843 I print_info: n_head           = 16
0.00.049.844 I print_info: n_head_kv        = 16
0.00.049.844 I print_info: n_rot            = 32
0.00.049.847 I print_info: n_swa            = 0
0.00.049.847 I print_info: n_embd_head_k    = 128
0.00.049.847 I print_info: n_embd_head_v    = 128
0.00.049.848 I print_info: n_gqa            = 1
0.00.049.849 I print_info: n_embd_k_gqa     = 2048
0.00.049.849 I print_info: n_embd_v_gqa     = 2048
0.00.049.850 I print_info: f_norm_eps       = 1.0e-05
0.00.049.850 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.850 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.850 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.851 I print_info: f_logit_scale    = 0.0e+00
0.00.049.851 I print_info: n_ff             = 8192
0.00.049.851 I print_info: n_expert         = 0
0.00.049.852 I print_info: n_expert_used    = 0
0.00.049.852 I print_info: causal attn      = 1
0.00.049.852 I print_info: pooling type     = 0
0.00.049.852 I print_info: rope type        = 2
0.00.049.858 I print_info: rope scaling     = linear
0.00.049.860 I print_info: freq_base_train  = 10000.0
0.00.049.861 I print_info: freq_scale_train = 1
0.00.049.861 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.862 I print_info: rope_finetuned   = unknown
0.00.049.862 I print_info: ssm_d_conv       = 0
0.00.049.862 I print_info: ssm_d_inner      = 0
0.00.049.862 I print_info: ssm_d_state      = 0
0.00.049.863 I print_info: ssm_dt_rank      = 0
0.00.049.863 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.863 I print_info: model type       = 1.4B
0.00.049.863 I print_info: model params     = 1.41 B
0.00.049.863 I print_info: general.name     = 1.4B
0.00.049.864 I print_info: vocab type       = BPE
0.00.049.864 I print_info: n_vocab          = 50304
0.00.049.864 I print_info: n_merges         = 50009
0.00.049.865 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.865 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.865 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.865 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.865 I print_info: LF token         = 128 'Ä'
0.00.049.866 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.866 I print_info: max token length = 1024
0.00.051.841 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.841 I load_tensors: offloading output layer to GPU
0.00.051.842 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.852 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.853 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.135 I llama_init_from_model: n_seq_max     = 1
0.00.052.135 I llama_init_from_model: n_ctx         = 2048
0.00.052.135 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.136 I llama_init_from_model: n_batch       = 2048
0.00.052.136 I llama_init_from_model: n_ubatch      = 512
0.00.052.136 I llama_init_from_model: flash_attn    = 0
0.00.052.136 I llama_init_from_model: freq_base     = 10000.0
0.00.052.136 I llama_init_from_model: freq_scale    = 1
0.00.052.137 I ggml_metal_init: allocating
0.00.052.143 I ggml_metal_init: found device: Apple M4
0.00.052.145 I ggml_metal_init: picking default device: Apple M4
0.00.052.713 I ggml_metal_init: using embedded metal library
0.00.055.086 I ggml_metal_init: GPU name:   Apple M4
0.00.055.087 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.087 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.088 I ggml_metal_init: simdgroup reduction   = true
0.00.055.088 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.088 I ggml_metal_init: has bfloat            = true
0.00.055.088 I ggml_metal_init: use bfloat            = true
0.00.055.089 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.572 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.492 I init:      Metal KV buffer size =   384.00 MiB
0.00.083.501 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.520 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.543 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.544 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.544 I llama_init_from_model: graph nodes  = 967
0.00.084.544 I llama_init_from_model: graph splits = 2
0.00.084.547 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.676 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.677 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.995 I main: llama threadpool init, n_threads = 4
0.00.691.041 I 
0.00.691.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.092 I 
0.00.691.330 I sampler seed: 1234
0.00.691.335 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.691.376 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.376 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.376 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.541.274 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.541.275 I llama_perf_context_print:        load time =     682.16 ms
0.01.541.276 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.60 tokens per second)
0.01.541.276 I llama_perf_context_print:        eval time =     795.31 ms /    63 runs   (   12.62 ms per token,    79.21 tokens per second)
0.01.541.277 I llama_perf_context_print:       total time =     850.29 ms /    70 tokens
0.01.541.471 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.109s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.008 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.646 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.653 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.653 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.654 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.654 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.654 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.655 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.656 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.660 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.357 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.092 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.093 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.094 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.094 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.094 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.095 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.095 I llama_model_loader: - type  f32:  194 tensors
0.00.025.095 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.096 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.096 I print_info: file format = GGUF V3 (latest)
0.00.025.097 I print_info: file type   = Q5_K - Medium
0.00.025.097 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.526 I load: special tokens cache size = 25
0.00.049.535 I load: token to piece cache size = 0.2984 MB
0.00.049.538 I print_info: arch             = gptneox
0.00.049.538 I print_info: vocab_only       = 0
0.00.049.538 I print_info: n_ctx_train      = 2048
0.00.049.539 I print_info: n_embd           = 2048
0.00.049.539 I print_info: n_layer          = 24
0.00.049.542 I print_info: n_head           = 16
0.00.049.543 I print_info: n_head_kv        = 16
0.00.049.543 I print_info: n_rot            = 32
0.00.049.543 I print_info: n_swa            = 0
0.00.049.543 I print_info: n_embd_head_k    = 128
0.00.049.543 I print_info: n_embd_head_v    = 128
0.00.049.544 I print_info: n_gqa            = 1
0.00.049.545 I print_info: n_embd_k_gqa     = 2048
0.00.049.545 I print_info: n_embd_v_gqa     = 2048
0.00.049.546 I print_info: f_norm_eps       = 1.0e-05
0.00.049.546 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.546 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.547 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.547 I print_info: f_logit_scale    = 0.0e+00
0.00.049.548 I print_info: n_ff             = 8192
0.00.049.548 I print_info: n_expert         = 0
0.00.049.548 I print_info: n_expert_used    = 0
0.00.049.548 I print_info: causal attn      = 1
0.00.049.548 I print_info: pooling type     = 0
0.00.049.548 I print_info: rope type        = 2
0.00.049.548 I print_info: rope scaling     = linear
0.00.049.552 I print_info: freq_base_train  = 10000.0
0.00.049.553 I print_info: freq_scale_train = 1
0.00.049.554 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.554 I print_info: rope_finetuned   = unknown
0.00.049.554 I print_info: ssm_d_conv       = 0
0.00.049.554 I print_info: ssm_d_inner      = 0
0.00.049.554 I print_info: ssm_d_state      = 0
0.00.049.554 I print_info: ssm_dt_rank      = 0
0.00.049.554 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.555 I print_info: model type       = 1.4B
0.00.049.555 I print_info: model params     = 1.41 B
0.00.049.555 I print_info: general.name     = 1.4B
0.00.049.556 I print_info: vocab type       = BPE
0.00.049.556 I print_info: n_vocab          = 50304
0.00.049.556 I print_info: n_merges         = 50009
0.00.049.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.561 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.561 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.562 I print_info: LF token         = 128 'Ä'
0.00.049.563 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.564 I print_info: max token length = 1024
0.00.051.340 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.341 I load_tensors: offloading output layer to GPU
0.00.051.341 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.346 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.347 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.791 I llama_init_from_model: n_seq_max     = 1
0.00.051.792 I llama_init_from_model: n_ctx         = 128
0.00.051.792 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.792 I llama_init_from_model: n_batch       = 128
0.00.051.792 I llama_init_from_model: n_ubatch      = 128
0.00.051.792 I llama_init_from_model: flash_attn    = 0
0.00.051.793 I llama_init_from_model: freq_base     = 10000.0
0.00.051.793 I llama_init_from_model: freq_scale    = 1
0.00.051.793 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.794 I ggml_metal_init: allocating
0.00.051.797 I ggml_metal_init: found device: Apple M4
0.00.051.798 I ggml_metal_init: picking default device: Apple M4
0.00.052.358 I ggml_metal_init: using embedded metal library
0.00.054.674 I ggml_metal_init: GPU name:   Apple M4
0.00.054.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.675 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.676 I ggml_metal_init: simdgroup reduction   = true
0.00.054.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.676 I ggml_metal_init: has bfloat            = true
0.00.054.676 I ggml_metal_init: use bfloat            = true
0.00.054.677 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.252 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.593 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.596 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.610 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.534 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.535 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.535 I llama_init_from_model: graph nodes  = 967
0.00.066.536 I llama_init_from_model: graph splits = 2
0.00.066.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.537 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.949 I 
0.00.645.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.040 I perplexity: tokenizing the input ..
0.00.653.268 I perplexity: tokenization took 8.226 ms
0.00.653.271 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.973 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.795.136 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.795.168 I llama_perf_context_print:        load time =     634.94 ms
0.00.795.169 I llama_perf_context_print: prompt eval time =     140.47 ms /   128 tokens (    1.10 ms per token,   911.20 tokens per second)
0.00.795.170 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.170 I llama_perf_context_print:       total time =     150.22 ms /   129 tokens
0.00.795.713 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.077s
sys	0m0.123s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.962 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.587 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.588 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.588 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.590 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.592 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.592 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.593 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.433 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.243 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.243 I llama_model_loader: - type  f32:  194 tensors
0.00.026.244 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.244 I print_info: file format = GGUF V3 (latest)
0.00.026.245 I print_info: file type   = Q6_K
0.00.026.246 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.013 I load: special tokens cache size = 25
0.00.050.779 I load: token to piece cache size = 0.2984 MB
0.00.050.781 I print_info: arch             = gptneox
0.00.050.781 I print_info: vocab_only       = 0
0.00.050.782 I print_info: n_ctx_train      = 2048
0.00.050.782 I print_info: n_embd           = 2048
0.00.050.782 I print_info: n_layer          = 24
0.00.050.785 I print_info: n_head           = 16
0.00.050.786 I print_info: n_head_kv        = 16
0.00.050.786 I print_info: n_rot            = 32
0.00.050.786 I print_info: n_swa            = 0
0.00.050.786 I print_info: n_embd_head_k    = 128
0.00.050.787 I print_info: n_embd_head_v    = 128
0.00.050.787 I print_info: n_gqa            = 1
0.00.050.788 I print_info: n_embd_k_gqa     = 2048
0.00.050.789 I print_info: n_embd_v_gqa     = 2048
0.00.050.789 I print_info: f_norm_eps       = 1.0e-05
0.00.050.790 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.790 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.790 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.790 I print_info: f_logit_scale    = 0.0e+00
0.00.050.791 I print_info: n_ff             = 8192
0.00.050.791 I print_info: n_expert         = 0
0.00.050.791 I print_info: n_expert_used    = 0
0.00.050.791 I print_info: causal attn      = 1
0.00.050.793 I print_info: pooling type     = 0
0.00.050.795 I print_info: rope type        = 2
0.00.050.795 I print_info: rope scaling     = linear
0.00.050.795 I print_info: freq_base_train  = 10000.0
0.00.050.796 I print_info: freq_scale_train = 1
0.00.050.796 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.796 I print_info: rope_finetuned   = unknown
0.00.050.796 I print_info: ssm_d_conv       = 0
0.00.050.796 I print_info: ssm_d_inner      = 0
0.00.050.797 I print_info: ssm_d_state      = 0
0.00.050.797 I print_info: ssm_dt_rank      = 0
0.00.050.797 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.797 I print_info: model type       = 1.4B
0.00.050.799 I print_info: model params     = 1.41 B
0.00.050.799 I print_info: general.name     = 1.4B
0.00.050.800 I print_info: vocab type       = BPE
0.00.050.800 I print_info: n_vocab          = 50304
0.00.050.800 I print_info: n_merges         = 50009
0.00.050.800 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.801 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.801 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.801 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.801 I print_info: LF token         = 128 'Ä'
0.00.050.801 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.805 I print_info: max token length = 1024
0.00.052.804 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.805 I load_tensors: offloading output layer to GPU
0.00.052.805 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.815 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.817 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.107 I llama_init_from_model: n_seq_max     = 1
0.00.053.108 I llama_init_from_model: n_ctx         = 2048
0.00.053.108 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.108 I llama_init_from_model: n_batch       = 2048
0.00.053.108 I llama_init_from_model: n_ubatch      = 512
0.00.053.109 I llama_init_from_model: flash_attn    = 0
0.00.053.109 I llama_init_from_model: freq_base     = 10000.0
0.00.053.109 I llama_init_from_model: freq_scale    = 1
0.00.053.110 I ggml_metal_init: allocating
0.00.053.113 I ggml_metal_init: found device: Apple M4
0.00.053.114 I ggml_metal_init: picking default device: Apple M4
0.00.053.717 I ggml_metal_init: using embedded metal library
0.00.056.059 I ggml_metal_init: GPU name:   Apple M4
0.00.056.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.060 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.061 I ggml_metal_init: simdgroup reduction   = true
0.00.056.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.061 I ggml_metal_init: has bfloat            = true
0.00.056.061 I ggml_metal_init: use bfloat            = true
0.00.056.062 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.062 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.810 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.579 I init:      Metal KV buffer size =   384.00 MiB
0.00.084.584 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.601 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.693 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.694 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.694 I llama_init_from_model: graph nodes  = 967
0.00.085.695 I llama_init_from_model: graph splits = 2
0.00.085.697 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.826 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.826 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.150 I main: llama threadpool init, n_threads = 4
0.00.743.188 I 
0.00.743.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.206 I 
0.00.743.434 I sampler seed: 1234
0.00.743.439 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.449 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.451 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.451 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.621.066 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.621.067 I llama_perf_context_print:        load time =     733.19 ms
0.01.621.068 I llama_perf_context_print: prompt eval time =      58.42 ms /     7 tokens (    8.35 ms per token,   119.83 tokens per second)
0.01.621.069 I llama_perf_context_print:        eval time =     816.30 ms /    63 runs   (   12.96 ms per token,    77.18 tokens per second)
0.01.621.069 I llama_perf_context_print:       total time =     877.92 ms /    70 tokens
0.01.621.295 I ggml_metal_free: deallocating

real	0m1.639s
user	0m0.108s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4472 (bc40c6b9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.912 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.766 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.770 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.773 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.775 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.778 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.778 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.779 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.779 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.782 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.783 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.783 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.689 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.553 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.554 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.554 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.555 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.555 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.555 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.556 I llama_model_loader: - type  f32:  194 tensors
0.00.024.556 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.557 I print_info: file format = GGUF V3 (latest)
0.00.024.557 I print_info: file type   = Q6_K
0.00.024.557 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.220 I load: special tokens cache size = 25
0.00.050.132 I load: token to piece cache size = 0.2984 MB
0.00.050.135 I print_info: arch             = gptneox
0.00.050.135 I print_info: vocab_only       = 0
0.00.050.135 I print_info: n_ctx_train      = 2048
0.00.050.136 I print_info: n_embd           = 2048
0.00.050.136 I print_info: n_layer          = 24
0.00.050.139 I print_info: n_head           = 16
0.00.050.140 I print_info: n_head_kv        = 16
0.00.050.140 I print_info: n_rot            = 32
0.00.050.140 I print_info: n_swa            = 0
0.00.050.140 I print_info: n_embd_head_k    = 128
0.00.050.140 I print_info: n_embd_head_v    = 128
0.00.050.141 I print_info: n_gqa            = 1
0.00.050.142 I print_info: n_embd_k_gqa     = 2048
0.00.050.143 I print_info: n_embd_v_gqa     = 2048
0.00.050.143 I print_info: f_norm_eps       = 1.0e-05
0.00.050.144 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.144 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.145 I print_info: f_logit_scale    = 0.0e+00
0.00.050.146 I print_info: n_ff             = 8192
0.00.050.146 I print_info: n_expert         = 0
0.00.050.146 I print_info: n_expert_used    = 0
0.00.050.146 I print_info: causal attn      = 1
0.00.050.146 I print_info: pooling type     = 0
0.00.050.146 I print_info: rope type        = 2
0.00.050.147 I print_info: rope scaling     = linear
0.00.050.147 I print_info: freq_base_train  = 10000.0
0.00.050.149 I print_info: freq_scale_train = 1
0.00.050.151 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.151 I print_info: rope_finetuned   = unknown
0.00.050.151 I print_info: ssm_d_conv       = 0
0.00.050.152 I print_info: ssm_d_inner      = 0
0.00.050.152 I print_info: ssm_d_state      = 0
0.00.050.152 I print_info: ssm_dt_rank      = 0
0.00.050.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.152 I print_info: model type       = 1.4B
0.00.050.153 I print_info: model params     = 1.41 B
0.00.050.154 I print_info: general.name     = 1.4B
0.00.050.155 I print_info: vocab type       = BPE
0.00.050.155 I print_info: n_vocab          = 50304
0.00.050.155 I print_info: n_merges         = 50009
0.00.050.155 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.157 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.157 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.157 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.157 I print_info: LF token         = 128 'Ä'
0.00.050.157 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.157 I print_info: max token length = 1024
0.00.052.239 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.239 I load_tensors: offloading output layer to GPU
0.00.052.239 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.250 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.251 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.540 I llama_init_from_model: n_seq_max     = 1
0.00.052.540 I llama_init_from_model: n_ctx         = 128
0.00.052.541 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.541 I llama_init_from_model: n_batch       = 128
0.00.052.541 I llama_init_from_model: n_ubatch      = 128
0.00.052.541 I llama_init_from_model: flash_attn    = 0
0.00.052.541 I llama_init_from_model: freq_base     = 10000.0
0.00.052.542 I llama_init_from_model: freq_scale    = 1
0.00.052.542 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.543 I ggml_metal_init: allocating
0.00.052.545 I ggml_metal_init: found device: Apple M4
0.00.052.547 I ggml_metal_init: picking default device: Apple M4
0.00.053.130 I ggml_metal_init: using embedded metal library
0.00.055.476 I ggml_metal_init: GPU name:   Apple M4
0.00.055.478 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.478 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.478 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.479 I ggml_metal_init: simdgroup reduction   = true
0.00.055.479 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.479 I ggml_metal_init: has bfloat            = true
0.00.055.479 I ggml_metal_init: use bfloat            = true
0.00.055.480 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.480 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.298 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.581 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.583 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.597 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.470 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.471 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.471 I llama_init_from_model: graph nodes  = 967
0.00.067.472 I llama_init_from_model: graph splits = 2
0.00.067.473 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.375.127 I 
0.00.375.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.375.170 I perplexity: tokenizing the input ..
0.00.382.755 I perplexity: tokenization took 7.584 ms
0.00.382.762 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.522.712 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.523.917 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.523.942 I llama_perf_context_print:        load time =     366.21 ms
0.00.523.942 I llama_perf_context_print: prompt eval time =     139.72 ms /   128 tokens (    1.09 ms per token,   916.10 tokens per second)
0.00.523.943 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.523.943 I llama_perf_context_print:       total time =     148.82 ms /   129 tokens
0.00.524.421 I ggml_metal_free: deallocating

real	0m0.538s
user	0m0.077s
sys	0m0.079s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4472 (bc40c6b9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b60a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b60aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b60aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b60b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b60bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b60c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b60c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b60cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b60d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b60d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b60dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b60e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b60ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b60f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b60fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b611870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b612760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b6135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b613e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b614560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b614820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b615aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b615fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b6162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b616a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b6177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b6183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b6191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b619650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b61a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b61a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b61ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b61b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b61bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b61c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b61c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b61ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b61d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b61da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b61e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b61e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b61ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b61f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b61f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b61fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b620280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b6209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b621320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b6217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b622100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b6225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b622a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b623820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b623cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14b624210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14b624760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14b624cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14b625200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14b625750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14b625ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14b6261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14b626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14b626c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14b6271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14b627730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14b627c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14b6281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14b628720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14b628c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14b6291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14b629710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14b629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14b62a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14b62a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14b62ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14b62b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14b62b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14b62bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14b61b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14b62c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14b62c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14b62cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14b62d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14b62d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14b62dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14b62e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14b62e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14b62ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14b62f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14b62f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14b62fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14b6302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14b630820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14b630d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b6316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b631ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b632dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b634050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b6344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b634990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b634e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b6352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b635c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b6360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b6369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b636e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b637330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b6377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b637c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b638110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b6385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b638a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b638ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b639390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b639830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b63a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b63a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b63aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b63af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b63b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b63b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b63bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b63c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b63c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b63cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b63cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14b63d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14b63d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14b63dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14b63e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14b63e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14b63eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14b63f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14b63f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14b63f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14b63fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14b640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14b640730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14b640bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14b641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14b641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14b6419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14b641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14b6422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14b642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14b642c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14b6430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14b643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14b643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14b643eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14b644350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14b6447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14b644c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14b645130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14b6455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14b645a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14b645f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14b6463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14b646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14b646cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14b647190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14b647630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14b647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14b647f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14b6484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14b648a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14b648f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14b6494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14b649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14b649d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14b64a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14b64a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14b64b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14b64b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14b64b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14b64bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14b64c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14b64cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14b64d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14b64d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14b64dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14b64e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14b64e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14b64ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14b64f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14b64f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14b64fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14b650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14b6507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14b650d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14b651260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14b6517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14b651d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14b652250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14b6527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14b652cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14b653240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14b653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14b653ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14b654230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14b654780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14b654cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14b655220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14b655770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14b655cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14b656210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14b656760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14b656cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14b657200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14b657750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14b657ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14b6581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14b658740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14b658c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14b6591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14b659730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14b659c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14b65a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14b65a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14b65ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14b65b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14b65b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14b65bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14b65c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14b65c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14b65cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14b65d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14b65d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14b65dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14b65e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14b65e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14b65ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14b65f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14b65f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14b65fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14b660170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14b6606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14b660c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14b6610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14b661550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14b6619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14b661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14b662330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14b6627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14b662c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14b663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14b6635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14b663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14b663ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14b664390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14b664830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14b664cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14b665170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14b6656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14b665de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14b666500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14b666c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14b667340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14b667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14b667df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14b6680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14b6686c0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.148.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.148.463 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b504bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b505040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b5054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b505920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b505d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b506200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b506670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b506ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b506f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b5073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b507830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b507f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b508a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b5091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b509a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b50a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b50a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b50af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b50b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b50bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b50c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b50cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b50d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b50da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b50e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b50e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b50e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b50eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b50efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b50f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b50f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b50fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b510230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b5104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b510960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b510dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b511240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b5116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b511b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b511f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b512400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b512870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b512ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b513150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b5135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b513a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b513ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b514310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b514780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b514bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b515060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b5154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b515940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b515db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b516220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b516690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b516c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b517100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b517570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b5179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b517e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b5182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b518730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b518ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b519010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b519480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b5198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b519d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b51a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b51a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b51aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b51af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b51b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14b51b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14b51bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14b51c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14b51c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14b51c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14b51ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14b51d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14b51d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14b51db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14b51dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14b51e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14b51e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14b51ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14b51f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14b51f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14b51fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14b51ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14b520370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14b5207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14b520c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14b5210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14b521530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14b5219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14b521e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14b522280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14b5226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14b522b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14b522fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14b523440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14b5238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14b523d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14b524190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14b524600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14b524a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14b524ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14b525350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14b5257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14b525c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14b5260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14b526510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b526980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b526df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b527260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b5276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b527b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b527fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b528420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b528890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b528d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b529170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b5295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b529a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b529ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b52a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b52a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b52ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b52b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b52b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b52b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b52bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b52c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b52c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b52cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b52cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b52d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b52d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b52dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b52e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b52e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b52ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b52eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b52f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b52f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b52fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b530060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b5304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b530940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b530db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b531220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b531690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b531b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b531f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14b5323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14b532850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14b532cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14b533130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14b5335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14b533a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14b533e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14b5342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14b534760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14b534bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14b535040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14b535c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14b535f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14b5361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14b536660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14b536ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14b536f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14b5373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14b537820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14b537c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14b538100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14b538570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14b5389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14b538e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14b5392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14b539730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14b539ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14b53a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14b53a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14b53a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14b53ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14b53b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14b53b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14b53bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14b53bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14b53c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14b53c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14b53cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14b53d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14b53d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14b53d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14b53de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14b53e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14b53e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14b53eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14b53eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14b53f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14b53f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14b53fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14b540340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14b5407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14b540c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14b541090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14b5415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14b541ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14b542630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14b5428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14b542eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14b543470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14b543a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14b543ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14b5445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14b544b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14b545130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14b5456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14b545cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14b546270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14b546830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14b546df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14b5473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14b547970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14b547f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14b5484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14b548ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14b549070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14b549630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14b549bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14b54a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14b54a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14b54ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14b54b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14b54b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14b54be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14b54c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14b54c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14b54cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14b54d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14b54db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14b54e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14b54e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14b54ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14b54f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14b54f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14b54fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14b550370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14b550930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14b550ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14b5514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14b551a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14b552030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14b5525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14b552bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14b553170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14b553730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14b553cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14b5542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14b554870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14b554e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14b5553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14b5559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14b555f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14b556530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14b556af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14b556ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14b5574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14b5579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14b557ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14b5583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14b5588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14b558df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14b5592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14b5597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14b559cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14b55a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14b55a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14b55abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14b55b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14b55b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14b55c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14b55c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14b55ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14b55d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14b55d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14b55e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14b55e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14b55e8e0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b668370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b64bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b649a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b64a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b61d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b61d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b61f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b64c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b614ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b61b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b61bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b61c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b61a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b61cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b61fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b62c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b6678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b616cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b64c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b64ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b6150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b6153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b615670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b668b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b668de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b6690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b669360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b669620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b6698e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b669ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b669e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b66a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b66a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b66a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b66a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b66ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b66aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b66b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b66b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b66b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b66b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b66bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b66bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b66c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b66c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b66c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b66ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b66cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b66cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b66d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b66d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b66d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b66dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b66dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b66e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b66e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b66e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b66e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b66eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b66ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b66f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b66f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b66f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b66f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b66fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b66fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b670160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b670420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b6706e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b6709a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b670c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14b670f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14b6711e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14b6714a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14b671760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14b671a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14b671ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14b671fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14b672260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14b672520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14b6727e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14b672aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14b672d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14b673020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14b6732e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14b6735a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14b673860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14b673b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14b673de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14b6740a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14b674360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14b674620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14b6748e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14b674ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14b674e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14b675120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14b6753e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14b6756a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14b675960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14b675c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14b675ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14b6761a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14b676460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14b676720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14b6769e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14b676ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14b676f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14b677220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14b6774e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14b6777a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14b677a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b677d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b677fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b6782a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b678560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b678820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b678ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b678da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b679060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b679320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b6795e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b6798a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b679b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b679e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b67a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b67a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b67a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b67a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b67abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b67aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b67b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b67b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b67b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b67b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b67bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b67bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b67c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b67c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b67c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b67ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b67cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b67cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b67d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b67d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b67d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b67daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b67dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b67e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b67e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b67e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b67e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b67eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b67ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14b67f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14b67f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14b67f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14b67f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14b67fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14b67fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14b680120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14b6803e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14b6806a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14b680960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14b680c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14b680ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14b6811a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14b681460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14b681720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14b6819e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14b681ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14b681f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14b682220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14b6824e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14b6827a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14b682a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14b682d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14b682fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14b6832a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14b683560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14b683820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14b683ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14b683da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14b684060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14b684320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14b6845e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14b6848a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14b684b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14b684e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14b6850e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14b6853a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14b685660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14b685920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14b685be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14b685ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14b686160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14b686420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14b6866e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14b6869a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14b686c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14b686f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14b6871e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14b6874a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14b687760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14b687a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14b687ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14b687fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14b688260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14b688520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14b688af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14b689040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14b689590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14b689ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14b68a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14b68a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14b68aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14b68b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14b68b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14b68bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14b68c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14b68c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14b68cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14b68d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14b68d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14b68daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14b68dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14b68e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14b68ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14b68efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14b68f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14b68fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14b68ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14b690520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14b690a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14b690fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14b691510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14b691a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14b691fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14b692500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14b692a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14b692fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14b6934f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14b693a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14b693f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14b6944e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14b694a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14b694f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14b6954d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14b695a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14b695f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14b6964c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14b696a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14b696f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14b6974b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14b697a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14b697f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14b6984a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14b6989f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14b698f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14b699490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14b6999e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14b699f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14b69a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14b69a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14b69af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14b69b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14b69b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14b69b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14b69bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14b69c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14b69c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14b69ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14b69ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14b69d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14b69d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14b69dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14b69e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14b69e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14b69e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14b69ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14b69f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14b69f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14b69fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14b6a07c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14b6a0ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14b6a1600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14b6a18c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14b6a1d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14b6a2330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14b6a2940 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.818s
user	0m0.306s
sys	0m0.330s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4472 (bc40c6b9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d908ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d909130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d9095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d90bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d90c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d90cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d90d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d90d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d90d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d90de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d90e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d90e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d90efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d90f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d90ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d9106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d910de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d911500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d911c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d9125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d912cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d913410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d913b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d914250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d914970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d914c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d9150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d915970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d915c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d9160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d9165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d916ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d916f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d9173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d917670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d917df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d9182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d918790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d918c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d919130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d919600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d919ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d919fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d91a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d91a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d91adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d91b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d91b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d91be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d91c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d91c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d91cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d91cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d91d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d91db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d91dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d91e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d91e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d91ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d91f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d91f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d91fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d920110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d9205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d920a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d920ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d921390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d921830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d921cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d922170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d922610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d922ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d922f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d9234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d9239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d923f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d924490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d9249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d924f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d925480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d9259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d925f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d926470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d9269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d926f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d927460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d9279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d927f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d928450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d9289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d928ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d929440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d929990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d929ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d92a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d92a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d92aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d91b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d92b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d92baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d92c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d92c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d92cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d92d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d92d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d92dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d92e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d92e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d92eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d92f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d92f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d92fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d930000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d9304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d930940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d930de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d931280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d931720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d931bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d932060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d932500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d9329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d932e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d933550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d933810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d933d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d934210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d934710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d934c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d935110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d935610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d935b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d936010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d936510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d936a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d936f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d937410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d937910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d937e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d938310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d938810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d938d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d939210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d939710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d939c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d93a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d93a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d93ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ce04cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ce053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ce05820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ce05c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ce06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ce06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ce069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ce06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ce072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ce07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ce07ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ce08010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ce08480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ce088f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ce08d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ce091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ce09640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ce09ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ce09f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ce0a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ce0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ce0ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ce0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ce0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ce0b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ce0be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ce0c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ce0c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ce0cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ce0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ce0d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ce0d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ce0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ce0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ce0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ce0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ce0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ce0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ce0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ce0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ce100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ce10530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ce109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ce10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ce11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ce116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ce11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ce11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ce12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ce128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ce12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ce13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ce13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ce13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ce13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ce14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ce147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ce14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ce150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ce15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ce15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ce15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ce169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ce16c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ce17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ce178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ce17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ce18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ce189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ce18fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ce19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ce19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ce1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ce1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ce1ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ce1b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ce1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ce1bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ce1c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ce1c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ce1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ce1d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ce1d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ce1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ce1e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ce1eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ce1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ce1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ce1fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ce20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ce20710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ce20cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ce21270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ce21820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ce21dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ce22380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ce22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ce22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ce23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ce23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ce23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ce245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ce24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ce25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ce256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ce25c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ce26210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ce267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ce26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ce27320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ce278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ce27e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ce28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ce289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ce28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ce29540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ce29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ce2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ce2a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ce2ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ce2b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ce2b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ce2bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ce2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ce2c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ce2ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ce2cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ce2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ce2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ce2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ce2e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ce2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ce2ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ce2f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ce2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ce30110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ce30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ce30f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ce31670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ce31930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ce32120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ce323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ce329f0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.105.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d91f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d91d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d90c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d90c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d92b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d915360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d915620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d93b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d93b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d93b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d93b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d93bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d93c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d93c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d93cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d93cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d93d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d93d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d93d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d93daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d93dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d93e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d93e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d93e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d93e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d93eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d93ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d93f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d93f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d93f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d93f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d93fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d93fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d940120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d9403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d9406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d940960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d940c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d940ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d9411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d941460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d941720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d9419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d941ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d941f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d942220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d9424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d9427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d942a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d942d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d942fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d9432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d943560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d943820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d943ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d943da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d944060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d944320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d9445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d9448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d944b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d944e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d9450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d9453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d945660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d945920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d945be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d945ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d946160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d946420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d9466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d9469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d946c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d946f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d9471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d9474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d947760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d947a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d947ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d947fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d948260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d948520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d9487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d948aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d948d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d949020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d9492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d9495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d949860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d949b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d949de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d94a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d94a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d94a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d94a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d94aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d94ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d94b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d94b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d94b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d94b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d94bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d94bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d94c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d94c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d94c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d94c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d94cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d94cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d94d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d94d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d94d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d94da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d94dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d94dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d94e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d94e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d94e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d94eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d94eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d94f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d94f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d94f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d94f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d94fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d94ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d950440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d9508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d950d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d951190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d951600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d951a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d951ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d952350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d9527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d952c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d9530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d953510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d953980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d953df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d954260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d9546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d954b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d954fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d955420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d955890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d955d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d956170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d9565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d956a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d956ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d957330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d9577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d957c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d958080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d9584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d958960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d958dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d959240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d9596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d959b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d959f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d95a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d95a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d95ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d95b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d95b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d95bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d95bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d95c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d95c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d95cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d95d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d95d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d95da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d95dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d95e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d95e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d95ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d95f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d95f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d95f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d95fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d960260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d9606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d960b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d960fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d961420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d961890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d961d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d962170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d9625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d962a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d962ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d963330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d9637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d963c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d964080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d9644f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d964960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d964dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d965240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d9657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d965ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d966150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d9665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d966a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d966ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d9673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d9678d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d968440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d968700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d968cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d969280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d969840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d969e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d96a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d96a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d96af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d96b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d96bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d96c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d96c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d96cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d96d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d96d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d96dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d96e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d96e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d96ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d96f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d96fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d96ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d970580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d970b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d971100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d9716c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d971c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d972240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d972800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d972dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d973380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d973940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d973f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d9744c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d974a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d975040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d975600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d975bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d976180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d976740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d976d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d9772c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d977880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d977e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d978400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d9789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d978f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d979540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d979b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d97a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d97a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d97ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d97b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d97b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d97bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d97c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d97c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d97ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d97d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d97d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d97dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d97e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d97e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d97ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d97f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d97f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d97fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d980000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d980500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d980a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d980f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d981400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d981e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d982530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d982c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d983370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d983630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d983e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d9840e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d9846f0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d9816c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d972500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d9713c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d96e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d96b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d97af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d9786c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d976440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d9741c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d96c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d969b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d96eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d96fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d975300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d971f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d979dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d96c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d96da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d974d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d976fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d96f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d970840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d975e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d9689c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d972ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d973080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d96d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d96e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d97b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d978c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d96a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d973c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d968f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d969540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d96b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d97ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d970e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d979240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d96f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d971980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d9758c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d96cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d977580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d96bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d97a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d977b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d973640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d97c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d96ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d97c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d96a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d97a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d974780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d976a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d979800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d978100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d970280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d9838f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d984b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d984e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d9850d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d985390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d985650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d985910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d985bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d985e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d986150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d986410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d9866d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d986990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d986c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d986f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d9871d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d987490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d987750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d987a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d987cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d987f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d988250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d988510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d9887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d988a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d988d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d989010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d9892d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d989590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d989850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d989b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d989dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d98a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d98a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d98a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d98a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d98ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d98ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d98b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d98b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d98b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d98b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d98bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d98bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d98c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d98c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d98c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d98c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d98cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d98cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d98d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d98d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d98d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d98da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d98dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d98dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d98e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d98e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d98e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d98ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d98ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d98f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d98f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d98f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d98f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d98fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d98fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d9900d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d990390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d990650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d990910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d990bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d990e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d991150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d991410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d9916d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d991990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d991c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d991f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d9921d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d992490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d992750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d992a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d992cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d992f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d993250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d993510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d9937d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d993a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d993d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d994010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d9942d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d994590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d994850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d994b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d994dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d995090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d995350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d995610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d9958d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d995b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d995e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d996110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d9963d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d996690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d996950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d996c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d996ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d997190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d997450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d997710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d9979d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d997c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d997f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d998210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d9984d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d998790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d998a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d998d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d998fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d999290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d999550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d999810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d999ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d999d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d99a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d99a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d99a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d99a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d99ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d99ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d99b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d99b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d99b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d99b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d99bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d99be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d99c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d99c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d99c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d99c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d99cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d99cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d99d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d99d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d99d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d99da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d99dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d99df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d99e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d99e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d99e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d99ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d99f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d99f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d99f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d99f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d99fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d9a0340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d9a0890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d9a0de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d9a1330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d9a1880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d9a1dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d9a2320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d9a2870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d9a2dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d9a3310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d9a3860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d9a3db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d9a4300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d9a4850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d9a4da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d9a52f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d9a5840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d9a5d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d9a62e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d9a6830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d9a6d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d9a72d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d9a7820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d9a7d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d9a82c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d9a8810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d9a8d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d9a92b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d9a9800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d9a9d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d9aa2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d9aa7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d9aad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d9ab290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d9ab7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d9abd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d9ac280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d9ac7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d9acd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d9ad270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d9ad7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d9add10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d9ae260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d9ae7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d9aed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d9af250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d9af7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d9afcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d9b0240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d9b0790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d9b0ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d9b1230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d9b14f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d9b17b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d9b1a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d9b1ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d9b2350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d9b27c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d9b2c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d9b30a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d9b3510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d9b3980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d9b3df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d9b4260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d9b46d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d9b4b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d9b4fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d9b5420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d9b5890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d9b6580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d9b6ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d9b73c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d9b7680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d9b7af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d9b80f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d9b8700 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.954s
user	0m0.245s
sys	0m0.138s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.58 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.17 sec*proc (2 tests)

Total Test time (real) =   1.18 sec
        1.26 real         0.72 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.28 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.15 user         0.05 sys
```
