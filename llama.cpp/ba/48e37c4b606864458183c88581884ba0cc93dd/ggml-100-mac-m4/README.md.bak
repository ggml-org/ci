### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.53 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.41 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.24 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.25 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.48 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.73 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.71 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.58 sec*proc (28 tests)

Total Test time (real) = 222.59 sec

real	3m42.623s
user	7m36.041s
sys	0m6.270s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.16 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.40 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.55 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.11 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.28 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.75 sec*proc (28 tests)

Total Test time (real) =  51.76 sec

real	0m51.774s
user	1m12.297s
sys	0m5.723s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.085 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.502 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.621 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.628 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.631 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.632 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.633 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.634 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.635 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.636 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.636 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.637 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.639 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.643 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.643 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.647 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.647 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.648 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.649 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.649 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.234 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.462 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.464 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.464 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.465 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.465 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.026.465 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.466 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.026.467 I llama_model_loader: - type  f32:  124 tensors
0.00.026.467 I llama_model_loader: - type  f16:   73 tensors
0.00.030.397 I llm_load_vocab: special tokens cache size = 5
0.00.032.482 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.032.486 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.032.486 I llm_load_print_meta: arch             = bert
0.00.032.487 I llm_load_print_meta: vocab type       = WPM
0.00.032.487 I llm_load_print_meta: n_vocab          = 30522
0.00.032.487 I llm_load_print_meta: n_merges         = 0
0.00.032.487 I llm_load_print_meta: vocab_only       = 0
0.00.032.488 I llm_load_print_meta: n_ctx_train      = 512
0.00.032.488 I llm_load_print_meta: n_embd           = 384
0.00.032.488 I llm_load_print_meta: n_layer          = 12
0.00.032.491 I llm_load_print_meta: n_head           = 12
0.00.032.500 I llm_load_print_meta: n_head_kv        = 12
0.00.032.500 I llm_load_print_meta: n_rot            = 32
0.00.032.501 I llm_load_print_meta: n_swa            = 0
0.00.032.501 I llm_load_print_meta: n_embd_head_k    = 32
0.00.032.503 I llm_load_print_meta: n_embd_head_v    = 32
0.00.032.504 I llm_load_print_meta: n_gqa            = 1
0.00.032.505 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.032.506 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.032.506 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.032.507 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.032.507 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.032.508 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.032.508 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.032.509 I llm_load_print_meta: n_ff             = 1536
0.00.032.509 I llm_load_print_meta: n_expert         = 0
0.00.032.509 I llm_load_print_meta: n_expert_used    = 0
0.00.032.509 I llm_load_print_meta: causal attn      = 0
0.00.032.509 I llm_load_print_meta: pooling type     = 2
0.00.032.510 I llm_load_print_meta: rope type        = 2
0.00.032.510 I llm_load_print_meta: rope scaling     = linear
0.00.032.516 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.032.517 I llm_load_print_meta: freq_scale_train = 1
0.00.032.517 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.032.517 I llm_load_print_meta: rope_finetuned   = unknown
0.00.032.517 I llm_load_print_meta: ssm_d_conv       = 0
0.00.032.517 I llm_load_print_meta: ssm_d_inner      = 0
0.00.032.518 I llm_load_print_meta: ssm_d_state      = 0
0.00.032.519 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.032.521 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.032.539 I llm_load_print_meta: model type       = 33M
0.00.032.545 I llm_load_print_meta: model ftype      = F16
0.00.032.546 I llm_load_print_meta: model params     = 33.21 M
0.00.032.547 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.032.549 I llm_load_print_meta: general.name     = Bge Small
0.00.032.549 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.032.550 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.032.550 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.032.550 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.032.550 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.032.551 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.032.551 I llm_load_print_meta: max token length = 21
0.00.034.585 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.034.586 I llm_load_tensors: offloading output layer to GPU
0.00.034.587 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.034.605 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.607 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.124 I llama_new_context_with_model: n_seq_max     = 1
0.00.035.126 I llama_new_context_with_model: n_ctx         = 512
0.00.035.126 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.035.126 I llama_new_context_with_model: n_batch       = 2048
0.00.035.126 I llama_new_context_with_model: n_ubatch      = 2048
0.00.035.127 I llama_new_context_with_model: flash_attn    = 0
0.00.035.127 I llama_new_context_with_model: freq_base     = 10000.0
0.00.035.128 I llama_new_context_with_model: freq_scale    = 1
0.00.035.129 I ggml_metal_init: allocating
0.00.035.139 I ggml_metal_init: found device: Apple M4
0.00.035.142 I ggml_metal_init: picking default device: Apple M4
0.00.035.966 I ggml_metal_init: using embedded metal library
0.00.040.039 I ggml_metal_init: GPU name:   Apple M4
0.00.040.042 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.042 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.043 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.043 I ggml_metal_init: simdgroup reduction   = true
0.00.040.043 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.044 I ggml_metal_init: has bfloat            = true
0.00.040.044 I ggml_metal_init: use bfloat            = true
0.00.040.044 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.045 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.407 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.053.091 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.093 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.095 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.053.870 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.053.871 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.053.872 I llama_new_context_with_model: graph nodes  = 429
0.00.053.872 I llama_new_context_with_model: graph splits = 2
0.00.053.887 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.001 I 
0.00.060.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.671 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.497 I llama_perf_context_print:        load time =      43.49 ms
0.00.065.498 I llama_perf_context_print: prompt eval time =       4.66 ms /     9 tokens (    0.52 ms per token,  1930.50 tokens per second)
0.00.065.499 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.499 I llama_perf_context_print:       total time =       5.50 ms /    10 tokens
0.00.065.622 I ggml_metal_free: deallocating

real	0m0.245s
user	0m0.048s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.240 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.248 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.252 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.253 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.254 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.256 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.257 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.258 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.258 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.258 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.258 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.259 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.261 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.261 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.262 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.262 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.262 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.263 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.263 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.824 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.504 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.505 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.505 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.506 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.506 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.506 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.507 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.507 I llama_model_loader: - type  f32:  124 tensors
0.00.014.507 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.157 I llm_load_vocab: special tokens cache size = 5
0.00.018.477 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.480 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.481 I llm_load_print_meta: arch             = bert
0.00.018.481 I llm_load_print_meta: vocab type       = WPM
0.00.018.481 I llm_load_print_meta: n_vocab          = 30522
0.00.018.481 I llm_load_print_meta: n_merges         = 0
0.00.018.481 I llm_load_print_meta: vocab_only       = 0
0.00.018.482 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.482 I llm_load_print_meta: n_embd           = 384
0.00.018.482 I llm_load_print_meta: n_layer          = 12
0.00.018.484 I llm_load_print_meta: n_head           = 12
0.00.018.491 I llm_load_print_meta: n_head_kv        = 12
0.00.018.491 I llm_load_print_meta: n_rot            = 32
0.00.018.492 I llm_load_print_meta: n_swa            = 0
0.00.018.492 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.492 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.492 I llm_load_print_meta: n_gqa            = 1
0.00.018.493 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.493 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.494 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.494 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.495 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.495 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.495 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.495 I llm_load_print_meta: n_ff             = 1536
0.00.018.496 I llm_load_print_meta: n_expert         = 0
0.00.018.496 I llm_load_print_meta: n_expert_used    = 0
0.00.018.496 I llm_load_print_meta: causal attn      = 0
0.00.018.496 I llm_load_print_meta: pooling type     = 2
0.00.018.496 I llm_load_print_meta: rope type        = 2
0.00.018.496 I llm_load_print_meta: rope scaling     = linear
0.00.018.498 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.498 I llm_load_print_meta: freq_scale_train = 1
0.00.018.498 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.498 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.499 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.499 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.499 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.499 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.499 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.504 I llm_load_print_meta: model type       = 33M
0.00.018.509 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.509 I llm_load_print_meta: model params     = 33.21 M
0.00.018.510 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.510 I llm_load_print_meta: general.name     = Bge Small
0.00.018.512 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.512 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.512 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.513 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.513 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.513 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.513 I llm_load_print_meta: max token length = 21
0.00.019.795 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.796 I llm_load_tensors: offloading output layer to GPU
0.00.019.796 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.802 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.802 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.153 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.154 I llama_new_context_with_model: n_ctx         = 512
0.00.020.154 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.154 I llama_new_context_with_model: n_batch       = 2048
0.00.020.155 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.155 I llama_new_context_with_model: flash_attn    = 0
0.00.020.155 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.156 I llama_new_context_with_model: freq_scale    = 1
0.00.020.156 I ggml_metal_init: allocating
0.00.020.159 I ggml_metal_init: found device: Apple M4
0.00.020.160 I ggml_metal_init: picking default device: Apple M4
0.00.020.779 I ggml_metal_init: using embedded metal library
0.00.023.280 I ggml_metal_init: GPU name:   Apple M4
0.00.023.281 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.282 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.282 I ggml_metal_init: simdgroup reduction   = true
0.00.023.282 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.283 I ggml_metal_init: has bfloat            = true
0.00.023.283 I ggml_metal_init: use bfloat            = true
0.00.023.283 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.284 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.560 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.028 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.030 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.032 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.649 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.650 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.650 I llama_new_context_with_model: graph nodes  = 429
0.00.034.650 I llama_new_context_with_model: graph splits = 2
0.00.034.664 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.445 I 
0.00.039.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.990 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.466 I llama_perf_context_print:        load time =      30.20 ms
0.00.044.467 I llama_perf_context_print: prompt eval time =       4.35 ms /     9 tokens (    0.48 ms per token,  2068.97 tokens per second)
0.00.044.468 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.468 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.044.664 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.171 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.088 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.384 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.393 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.394 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.395 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.396 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.406 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.407 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.407 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.408 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.409 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.412 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.413 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.414 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.700 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.806 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.807 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.807 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.808 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.808 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.051.808 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.809 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.809 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.809 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.810 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.051.810 I llama_model_loader: - type  f32:   40 tensors
0.00.051.811 I llama_model_loader: - type  f16:   30 tensors
0.00.070.875 W llm_load_vocab: empty token at index 5
0.00.075.725 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.077.126 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.077.157 I llm_load_vocab: special tokens cache size = 5
0.00.339.986 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.340.001 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.340.003 I llm_load_print_meta: arch             = jina-bert-v2
0.00.340.003 I llm_load_print_meta: vocab type       = BPE
0.00.340.003 I llm_load_print_meta: n_vocab          = 61056
0.00.340.004 I llm_load_print_meta: n_merges         = 39382
0.00.340.004 I llm_load_print_meta: vocab_only       = 0
0.00.340.004 I llm_load_print_meta: n_ctx_train      = 8192
0.00.340.004 I llm_load_print_meta: n_embd           = 384
0.00.340.004 I llm_load_print_meta: n_layer          = 4
0.00.340.011 I llm_load_print_meta: n_head           = 12
0.00.340.035 I llm_load_print_meta: n_head_kv        = 12
0.00.340.036 I llm_load_print_meta: n_rot            = 32
0.00.340.036 I llm_load_print_meta: n_swa            = 0
0.00.340.037 I llm_load_print_meta: n_embd_head_k    = 32
0.00.340.037 I llm_load_print_meta: n_embd_head_v    = 32
0.00.340.040 I llm_load_print_meta: n_gqa            = 1
0.00.340.041 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.340.041 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.340.046 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.340.047 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.340.047 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.340.060 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.340.061 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.340.063 I llm_load_print_meta: n_ff             = 1536
0.00.340.064 I llm_load_print_meta: n_expert         = 0
0.00.340.064 I llm_load_print_meta: n_expert_used    = 0
0.00.340.065 I llm_load_print_meta: causal attn      = 0
0.00.340.065 I llm_load_print_meta: pooling type     = -1
0.00.340.065 I llm_load_print_meta: rope type        = -1
0.00.340.065 I llm_load_print_meta: rope scaling     = linear
0.00.340.066 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.340.066 I llm_load_print_meta: freq_scale_train = 1
0.00.340.066 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.340.067 I llm_load_print_meta: rope_finetuned   = unknown
0.00.340.067 I llm_load_print_meta: ssm_d_conv       = 0
0.00.340.067 I llm_load_print_meta: ssm_d_inner      = 0
0.00.340.067 I llm_load_print_meta: ssm_d_state      = 0
0.00.340.067 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.340.067 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.340.068 I llm_load_print_meta: model type       = 33M
0.00.340.090 I llm_load_print_meta: model ftype      = F16
0.00.340.091 I llm_load_print_meta: model params     = 32.90 M
0.00.340.091 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.340.091 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.340.095 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.340.096 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.340.096 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.340.096 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.340.096 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.340.096 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.340.097 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.340.097 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.340.097 I llm_load_print_meta: max token length = 45
0.00.341.265 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.341.265 I llm_load_tensors: offloading output layer to GPU
0.00.341.266 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.341.289 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.341.290 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.342.161 I llama_new_context_with_model: n_seq_max     = 1
0.00.342.163 I llama_new_context_with_model: n_ctx         = 8192
0.00.342.163 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.342.163 I llama_new_context_with_model: n_batch       = 2048
0.00.342.163 I llama_new_context_with_model: n_ubatch      = 2048
0.00.342.163 I llama_new_context_with_model: flash_attn    = 0
0.00.342.164 I llama_new_context_with_model: freq_base     = 10000.0
0.00.342.164 I llama_new_context_with_model: freq_scale    = 1
0.00.342.165 I ggml_metal_init: allocating
0.00.342.167 I ggml_metal_init: found device: Apple M4
0.00.342.170 I ggml_metal_init: picking default device: Apple M4
0.00.343.180 I ggml_metal_init: using embedded metal library
0.00.345.956 I ggml_metal_init: GPU name:   Apple M4
0.00.345.958 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.958 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.959 I ggml_metal_init: simdgroup reduction   = true
0.00.345.959 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.959 I ggml_metal_init: has bfloat            = true
0.00.345.959 I ggml_metal_init: use bfloat            = true
0.00.345.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.355.441 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.357.845 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.357.850 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.357.852 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.358.415 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.358.416 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.358.416 I llama_new_context_with_model: graph nodes  = 154
0.00.358.417 I llama_new_context_with_model: graph splits = 2
0.00.358.434 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.358.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.371.471 I 
0.00.371.503 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.371.783 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.371.784 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.371.787 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.371.787 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.371.789 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.371.790 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.372.321 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.376.061 I llama_perf_context_print:        load time =     346.38 ms
0.00.376.062 I llama_perf_context_print: prompt eval time =       3.73 ms /    62 tokens (    0.06 ms per token, 16613.08 tokens per second)
0.00.376.063 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.376.063 I llama_perf_context_print:       total time =       4.59 ms /    63 tokens
0.00.376.291 I ggml_metal_free: deallocating

real	0m1.107s
user	0m0.344s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.126 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.227 I main: llama backend init
0.00.000.235 I main: load the model and apply lora adapter, if any
0.00.028.140 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.297 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.325 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.328 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.329 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.329 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.330 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.331 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.333 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.336 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.340 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.345 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.346 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.347 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.571 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.842 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.058.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.943 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.944 I llama_model_loader: - type  f32:  194 tensors
0.00.058.945 I llama_model_loader: - type  f16:   98 tensors
0.00.090.540 I llm_load_vocab: special tokens cache size = 25
0.00.097.480 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.097.483 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.097.484 I llm_load_print_meta: arch             = gptneox
0.00.097.484 I llm_load_print_meta: vocab type       = BPE
0.00.097.484 I llm_load_print_meta: n_vocab          = 50304
0.00.097.484 I llm_load_print_meta: n_merges         = 50009
0.00.097.484 I llm_load_print_meta: vocab_only       = 0
0.00.097.485 I llm_load_print_meta: n_ctx_train      = 2048
0.00.097.485 I llm_load_print_meta: n_embd           = 2048
0.00.097.485 I llm_load_print_meta: n_layer          = 24
0.00.097.488 I llm_load_print_meta: n_head           = 16
0.00.097.502 I llm_load_print_meta: n_head_kv        = 16
0.00.097.503 I llm_load_print_meta: n_rot            = 32
0.00.097.503 I llm_load_print_meta: n_swa            = 0
0.00.097.503 I llm_load_print_meta: n_embd_head_k    = 128
0.00.097.503 I llm_load_print_meta: n_embd_head_v    = 128
0.00.097.504 I llm_load_print_meta: n_gqa            = 1
0.00.097.504 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.097.505 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.097.505 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.097.506 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.097.506 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.097.506 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.097.506 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.097.507 I llm_load_print_meta: n_ff             = 8192
0.00.097.507 I llm_load_print_meta: n_expert         = 0
0.00.097.507 I llm_load_print_meta: n_expert_used    = 0
0.00.097.508 I llm_load_print_meta: causal attn      = 1
0.00.097.508 I llm_load_print_meta: pooling type     = 0
0.00.097.508 I llm_load_print_meta: rope type        = 2
0.00.097.508 I llm_load_print_meta: rope scaling     = linear
0.00.097.508 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.097.509 I llm_load_print_meta: freq_scale_train = 1
0.00.097.509 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.097.509 I llm_load_print_meta: rope_finetuned   = unknown
0.00.097.509 I llm_load_print_meta: ssm_d_conv       = 0
0.00.097.509 I llm_load_print_meta: ssm_d_inner      = 0
0.00.097.509 I llm_load_print_meta: ssm_d_state      = 0
0.00.097.509 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.097.510 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.097.510 I llm_load_print_meta: model type       = 1.4B
0.00.097.520 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.097.520 I llm_load_print_meta: model params     = 1.41 B
0.00.097.521 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.097.521 I llm_load_print_meta: general.name     = 1.4B
0.00.097.522 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.097.522 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.097.522 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.097.522 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.097.522 I llm_load_print_meta: LF token         = 128 ''
0.00.097.523 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.097.523 I llm_load_print_meta: max token length = 1024
0.00.100.164 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.164 I llm_load_tensors: offloading output layer to GPU
0.00.100.165 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.183 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.184 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.116 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.117 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.117 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.117 I llama_new_context_with_model: n_batch       = 2048
0.00.101.117 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.117 I llama_new_context_with_model: flash_attn    = 0
0.00.101.118 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.118 I llama_new_context_with_model: freq_scale    = 1
0.00.101.119 I ggml_metal_init: allocating
0.00.101.122 I ggml_metal_init: found device: Apple M4
0.00.101.124 I ggml_metal_init: picking default device: Apple M4
0.00.101.807 I ggml_metal_init: using embedded metal library
0.00.111.209 I ggml_metal_init: GPU name:   Apple M4
0.00.111.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.111.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.111.212 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.111.212 I ggml_metal_init: simdgroup reduction   = true
0.00.111.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.111.213 I ggml_metal_init: has bfloat            = true
0.00.111.213 I ggml_metal_init: use bfloat            = true
0.00.111.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.111.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.135.124 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.155.336 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.155.345 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.155.366 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.156.318 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.156.320 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.156.320 I llama_new_context_with_model: graph nodes  = 967
0.00.156.321 I llama_new_context_with_model: graph splits = 2
0.00.156.345 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.156.483 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.156.483 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.235.753 I main: llama threadpool init, n_threads = 4
0.00.235.791 I 
0.00.235.828 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.235.829 I 
0.00.235.901 I sampler seed: 1234
0.00.235.906 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.235.929 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.235.931 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.235.931 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.075.964 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.02.075.964 I llama_perf_context_print:        load time =     207.60 ms
0.02.075.965 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   160.00 tokens per second)
0.02.075.967 I llama_perf_context_print:        eval time =    1793.27 ms /    63 runs   (   28.46 ms per token,    35.13 tokens per second)
0.02.075.967 I llama_perf_context_print:       total time =    1840.21 ms /    70 tokens
0.02.076.174 I ggml_metal_free: deallocating

real	0m2.447s
user	0m0.145s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.427 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.067 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.091 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.108 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.110 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.984 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.046 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.997 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.998 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.998 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.999 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.999 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.000 I llama_model_loader: - type  f32:  194 tensors
0.00.036.000 I llama_model_loader: - type  f16:   98 tensors
0.00.058.176 I llm_load_vocab: special tokens cache size = 25
0.00.066.611 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.615 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.616 I llm_load_print_meta: arch             = gptneox
0.00.066.616 I llm_load_print_meta: vocab type       = BPE
0.00.066.616 I llm_load_print_meta: n_vocab          = 50304
0.00.066.616 I llm_load_print_meta: n_merges         = 50009
0.00.066.616 I llm_load_print_meta: vocab_only       = 0
0.00.066.617 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.617 I llm_load_print_meta: n_embd           = 2048
0.00.066.617 I llm_load_print_meta: n_layer          = 24
0.00.066.621 I llm_load_print_meta: n_head           = 16
0.00.066.635 I llm_load_print_meta: n_head_kv        = 16
0.00.066.636 I llm_load_print_meta: n_rot            = 32
0.00.066.636 I llm_load_print_meta: n_swa            = 0
0.00.066.636 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.636 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.638 I llm_load_print_meta: n_gqa            = 1
0.00.066.639 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.640 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.640 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.643 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.643 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.644 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.644 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.644 I llm_load_print_meta: n_ff             = 8192
0.00.066.645 I llm_load_print_meta: n_expert         = 0
0.00.066.645 I llm_load_print_meta: n_expert_used    = 0
0.00.066.645 I llm_load_print_meta: causal attn      = 1
0.00.066.645 I llm_load_print_meta: pooling type     = 0
0.00.066.645 I llm_load_print_meta: rope type        = 2
0.00.066.646 I llm_load_print_meta: rope scaling     = linear
0.00.066.646 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.646 I llm_load_print_meta: freq_scale_train = 1
0.00.066.647 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.647 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.647 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.647 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.647 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.647 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.648 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.648 I llm_load_print_meta: model type       = 1.4B
0.00.066.659 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.066.659 I llm_load_print_meta: model params     = 1.41 B
0.00.066.660 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.066.660 I llm_load_print_meta: general.name     = 1.4B
0.00.066.660 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.660 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.661 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.661 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.661 I llm_load_print_meta: LF token         = 128 ''
0.00.066.661 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.662 I llm_load_print_meta: max token length = 1024
0.00.069.189 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.189 I llm_load_tensors: offloading output layer to GPU
0.00.069.190 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.200 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.069.202 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.070.291 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.292 I llama_new_context_with_model: n_ctx         = 128
0.00.070.292 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.292 I llama_new_context_with_model: n_batch       = 128
0.00.070.293 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.293 I llama_new_context_with_model: flash_attn    = 0
0.00.070.293 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.294 I llama_new_context_with_model: freq_scale    = 1
0.00.070.294 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.295 I ggml_metal_init: allocating
0.00.070.304 I ggml_metal_init: found device: Apple M4
0.00.070.307 I ggml_metal_init: picking default device: Apple M4
0.00.070.984 I ggml_metal_init: using embedded metal library
0.00.073.625 I ggml_metal_init: GPU name:   Apple M4
0.00.073.627 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.628 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.628 I ggml_metal_init: simdgroup reduction   = true
0.00.073.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.628 I ggml_metal_init: has bfloat            = true
0.00.073.629 I ggml_metal_init: use bfloat            = true
0.00.073.629 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.630 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.229 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.506 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.510 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.527 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.432 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.434 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.434 I llama_new_context_with_model: graph nodes  = 967
0.00.086.434 I llama_new_context_with_model: graph splits = 2
0.00.086.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.086.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.998.583 I 
0.00.998.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.998.700 I perplexity: tokenizing the input ..
0.01.011.613 I perplexity: tokenization took 12.91 ms
0.01.011.629 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.133.516 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.135.204 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.135.224 I llama_perf_context_print:        load time =     981.50 ms
0.01.135.226 I llama_perf_context_print: prompt eval time =     121.16 ms /   128 tokens (    0.95 ms per token,  1056.43 tokens per second)
0.01.135.227 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.135.228 I llama_perf_context_print:       total time =     136.65 ms /   129 tokens
0.01.135.960 I ggml_metal_free: deallocating

real	0m1.318s
user	0m0.105s
sys	0m0.199s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.863 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.878 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.890 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.892 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.892 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.893 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.894 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.849 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.915 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.163 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.165 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.166 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.166 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.166 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.167 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.167 I llama_model_loader: - type  f32:  194 tensors
0.00.041.168 I llama_model_loader: - type q8_0:   98 tensors
0.00.066.108 I llm_load_vocab: special tokens cache size = 25
0.00.074.233 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.237 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.238 I llm_load_print_meta: arch             = gptneox
0.00.074.238 I llm_load_print_meta: vocab type       = BPE
0.00.074.238 I llm_load_print_meta: n_vocab          = 50304
0.00.074.239 I llm_load_print_meta: n_merges         = 50009
0.00.074.239 I llm_load_print_meta: vocab_only       = 0
0.00.074.239 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.239 I llm_load_print_meta: n_embd           = 2048
0.00.074.239 I llm_load_print_meta: n_layer          = 24
0.00.074.246 I llm_load_print_meta: n_head           = 16
0.00.074.261 I llm_load_print_meta: n_head_kv        = 16
0.00.074.262 I llm_load_print_meta: n_rot            = 32
0.00.074.262 I llm_load_print_meta: n_swa            = 0
0.00.074.262 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.262 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.263 I llm_load_print_meta: n_gqa            = 1
0.00.074.264 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.265 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.266 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.267 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.267 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.268 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.268 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.271 I llm_load_print_meta: n_ff             = 8192
0.00.074.272 I llm_load_print_meta: n_expert         = 0
0.00.074.272 I llm_load_print_meta: n_expert_used    = 0
0.00.074.273 I llm_load_print_meta: causal attn      = 1
0.00.074.273 I llm_load_print_meta: pooling type     = 0
0.00.074.274 I llm_load_print_meta: rope type        = 2
0.00.074.275 I llm_load_print_meta: rope scaling     = linear
0.00.074.275 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.276 I llm_load_print_meta: freq_scale_train = 1
0.00.074.276 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.276 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.276 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.276 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.277 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.277 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.277 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.278 I llm_load_print_meta: model type       = 1.4B
0.00.074.289 I llm_load_print_meta: model ftype      = Q8_0
0.00.074.289 I llm_load_print_meta: model params     = 1.41 B
0.00.074.290 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.074.290 I llm_load_print_meta: general.name     = 1.4B
0.00.074.291 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.291 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.291 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.293 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.293 I llm_load_print_meta: LF token         = 128 ''
0.00.074.293 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.294 I llm_load_print_meta: max token length = 1024
0.00.077.351 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.352 I llm_load_tensors: offloading output layer to GPU
0.00.077.352 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.364 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.077.366 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.078.779 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.780 I llama_new_context_with_model: n_ctx         = 2048
0.00.078.781 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.078.781 I llama_new_context_with_model: n_batch       = 2048
0.00.078.781 I llama_new_context_with_model: n_ubatch      = 512
0.00.078.781 I llama_new_context_with_model: flash_attn    = 0
0.00.078.782 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.782 I llama_new_context_with_model: freq_scale    = 1
0.00.078.783 I ggml_metal_init: allocating
0.00.078.787 I ggml_metal_init: found device: Apple M4
0.00.078.790 I ggml_metal_init: picking default device: Apple M4
0.00.079.745 I ggml_metal_init: using embedded metal library
0.00.083.570 I ggml_metal_init: GPU name:   Apple M4
0.00.083.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.574 I ggml_metal_init: simdgroup reduction   = true
0.00.083.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.574 I ggml_metal_init: has bfloat            = true
0.00.083.574 I ggml_metal_init: use bfloat            = true
0.00.083.575 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.216 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.121.798 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.812 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.121.841 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.122.887 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.122.890 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.122.890 I llama_new_context_with_model: graph nodes  = 967
0.00.122.891 I llama_new_context_with_model: graph splits = 2
0.00.122.914 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.123.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.123.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.330.039 I main: llama threadpool init, n_threads = 4
0.01.330.080 I 
0.01.330.114 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.330.114 I 
0.01.330.380 I sampler seed: 1234
0.01.330.385 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.330.426 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.330.437 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.330.437 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.416.362 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63223.51 tokens per second)
0.02.416.363 I llama_perf_context_print:        load time =    1320.17 ms
0.02.416.364 I llama_perf_context_print: prompt eval time =      39.97 ms /     7 tokens (    5.71 ms per token,   175.12 tokens per second)
0.02.416.364 I llama_perf_context_print:        eval time =    1043.13 ms /    63 runs   (   16.56 ms per token,    60.40 tokens per second)
0.02.416.366 I llama_perf_context_print:       total time =    1086.33 ms /    70 tokens
0.02.416.593 I ggml_metal_free: deallocating

real	0m2.435s
user	0m0.125s
sys	0m0.239s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.130 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.428 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.623 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.628 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.632 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.633 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.633 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.634 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.635 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.635 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.635 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.636 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.641 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.641 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.641 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.956 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.763 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.765 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.765 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.766 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.766 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.767 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.767 I llama_model_loader: - type  f32:  194 tensors
0.00.032.768 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.269 I llm_load_vocab: special tokens cache size = 25
0.00.063.526 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.529 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.529 I llm_load_print_meta: arch             = gptneox
0.00.063.529 I llm_load_print_meta: vocab type       = BPE
0.00.063.529 I llm_load_print_meta: n_vocab          = 50304
0.00.063.530 I llm_load_print_meta: n_merges         = 50009
0.00.063.530 I llm_load_print_meta: vocab_only       = 0
0.00.063.530 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.530 I llm_load_print_meta: n_embd           = 2048
0.00.063.530 I llm_load_print_meta: n_layer          = 24
0.00.063.533 I llm_load_print_meta: n_head           = 16
0.00.063.546 I llm_load_print_meta: n_head_kv        = 16
0.00.063.546 I llm_load_print_meta: n_rot            = 32
0.00.063.546 I llm_load_print_meta: n_swa            = 0
0.00.063.547 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.547 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.548 I llm_load_print_meta: n_gqa            = 1
0.00.063.548 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.549 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.549 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.550 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.550 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.550 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.550 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.551 I llm_load_print_meta: n_ff             = 8192
0.00.063.553 I llm_load_print_meta: n_expert         = 0
0.00.063.553 I llm_load_print_meta: n_expert_used    = 0
0.00.063.553 I llm_load_print_meta: causal attn      = 1
0.00.063.553 I llm_load_print_meta: pooling type     = 0
0.00.063.553 I llm_load_print_meta: rope type        = 2
0.00.063.553 I llm_load_print_meta: rope scaling     = linear
0.00.063.554 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.554 I llm_load_print_meta: freq_scale_train = 1
0.00.063.554 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.554 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.555 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.555 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.555 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.555 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.555 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.556 I llm_load_print_meta: model type       = 1.4B
0.00.063.565 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.566 I llm_load_print_meta: model params     = 1.41 B
0.00.063.566 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.566 I llm_load_print_meta: general.name     = 1.4B
0.00.063.566 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.567 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.567 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.567 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.568 I llm_load_print_meta: LF token         = 128 ''
0.00.063.568 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.569 I llm_load_print_meta: max token length = 1024
0.00.065.684 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.684 I llm_load_tensors: offloading output layer to GPU
0.00.065.685 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.695 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.696 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.576 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.576 I llama_new_context_with_model: n_ctx         = 128
0.00.066.576 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.577 I llama_new_context_with_model: n_batch       = 128
0.00.066.577 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.577 I llama_new_context_with_model: flash_attn    = 0
0.00.066.577 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.578 I llama_new_context_with_model: freq_scale    = 1
0.00.066.578 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.578 I ggml_metal_init: allocating
0.00.066.584 I ggml_metal_init: found device: Apple M4
0.00.066.587 I ggml_metal_init: picking default device: Apple M4
0.00.067.145 I ggml_metal_init: using embedded metal library
0.00.069.538 I ggml_metal_init: GPU name:   Apple M4
0.00.069.539 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.540 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.540 I ggml_metal_init: simdgroup reduction   = true
0.00.069.540 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.540 I ggml_metal_init: has bfloat            = true
0.00.069.541 I ggml_metal_init: use bfloat            = true
0.00.069.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.541 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.355 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.683 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.686 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.703 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.080.570 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.080.571 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.080.571 I llama_new_context_with_model: graph nodes  = 967
0.00.080.572 I llama_new_context_with_model: graph splits = 2
0.00.080.584 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.585 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.942.090 I 
0.00.942.133 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.942.160 I perplexity: tokenizing the input ..
0.00.949.847 I perplexity: tokenization took 7.685 ms
0.00.949.850 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.073.458 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.074.617 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.074.635 I llama_perf_context_print:        load time =     930.65 ms
0.01.074.636 I llama_perf_context_print: prompt eval time =     123.38 ms /   128 tokens (    0.96 ms per token,  1037.45 tokens per second)
0.01.074.639 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.074.639 I llama_perf_context_print:       total time =     132.55 ms /   129 tokens
0.01.075.070 I ggml_metal_free: deallocating

real	0m1.093s
user	0m0.091s
sys	0m0.154s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.016.466 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.948 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.958 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.958 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.958 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.960 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.962 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.962 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.962 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.962 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.963 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.963 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.963 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.967 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.968 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.417 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.631 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.161 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.161 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.162 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.162 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.163 I llama_model_loader: - type  f32:  194 tensors
0.00.046.163 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.163 I llama_model_loader: - type q6_K:    1 tensors
0.00.075.568 I llm_load_vocab: special tokens cache size = 25
0.00.085.412 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.416 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.417 I llm_load_print_meta: arch             = gptneox
0.00.085.417 I llm_load_print_meta: vocab type       = BPE
0.00.085.417 I llm_load_print_meta: n_vocab          = 50304
0.00.085.418 I llm_load_print_meta: n_merges         = 50009
0.00.085.418 I llm_load_print_meta: vocab_only       = 0
0.00.085.418 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.422 I llm_load_print_meta: n_embd           = 2048
0.00.085.422 I llm_load_print_meta: n_layer          = 24
0.00.085.427 I llm_load_print_meta: n_head           = 16
0.00.085.442 I llm_load_print_meta: n_head_kv        = 16
0.00.085.442 I llm_load_print_meta: n_rot            = 32
0.00.085.442 I llm_load_print_meta: n_swa            = 0
0.00.085.443 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.443 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.444 I llm_load_print_meta: n_gqa            = 1
0.00.085.445 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.446 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.446 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.447 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.447 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.448 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.448 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.449 I llm_load_print_meta: n_ff             = 8192
0.00.085.449 I llm_load_print_meta: n_expert         = 0
0.00.085.449 I llm_load_print_meta: n_expert_used    = 0
0.00.085.452 I llm_load_print_meta: causal attn      = 1
0.00.085.453 I llm_load_print_meta: pooling type     = 0
0.00.085.454 I llm_load_print_meta: rope type        = 2
0.00.085.454 I llm_load_print_meta: rope scaling     = linear
0.00.085.454 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.455 I llm_load_print_meta: freq_scale_train = 1
0.00.085.455 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.455 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.456 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.456 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.456 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.456 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.456 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.457 I llm_load_print_meta: model type       = 1.4B
0.00.085.468 I llm_load_print_meta: model ftype      = Q4_0
0.00.085.468 I llm_load_print_meta: model params     = 1.41 B
0.00.085.469 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.085.469 I llm_load_print_meta: general.name     = 1.4B
0.00.085.470 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.470 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.470 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.471 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.471 I llm_load_print_meta: LF token         = 128 ''
0.00.085.471 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.473 I llm_load_print_meta: max token length = 1024
0.00.088.524 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.526 I llm_load_tensors: offloading output layer to GPU
0.00.088.526 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.538 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.088.540 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.090.024 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.025 I llama_new_context_with_model: n_ctx         = 2048
0.00.090.026 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.090.026 I llama_new_context_with_model: n_batch       = 2048
0.00.090.026 I llama_new_context_with_model: n_ubatch      = 512
0.00.090.026 I llama_new_context_with_model: flash_attn    = 0
0.00.090.027 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.027 I llama_new_context_with_model: freq_scale    = 1
0.00.090.028 I ggml_metal_init: allocating
0.00.090.032 I ggml_metal_init: found device: Apple M4
0.00.090.035 I ggml_metal_init: picking default device: Apple M4
0.00.091.004 I ggml_metal_init: using embedded metal library
0.00.094.819 I ggml_metal_init: GPU name:   Apple M4
0.00.094.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.823 I ggml_metal_init: simdgroup reduction   = true
0.00.094.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.824 I ggml_metal_init: has bfloat            = true
0.00.094.826 I ggml_metal_init: use bfloat            = true
0.00.094.828 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.153 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.132.082 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.132.090 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.132.116 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.133.238 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.133.240 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.133.241 I llama_new_context_with_model: graph nodes  = 967
0.00.133.241 I llama_new_context_with_model: graph splits = 2
0.00.133.261 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.133.393 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.133.393 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.839.912 I main: llama threadpool init, n_threads = 4
0.00.839.963 I 
0.00.839.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.839.991 I 
0.00.840.229 I sampler seed: 1234
0.00.840.233 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.840.255 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.840.256 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.840.256 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.513.313 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.01.513.313 I llama_perf_context_print:        load time =     823.44 ms
0.01.513.314 I llama_perf_context_print: prompt eval time =      39.84 ms /     7 tokens (    5.69 ms per token,   175.69 tokens per second)
0.01.513.315 I llama_perf_context_print:        eval time =     630.62 ms /    63 runs   (   10.01 ms per token,    99.90 tokens per second)
0.01.513.315 I llama_perf_context_print:       total time =     673.40 ms /    70 tokens
0.01.513.569 I ggml_metal_free: deallocating

real	0m1.535s
user	0m0.131s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.558 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.560 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.561 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.679 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.780 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.782 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.782 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.782 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.783 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.783 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.783 I llama_model_loader: - type  f32:  194 tensors
0.00.024.784 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.784 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.085 I llm_load_vocab: special tokens cache size = 25
0.00.052.011 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.013 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.014 I llm_load_print_meta: arch             = gptneox
0.00.052.014 I llm_load_print_meta: vocab type       = BPE
0.00.052.014 I llm_load_print_meta: n_vocab          = 50304
0.00.052.014 I llm_load_print_meta: n_merges         = 50009
0.00.052.015 I llm_load_print_meta: vocab_only       = 0
0.00.052.015 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.015 I llm_load_print_meta: n_embd           = 2048
0.00.052.015 I llm_load_print_meta: n_layer          = 24
0.00.052.018 I llm_load_print_meta: n_head           = 16
0.00.052.031 I llm_load_print_meta: n_head_kv        = 16
0.00.052.031 I llm_load_print_meta: n_rot            = 32
0.00.052.031 I llm_load_print_meta: n_swa            = 0
0.00.052.031 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.031 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.032 I llm_load_print_meta: n_gqa            = 1
0.00.052.033 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.033 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.034 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.034 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.034 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.035 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.035 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.035 I llm_load_print_meta: n_ff             = 8192
0.00.052.036 I llm_load_print_meta: n_expert         = 0
0.00.052.037 I llm_load_print_meta: n_expert_used    = 0
0.00.052.037 I llm_load_print_meta: causal attn      = 1
0.00.052.037 I llm_load_print_meta: pooling type     = 0
0.00.052.037 I llm_load_print_meta: rope type        = 2
0.00.052.038 I llm_load_print_meta: rope scaling     = linear
0.00.052.038 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.038 I llm_load_print_meta: freq_scale_train = 1
0.00.052.038 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.039 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.039 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.039 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.039 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.039 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.039 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.040 I llm_load_print_meta: model type       = 1.4B
0.00.052.049 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.049 I llm_load_print_meta: model params     = 1.41 B
0.00.052.050 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.050 I llm_load_print_meta: general.name     = 1.4B
0.00.052.051 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.052 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.052 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.052 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.052 I llm_load_print_meta: LF token         = 128 ''
0.00.052.052 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.052 I llm_load_print_meta: max token length = 1024
0.00.054.079 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.080 I llm_load_tensors: offloading output layer to GPU
0.00.054.080 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.091 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.092 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.033 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.033 I llama_new_context_with_model: n_ctx         = 128
0.00.055.033 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.034 I llama_new_context_with_model: n_batch       = 128
0.00.055.034 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.034 I llama_new_context_with_model: flash_attn    = 0
0.00.055.034 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.035 I llama_new_context_with_model: freq_scale    = 1
0.00.055.035 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.035 I ggml_metal_init: allocating
0.00.055.041 I ggml_metal_init: found device: Apple M4
0.00.055.043 I ggml_metal_init: picking default device: Apple M4
0.00.055.590 I ggml_metal_init: using embedded metal library
0.00.057.914 I ggml_metal_init: GPU name:   Apple M4
0.00.057.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.916 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.916 I ggml_metal_init: simdgroup reduction   = true
0.00.057.917 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.917 I ggml_metal_init: has bfloat            = true
0.00.057.917 I ggml_metal_init: use bfloat            = true
0.00.057.917 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.919 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.818 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.047 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.049 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.062 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.924 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.925 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.926 I llama_new_context_with_model: graph nodes  = 967
0.00.069.926 I llama_new_context_with_model: graph splits = 2
0.00.069.939 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.939 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.851 I 
0.00.610.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.908 I perplexity: tokenizing the input ..
0.00.619.240 I perplexity: tokenization took 8.33 ms
0.00.619.244 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.742.167 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.743.421 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.743.438 I llama_perf_context_print:        load time =     601.29 ms
0.00.743.440 I llama_perf_context_print: prompt eval time =     122.70 ms /   128 tokens (    0.96 ms per token,  1043.21 tokens per second)
0.00.743.440 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.441 I llama_perf_context_print:       total time =     132.59 ms /   129 tokens
0.00.743.970 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.080s
sys	0m0.099s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.789 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.237 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.243 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.243 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.243 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.244 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.245 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.245 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.246 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.246 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.247 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.252 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.206 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.078 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.078 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.079 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.080 I llama_model_loader: - type  f32:  194 tensors
0.00.024.080 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.081 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.157 I llm_load_vocab: special tokens cache size = 25
0.00.051.004 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.007 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.007 I llm_load_print_meta: arch             = gptneox
0.00.051.008 I llm_load_print_meta: vocab type       = BPE
0.00.051.008 I llm_load_print_meta: n_vocab          = 50304
0.00.051.008 I llm_load_print_meta: n_merges         = 50009
0.00.051.008 I llm_load_print_meta: vocab_only       = 0
0.00.051.008 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.009 I llm_load_print_meta: n_embd           = 2048
0.00.051.009 I llm_load_print_meta: n_layer          = 24
0.00.051.011 I llm_load_print_meta: n_head           = 16
0.00.051.023 I llm_load_print_meta: n_head_kv        = 16
0.00.051.023 I llm_load_print_meta: n_rot            = 32
0.00.051.024 I llm_load_print_meta: n_swa            = 0
0.00.051.024 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.024 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.025 I llm_load_print_meta: n_gqa            = 1
0.00.051.026 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.026 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.027 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.027 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.028 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.028 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.028 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.029 I llm_load_print_meta: n_ff             = 8192
0.00.051.029 I llm_load_print_meta: n_expert         = 0
0.00.051.029 I llm_load_print_meta: n_expert_used    = 0
0.00.051.029 I llm_load_print_meta: causal attn      = 1
0.00.051.029 I llm_load_print_meta: pooling type     = 0
0.00.051.030 I llm_load_print_meta: rope type        = 2
0.00.051.031 I llm_load_print_meta: rope scaling     = linear
0.00.051.031 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.031 I llm_load_print_meta: freq_scale_train = 1
0.00.051.031 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.032 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.032 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.032 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.032 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.032 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.032 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.033 I llm_load_print_meta: model type       = 1.4B
0.00.051.042 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.042 I llm_load_print_meta: model params     = 1.41 B
0.00.051.043 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.044 I llm_load_print_meta: general.name     = 1.4B
0.00.051.044 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.045 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.045 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.045 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.045 I llm_load_print_meta: LF token         = 128 ''
0.00.051.046 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.046 I llm_load_print_meta: max token length = 1024
0.00.052.689 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.690 I llm_load_tensors: offloading output layer to GPU
0.00.052.690 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.700 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.701 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.554 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.555 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.555 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.555 I llama_new_context_with_model: n_batch       = 2048
0.00.053.555 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.556 I llama_new_context_with_model: flash_attn    = 0
0.00.053.556 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.556 I llama_new_context_with_model: freq_scale    = 1
0.00.053.557 I ggml_metal_init: allocating
0.00.053.564 I ggml_metal_init: found device: Apple M4
0.00.053.566 I ggml_metal_init: picking default device: Apple M4
0.00.054.157 I ggml_metal_init: using embedded metal library
0.00.056.500 I ggml_metal_init: GPU name:   Apple M4
0.00.056.501 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.501 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.502 I ggml_metal_init: simdgroup reduction   = true
0.00.056.502 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.502 I ggml_metal_init: has bfloat            = true
0.00.056.502 I ggml_metal_init: use bfloat            = true
0.00.056.503 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.503 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.967 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.314 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.331 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.360 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.423 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.425 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.426 I llama_new_context_with_model: graph nodes  = 967
0.00.086.426 I llama_new_context_with_model: graph splits = 2
0.00.086.441 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.558 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.559 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.299 I main: llama threadpool init, n_threads = 4
0.00.718.366 I 
0.00.718.400 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.401 I 
0.00.718.654 I sampler seed: 1234
0.00.718.660 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.718.700 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.718.702 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.718.702 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.445.579 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64311.59 tokens per second)
0.01.445.579 I llama_perf_context_print:        load time =     709.50 ms
0.01.445.581 I llama_perf_context_print: prompt eval time =      43.97 ms /     7 tokens (    6.28 ms per token,   159.22 tokens per second)
0.01.445.582 I llama_perf_context_print:        eval time =     680.09 ms /    63 runs   (   10.80 ms per token,    92.64 tokens per second)
0.01.445.582 I llama_perf_context_print:       total time =     727.29 ms /    70 tokens
0.01.445.846 I ggml_metal_free: deallocating

real	0m1.465s
user	0m0.111s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.817 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.501 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.506 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.508 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.509 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.510 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.511 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.513 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.514 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.379 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.380 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.380 I llama_model_loader: - type  f32:  194 tensors
0.00.023.380 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.381 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.554 I llm_load_vocab: special tokens cache size = 25
0.00.049.547 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.550 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.550 I llm_load_print_meta: arch             = gptneox
0.00.049.550 I llm_load_print_meta: vocab type       = BPE
0.00.049.551 I llm_load_print_meta: n_vocab          = 50304
0.00.049.551 I llm_load_print_meta: n_merges         = 50009
0.00.049.551 I llm_load_print_meta: vocab_only       = 0
0.00.049.551 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.551 I llm_load_print_meta: n_embd           = 2048
0.00.049.551 I llm_load_print_meta: n_layer          = 24
0.00.049.561 I llm_load_print_meta: n_head           = 16
0.00.049.569 I llm_load_print_meta: n_head_kv        = 16
0.00.049.570 I llm_load_print_meta: n_rot            = 32
0.00.049.570 I llm_load_print_meta: n_swa            = 0
0.00.049.570 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.570 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.571 I llm_load_print_meta: n_gqa            = 1
0.00.049.572 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.573 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.573 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.576 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.576 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.576 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.576 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.577 I llm_load_print_meta: n_ff             = 8192
0.00.049.577 I llm_load_print_meta: n_expert         = 0
0.00.049.577 I llm_load_print_meta: n_expert_used    = 0
0.00.049.577 I llm_load_print_meta: causal attn      = 1
0.00.049.577 I llm_load_print_meta: pooling type     = 0
0.00.049.577 I llm_load_print_meta: rope type        = 2
0.00.049.578 I llm_load_print_meta: rope scaling     = linear
0.00.049.579 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.580 I llm_load_print_meta: freq_scale_train = 1
0.00.049.580 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.581 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.581 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.581 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.581 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.581 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.581 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.582 I llm_load_print_meta: model type       = 1.4B
0.00.049.586 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.587 I llm_load_print_meta: model params     = 1.41 B
0.00.049.587 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.587 I llm_load_print_meta: general.name     = 1.4B
0.00.049.588 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.588 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.588 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.588 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.589 I llm_load_print_meta: LF token         = 128 ''
0.00.049.589 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.590 I llm_load_print_meta: max token length = 1024
0.00.051.346 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.347 I llm_load_tensors: offloading output layer to GPU
0.00.051.347 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.352 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.352 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.328 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.329 I llama_new_context_with_model: n_ctx         = 128
0.00.052.329 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.329 I llama_new_context_with_model: n_batch       = 128
0.00.052.330 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.330 I llama_new_context_with_model: flash_attn    = 0
0.00.052.330 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.330 I llama_new_context_with_model: freq_scale    = 1
0.00.052.331 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.331 I ggml_metal_init: allocating
0.00.052.339 I ggml_metal_init: found device: Apple M4
0.00.052.344 I ggml_metal_init: picking default device: Apple M4
0.00.052.915 I ggml_metal_init: using embedded metal library
0.00.055.224 I ggml_metal_init: GPU name:   Apple M4
0.00.055.225 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.226 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.226 I ggml_metal_init: simdgroup reduction   = true
0.00.055.226 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.226 I ggml_metal_init: has bfloat            = true
0.00.055.227 I ggml_metal_init: use bfloat            = true
0.00.055.227 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.777 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.021 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.023 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.037 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.934 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.935 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.935 I llama_new_context_with_model: graph nodes  = 967
0.00.066.935 I llama_new_context_with_model: graph splits = 2
0.00.066.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.949 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.111 I 
0.00.662.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.195 I perplexity: tokenizing the input ..
0.00.669.983 I perplexity: tokenization took 7.787 ms
0.00.669.991 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.756 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.793.919 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.793.937 I llama_perf_context_print:        load time =     653.29 ms
0.00.793.938 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.57 tokens per second)
0.00.793.939 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.940 I llama_perf_context_print:       total time =     131.83 ms /   129 tokens
0.00.794.360 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.078s
sys	0m0.108s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.423 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.026 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.031 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.032 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.033 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.033 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.035 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.036 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.036 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.037 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.037 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.038 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.038 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.039 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.051 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.159 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.144 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.144 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.145 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.145 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.145 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.146 I llama_model_loader: - type  f32:  194 tensors
0.00.025.146 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.146 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.356 I llm_load_vocab: special tokens cache size = 25
0.00.052.226 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.229 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.230 I llm_load_print_meta: arch             = gptneox
0.00.052.230 I llm_load_print_meta: vocab type       = BPE
0.00.052.230 I llm_load_print_meta: n_vocab          = 50304
0.00.052.230 I llm_load_print_meta: n_merges         = 50009
0.00.052.231 I llm_load_print_meta: vocab_only       = 0
0.00.052.231 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.231 I llm_load_print_meta: n_embd           = 2048
0.00.052.231 I llm_load_print_meta: n_layer          = 24
0.00.052.233 I llm_load_print_meta: n_head           = 16
0.00.052.246 I llm_load_print_meta: n_head_kv        = 16
0.00.052.246 I llm_load_print_meta: n_rot            = 32
0.00.052.246 I llm_load_print_meta: n_swa            = 0
0.00.052.247 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.247 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.247 I llm_load_print_meta: n_gqa            = 1
0.00.052.248 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.249 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.249 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.250 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.250 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.250 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.250 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.251 I llm_load_print_meta: n_ff             = 8192
0.00.052.251 I llm_load_print_meta: n_expert         = 0
0.00.052.251 I llm_load_print_meta: n_expert_used    = 0
0.00.052.252 I llm_load_print_meta: causal attn      = 1
0.00.052.254 I llm_load_print_meta: pooling type     = 0
0.00.052.254 I llm_load_print_meta: rope type        = 2
0.00.052.254 I llm_load_print_meta: rope scaling     = linear
0.00.052.255 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.255 I llm_load_print_meta: freq_scale_train = 1
0.00.052.255 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.256 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.256 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.256 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.256 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.256 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.256 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.257 I llm_load_print_meta: model type       = 1.4B
0.00.052.266 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.267 I llm_load_print_meta: model params     = 1.41 B
0.00.052.267 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.267 I llm_load_print_meta: general.name     = 1.4B
0.00.052.267 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.269 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.269 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.269 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.269 I llm_load_print_meta: LF token         = 128 ''
0.00.052.269 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.269 I llm_load_print_meta: max token length = 1024
0.00.054.318 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.319 I llm_load_tensors: offloading output layer to GPU
0.00.054.319 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.329 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.330 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.250 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.250 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.251 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.251 I llama_new_context_with_model: n_batch       = 2048
0.00.055.251 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.251 I llama_new_context_with_model: flash_attn    = 0
0.00.055.251 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.252 I llama_new_context_with_model: freq_scale    = 1
0.00.055.252 I ggml_metal_init: allocating
0.00.055.256 I ggml_metal_init: found device: Apple M4
0.00.055.258 I ggml_metal_init: picking default device: Apple M4
0.00.055.882 I ggml_metal_init: using embedded metal library
0.00.058.268 I ggml_metal_init: GPU name:   Apple M4
0.00.058.269 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.270 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.270 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.270 I ggml_metal_init: simdgroup reduction   = true
0.00.058.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.271 I ggml_metal_init: has bfloat            = true
0.00.058.271 I ggml_metal_init: use bfloat            = true
0.00.058.271 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.205 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.149 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.154 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.171 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.144 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.145 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.146 I llama_new_context_with_model: graph nodes  = 967
0.00.089.146 I llama_new_context_with_model: graph splits = 2
0.00.089.157 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.285 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.621 I main: llama threadpool init, n_threads = 4
0.00.747.663 I 
0.00.747.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.705 I 
0.00.747.944 I sampler seed: 1234
0.00.747.949 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.960 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.960 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.960 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.538.344 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.538.344 I llama_perf_context_print:        load time =     738.19 ms
0.01.538.345 I llama_perf_context_print: prompt eval time =      43.09 ms /     7 tokens (    6.16 ms per token,   162.45 tokens per second)
0.01.538.346 I llama_perf_context_print:        eval time =     744.31 ms /    63 runs   (   11.81 ms per token,    84.64 tokens per second)
0.01.538.346 I llama_perf_context_print:       total time =     790.73 ms /    70 tokens
0.01.538.573 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.449 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.438 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.445 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.446 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.446 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.446 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.447 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.447 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.448 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.448 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.448 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.449 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.454 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.358 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.414 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.420 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.420 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.420 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.421 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.421 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.421 I llama_model_loader: - type  f32:  194 tensors
0.00.024.422 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.422 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.495 I llm_load_vocab: special tokens cache size = 25
0.00.051.437 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.440 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.440 I llm_load_print_meta: arch             = gptneox
0.00.051.441 I llm_load_print_meta: vocab type       = BPE
0.00.051.441 I llm_load_print_meta: n_vocab          = 50304
0.00.051.441 I llm_load_print_meta: n_merges         = 50009
0.00.051.441 I llm_load_print_meta: vocab_only       = 0
0.00.051.442 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.442 I llm_load_print_meta: n_embd           = 2048
0.00.051.442 I llm_load_print_meta: n_layer          = 24
0.00.051.445 I llm_load_print_meta: n_head           = 16
0.00.051.457 I llm_load_print_meta: n_head_kv        = 16
0.00.051.458 I llm_load_print_meta: n_rot            = 32
0.00.051.458 I llm_load_print_meta: n_swa            = 0
0.00.051.458 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.458 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.459 I llm_load_print_meta: n_gqa            = 1
0.00.051.460 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.460 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.461 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.461 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.462 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.462 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.462 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.463 I llm_load_print_meta: n_ff             = 8192
0.00.051.463 I llm_load_print_meta: n_expert         = 0
0.00.051.463 I llm_load_print_meta: n_expert_used    = 0
0.00.051.463 I llm_load_print_meta: causal attn      = 1
0.00.051.463 I llm_load_print_meta: pooling type     = 0
0.00.051.463 I llm_load_print_meta: rope type        = 2
0.00.051.463 I llm_load_print_meta: rope scaling     = linear
0.00.051.464 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.464 I llm_load_print_meta: freq_scale_train = 1
0.00.051.464 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.465 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.465 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.465 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.465 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.467 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.467 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.467 I llm_load_print_meta: model type       = 1.4B
0.00.051.477 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.477 I llm_load_print_meta: model params     = 1.41 B
0.00.051.478 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.478 I llm_load_print_meta: general.name     = 1.4B
0.00.051.478 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: LF token         = 128 ''
0.00.051.479 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: max token length = 1024
0.00.053.539 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.540 I llm_load_tensors: offloading output layer to GPU
0.00.053.540 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.550 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.551 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.452 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.453 I llama_new_context_with_model: n_ctx         = 128
0.00.054.453 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.453 I llama_new_context_with_model: n_batch       = 128
0.00.054.454 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.454 I llama_new_context_with_model: flash_attn    = 0
0.00.054.454 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.455 I llama_new_context_with_model: freq_scale    = 1
0.00.054.455 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.456 I ggml_metal_init: allocating
0.00.054.462 I ggml_metal_init: found device: Apple M4
0.00.054.464 I ggml_metal_init: picking default device: Apple M4
0.00.055.026 I ggml_metal_init: using embedded metal library
0.00.057.352 I ggml_metal_init: GPU name:   Apple M4
0.00.057.354 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.354 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.355 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.355 I ggml_metal_init: simdgroup reduction   = true
0.00.057.355 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.355 I ggml_metal_init: has bfloat            = true
0.00.057.355 I ggml_metal_init: use bfloat            = true
0.00.057.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.723 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.144 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.147 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.160 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.023 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.024 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.024 I llama_new_context_with_model: graph nodes  = 967
0.00.069.024 I llama_new_context_with_model: graph splits = 2
0.00.069.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.037 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.566 I 
0.00.734.612 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.625 I perplexity: tokenizing the input ..
0.00.742.825 I perplexity: tokenization took 8.199 ms
0.00.742.829 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.557 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.878.719 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.878.733 I llama_perf_context_print:        load time =     725.11 ms
0.00.878.733 I llama_perf_context_print: prompt eval time =     134.50 ms /   128 tokens (    1.05 ms per token,   951.67 tokens per second)
0.00.878.734 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.878.735 I llama_perf_context_print:       total time =     144.17 ms /   129 tokens
0.00.879.217 I ggml_metal_free: deallocating

real	0m0.894s
user	0m0.079s
sys	0m0.126s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.660 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.209 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.215 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.215 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.216 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.216 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.218 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.219 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.224 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.218 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.133 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.134 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.134 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.134 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.135 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.136 I llama_model_loader: - type  f32:  194 tensors
0.00.024.136 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.136 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.408 I llm_load_vocab: special tokens cache size = 25
0.00.050.293 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.296 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.296 I llm_load_print_meta: arch             = gptneox
0.00.050.296 I llm_load_print_meta: vocab type       = BPE
0.00.050.297 I llm_load_print_meta: n_vocab          = 50304
0.00.050.297 I llm_load_print_meta: n_merges         = 50009
0.00.050.297 I llm_load_print_meta: vocab_only       = 0
0.00.050.297 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.297 I llm_load_print_meta: n_embd           = 2048
0.00.050.297 I llm_load_print_meta: n_layer          = 24
0.00.050.301 I llm_load_print_meta: n_head           = 16
0.00.050.313 I llm_load_print_meta: n_head_kv        = 16
0.00.050.314 I llm_load_print_meta: n_rot            = 32
0.00.050.314 I llm_load_print_meta: n_swa            = 0
0.00.050.314 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.315 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.315 I llm_load_print_meta: n_gqa            = 1
0.00.050.316 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.317 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.317 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.318 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.318 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.319 I llm_load_print_meta: n_ff             = 8192
0.00.050.319 I llm_load_print_meta: n_expert         = 0
0.00.050.319 I llm_load_print_meta: n_expert_used    = 0
0.00.050.320 I llm_load_print_meta: causal attn      = 1
0.00.050.323 I llm_load_print_meta: pooling type     = 0
0.00.050.323 I llm_load_print_meta: rope type        = 2
0.00.050.323 I llm_load_print_meta: rope scaling     = linear
0.00.050.323 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.324 I llm_load_print_meta: freq_scale_train = 1
0.00.050.324 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.324 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.324 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.324 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.325 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.325 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.325 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.325 I llm_load_print_meta: model type       = 1.4B
0.00.050.335 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.335 I llm_load_print_meta: model params     = 1.41 B
0.00.050.336 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.336 I llm_load_print_meta: general.name     = 1.4B
0.00.050.337 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.338 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.338 I llm_load_print_meta: LF token         = 128 ''
0.00.050.338 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.338 I llm_load_print_meta: max token length = 1024
0.00.052.302 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.303 I llm_load_tensors: offloading output layer to GPU
0.00.052.303 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.314 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.315 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.196 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.197 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.197 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.197 I llama_new_context_with_model: n_batch       = 2048
0.00.053.197 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.198 I llama_new_context_with_model: flash_attn    = 0
0.00.053.198 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.198 I llama_new_context_with_model: freq_scale    = 1
0.00.053.199 I ggml_metal_init: allocating
0.00.053.202 I ggml_metal_init: found device: Apple M4
0.00.053.204 I ggml_metal_init: picking default device: Apple M4
0.00.053.805 I ggml_metal_init: using embedded metal library
0.00.056.175 I ggml_metal_init: GPU name:   Apple M4
0.00.056.177 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.177 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.178 I ggml_metal_init: simdgroup reduction   = true
0.00.056.179 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.179 I ggml_metal_init: has bfloat            = true
0.00.056.179 I ggml_metal_init: use bfloat            = true
0.00.056.180 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.907 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.281 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.287 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.306 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.350 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.351 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.352 I llama_new_context_with_model: graph nodes  = 967
0.00.086.352 I llama_new_context_with_model: graph splits = 2
0.00.086.368 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.512 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.512 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.992 I main: llama threadpool init, n_threads = 4
0.00.717.032 I 
0.00.717.085 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.087 I 
0.00.717.327 I sampler seed: 1234
0.00.717.331 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.365 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.385 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.386 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.553.936 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.553.937 I llama_perf_context_print:        load time =     708.33 ms
0.01.553.937 I llama_perf_context_print: prompt eval time =      42.30 ms /     7 tokens (    6.04 ms per token,   165.47 tokens per second)
0.01.553.938 I llama_perf_context_print:        eval time =     791.16 ms /    63 runs   (   12.56 ms per token,    79.63 tokens per second)
0.01.553.942 I llama_perf_context_print:       total time =     836.95 ms /    70 tokens
0.01.554.143 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.110s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.866 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.603 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.609 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.610 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.610 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.610 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.610 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.611 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.612 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.612 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.612 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.613 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.613 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.616 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.618 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.618 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.552 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.608 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.587 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.588 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.590 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.590 I llama_model_loader: - type  f32:  194 tensors
0.00.023.591 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.591 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.923 I llm_load_vocab: special tokens cache size = 25
0.00.049.907 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.909 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.910 I llm_load_print_meta: arch             = gptneox
0.00.049.910 I llm_load_print_meta: vocab type       = BPE
0.00.049.910 I llm_load_print_meta: n_vocab          = 50304
0.00.049.910 I llm_load_print_meta: n_merges         = 50009
0.00.049.910 I llm_load_print_meta: vocab_only       = 0
0.00.049.911 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.911 I llm_load_print_meta: n_embd           = 2048
0.00.049.911 I llm_load_print_meta: n_layer          = 24
0.00.049.914 I llm_load_print_meta: n_head           = 16
0.00.049.926 I llm_load_print_meta: n_head_kv        = 16
0.00.049.926 I llm_load_print_meta: n_rot            = 32
0.00.049.928 I llm_load_print_meta: n_swa            = 0
0.00.049.929 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.929 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.930 I llm_load_print_meta: n_gqa            = 1
0.00.049.930 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.931 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.932 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.932 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.932 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.932 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.932 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.933 I llm_load_print_meta: n_ff             = 8192
0.00.049.933 I llm_load_print_meta: n_expert         = 0
0.00.049.933 I llm_load_print_meta: n_expert_used    = 0
0.00.049.933 I llm_load_print_meta: causal attn      = 1
0.00.049.933 I llm_load_print_meta: pooling type     = 0
0.00.049.933 I llm_load_print_meta: rope type        = 2
0.00.049.934 I llm_load_print_meta: rope scaling     = linear
0.00.049.935 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.935 I llm_load_print_meta: freq_scale_train = 1
0.00.049.935 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.936 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.936 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.936 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.936 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.936 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.936 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.937 I llm_load_print_meta: model type       = 1.4B
0.00.049.946 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.947 I llm_load_print_meta: model params     = 1.41 B
0.00.049.948 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.948 I llm_load_print_meta: general.name     = 1.4B
0.00.049.948 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.948 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.949 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.949 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.952 I llm_load_print_meta: LF token         = 128 ''
0.00.049.952 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.952 I llm_load_print_meta: max token length = 1024
0.00.051.984 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.985 I llm_load_tensors: offloading output layer to GPU
0.00.051.985 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.995 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.996 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.878 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.878 I llama_new_context_with_model: n_ctx         = 128
0.00.052.879 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.879 I llama_new_context_with_model: n_batch       = 128
0.00.052.879 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.879 I llama_new_context_with_model: flash_attn    = 0
0.00.052.880 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.880 I llama_new_context_with_model: freq_scale    = 1
0.00.052.880 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.881 I ggml_metal_init: allocating
0.00.052.884 I ggml_metal_init: found device: Apple M4
0.00.052.886 I ggml_metal_init: picking default device: Apple M4
0.00.053.455 I ggml_metal_init: using embedded metal library
0.00.055.782 I ggml_metal_init: GPU name:   Apple M4
0.00.055.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.784 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.784 I ggml_metal_init: simdgroup reduction   = true
0.00.055.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.784 I ggml_metal_init: has bfloat            = true
0.00.055.785 I ggml_metal_init: use bfloat            = true
0.00.055.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.786 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.360 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.722 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.726 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.742 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.592 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.593 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.594 I llama_new_context_with_model: graph nodes  = 967
0.00.067.594 I llama_new_context_with_model: graph splits = 2
0.00.067.606 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.048 I 
0.00.660.090 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.103 I perplexity: tokenizing the input ..
0.00.668.252 I perplexity: tokenization took 8.148 ms
0.00.668.258 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.350 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.804.596 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.804.624 I llama_perf_context_print:        load time =     651.18 ms
0.00.804.624 I llama_perf_context_print: prompt eval time =     134.87 ms /   128 tokens (    1.05 ms per token,   949.08 tokens per second)
0.00.804.625 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.625 I llama_perf_context_print:       total time =     144.58 ms /   129 tokens
0.00.805.104 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.078s
sys	0m0.119s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.873 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.382 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.387 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.389 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.390 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.390 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.391 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.391 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.392 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.392 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.393 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.396 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.283 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.360 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.250 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.251 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.251 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.252 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.252 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.252 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.253 I llama_model_loader: - type  f32:  194 tensors
0.00.024.253 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.253 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.254 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.582 I llm_load_vocab: special tokens cache size = 25
0.00.050.450 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.454 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.454 I llm_load_print_meta: arch             = gptneox
0.00.050.454 I llm_load_print_meta: vocab type       = BPE
0.00.050.455 I llm_load_print_meta: n_vocab          = 50304
0.00.050.455 I llm_load_print_meta: n_merges         = 50009
0.00.050.455 I llm_load_print_meta: vocab_only       = 0
0.00.050.455 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.455 I llm_load_print_meta: n_embd           = 2048
0.00.050.455 I llm_load_print_meta: n_layer          = 24
0.00.050.459 I llm_load_print_meta: n_head           = 16
0.00.050.467 I llm_load_print_meta: n_head_kv        = 16
0.00.050.467 I llm_load_print_meta: n_rot            = 32
0.00.050.467 I llm_load_print_meta: n_swa            = 0
0.00.050.467 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.467 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.468 I llm_load_print_meta: n_gqa            = 1
0.00.050.469 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.470 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.470 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.473 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.473 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.473 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.473 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.474 I llm_load_print_meta: n_ff             = 8192
0.00.050.474 I llm_load_print_meta: n_expert         = 0
0.00.050.474 I llm_load_print_meta: n_expert_used    = 0
0.00.050.475 I llm_load_print_meta: causal attn      = 1
0.00.050.475 I llm_load_print_meta: pooling type     = 0
0.00.050.475 I llm_load_print_meta: rope type        = 2
0.00.050.475 I llm_load_print_meta: rope scaling     = linear
0.00.050.475 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.477 I llm_load_print_meta: freq_scale_train = 1
0.00.050.477 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.477 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.478 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.478 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.478 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.478 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.478 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.479 I llm_load_print_meta: model type       = 1.4B
0.00.050.483 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.483 I llm_load_print_meta: model params     = 1.41 B
0.00.050.484 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.485 I llm_load_print_meta: general.name     = 1.4B
0.00.050.485 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.485 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.485 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.485 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.486 I llm_load_print_meta: LF token         = 128 ''
0.00.050.486 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.486 I llm_load_print_meta: max token length = 1024
0.00.052.238 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.239 I llm_load_tensors: offloading output layer to GPU
0.00.052.239 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.245 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.245 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.158 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.158 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.159 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.159 I llama_new_context_with_model: n_batch       = 2048
0.00.053.159 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.159 I llama_new_context_with_model: flash_attn    = 0
0.00.053.160 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.160 I llama_new_context_with_model: freq_scale    = 1
0.00.053.160 I ggml_metal_init: allocating
0.00.053.163 I ggml_metal_init: found device: Apple M4
0.00.053.166 I ggml_metal_init: picking default device: Apple M4
0.00.053.780 I ggml_metal_init: using embedded metal library
0.00.056.098 I ggml_metal_init: GPU name:   Apple M4
0.00.056.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.101 I ggml_metal_init: simdgroup reduction   = true
0.00.056.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.101 I ggml_metal_init: has bfloat            = true
0.00.056.101 I ggml_metal_init: use bfloat            = true
0.00.056.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.857 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.200 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.207 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.226 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.270 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.272 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.272 I llama_new_context_with_model: graph nodes  = 967
0.00.087.272 I llama_new_context_with_model: graph splits = 2
0.00.087.282 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.430 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.444.123 I main: llama threadpool init, n_threads = 4
0.00.444.170 I 
0.00.444.203 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.444.204 I 
0.00.444.437 I sampler seed: 1234
0.00.444.443 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.444.480 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.444.484 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.444.484 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.126.285 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.126.285 I llama_perf_context_print:        load time =     434.24 ms
0.01.126.286 I llama_perf_context_print: prompt eval time =      39.68 ms /     7 tokens (    5.67 ms per token,   176.40 tokens per second)
0.01.126.287 I llama_perf_context_print:        eval time =     639.24 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.126.288 I llama_perf_context_print:       total time =     682.17 ms /    70 tokens
0.01.126.574 I ggml_metal_free: deallocating

real	0m1.144s
user	0m0.110s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.069 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.594 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.598 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.600 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.601 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.601 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.601 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.601 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.602 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.603 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.603 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.604 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.604 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.605 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.608 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.608 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.609 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.449 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.520 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.434 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.436 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.436 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.437 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.437 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.437 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.438 I llama_model_loader: - type  f32:  194 tensors
0.00.024.438 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.438 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.438 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.754 I llm_load_vocab: special tokens cache size = 25
0.00.050.615 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.618 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.619 I llm_load_print_meta: arch             = gptneox
0.00.050.619 I llm_load_print_meta: vocab type       = BPE
0.00.050.619 I llm_load_print_meta: n_vocab          = 50304
0.00.050.619 I llm_load_print_meta: n_merges         = 50009
0.00.050.620 I llm_load_print_meta: vocab_only       = 0
0.00.050.620 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.620 I llm_load_print_meta: n_embd           = 2048
0.00.050.620 I llm_load_print_meta: n_layer          = 24
0.00.050.623 I llm_load_print_meta: n_head           = 16
0.00.050.635 I llm_load_print_meta: n_head_kv        = 16
0.00.050.635 I llm_load_print_meta: n_rot            = 32
0.00.050.636 I llm_load_print_meta: n_swa            = 0
0.00.050.636 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.637 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.639 I llm_load_print_meta: n_gqa            = 1
0.00.050.640 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.641 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.641 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.641 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.642 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.642 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.642 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.642 I llm_load_print_meta: n_ff             = 8192
0.00.050.643 I llm_load_print_meta: n_expert         = 0
0.00.050.643 I llm_load_print_meta: n_expert_used    = 0
0.00.050.643 I llm_load_print_meta: causal attn      = 1
0.00.050.643 I llm_load_print_meta: pooling type     = 0
0.00.050.643 I llm_load_print_meta: rope type        = 2
0.00.050.643 I llm_load_print_meta: rope scaling     = linear
0.00.050.644 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.644 I llm_load_print_meta: freq_scale_train = 1
0.00.050.644 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.644 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.645 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.646 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.646 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.646 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.646 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.646 I llm_load_print_meta: model type       = 1.4B
0.00.050.656 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.656 I llm_load_print_meta: model params     = 1.41 B
0.00.050.657 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.657 I llm_load_print_meta: general.name     = 1.4B
0.00.050.657 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.658 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.658 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.658 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.658 I llm_load_print_meta: LF token         = 128 ''
0.00.050.659 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.659 I llm_load_print_meta: max token length = 1024
0.00.052.562 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.563 I llm_load_tensors: offloading output layer to GPU
0.00.052.563 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.574 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.575 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.534 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.535 I llama_new_context_with_model: n_ctx         = 128
0.00.053.535 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.535 I llama_new_context_with_model: n_batch       = 128
0.00.053.535 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.535 I llama_new_context_with_model: flash_attn    = 0
0.00.053.536 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.536 I llama_new_context_with_model: freq_scale    = 1
0.00.053.536 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.537 I ggml_metal_init: allocating
0.00.053.540 I ggml_metal_init: found device: Apple M4
0.00.053.542 I ggml_metal_init: picking default device: Apple M4
0.00.054.100 I ggml_metal_init: using embedded metal library
0.00.056.419 I ggml_metal_init: GPU name:   Apple M4
0.00.056.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.421 I ggml_metal_init: simdgroup reduction   = true
0.00.056.421 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.421 I ggml_metal_init: has bfloat            = true
0.00.056.422 I ggml_metal_init: use bfloat            = true
0.00.056.422 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.423 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.180 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.521 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.530 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.548 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.436 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.437 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.437 I llama_new_context_with_model: graph nodes  = 967
0.00.068.438 I llama_new_context_with_model: graph splits = 2
0.00.068.450 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.451 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.391.527 I 
0.00.391.612 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.391.635 I perplexity: tokenizing the input ..
0.00.399.269 I perplexity: tokenization took 7.633 ms
0.00.399.272 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.531.586 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.532.773 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.532.787 I llama_perf_context_print:        load time =     381.45 ms
0.00.532.788 I llama_perf_context_print: prompt eval time =     132.09 ms /   128 tokens (    1.03 ms per token,   969.05 tokens per second)
0.00.532.788 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.532.789 I llama_perf_context_print:       total time =     141.26 ms /   129 tokens
0.00.533.261 I ggml_metal_free: deallocating

real	0m0.548s
user	0m0.078s
sys	0m0.077s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.814 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.227 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.228 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.231 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.231 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.234 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.218 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.219 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.219 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.220 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.220 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.220 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.221 I llama_model_loader: - type  f32:  194 tensors
0.00.025.222 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.222 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.222 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.222 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.814 I llm_load_vocab: special tokens cache size = 25
0.00.052.966 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.971 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.972 I llm_load_print_meta: arch             = gptneox
0.00.052.972 I llm_load_print_meta: vocab type       = BPE
0.00.052.972 I llm_load_print_meta: n_vocab          = 50304
0.00.052.972 I llm_load_print_meta: n_merges         = 50009
0.00.052.973 I llm_load_print_meta: vocab_only       = 0
0.00.052.975 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.975 I llm_load_print_meta: n_embd           = 2048
0.00.052.975 I llm_load_print_meta: n_layer          = 24
0.00.052.980 I llm_load_print_meta: n_head           = 16
0.00.052.993 I llm_load_print_meta: n_head_kv        = 16
0.00.052.994 I llm_load_print_meta: n_rot            = 32
0.00.052.994 I llm_load_print_meta: n_swa            = 0
0.00.052.994 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.994 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.995 I llm_load_print_meta: n_gqa            = 1
0.00.052.995 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.996 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.997 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.997 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.997 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.997 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.997 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.998 I llm_load_print_meta: n_ff             = 8192
0.00.052.998 I llm_load_print_meta: n_expert         = 0
0.00.052.998 I llm_load_print_meta: n_expert_used    = 0
0.00.052.998 I llm_load_print_meta: causal attn      = 1
0.00.052.998 I llm_load_print_meta: pooling type     = 0
0.00.052.998 I llm_load_print_meta: rope type        = 2
0.00.052.999 I llm_load_print_meta: rope scaling     = linear
0.00.052.999 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.999 I llm_load_print_meta: freq_scale_train = 1
0.00.052.999 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.999 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.000 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.000 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.000 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.000 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.000 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.001 I llm_load_print_meta: model type       = 1.4B
0.00.053.011 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.011 I llm_load_print_meta: model params     = 1.41 B
0.00.053.011 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.012 I llm_load_print_meta: general.name     = 1.4B
0.00.053.012 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.012 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.012 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.012 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.012 I llm_load_print_meta: LF token         = 128 ''
0.00.053.013 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.013 I llm_load_print_meta: max token length = 1024
0.00.054.948 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.949 I llm_load_tensors: offloading output layer to GPU
0.00.054.949 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.960 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.962 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.856 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.857 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.857 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.857 I llama_new_context_with_model: n_batch       = 2048
0.00.055.858 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.858 I llama_new_context_with_model: flash_attn    = 0
0.00.055.858 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.859 I llama_new_context_with_model: freq_scale    = 1
0.00.055.859 I ggml_metal_init: allocating
0.00.055.866 I ggml_metal_init: found device: Apple M4
0.00.055.868 I ggml_metal_init: picking default device: Apple M4
0.00.056.520 I ggml_metal_init: using embedded metal library
0.00.058.925 I ggml_metal_init: GPU name:   Apple M4
0.00.058.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.928 I ggml_metal_init: simdgroup reduction   = true
0.00.058.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.928 I ggml_metal_init: has bfloat            = true
0.00.058.928 I ggml_metal_init: use bfloat            = true
0.00.058.928 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.929 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.933 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.210 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.217 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.236 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.224 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.225 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.225 I llama_new_context_with_model: graph nodes  = 967
0.00.088.226 I llama_new_context_with_model: graph splits = 2
0.00.088.241 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.735 I main: llama threadpool init, n_threads = 4
0.00.506.776 I 
0.00.506.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.808 I 
0.00.506.951 I sampler seed: 1234
0.00.506.956 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.506.965 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.506.966 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.506.966 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.253.251 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.253.252 I llama_perf_context_print:        load time =     497.91 ms
0.01.253.253 I llama_perf_context_print: prompt eval time =      40.45 ms /     7 tokens (    5.78 ms per token,   173.05 tokens per second)
0.01.253.254 I llama_perf_context_print:        eval time =     702.89 ms /    63 runs   (   11.16 ms per token,    89.63 tokens per second)
0.01.253.254 I llama_perf_context_print:       total time =     746.52 ms /    70 tokens
0.01.253.542 I ggml_metal_free: deallocating

real	0m1.276s
user	0m0.109s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.815 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.715 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.716 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.717 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.717 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.718 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.719 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.719 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.720 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.720 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.721 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.723 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.723 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.723 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.446 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.446 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.447 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.448 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.448 I llama_model_loader: - type  f32:  194 tensors
0.00.023.448 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.449 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.449 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.449 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.597 I llm_load_vocab: special tokens cache size = 25
0.00.049.428 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.431 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.431 I llm_load_print_meta: arch             = gptneox
0.00.049.431 I llm_load_print_meta: vocab type       = BPE
0.00.049.432 I llm_load_print_meta: n_vocab          = 50304
0.00.049.432 I llm_load_print_meta: n_merges         = 50009
0.00.049.432 I llm_load_print_meta: vocab_only       = 0
0.00.049.432 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.432 I llm_load_print_meta: n_embd           = 2048
0.00.049.432 I llm_load_print_meta: n_layer          = 24
0.00.049.435 I llm_load_print_meta: n_head           = 16
0.00.049.447 I llm_load_print_meta: n_head_kv        = 16
0.00.049.448 I llm_load_print_meta: n_rot            = 32
0.00.049.449 I llm_load_print_meta: n_swa            = 0
0.00.049.449 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.449 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.452 I llm_load_print_meta: n_gqa            = 1
0.00.049.453 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.453 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.454 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.454 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.454 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.454 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.454 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.455 I llm_load_print_meta: n_ff             = 8192
0.00.049.455 I llm_load_print_meta: n_expert         = 0
0.00.049.455 I llm_load_print_meta: n_expert_used    = 0
0.00.049.455 I llm_load_print_meta: causal attn      = 1
0.00.049.456 I llm_load_print_meta: pooling type     = 0
0.00.049.456 I llm_load_print_meta: rope type        = 2
0.00.049.456 I llm_load_print_meta: rope scaling     = linear
0.00.049.456 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.456 I llm_load_print_meta: freq_scale_train = 1
0.00.049.457 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.457 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.457 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.457 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.457 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.458 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.458 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.459 I llm_load_print_meta: model type       = 1.4B
0.00.049.468 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.469 I llm_load_print_meta: model params     = 1.41 B
0.00.049.469 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.469 I llm_load_print_meta: general.name     = 1.4B
0.00.049.469 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.470 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.470 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.470 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.470 I llm_load_print_meta: LF token         = 128 ''
0.00.049.471 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.472 I llm_load_print_meta: max token length = 1024
0.00.051.414 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.415 I llm_load_tensors: offloading output layer to GPU
0.00.051.415 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.425 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.426 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.303 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.304 I llama_new_context_with_model: n_ctx         = 128
0.00.052.304 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.304 I llama_new_context_with_model: n_batch       = 128
0.00.052.304 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.304 I llama_new_context_with_model: flash_attn    = 0
0.00.052.305 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.305 I llama_new_context_with_model: freq_scale    = 1
0.00.052.306 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.306 I ggml_metal_init: allocating
0.00.052.309 I ggml_metal_init: found device: Apple M4
0.00.052.311 I ggml_metal_init: picking default device: Apple M4
0.00.052.878 I ggml_metal_init: using embedded metal library
0.00.055.209 I ggml_metal_init: GPU name:   Apple M4
0.00.055.210 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.210 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.211 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.211 I ggml_metal_init: simdgroup reduction   = true
0.00.055.211 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.211 I ggml_metal_init: has bfloat            = true
0.00.055.211 I ggml_metal_init: use bfloat            = true
0.00.055.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.825 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.060 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.064 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.079 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.994 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.995 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.995 I llama_new_context_with_model: graph nodes  = 967
0.00.066.995 I llama_new_context_with_model: graph splits = 2
0.00.067.008 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.009 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.482.213 I 
0.00.482.251 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.482.265 I perplexity: tokenizing the input ..
0.00.490.187 I perplexity: tokenization took 7.92 ms
0.00.490.190 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.622.759 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.624.024 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.624.040 I llama_perf_context_print:        load time =     473.39 ms
0.00.624.041 I llama_perf_context_print: prompt eval time =     132.34 ms /   128 tokens (    1.03 ms per token,   967.18 tokens per second)
0.00.624.041 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.624.042 I llama_perf_context_print:       total time =     141.83 ms /   129 tokens
0.00.624.545 I ggml_metal_free: deallocating

real	0m0.639s
user	0m0.077s
sys	0m0.091s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.011.768 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.100 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.026.104 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.106 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.106 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.107 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.107 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.107 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.108 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.108 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.109 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.109 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.109 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.110 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.110 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.113 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.113 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.204 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.357 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.358 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.359 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.359 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.359 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.035.360 I llama_model_loader: - type  f32:  194 tensors
0.00.035.360 I llama_model_loader: - type q4_K:   61 tensors
0.00.035.360 I llama_model_loader: - type q5_K:   24 tensors
0.00.035.360 I llama_model_loader: - type q6_K:   13 tensors
0.00.060.722 I llm_load_vocab: special tokens cache size = 25
0.00.067.393 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.396 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.396 I llm_load_print_meta: arch             = gptneox
0.00.067.397 I llm_load_print_meta: vocab type       = BPE
0.00.067.397 I llm_load_print_meta: n_vocab          = 50304
0.00.067.397 I llm_load_print_meta: n_merges         = 50009
0.00.067.397 I llm_load_print_meta: vocab_only       = 0
0.00.067.397 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.398 I llm_load_print_meta: n_embd           = 2048
0.00.067.398 I llm_load_print_meta: n_layer          = 24
0.00.067.400 I llm_load_print_meta: n_head           = 16
0.00.067.412 I llm_load_print_meta: n_head_kv        = 16
0.00.067.412 I llm_load_print_meta: n_rot            = 32
0.00.067.412 I llm_load_print_meta: n_swa            = 0
0.00.067.413 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.413 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.414 I llm_load_print_meta: n_gqa            = 1
0.00.067.414 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.416 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.416 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.416 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.417 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.417 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.419 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.419 I llm_load_print_meta: n_ff             = 8192
0.00.067.420 I llm_load_print_meta: n_expert         = 0
0.00.067.421 I llm_load_print_meta: n_expert_used    = 0
0.00.067.422 I llm_load_print_meta: causal attn      = 1
0.00.067.423 I llm_load_print_meta: pooling type     = 0
0.00.067.423 I llm_load_print_meta: rope type        = 2
0.00.067.423 I llm_load_print_meta: rope scaling     = linear
0.00.067.423 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.424 I llm_load_print_meta: freq_scale_train = 1
0.00.067.424 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.424 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.424 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.424 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.424 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.425 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.425 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.425 I llm_load_print_meta: model type       = 1.4B
0.00.067.434 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.067.435 I llm_load_print_meta: model params     = 1.41 B
0.00.067.435 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.067.435 I llm_load_print_meta: general.name     = 1.4B
0.00.067.436 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.436 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.436 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.437 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.438 I llm_load_print_meta: LF token         = 128 ''
0.00.067.438 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.438 I llm_load_print_meta: max token length = 1024
0.00.069.179 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.180 I llm_load_tensors: offloading output layer to GPU
0.00.069.181 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.190 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.069.192 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.070.133 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.134 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.135 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.135 I llama_new_context_with_model: n_batch       = 2048
0.00.070.135 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.135 I llama_new_context_with_model: flash_attn    = 0
0.00.070.136 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.136 I llama_new_context_with_model: freq_scale    = 1
0.00.070.137 I ggml_metal_init: allocating
0.00.070.139 I ggml_metal_init: found device: Apple M4
0.00.070.142 I ggml_metal_init: picking default device: Apple M4
0.00.070.801 I ggml_metal_init: using embedded metal library
0.00.073.669 I ggml_metal_init: GPU name:   Apple M4
0.00.073.671 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.673 I ggml_metal_init: simdgroup reduction   = true
0.00.073.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.674 I ggml_metal_init: has bfloat            = true
0.00.073.674 I ggml_metal_init: use bfloat            = true
0.00.073.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.250 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.886 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.894 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.922 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.019 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.020 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.020 I llama_new_context_with_model: graph nodes  = 967
0.00.107.021 I llama_new_context_with_model: graph splits = 2
0.00.107.036 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.176 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.176 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.910 I main: llama threadpool init, n_threads = 4
0.00.653.946 I 
0.00.653.979 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.981 I 
0.00.654.205 I sampler seed: 1234
0.00.654.212 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.654.285 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.654.290 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.654.290 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.418.630 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53423.63 tokens per second)
0.01.418.630 I llama_perf_context_print:        load time =     642.14 ms
0.01.418.631 I llama_perf_context_print: prompt eval time =      50.17 ms /     7 tokens (    7.17 ms per token,   139.53 tokens per second)
0.01.418.632 I llama_perf_context_print:        eval time =     711.09 ms /    63 runs   (   11.29 ms per token,    88.60 tokens per second)
0.01.418.633 I llama_perf_context_print:       total time =     764.72 ms /    70 tokens
0.01.418.851 I ggml_metal_free: deallocating

real	0m1.436s
user	0m0.117s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.209 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.111 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.116 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.119 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.120 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.121 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.121 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.122 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.122 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.122 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.123 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.124 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.124 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.124 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.036 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.988 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.989 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.989 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.990 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.990 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.990 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.991 I llama_model_loader: - type  f32:  194 tensors
0.00.023.991 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.991 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.991 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.170 I llm_load_vocab: special tokens cache size = 25
0.00.050.073 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.076 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.077 I llm_load_print_meta: arch             = gptneox
0.00.050.077 I llm_load_print_meta: vocab type       = BPE
0.00.050.077 I llm_load_print_meta: n_vocab          = 50304
0.00.050.077 I llm_load_print_meta: n_merges         = 50009
0.00.050.077 I llm_load_print_meta: vocab_only       = 0
0.00.050.078 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.078 I llm_load_print_meta: n_embd           = 2048
0.00.050.078 I llm_load_print_meta: n_layer          = 24
0.00.050.081 I llm_load_print_meta: n_head           = 16
0.00.050.093 I llm_load_print_meta: n_head_kv        = 16
0.00.050.093 I llm_load_print_meta: n_rot            = 32
0.00.050.093 I llm_load_print_meta: n_swa            = 0
0.00.050.094 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.094 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.095 I llm_load_print_meta: n_gqa            = 1
0.00.050.095 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.096 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.097 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.099 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.099 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.100 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.100 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.100 I llm_load_print_meta: n_ff             = 8192
0.00.050.100 I llm_load_print_meta: n_expert         = 0
0.00.050.101 I llm_load_print_meta: n_expert_used    = 0
0.00.050.102 I llm_load_print_meta: causal attn      = 1
0.00.050.102 I llm_load_print_meta: pooling type     = 0
0.00.050.102 I llm_load_print_meta: rope type        = 2
0.00.050.102 I llm_load_print_meta: rope scaling     = linear
0.00.050.102 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.102 I llm_load_print_meta: freq_scale_train = 1
0.00.050.103 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.103 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.103 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.103 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.103 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.103 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.103 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.104 I llm_load_print_meta: model type       = 1.4B
0.00.050.113 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.115 I llm_load_print_meta: model params     = 1.41 B
0.00.050.115 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.116 I llm_load_print_meta: general.name     = 1.4B
0.00.050.116 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.116 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.116 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.116 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.116 I llm_load_print_meta: LF token         = 128 ''
0.00.050.117 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.117 I llm_load_print_meta: max token length = 1024
0.00.052.142 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.143 I llm_load_tensors: offloading output layer to GPU
0.00.052.143 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.154 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.155 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.061 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.061 I llama_new_context_with_model: n_ctx         = 128
0.00.053.062 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.062 I llama_new_context_with_model: n_batch       = 128
0.00.053.062 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.062 I llama_new_context_with_model: flash_attn    = 0
0.00.053.062 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.063 I llama_new_context_with_model: freq_scale    = 1
0.00.053.063 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.064 I ggml_metal_init: allocating
0.00.053.067 I ggml_metal_init: found device: Apple M4
0.00.053.069 I ggml_metal_init: picking default device: Apple M4
0.00.053.646 I ggml_metal_init: using embedded metal library
0.00.055.936 I ggml_metal_init: GPU name:   Apple M4
0.00.055.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.937 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.938 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.938 I ggml_metal_init: simdgroup reduction   = true
0.00.055.938 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.938 I ggml_metal_init: has bfloat            = true
0.00.055.938 I ggml_metal_init: use bfloat            = true
0.00.055.939 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.628 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.930 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.934 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.951 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.817 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.818 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.818 I llama_new_context_with_model: graph nodes  = 967
0.00.067.818 I llama_new_context_with_model: graph splits = 2
0.00.067.825 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.826 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.548.931 I 
0.00.548.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.548.999 I perplexity: tokenizing the input ..
0.00.556.545 I perplexity: tokenization took 7.544 ms
0.00.556.550 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.689.630 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.691.039 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.691.057 I llama_perf_context_print:        load time =     539.72 ms
0.00.691.058 I llama_perf_context_print: prompt eval time =     132.84 ms /   128 tokens (    1.04 ms per token,   963.57 tokens per second)
0.00.691.058 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.691.059 I llama_perf_context_print:       total time =     142.13 ms /   129 tokens
0.00.691.402 I ggml_metal_free: deallocating

real	0m0.706s
user	0m0.078s
sys	0m0.090s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.185 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.307 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.025.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.318 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.318 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.319 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.321 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.321 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.322 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.322 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.323 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.326 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.326 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.327 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.329 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.329 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.406 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.684 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.684 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.684 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.684 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.034.685 I llama_model_loader: - type  f32:  194 tensors
0.00.034.685 I llama_model_loader: - type q5_K:   61 tensors
0.00.034.686 I llama_model_loader: - type q6_K:   37 tensors
0.00.057.969 I llm_load_vocab: special tokens cache size = 25
0.00.064.094 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.097 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.097 I llm_load_print_meta: arch             = gptneox
0.00.064.097 I llm_load_print_meta: vocab type       = BPE
0.00.064.097 I llm_load_print_meta: n_vocab          = 50304
0.00.064.098 I llm_load_print_meta: n_merges         = 50009
0.00.064.098 I llm_load_print_meta: vocab_only       = 0
0.00.064.098 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.098 I llm_load_print_meta: n_embd           = 2048
0.00.064.098 I llm_load_print_meta: n_layer          = 24
0.00.064.101 I llm_load_print_meta: n_head           = 16
0.00.064.108 I llm_load_print_meta: n_head_kv        = 16
0.00.064.108 I llm_load_print_meta: n_rot            = 32
0.00.064.109 I llm_load_print_meta: n_swa            = 0
0.00.064.109 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.109 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.111 I llm_load_print_meta: n_gqa            = 1
0.00.064.112 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.112 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.113 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.113 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.113 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.113 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.113 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.114 I llm_load_print_meta: n_ff             = 8192
0.00.064.114 I llm_load_print_meta: n_expert         = 0
0.00.064.114 I llm_load_print_meta: n_expert_used    = 0
0.00.064.114 I llm_load_print_meta: causal attn      = 1
0.00.064.115 I llm_load_print_meta: pooling type     = 0
0.00.064.115 I llm_load_print_meta: rope type        = 2
0.00.064.115 I llm_load_print_meta: rope scaling     = linear
0.00.064.115 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.115 I llm_load_print_meta: freq_scale_train = 1
0.00.064.119 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.119 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.119 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.119 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.119 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.119 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.120 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.120 I llm_load_print_meta: model type       = 1.4B
0.00.064.125 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.064.125 I llm_load_print_meta: model params     = 1.41 B
0.00.064.126 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.064.126 I llm_load_print_meta: general.name     = 1.4B
0.00.064.126 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.126 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.127 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.127 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.127 I llm_load_print_meta: LF token         = 128 ''
0.00.064.128 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.128 I llm_load_print_meta: max token length = 1024
0.00.065.971 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.971 I llm_load_tensors: offloading output layer to GPU
0.00.065.972 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.976 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.065.977 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.066.887 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.888 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.889 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.889 I llama_new_context_with_model: n_batch       = 2048
0.00.066.889 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.889 I llama_new_context_with_model: flash_attn    = 0
0.00.066.890 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.890 I llama_new_context_with_model: freq_scale    = 1
0.00.066.890 I ggml_metal_init: allocating
0.00.066.893 I ggml_metal_init: found device: Apple M4
0.00.066.895 I ggml_metal_init: picking default device: Apple M4
0.00.067.496 I ggml_metal_init: using embedded metal library
0.00.070.082 I ggml_metal_init: GPU name:   Apple M4
0.00.070.083 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.084 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.084 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.084 I ggml_metal_init: simdgroup reduction   = true
0.00.070.084 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.084 I ggml_metal_init: has bfloat            = true
0.00.070.085 I ggml_metal_init: use bfloat            = true
0.00.070.085 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.085 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.692 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.374 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.380 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.398 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.431 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.101.433 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.101.433 I llama_new_context_with_model: graph nodes  = 967
0.00.101.433 I llama_new_context_with_model: graph splits = 2
0.00.101.449 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.101.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.191 I main: llama threadpool init, n_threads = 4
0.00.761.231 I 
0.00.761.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.263 I 
0.00.761.488 I sampler seed: 1234
0.00.761.493 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.504 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.504 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.504 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.611.938 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62831.86 tokens per second)
0.01.611.939 I llama_perf_context_print:        load time =     752.00 ms
0.01.611.940 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.77 tokens per second)
0.01.611.940 I llama_perf_context_print:        eval time =     796.05 ms /    63 runs   (   12.64 ms per token,    79.14 tokens per second)
0.01.611.941 I llama_perf_context_print:       total time =     850.75 ms /    70 tokens
0.01.612.164 I ggml_metal_free: deallocating

real	0m1.628s
user	0m0.112s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.481 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.431 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.434 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.435 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.435 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.435 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.436 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.436 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.437 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.437 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.437 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.438 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.442 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.442 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.443 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.568 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.686 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.687 I llama_model_loader: - type  f32:  194 tensors
0.00.023.687 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.687 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.916 I llm_load_vocab: special tokens cache size = 25
0.00.050.943 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.947 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.948 I llm_load_print_meta: arch             = gptneox
0.00.050.948 I llm_load_print_meta: vocab type       = BPE
0.00.050.948 I llm_load_print_meta: n_vocab          = 50304
0.00.050.949 I llm_load_print_meta: n_merges         = 50009
0.00.050.949 I llm_load_print_meta: vocab_only       = 0
0.00.050.949 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.949 I llm_load_print_meta: n_embd           = 2048
0.00.050.949 I llm_load_print_meta: n_layer          = 24
0.00.050.954 I llm_load_print_meta: n_head           = 16
0.00.050.963 I llm_load_print_meta: n_head_kv        = 16
0.00.050.965 I llm_load_print_meta: n_rot            = 32
0.00.050.965 I llm_load_print_meta: n_swa            = 0
0.00.050.966 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.966 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.966 I llm_load_print_meta: n_gqa            = 1
0.00.050.967 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.967 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.968 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.969 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.969 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.974 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.974 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.977 I llm_load_print_meta: n_ff             = 8192
0.00.050.979 I llm_load_print_meta: n_expert         = 0
0.00.050.979 I llm_load_print_meta: n_expert_used    = 0
0.00.050.979 I llm_load_print_meta: causal attn      = 1
0.00.050.979 I llm_load_print_meta: pooling type     = 0
0.00.050.979 I llm_load_print_meta: rope type        = 2
0.00.050.979 I llm_load_print_meta: rope scaling     = linear
0.00.050.980 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.980 I llm_load_print_meta: freq_scale_train = 1
0.00.050.980 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.980 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.980 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.982 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.982 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.982 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.982 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.983 I llm_load_print_meta: model type       = 1.4B
0.00.050.988 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.988 I llm_load_print_meta: model params     = 1.41 B
0.00.050.989 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.989 I llm_load_print_meta: general.name     = 1.4B
0.00.050.989 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.990 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.991 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.991 I llm_load_print_meta: LF token         = 128 ''
0.00.050.991 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.991 I llm_load_print_meta: max token length = 1024
0.00.052.706 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.707 I llm_load_tensors: offloading output layer to GPU
0.00.052.707 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.713 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.714 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.592 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.592 I llama_new_context_with_model: n_ctx         = 128
0.00.053.593 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.593 I llama_new_context_with_model: n_batch       = 128
0.00.053.593 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.593 I llama_new_context_with_model: flash_attn    = 0
0.00.053.594 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.594 I llama_new_context_with_model: freq_scale    = 1
0.00.053.594 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.595 I ggml_metal_init: allocating
0.00.053.599 I ggml_metal_init: found device: Apple M4
0.00.053.603 I ggml_metal_init: picking default device: Apple M4
0.00.054.227 I ggml_metal_init: using embedded metal library
0.00.057.066 I ggml_metal_init: GPU name:   Apple M4
0.00.057.068 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.068 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.070 I ggml_metal_init: simdgroup reduction   = true
0.00.057.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.070 I ggml_metal_init: has bfloat            = true
0.00.057.071 I ggml_metal_init: use bfloat            = true
0.00.057.071 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.299 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.603 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.606 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.620 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.505 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.506 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.506 I llama_new_context_with_model: graph nodes  = 967
0.00.068.507 I llama_new_context_with_model: graph splits = 2
0.00.068.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.515 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.293 I 
0.00.630.328 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.339 I perplexity: tokenizing the input ..
0.00.638.899 I perplexity: tokenization took 8.558 ms
0.00.638.905 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.777 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.780.152 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.780.164 I llama_perf_context_print:        load time =     621.81 ms
0.00.780.165 I llama_perf_context_print: prompt eval time =     139.64 ms /   128 tokens (    1.09 ms per token,   916.64 tokens per second)
0.00.780.166 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.166 I llama_perf_context_print:       total time =     149.87 ms /   129 tokens
0.00.780.464 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.080s
sys	0m0.109s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.881 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.746 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.757 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.758 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.758 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.759 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.759 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.760 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.760 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.761 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.761 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.761 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.762 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.762 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.764 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.764 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.764 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.701 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.608 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.609 I llama_model_loader: - type  f32:  194 tensors
0.00.025.610 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.623 I llm_load_vocab: special tokens cache size = 25
0.00.052.524 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.526 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.527 I llm_load_print_meta: arch             = gptneox
0.00.052.527 I llm_load_print_meta: vocab type       = BPE
0.00.052.527 I llm_load_print_meta: n_vocab          = 50304
0.00.052.528 I llm_load_print_meta: n_merges         = 50009
0.00.052.528 I llm_load_print_meta: vocab_only       = 0
0.00.052.528 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.528 I llm_load_print_meta: n_embd           = 2048
0.00.052.528 I llm_load_print_meta: n_layer          = 24
0.00.052.531 I llm_load_print_meta: n_head           = 16
0.00.052.542 I llm_load_print_meta: n_head_kv        = 16
0.00.052.543 I llm_load_print_meta: n_rot            = 32
0.00.052.543 I llm_load_print_meta: n_swa            = 0
0.00.052.543 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.543 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.544 I llm_load_print_meta: n_gqa            = 1
0.00.052.545 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.545 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.546 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.547 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.547 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.547 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.549 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.550 I llm_load_print_meta: n_ff             = 8192
0.00.052.550 I llm_load_print_meta: n_expert         = 0
0.00.052.550 I llm_load_print_meta: n_expert_used    = 0
0.00.052.550 I llm_load_print_meta: causal attn      = 1
0.00.052.551 I llm_load_print_meta: pooling type     = 0
0.00.052.552 I llm_load_print_meta: rope type        = 2
0.00.052.552 I llm_load_print_meta: rope scaling     = linear
0.00.052.552 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.552 I llm_load_print_meta: freq_scale_train = 1
0.00.052.553 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.553 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.553 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.553 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.553 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.553 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.553 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.554 I llm_load_print_meta: model type       = 1.4B
0.00.052.563 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.563 I llm_load_print_meta: model params     = 1.41 B
0.00.052.564 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.564 I llm_load_print_meta: general.name     = 1.4B
0.00.052.564 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.565 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.565 I llm_load_print_meta: LF token         = 128 ''
0.00.052.566 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.566 I llm_load_print_meta: max token length = 1024
0.00.054.183 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.184 I llm_load_tensors: offloading output layer to GPU
0.00.054.184 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.194 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.196 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.022 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.023 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.023 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.023 I llama_new_context_with_model: n_batch       = 2048
0.00.055.023 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.024 I llama_new_context_with_model: flash_attn    = 0
0.00.055.024 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.024 I llama_new_context_with_model: freq_scale    = 1
0.00.055.025 I ggml_metal_init: allocating
0.00.055.031 I ggml_metal_init: found device: Apple M4
0.00.055.033 I ggml_metal_init: picking default device: Apple M4
0.00.055.608 I ggml_metal_init: using embedded metal library
0.00.057.919 I ggml_metal_init: GPU name:   Apple M4
0.00.057.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.922 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.922 I ggml_metal_init: simdgroup reduction   = true
0.00.057.922 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.922 I ggml_metal_init: has bfloat            = true
0.00.057.922 I ggml_metal_init: use bfloat            = true
0.00.057.923 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.455 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.629 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.641 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.658 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.703 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.705 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.705 I llama_new_context_with_model: graph nodes  = 967
0.00.087.705 I llama_new_context_with_model: graph splits = 2
0.00.087.719 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.873 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.874 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.538 I main: llama threadpool init, n_threads = 4
0.00.766.574 I 
0.00.766.617 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.619 I 
0.00.766.843 I sampler seed: 1234
0.00.766.848 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.875 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.878 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.878 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.651.017 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.01.651.018 I llama_perf_context_print:        load time =     756.65 ms
0.01.651.020 I llama_perf_context_print: prompt eval time =      54.39 ms /     7 tokens (    7.77 ms per token,   128.71 tokens per second)
0.01.651.020 I llama_perf_context_print:        eval time =     826.79 ms /    63 runs   (   13.12 ms per token,    76.20 tokens per second)
0.01.651.021 I llama_perf_context_print:       total time =     884.48 ms /    70 tokens
0.01.651.282 I ggml_metal_free: deallocating

real	0m1.670s
user	0m0.110s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4399 (ba48e37c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.170 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.812 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.826 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.829 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.830 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.831 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.832 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.835 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.835 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.882 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.883 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.884 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.884 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.884 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.885 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.885 I llama_model_loader: - type  f32:  194 tensors
0.00.026.886 I llama_model_loader: - type q6_K:   98 tensors
0.00.048.833 I llm_load_vocab: special tokens cache size = 25
0.00.054.807 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.812 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.812 I llm_load_print_meta: arch             = gptneox
0.00.054.813 I llm_load_print_meta: vocab type       = BPE
0.00.054.813 I llm_load_print_meta: n_vocab          = 50304
0.00.054.813 I llm_load_print_meta: n_merges         = 50009
0.00.054.813 I llm_load_print_meta: vocab_only       = 0
0.00.054.813 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.814 I llm_load_print_meta: n_embd           = 2048
0.00.054.814 I llm_load_print_meta: n_layer          = 24
0.00.054.818 I llm_load_print_meta: n_head           = 16
0.00.054.832 I llm_load_print_meta: n_head_kv        = 16
0.00.054.832 I llm_load_print_meta: n_rot            = 32
0.00.054.833 I llm_load_print_meta: n_swa            = 0
0.00.054.833 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.833 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.833 I llm_load_print_meta: n_gqa            = 1
0.00.054.834 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.834 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.837 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.837 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.837 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.837 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.837 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.838 I llm_load_print_meta: n_ff             = 8192
0.00.054.838 I llm_load_print_meta: n_expert         = 0
0.00.054.838 I llm_load_print_meta: n_expert_used    = 0
0.00.054.838 I llm_load_print_meta: causal attn      = 1
0.00.054.839 I llm_load_print_meta: pooling type     = 0
0.00.054.839 I llm_load_print_meta: rope type        = 2
0.00.054.839 I llm_load_print_meta: rope scaling     = linear
0.00.054.839 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.839 I llm_load_print_meta: freq_scale_train = 1
0.00.054.841 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.842 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.842 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.842 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.842 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.842 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.842 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.843 I llm_load_print_meta: model type       = 1.4B
0.00.054.853 I llm_load_print_meta: model ftype      = Q6_K
0.00.054.853 I llm_load_print_meta: model params     = 1.41 B
0.00.054.854 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.054.854 I llm_load_print_meta: general.name     = 1.4B
0.00.054.854 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.854 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.855 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.855 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.855 I llm_load_print_meta: LF token         = 128 ''
0.00.054.855 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.855 I llm_load_print_meta: max token length = 1024
0.00.056.900 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.901 I llm_load_tensors: offloading output layer to GPU
0.00.056.901 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.912 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.056.913 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.057.764 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.765 I llama_new_context_with_model: n_ctx         = 128
0.00.057.765 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.057.765 I llama_new_context_with_model: n_batch       = 128
0.00.057.766 I llama_new_context_with_model: n_ubatch      = 128
0.00.057.766 I llama_new_context_with_model: flash_attn    = 0
0.00.057.766 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.766 I llama_new_context_with_model: freq_scale    = 1
0.00.057.767 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.767 I ggml_metal_init: allocating
0.00.057.771 I ggml_metal_init: found device: Apple M4
0.00.057.773 I ggml_metal_init: picking default device: Apple M4
0.00.058.363 I ggml_metal_init: using embedded metal library
0.00.060.690 I ggml_metal_init: GPU name:   Apple M4
0.00.060.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.693 I ggml_metal_init: simdgroup reduction   = true
0.00.060.693 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.694 I ggml_metal_init: has bfloat            = true
0.00.060.694 I ggml_metal_init: use bfloat            = true
0.00.060.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.736 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.957 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.964 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.979 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.847 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.848 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.848 I llama_new_context_with_model: graph nodes  = 967
0.00.072.848 I llama_new_context_with_model: graph splits = 2
0.00.072.862 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.862 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.282.292 I 
0.00.282.326 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.282.339 I perplexity: tokenizing the input ..
0.00.289.990 I perplexity: tokenization took 7.65 ms
0.00.289.993 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.430.140 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.431.359 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.431.375 I llama_perf_context_print:        load time =     272.11 ms
0.00.431.376 I llama_perf_context_print: prompt eval time =     139.88 ms /   128 tokens (    1.09 ms per token,   915.06 tokens per second)
0.00.431.376 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.431.377 I llama_perf_context_print:       total time =     149.09 ms /   129 tokens
0.00.431.930 I ggml_metal_free: deallocating

real	0m0.448s
user	0m0.081s
sys	0m0.054s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4399 (ba48e37c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d70a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d70a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d70aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d70b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d70ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d70bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d70c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d70cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d70d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d70d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d70daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d70dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d70eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d70f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d70fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d7101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d711030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d711750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d711f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d712d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d713480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d713d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d714440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d714700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d714d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d715980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d715ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d716180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d7168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d7176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d717970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d717e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d7182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d718750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d718bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d719090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d7199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d719e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d71a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d71a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d71abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d71b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d71bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d71c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d71c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d71cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d71d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d71d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d71df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d71e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d71ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d71f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d71f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d71f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d720160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d720420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d7208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d720d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d721200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d7216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d721b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d721fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d722920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d722dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d723260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d723700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d723ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d7240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d724640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d724b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d7250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d725630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d7260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d726620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d726b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d7270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d727610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d727b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d7280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d728b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d7290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d7295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d729b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d72a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d72a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d72ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d72b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d72b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d72bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d71b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d72bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d72c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d72cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d72d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d72d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d72dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d72e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d72e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d72ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d72f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d72f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d72fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d7301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d730700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d730c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d7310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d731590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d731a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d731ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d732370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d732810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d732cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d733150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d7335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d733a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d733f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d7343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d734870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d734d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d7351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d735650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d735af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d736430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d7368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d736d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d737210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d7376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d737b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d737ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d738490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d738930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d738dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d739270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d739710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d73a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d73a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d73a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d73ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d73b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d73b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d73bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d73c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d73c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d73c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d73ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d73d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d73d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d73dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d73e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d73e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d73ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d73eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d73f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d73f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d73fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d740170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d740610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d740ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d740f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d7413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d741890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d741d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d7421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d742670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d742b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d742fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d743450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d7438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d743d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d744230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d7446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d744b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d745010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d7454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d745950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d745df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d746290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d746730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d746bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d747070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d747510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d7479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d747e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d7483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d7488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d748e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d749650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d74a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d74a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10d74b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d74b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d74b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d74bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10d74c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d74cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d74d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d74d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d74d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d74e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d74e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d74ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d74f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d74f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d74fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d7506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d751140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d751690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d751be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d752680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d752bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d753120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d753670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d753bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d754110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d754660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d754bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d755100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d755650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d755ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d7560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d756640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d7570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d757b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d7580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d758620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d758b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d7590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d759610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d759b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d75a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d75a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d75ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d75b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d75b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d75bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d75c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d75c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d75cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d75d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d75d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d75db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d75e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d75e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d75eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d75f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d75f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d75fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d760050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d7605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d760af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10d760f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10d761430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d7618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d761d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d762210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d7626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d762b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d762ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d763490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d763930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d763dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d764270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d764710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d764bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d765050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d7655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d765cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d7663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d766b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d767220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d7674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10d767cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d767f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d7685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.153 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.157 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d304d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d3051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d305630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d305aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d305f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d306380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d3067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d306c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d3070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d307540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d3079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d3080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d308bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d309370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d309b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d30a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d30a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d30b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d30b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d30bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d30c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d30cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d30d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d30dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d30e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d30e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d30e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d30ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d30f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d30f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d30fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d30ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d3103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d310670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d310ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d310f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d3113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d311830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d311ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d312110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d312580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d3129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d312e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d3132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d313740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d313bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d314020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d314490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d314900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d314d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d3151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d315650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d315ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d315f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d3163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d316810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d316d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d317280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d3176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d317b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d317fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d318440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d3188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d318d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d319190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d319600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d319a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d319ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d31a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d31a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d31ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d31b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d31b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d31b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d31bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d31c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d31c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d31cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d31cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d31d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d31d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d31dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d31e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d31e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d31ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d31eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d31f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d31f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d31fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d320080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d3204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d320960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d320dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d321240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d3216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d321b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d321f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d322400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d322870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d322ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d323150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d3235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d323a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d323ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d324310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d324780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d324bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d325060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d3254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d325940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d325db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d326220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d326690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d326b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d326f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d3273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d327850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d327cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d328130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d3285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d328a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d328e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d3292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d329760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d329bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d32a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d32a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d32a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d32ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d32b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d32b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d32bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d32bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d32c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d32c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d32cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d32d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d32d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d32d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d32de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d32e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d32e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d32ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d32f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d32f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d32f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d32fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d3301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d330650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d330ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d330f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d3313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d331810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d331c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d3320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d332560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d3329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d332e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d3332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d333720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d333b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d334000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d334470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d3348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d334d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d3351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d335df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d3360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d336370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d3367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d336c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d3370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d337530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d3379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d337e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d338280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d3386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d338b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d338fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d339440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d3398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d339d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d33a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d33a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d33aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d33aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d33b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d33b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d33bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d33c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d33c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d33c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d33cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d33d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d33d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d33db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d33dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d33e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d33e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d33ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d33f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10d33f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d33fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d340050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d3404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10d340930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d340da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d341210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d341730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d341c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d3427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d342a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d343030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d3435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d343bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d344170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d344730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d344cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d3452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d345870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d345e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d3463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d3469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d346f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d347530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d347af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d3480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d348670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d348c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d3491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d3497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d349d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d34a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d34a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d34aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d34b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d34ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d34bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d34c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d34cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d34d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d34d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d34dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d34e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d34e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d34edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d34f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d34f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d34ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d3504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d350ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d351070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d351630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d351bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d3521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d352770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d352d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d3532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d3538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d353e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d354430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d3549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d354fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d355570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d355b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d3560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d3566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10d356c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10d357170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d357670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d357b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d358070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d358570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d358a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d358f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d359470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d359970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d359e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d35a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d35a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d35ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d35b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d35b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d35c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d35c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d35cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d35d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d35d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10d35e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d35e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d35ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f7044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f7056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f7063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f7078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f7083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f70a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f70a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f70b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f70b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f70bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f70c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f70cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f70d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f70db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f70de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f70e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f70e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f70e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f70ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f70f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f70f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f70fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f70ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f7107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f7110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f7119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f7138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f7141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f7157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f7160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f7185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f71a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f71a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f71a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f71adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f71b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f71b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f71bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f71bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f71c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f71ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f71d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f71d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f71da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f71de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f71e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f71e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f71ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f71f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f71f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f71f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f71fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f7213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f7229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f7232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f723e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f724290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f724700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f724fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f7258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f725d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f7261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f726a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f726ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f727360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f7277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f727c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f7280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f728520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f729270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f7296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f729fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f72a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f72a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f72ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f72b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f72b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f72ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f72bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f72c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f72c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f72cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f72d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f72d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f72d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f72dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f72e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f72e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f72eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f72efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f72f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f72f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f72fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f7305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f730a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f732070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f7324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f732950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f7336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f7343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f734860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f7355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f735a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f735e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f736770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f736be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f737050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f7374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f738210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f738af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f738f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f7393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f73a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f73a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f73aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f73ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f73b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f73b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f73bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f73c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f73c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f73c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f73cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f73d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f73d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f73dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f73df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f73e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f73e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f73ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f73f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f73f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f73f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f73fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f7402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f740ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f741b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f742110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f742580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f7429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f742e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f7432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f743740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f744020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f744490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f744900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f7451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f745650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f745ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f745f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f7463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f746810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f7470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f7479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f747e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f7482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f748b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f749000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f749470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f7498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f749d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f74a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f74a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f74aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f74af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f74b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f74b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f74bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f74c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f74c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f74c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f74ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f74d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f74d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f74db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f74dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f74e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f74e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f74ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f74f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f74f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f74fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f74fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f750360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f7507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f750c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f7510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f751990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f751e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f752270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f7526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f752b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f752fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f7538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f753d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f754180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f7545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f754a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f754ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f755340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f7557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f756940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f757060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f757780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f757eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f7584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f758ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.821s
user	0m0.293s
sys	0m0.325s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4399 (ba48e37c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137f0d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137f0df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137f0e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137f0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137f0f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137f0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137f10150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137f10650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137f10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137f11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137f11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137f12320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137f12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137f13250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137f13970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137f14090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137f147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137f14f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137f156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137f15dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137f164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137f16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137f174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137f17760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137f17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137f189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137f18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137f191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137f19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137f19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137f1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137f1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137f1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137f1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137f1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137f1bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137f1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137f1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137f1ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137f1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137f1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137f1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137f1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137f1e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137f1eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137f1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137f1f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137f1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137f203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137f209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137f20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137f217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137f21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137f22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137f223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137f229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137f231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137f23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137f23920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137f23dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137f24260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137f24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137f24ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137f25040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137f254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137f25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137f25e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137f262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137f26760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137f26c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137f27150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137f276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137f27bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137f28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137f28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137f28be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137f29130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137f29680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137f29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137f2a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137f2a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137f2abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137f2b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137f2b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137f2bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137f2c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137f2c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137f2cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137f2d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137f2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137f2db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137f2e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137f2e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137f2eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137f1e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137f2eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137f2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137f2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137f30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137f30790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137f30ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137f31230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137f31780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137f31cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137f32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137f32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137f32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137f33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137f33760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137f33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137f34150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137f345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137f34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137f34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137f353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137f35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137f35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137f361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137f36650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137f36af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137f36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137f37430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137f378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137f37d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137f38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137f386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137f38b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137f38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137f39490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137f39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137f39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137f3a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137f3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137f3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137f3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137f3b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137f3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137f3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137f3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137f3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137f3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137f3d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137f3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137f3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137f3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137f3e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137f3e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137f3ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137f3f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137f3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137f3fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137f3fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137f40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137f40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137f40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137f41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137f41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137f41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137f41f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137f423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137f42890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137f42d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137f431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137f43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137f43b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137f43fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137f44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137f448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137f44d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137f45230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137f456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137f45b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137f46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137f464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137f46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137f46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137f47290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137f47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137f47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137f48070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137f48510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137f489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137f48e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137f492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137f49790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137f49c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137f4a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137f4a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137f4aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137f4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137f4b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137f4b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137f4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137f4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137f4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137f4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137f4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137f4d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137f4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137f4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137f4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137f4ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137f4f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137f4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137f500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137f50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137f50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137f511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137f51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137f51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137f521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137f52710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137f52c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137f531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137f53700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137f53c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137f541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137f546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137f54c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137f55190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137f556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137f55c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137f56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137f566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137f56c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137f57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137f576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137f57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137f58160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137f586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137f58c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137f59150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137f596a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137f59bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137f5a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137f5abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137f5b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137f5b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137f5bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137f5c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137f5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137f5cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137f5d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137f5d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137f5dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137f5e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137f5e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137f5eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137f5f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137f5f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137f5fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137f600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137f60630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137f60b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137f610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137f61620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137f61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137f620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137f62610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137f62b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137f630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137f63600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137f63b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137f63ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137f64490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137f64930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137f64dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137f65270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137f65710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137f65bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137f66050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137f664f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137f66990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137f66e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137f672d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137f67770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137f67c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137f680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137f68600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137f68d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137f69440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137f69b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137f6a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137f6a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137f6ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137f6aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137f6b600 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137e05310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137e05780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137e05bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137e06060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137e064d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137e06940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137e06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137e07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137e07690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137e07b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137e07f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137e08600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137e09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137e098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137e0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137e0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137e0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137e0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137e0bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137e0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137e0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137e0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137e0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137e0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137e0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137e0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137e0ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137e0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137e0f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137e0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137e10010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137e10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137e109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137e10c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137e110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137e11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137e119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137e11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137e122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137e12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137e12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137e12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137e13460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137e138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137e13d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137e141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137e14620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137e14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137e14f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137e15370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137e157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137e15c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137e160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137e16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137e169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137e16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137e17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137e17880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137e17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137e18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137e185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137e18a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137e18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137e19320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137e19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137e19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137e1a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137e1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137e1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137e1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137e1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137e1b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137e1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137e1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137e1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137e1c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137e1ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137e1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137e1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137e1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137e1de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137e1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137e1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137e1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137e1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137e1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137e1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137e1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137e20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137e20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137e20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137e20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137e213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137e21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137e21cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137e22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137e22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137e22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137e22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137e232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137e23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137e23bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137e24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137e244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137e24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137e24d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137e251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137e25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137e25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137e25f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137e263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137e26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137e26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137e27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137e27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137e279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137e27e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137e282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137e28730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137e28ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137e29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137e29480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137e298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137e29d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137e2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137e2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137e2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137e2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137e2b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137e2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137e2bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137e2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137e2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137e2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137e2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137e2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137e2d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137e2db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137e2dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137e2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137e2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137e2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137e2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137e2f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137e2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137e2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137e30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137e307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137e30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137e310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137e31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137e319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137e31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137e32280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137e326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137e32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137e32fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137e33440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137e338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137e33d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137e34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137e34600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137e34a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137e34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137e35350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137e357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137e363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137e366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137e36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137e36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137e37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137e376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137e37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137e37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137e38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137e38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137e38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137e39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137e395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137e39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137e39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137e3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137e3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137e3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137e3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137e3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137e3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137e3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137e3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137e3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137e3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137e3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137e3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137e3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137e3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137e3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137e3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137e3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137e3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137e3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137e3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137e3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137e40140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137e40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137e40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137e40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137e413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137e41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137e41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137e42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137e42db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137e43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137e43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137e43bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137e441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137e44770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137e44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137e452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137e458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137e45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137e46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137e469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137e46fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137e47570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137e47b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137e480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137e486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137e48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137e49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137e497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137e49db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137e4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137e4a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137e4aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137e4b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137e4ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137e4c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137e4c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137e4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137e4d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137e4d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137e4dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137e4e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137e4e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137e4ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137e4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137e4f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137e4ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137e50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137e50af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137e510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137e51670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137e51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137e521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137e527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137e52d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137e53330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137e538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137e53eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137e54470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137e54a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137e54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137e555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137e55b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137e56130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137e566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137e56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137e57270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137e57770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137e57c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137e58170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137e58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137e58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137e59070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137e59570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137e59a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137e59f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137e5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137e5a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137e5ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137e5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137e5b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137e5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137e5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137e5cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137e5d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137e5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137e5dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137e5e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137e5ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137e5f060 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137e5c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137e4ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137e4bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137e48970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137e46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137e55870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137e53030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137e50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137e4eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137e46cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137e44470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137e494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137e4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137e4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137e4c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137e54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137e47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137e4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137e4a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137e43330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137e4d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137e48f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137e535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137e4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137e43eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137e45b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137e563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137e4b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137e53bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137e49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137e4c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137e50230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137e4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137e47830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137e51ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137e466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137e54cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137e524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137e4dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137e56f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137e455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137e569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137e44a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137e552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137e4f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137e51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137e54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137e52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137e4abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137e42500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137e08230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137e04eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137e5e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137e5f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137e5fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137e5fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137e5ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137e60240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137e60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137e607c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137e60a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137e60d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137e61000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137e612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137e61580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137e61840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137e61b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137e61dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137e62080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137e62340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137e62600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137e628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137e62b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137e62e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137e63100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137e633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137e63680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127f04430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127f048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127f04d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127f05180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127f055f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127f05a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127f05ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127f06340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127f067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127f06c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127f07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127f07500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127f07970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127f07de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127f08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127f086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127f08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127f08fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127f09410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127f09880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127f09cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127f0a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127f0a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127f0b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127f0bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127f0c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127f0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127f0cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127f0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127f0d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127f0dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127f0e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127f0e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127f0ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127f0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127f0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127f0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127f102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127f107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127f10cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127f111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127f116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127f11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127f120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127f125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127f12ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127f12fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127f134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127f139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127f13ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127f143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127f148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127f14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127f152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127f157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127f15cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127f161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127f166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127f16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127f170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127f175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127f17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127f17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127f184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127f189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127f18ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127f198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127f19dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127f1a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127f1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127f1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127f1b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127f1b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127f1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127f1c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127f1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127f1cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127f1cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127f1d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127f1d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127f1ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127f1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127f1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127f1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127f1f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127f1f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127f1fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127f201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127f206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127f20bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127f210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127f215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127f21ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127f21fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127f224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127f229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127f22ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127f233d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127f238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127f23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127f242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127f247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127f24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127f251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127f256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127f25bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127f260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127f265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127f26ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127f26fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127f274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127f279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127f27ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127f283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127f28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127f28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127f294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127f29a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127f2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127f2a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127f2acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127f2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127f2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127f2bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127f2c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127f2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127f2d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127f2d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127f2d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127f2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127f2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127f2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127f2f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127f2f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f2faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f30040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f30590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127f30ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127f31030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127f31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127f31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127f32020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127f32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127f32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127f33010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127f33560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127f33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127f34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127f34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127f34aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f34ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127f35540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127f35a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127f35fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127f36530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127f36a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f36fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127f37520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127f37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127f37fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127f38510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127f38a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127f38fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127f39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127f39a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127f39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127f3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127f3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127f3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127f3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127f3ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127f3bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127f3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f3ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127f3cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f3d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127f3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127f3e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127f3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127f3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127f3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127f3f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f40490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127f409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127f40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127f413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127f41870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127f41d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127f421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127f42650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127f42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127f42f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127f43430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127f438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127f43d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127f44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127f446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127f44b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127f44ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f45490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127f459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127f46100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127f46820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127f46f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127f47660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127f47920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127f48110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127f483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127f489e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.922s
user	0m0.243s
sys	0m0.138s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.16 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.53 real         0.15 user         0.04 sys
```
