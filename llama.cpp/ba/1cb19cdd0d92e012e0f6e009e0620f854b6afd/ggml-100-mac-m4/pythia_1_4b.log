Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.668s
user	0m0.707s
sys	0m0.999s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  7%] Built target sha1
[  7%] Built target build_info
[  7%] Built target sha256
[  7%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 13%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 15%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 21%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 23%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Built target llava
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX static library libcommon.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 33%] Linking CXX static library libllava_static.a
[ 33%] Built target test-c
[ 33%] Built target llama-simple-chat
[ 33%] Built target llama-quantize-stats
[ 33%] Built target llama-simple
[ 33%] Built target llava_static
[ 33%] Built target common
[ 33%] Built target llava_shared
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Linking CXX executable ../bin/test-sampling
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-log
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-sampling
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Built target test-llama-grammar
[ 47%] Built target test-grammar-parser
[ 47%] Built target test-json-schema-to-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 47%] Built target test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 56%] Linking CXX executable ../bin/test-autorelease
[ 56%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-quantize-fns
[ 57%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Built target test-chat-template
[ 59%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 59%] Built target test-backend-ops
[ 60%] Built target test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Built target test-autorelease
[ 61%] Built target test-quantize-fns
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Built target test-quantize-perf
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target llama-batched-bench
[ 69%] Built target llama-batched
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Built target llama-embedding
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-gguf-split
[ 70%] Built target llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookup
[ 80%] Built target llama-bench
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Built target llama-lookahead
[ 81%] Generating loading.html.hpp
[ 82%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 83%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-create
[ 83%] Generating index.html.hpp
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Built target llama-passkey
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Built target llama-perplexity
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Built target llama-retrieval
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Built target llama-run
[ 92%] Built target llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-speculative
[ 93%] Built target llama-save-load-state
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Built target llama-tokenize
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.517s
user	0m5.290s
sys	0m8.621s

main: quantize time =  2527.33 ms
main:    total time =  2527.33 ms

main: quantize time =  1304.92 ms
main:    total time =  1304.92 ms

main: quantize time =  1307.45 ms
main:    total time =  1307.45 ms

main: quantize time =  1371.56 ms
main:    total time =  1371.56 ms

main: quantize time =  1545.13 ms
main:    total time =  1545.13 ms

main: quantize time =  5008.91 ms
main:    total time =  5008.91 ms

main: quantize time =  5844.59 ms
main:    total time =  5844.59 ms

main: quantize time =  7025.63 ms
main:    total time =  7025.63 ms

main: quantize time =  5882.72 ms
main:    total time =  5882.72 ms

main: quantize time =  4495.79 ms
main:    total time =  4495.79 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.104 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.231 I main: llama backend init
0.00.000.238 I main: load the model and apply lora adapter, if any
0.00.051.509 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.062.559 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.062.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.062.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.062.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.062.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.062.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.062.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.062.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.062.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.062.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.062.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.062.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.062.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.062.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.062.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.062.592 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.062.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.069.516 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.071.741 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.769 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.078.777 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.778 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.778 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.779 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.781 I llama_model_loader: - type  f32:  194 tensors
0.00.078.781 I llama_model_loader: - type  f16:   98 tensors
0.00.117.251 I llm_load_vocab: special tokens cache size = 25
0.00.124.773 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.124.776 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.124.776 I llm_load_print_meta: arch             = gptneox
0.00.124.776 I llm_load_print_meta: vocab type       = BPE
0.00.124.777 I llm_load_print_meta: n_vocab          = 50304
0.00.124.777 I llm_load_print_meta: n_merges         = 50009
0.00.124.777 I llm_load_print_meta: vocab_only       = 0
0.00.124.777 I llm_load_print_meta: n_ctx_train      = 2048
0.00.124.777 I llm_load_print_meta: n_embd           = 2048
0.00.124.777 I llm_load_print_meta: n_layer          = 24
0.00.124.781 I llm_load_print_meta: n_head           = 16
0.00.124.783 I llm_load_print_meta: n_head_kv        = 16
0.00.124.783 I llm_load_print_meta: n_rot            = 32
0.00.124.783 I llm_load_print_meta: n_swa            = 0
0.00.124.783 I llm_load_print_meta: n_embd_head_k    = 128
0.00.124.784 I llm_load_print_meta: n_embd_head_v    = 128
0.00.124.784 I llm_load_print_meta: n_gqa            = 1
0.00.124.785 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.124.786 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.124.786 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.124.787 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.124.787 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.124.787 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.124.787 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.124.788 I llm_load_print_meta: n_ff             = 8192
0.00.124.788 I llm_load_print_meta: n_expert         = 0
0.00.124.788 I llm_load_print_meta: n_expert_used    = 0
0.00.124.788 I llm_load_print_meta: causal attn      = 1
0.00.124.788 I llm_load_print_meta: pooling type     = 0
0.00.124.788 I llm_load_print_meta: rope type        = 2
0.00.124.791 I llm_load_print_meta: rope scaling     = linear
0.00.124.791 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.124.792 I llm_load_print_meta: freq_scale_train = 1
0.00.124.792 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.124.792 I llm_load_print_meta: rope_finetuned   = unknown
0.00.124.792 I llm_load_print_meta: ssm_d_conv       = 0
0.00.124.792 I llm_load_print_meta: ssm_d_inner      = 0
0.00.124.793 I llm_load_print_meta: ssm_d_state      = 0
0.00.124.793 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.124.793 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.124.793 I llm_load_print_meta: model type       = 1.4B
0.00.124.794 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.124.794 I llm_load_print_meta: model params     = 1.41 B
0.00.124.795 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.124.795 I llm_load_print_meta: general.name     = 1.4B
0.00.124.795 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.124.795 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.124.795 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.124.795 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.124.796 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.124.796 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.124.800 I llm_load_print_meta: max token length = 1024
0.00.127.512 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.127.512 I llm_load_tensors: offloading output layer to GPU
0.00.127.513 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.127.532 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.127.533 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.128.537 I llama_new_context_with_model: n_seq_max     = 1
0.00.128.538 I llama_new_context_with_model: n_ctx         = 2048
0.00.128.539 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.128.539 I llama_new_context_with_model: n_batch       = 2048
0.00.128.539 I llama_new_context_with_model: n_ubatch      = 512
0.00.128.539 I llama_new_context_with_model: flash_attn    = 0
0.00.128.540 I llama_new_context_with_model: freq_base     = 10000.0
0.00.128.540 I llama_new_context_with_model: freq_scale    = 1
0.00.128.540 I ggml_metal_init: allocating
0.00.128.544 I ggml_metal_init: found device: Apple M4
0.00.128.546 I ggml_metal_init: picking default device: Apple M4
0.00.129.251 I ggml_metal_init: using embedded metal library
0.00.138.983 I ggml_metal_init: GPU name:   Apple M4
0.00.138.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.138.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.138.986 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.138.986 I ggml_metal_init: simdgroup reduction   = true
0.00.138.986 I ggml_metal_init: simdgroup matrix mul. = true
0.00.138.986 I ggml_metal_init: has bfloat            = true
0.00.138.986 I ggml_metal_init: use bfloat            = true
0.00.138.987 I ggml_metal_init: hasUnifiedMemory      = true
0.00.138.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.185.622 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.185.628 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.185.649 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.186.610 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.186.612 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.186.613 I llama_new_context_with_model: graph nodes  = 967
0.00.186.613 I llama_new_context_with_model: graph splits = 2
0.00.186.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.265.429 I main: llama threadpool init, n_threads = 4
0.00.265.461 I 
0.00.265.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.265.506 I 
0.00.265.594 I sampler seed: 1234
0.00.265.599 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.265.623 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.265.625 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.265.625 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.115.813 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.02.115.813 I llama_perf_context_print:        load time =     213.91 ms
0.02.115.814 I llama_perf_context_print: prompt eval time =      43.83 ms /     7 tokens (    6.26 ms per token,   159.70 tokens per second)
0.02.115.815 I llama_perf_context_print:        eval time =    1803.30 ms /    63 runs   (   28.62 ms per token,    34.94 tokens per second)
0.02.115.815 I llama_perf_context_print:       total time =    1850.39 ms /    70 tokens
0.02.115.986 I ggml_metal_free: deallocating

real	0m2.420s
user	0m0.149s
sys	0m0.102s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.633 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.381 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.388 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.389 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.390 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.391 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.391 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.392 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.392 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.394 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.206 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.317 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.319 I llama_model_loader: - type  f32:  194 tensors
0.00.031.319 I llama_model_loader: - type q8_0:   98 tensors
0.00.052.928 I llm_load_vocab: special tokens cache size = 25
0.00.059.037 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.039 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.040 I llm_load_print_meta: arch             = gptneox
0.00.059.040 I llm_load_print_meta: vocab type       = BPE
0.00.059.040 I llm_load_print_meta: n_vocab          = 50304
0.00.059.041 I llm_load_print_meta: n_merges         = 50009
0.00.059.041 I llm_load_print_meta: vocab_only       = 0
0.00.059.041 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.041 I llm_load_print_meta: n_embd           = 2048
0.00.059.041 I llm_load_print_meta: n_layer          = 24
0.00.059.045 I llm_load_print_meta: n_head           = 16
0.00.059.046 I llm_load_print_meta: n_head_kv        = 16
0.00.059.046 I llm_load_print_meta: n_rot            = 32
0.00.059.046 I llm_load_print_meta: n_swa            = 0
0.00.059.046 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.047 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.047 I llm_load_print_meta: n_gqa            = 1
0.00.059.048 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.049 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.049 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.050 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.050 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.050 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.050 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.051 I llm_load_print_meta: n_ff             = 8192
0.00.059.051 I llm_load_print_meta: n_expert         = 0
0.00.059.054 I llm_load_print_meta: n_expert_used    = 0
0.00.059.054 I llm_load_print_meta: causal attn      = 1
0.00.059.054 I llm_load_print_meta: pooling type     = 0
0.00.059.054 I llm_load_print_meta: rope type        = 2
0.00.059.054 I llm_load_print_meta: rope scaling     = linear
0.00.059.055 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.055 I llm_load_print_meta: freq_scale_train = 1
0.00.059.055 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.056 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.056 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.056 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.058 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.058 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.058 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.058 I llm_load_print_meta: model type       = 1.4B
0.00.059.059 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.059 I llm_load_print_meta: model params     = 1.41 B
0.00.059.060 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.060 I llm_load_print_meta: general.name     = 1.4B
0.00.059.060 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.060 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.060 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.061 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.065 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.066 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.066 I llm_load_print_meta: max token length = 1024
0.00.061.122 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.122 I llm_load_tensors: offloading output layer to GPU
0.00.061.123 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.129 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.131 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.155 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.155 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.156 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.156 I llama_new_context_with_model: n_batch       = 2048
0.00.062.156 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.156 I llama_new_context_with_model: flash_attn    = 0
0.00.062.157 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.157 I llama_new_context_with_model: freq_scale    = 1
0.00.062.157 I ggml_metal_init: allocating
0.00.062.161 I ggml_metal_init: found device: Apple M4
0.00.062.163 I ggml_metal_init: picking default device: Apple M4
0.00.062.893 I ggml_metal_init: using embedded metal library
0.00.065.401 I ggml_metal_init: GPU name:   Apple M4
0.00.065.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.403 I ggml_metal_init: simdgroup reduction   = true
0.00.065.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.404 I ggml_metal_init: has bfloat            = true
0.00.065.404 I ggml_metal_init: use bfloat            = true
0.00.065.404 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.405 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.750 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.760 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.785 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.046 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.101.049 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.101.049 I llama_new_context_with_model: graph nodes  = 967
0.00.101.050 I llama_new_context_with_model: graph splits = 2
0.00.101.069 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.441.059 I main: llama threadpool init, n_threads = 4
0.01.441.093 I 
0.01.441.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.441.131 I 
0.01.441.349 I sampler seed: 1234
0.01.441.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.441.400 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.441.404 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.441.405 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.528.461 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.02.528.461 I llama_perf_context_print:        load time =    1431.42 ms
0.02.528.463 I llama_perf_context_print: prompt eval time =      39.83 ms /     7 tokens (    5.69 ms per token,   175.75 tokens per second)
0.02.528.464 I llama_perf_context_print:        eval time =    1044.34 ms /    63 runs   (   16.58 ms per token,    60.32 tokens per second)
0.02.528.465 I llama_perf_context_print:       total time =    1087.40 ms /    70 tokens
0.02.528.713 I ggml_metal_free: deallocating

real	0m2.546s
user	0m0.112s
sys	0m0.232s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.017.527 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.439 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.440 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.440 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.442 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.443 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.444 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.446 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.446 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.446 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.573 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.617 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.620 I llama_model_loader: - type  f32:  194 tensors
0.00.043.620 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.621 I llama_model_loader: - type q6_K:    1 tensors
0.00.066.211 I llm_load_vocab: special tokens cache size = 25
0.00.072.499 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.504 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.505 I llm_load_print_meta: arch             = gptneox
0.00.072.505 I llm_load_print_meta: vocab type       = BPE
0.00.072.505 I llm_load_print_meta: n_vocab          = 50304
0.00.072.506 I llm_load_print_meta: n_merges         = 50009
0.00.072.506 I llm_load_print_meta: vocab_only       = 0
0.00.072.506 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.509 I llm_load_print_meta: n_embd           = 2048
0.00.072.509 I llm_load_print_meta: n_layer          = 24
0.00.072.513 I llm_load_print_meta: n_head           = 16
0.00.072.514 I llm_load_print_meta: n_head_kv        = 16
0.00.072.514 I llm_load_print_meta: n_rot            = 32
0.00.072.514 I llm_load_print_meta: n_swa            = 0
0.00.072.515 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.515 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.516 I llm_load_print_meta: n_gqa            = 1
0.00.072.516 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.517 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.517 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.518 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.518 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.518 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.518 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.519 I llm_load_print_meta: n_ff             = 8192
0.00.072.519 I llm_load_print_meta: n_expert         = 0
0.00.072.519 I llm_load_print_meta: n_expert_used    = 0
0.00.072.519 I llm_load_print_meta: causal attn      = 1
0.00.072.519 I llm_load_print_meta: pooling type     = 0
0.00.072.519 I llm_load_print_meta: rope type        = 2
0.00.072.521 I llm_load_print_meta: rope scaling     = linear
0.00.072.522 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.523 I llm_load_print_meta: freq_scale_train = 1
0.00.072.523 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.523 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.524 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.524 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.524 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.524 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.524 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.525 I llm_load_print_meta: model type       = 1.4B
0.00.072.525 I llm_load_print_meta: model ftype      = Q4_0
0.00.072.525 I llm_load_print_meta: model params     = 1.41 B
0.00.072.526 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.072.526 I llm_load_print_meta: general.name     = 1.4B
0.00.072.526 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.528 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.528 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.528 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.528 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.528 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.529 I llm_load_print_meta: max token length = 1024
0.00.074.428 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.428 I llm_load_tensors: offloading output layer to GPU
0.00.074.428 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.439 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.074.441 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.075.366 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.367 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.367 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.367 I llama_new_context_with_model: n_batch       = 2048
0.00.075.367 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.368 I llama_new_context_with_model: flash_attn    = 0
0.00.075.368 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.368 I llama_new_context_with_model: freq_scale    = 1
0.00.075.369 I ggml_metal_init: allocating
0.00.075.374 I ggml_metal_init: found device: Apple M4
0.00.075.376 I ggml_metal_init: picking default device: Apple M4
0.00.076.023 I ggml_metal_init: using embedded metal library
0.00.078.481 I ggml_metal_init: GPU name:   Apple M4
0.00.078.483 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.483 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.483 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.484 I ggml_metal_init: simdgroup reduction   = true
0.00.078.484 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.484 I ggml_metal_init: has bfloat            = true
0.00.078.484 I ggml_metal_init: use bfloat            = true
0.00.078.485 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.096 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.111.101 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.111.122 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.050 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.112.051 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.112.051 I llama_new_context_with_model: graph nodes  = 967
0.00.112.052 I llama_new_context_with_model: graph splits = 2
0.00.112.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.959.829 I main: llama threadpool init, n_threads = 4
0.00.959.878 I 
0.00.959.919 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.959.920 I 
0.00.960.147 I sampler seed: 1234
0.00.960.152 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.960.181 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.960.182 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.960.182 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.639.836 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.639.837 I llama_perf_context_print:        load time =     942.29 ms
0.01.639.837 I llama_perf_context_print: prompt eval time =      39.61 ms /     7 tokens (    5.66 ms per token,   176.74 tokens per second)
0.01.639.838 I llama_perf_context_print:        eval time =     637.16 ms /    63 runs   (   10.11 ms per token,    98.88 tokens per second)
0.01.639.838 I llama_perf_context_print:       total time =     680.01 ms /    70 tokens
0.01.640.068 I ggml_metal_free: deallocating

real	0m1.660s
user	0m0.114s
sys	0m0.155s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.733 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.213 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.218 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.219 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.220 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.220 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.220 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.222 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.222 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.223 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.223 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.223 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.224 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.227 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.227 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.117 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.244 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.165 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.166 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.166 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.166 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.167 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.167 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.167 I llama_model_loader: - type  f32:  194 tensors
0.00.024.168 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.168 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.259 I llm_load_vocab: special tokens cache size = 25
0.00.051.312 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.315 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.315 I llm_load_print_meta: arch             = gptneox
0.00.051.316 I llm_load_print_meta: vocab type       = BPE
0.00.051.316 I llm_load_print_meta: n_vocab          = 50304
0.00.051.316 I llm_load_print_meta: n_merges         = 50009
0.00.051.316 I llm_load_print_meta: vocab_only       = 0
0.00.051.316 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.317 I llm_load_print_meta: n_embd           = 2048
0.00.051.317 I llm_load_print_meta: n_layer          = 24
0.00.051.319 I llm_load_print_meta: n_head           = 16
0.00.051.320 I llm_load_print_meta: n_head_kv        = 16
0.00.051.320 I llm_load_print_meta: n_rot            = 32
0.00.051.320 I llm_load_print_meta: n_swa            = 0
0.00.051.321 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.321 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.322 I llm_load_print_meta: n_gqa            = 1
0.00.051.322 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.323 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.324 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.324 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.324 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.324 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.325 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.325 I llm_load_print_meta: n_ff             = 8192
0.00.051.325 I llm_load_print_meta: n_expert         = 0
0.00.051.326 I llm_load_print_meta: n_expert_used    = 0
0.00.051.327 I llm_load_print_meta: causal attn      = 1
0.00.051.330 I llm_load_print_meta: pooling type     = 0
0.00.051.330 I llm_load_print_meta: rope type        = 2
0.00.051.330 I llm_load_print_meta: rope scaling     = linear
0.00.051.330 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.331 I llm_load_print_meta: freq_scale_train = 1
0.00.051.331 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.331 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.331 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.331 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.331 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.331 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.332 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.332 I llm_load_print_meta: model type       = 1.4B
0.00.051.332 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.333 I llm_load_print_meta: model params     = 1.41 B
0.00.051.333 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.333 I llm_load_print_meta: general.name     = 1.4B
0.00.051.334 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.338 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.338 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.338 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.339 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.339 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.339 I llm_load_print_meta: max token length = 1024
0.00.053.343 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.343 I llm_load_tensors: offloading output layer to GPU
0.00.053.344 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.354 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.355 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.243 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.244 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.244 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.244 I llama_new_context_with_model: n_batch       = 2048
0.00.054.244 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.244 I llama_new_context_with_model: flash_attn    = 0
0.00.054.245 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.245 I llama_new_context_with_model: freq_scale    = 1
0.00.054.246 I ggml_metal_init: allocating
0.00.054.252 I ggml_metal_init: found device: Apple M4
0.00.054.255 I ggml_metal_init: picking default device: Apple M4
0.00.054.844 I ggml_metal_init: using embedded metal library
0.00.057.219 I ggml_metal_init: GPU name:   Apple M4
0.00.057.221 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.221 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.222 I ggml_metal_init: simdgroup reduction   = true
0.00.057.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.222 I ggml_metal_init: has bfloat            = true
0.00.057.224 I ggml_metal_init: use bfloat            = true
0.00.057.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.226 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.939 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.950 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.972 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.997 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.998 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.998 I llama_new_context_with_model: graph nodes  = 967
0.00.087.999 I llama_new_context_with_model: graph splits = 2
0.00.088.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.182 I main: llama threadpool init, n_threads = 4
0.00.721.231 I 
0.00.721.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.265 I 
0.00.721.513 I sampler seed: 1234
0.00.721.517 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.721.528 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.721.529 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.721.529 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.441.812 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64311.59 tokens per second)
0.01.441.813 I llama_perf_context_print:        load time =     712.44 ms
0.01.441.814 I llama_perf_context_print: prompt eval time =      39.65 ms /     7 tokens (    5.66 ms per token,   176.57 tokens per second)
0.01.441.815 I llama_perf_context_print:        eval time =     677.73 ms /    63 runs   (   10.76 ms per token,    92.96 tokens per second)
0.01.441.815 I llama_perf_context_print:       total time =     720.63 ms /    70 tokens
0.01.442.031 I ggml_metal_free: deallocating

real	0m1.457s
user	0m0.110s
sys	0m0.146s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.627 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.133 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.133 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.134 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.135 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.137 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.137 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.137 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.138 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.139 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.033 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.792 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.793 I llama_model_loader: - type  f32:  194 tensors
0.00.024.793 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.793 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.980 I llm_load_vocab: special tokens cache size = 25
0.00.050.953 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.955 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.955 I llm_load_print_meta: arch             = gptneox
0.00.050.955 I llm_load_print_meta: vocab type       = BPE
0.00.050.956 I llm_load_print_meta: n_vocab          = 50304
0.00.050.956 I llm_load_print_meta: n_merges         = 50009
0.00.050.956 I llm_load_print_meta: vocab_only       = 0
0.00.050.956 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.956 I llm_load_print_meta: n_embd           = 2048
0.00.050.957 I llm_load_print_meta: n_layer          = 24
0.00.050.959 I llm_load_print_meta: n_head           = 16
0.00.050.960 I llm_load_print_meta: n_head_kv        = 16
0.00.050.960 I llm_load_print_meta: n_rot            = 32
0.00.050.961 I llm_load_print_meta: n_swa            = 0
0.00.050.961 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.961 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.962 I llm_load_print_meta: n_gqa            = 1
0.00.050.963 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.963 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.965 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.965 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.966 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.966 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.966 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.967 I llm_load_print_meta: n_ff             = 8192
0.00.050.968 I llm_load_print_meta: n_expert         = 0
0.00.050.968 I llm_load_print_meta: n_expert_used    = 0
0.00.050.968 I llm_load_print_meta: causal attn      = 1
0.00.050.968 I llm_load_print_meta: pooling type     = 0
0.00.050.968 I llm_load_print_meta: rope type        = 2
0.00.050.969 I llm_load_print_meta: rope scaling     = linear
0.00.050.969 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.970 I llm_load_print_meta: freq_scale_train = 1
0.00.050.970 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.970 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.970 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.970 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.972 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.972 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.972 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.973 I llm_load_print_meta: model type       = 1.4B
0.00.050.973 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.973 I llm_load_print_meta: model params     = 1.41 B
0.00.050.974 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.974 I llm_load_print_meta: general.name     = 1.4B
0.00.050.974 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.974 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.975 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.975 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.975 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.979 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.979 I llm_load_print_meta: max token length = 1024
0.00.053.040 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.040 I llm_load_tensors: offloading output layer to GPU
0.00.053.040 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.050 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.052 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.999 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.000 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.001 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.001 I llama_new_context_with_model: n_batch       = 2048
0.00.054.001 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.001 I llama_new_context_with_model: flash_attn    = 0
0.00.054.001 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.002 I llama_new_context_with_model: freq_scale    = 1
0.00.054.002 I ggml_metal_init: allocating
0.00.054.005 I ggml_metal_init: found device: Apple M4
0.00.054.007 I ggml_metal_init: picking default device: Apple M4
0.00.054.596 I ggml_metal_init: using embedded metal library
0.00.056.905 I ggml_metal_init: GPU name:   Apple M4
0.00.056.906 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.907 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.907 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.907 I ggml_metal_init: simdgroup reduction   = true
0.00.056.908 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.908 I ggml_metal_init: has bfloat            = true
0.00.056.908 I ggml_metal_init: use bfloat            = true
0.00.056.908 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.318 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.323 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.341 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.367 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.369 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.369 I llama_new_context_with_model: graph nodes  = 967
0.00.086.369 I llama_new_context_with_model: graph splits = 2
0.00.086.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.121 I main: llama threadpool init, n_threads = 4
0.00.744.157 I 
0.00.744.185 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.186 I 
0.00.744.402 I sampler seed: 1234
0.00.744.407 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.443 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.448 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.448 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.532.690 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.532.691 I llama_perf_context_print:        load time =     734.49 ms
0.01.532.692 I llama_perf_context_print: prompt eval time =      43.28 ms /     7 tokens (    6.18 ms per token,   161.74 tokens per second)
0.01.532.692 I llama_perf_context_print:        eval time =     741.92 ms /    63 runs   (   11.78 ms per token,    84.92 tokens per second)
0.01.532.693 I llama_perf_context_print:       total time =     788.57 ms /    70 tokens
0.01.532.882 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.108s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.621 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.194 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.195 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.196 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.197 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.200 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.200 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.201 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.166 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.972 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.973 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.973 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.973 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.974 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.974 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.975 I llama_model_loader: - type  f32:  194 tensors
0.00.023.975 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.975 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.086 I llm_load_vocab: special tokens cache size = 25
0.00.049.896 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.899 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.899 I llm_load_print_meta: arch             = gptneox
0.00.049.899 I llm_load_print_meta: vocab type       = BPE
0.00.049.899 I llm_load_print_meta: n_vocab          = 50304
0.00.049.900 I llm_load_print_meta: n_merges         = 50009
0.00.049.900 I llm_load_print_meta: vocab_only       = 0
0.00.049.900 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.900 I llm_load_print_meta: n_embd           = 2048
0.00.049.900 I llm_load_print_meta: n_layer          = 24
0.00.049.903 I llm_load_print_meta: n_head           = 16
0.00.049.904 I llm_load_print_meta: n_head_kv        = 16
0.00.049.904 I llm_load_print_meta: n_rot            = 32
0.00.049.904 I llm_load_print_meta: n_swa            = 0
0.00.049.904 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.905 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.905 I llm_load_print_meta: n_gqa            = 1
0.00.049.906 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.907 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.907 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.908 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.908 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.908 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.909 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.910 I llm_load_print_meta: n_ff             = 8192
0.00.049.910 I llm_load_print_meta: n_expert         = 0
0.00.049.910 I llm_load_print_meta: n_expert_used    = 0
0.00.049.912 I llm_load_print_meta: causal attn      = 1
0.00.049.913 I llm_load_print_meta: pooling type     = 0
0.00.049.913 I llm_load_print_meta: rope type        = 2
0.00.049.914 I llm_load_print_meta: rope scaling     = linear
0.00.049.914 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.914 I llm_load_print_meta: freq_scale_train = 1
0.00.049.915 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.915 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.915 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.915 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.915 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.915 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.915 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.916 I llm_load_print_meta: model type       = 1.4B
0.00.049.916 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.917 I llm_load_print_meta: model params     = 1.41 B
0.00.049.917 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.917 I llm_load_print_meta: general.name     = 1.4B
0.00.049.917 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.918 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.918 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.922 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.922 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.923 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.924 I llm_load_print_meta: max token length = 1024
0.00.051.877 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.878 I llm_load_tensors: offloading output layer to GPU
0.00.051.878 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.888 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.889 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.785 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.786 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.786 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.786 I llama_new_context_with_model: n_batch       = 2048
0.00.052.786 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.787 I llama_new_context_with_model: flash_attn    = 0
0.00.052.787 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.787 I llama_new_context_with_model: freq_scale    = 1
0.00.052.788 I ggml_metal_init: allocating
0.00.052.790 I ggml_metal_init: found device: Apple M4
0.00.052.792 I ggml_metal_init: picking default device: Apple M4
0.00.053.387 I ggml_metal_init: using embedded metal library
0.00.055.691 I ggml_metal_init: GPU name:   Apple M4
0.00.055.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.693 I ggml_metal_init: simdgroup reduction   = true
0.00.055.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.695 I ggml_metal_init: has bfloat            = true
0.00.055.695 I ggml_metal_init: use bfloat            = true
0.00.055.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.696 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.395 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.399 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.416 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.349 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.351 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.351 I llama_new_context_with_model: graph nodes  = 967
0.00.085.351 I llama_new_context_with_model: graph splits = 2
0.00.085.365 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.095 I main: llama threadpool init, n_threads = 4
0.00.705.138 I 
0.00.705.188 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.189 I 
0.00.705.427 I sampler seed: 1234
0.00.705.431 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.470 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.470 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.470 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.549.119 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.549.120 I llama_perf_context_print:        load time =     696.47 ms
0.01.549.121 I llama_perf_context_print: prompt eval time =      42.23 ms /     7 tokens (    6.03 ms per token,   165.77 tokens per second)
0.01.549.121 I llama_perf_context_print:        eval time =     798.54 ms /    63 runs   (   12.68 ms per token,    78.89 tokens per second)
0.01.549.122 I llama_perf_context_print:       total time =     844.03 ms /    70 tokens
0.01.549.356 I ggml_metal_free: deallocating

real	0m1.565s
user	0m0.108s
sys	0m0.164s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.140 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.582 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.584 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.584 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.585 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.585 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.586 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.586 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.587 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.590 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.593 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.417 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.483 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.483 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.484 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.484 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.485 I llama_model_loader: - type  f32:  194 tensors
0.00.023.485 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.485 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.485 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.537 I llm_load_vocab: special tokens cache size = 25
0.00.050.512 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.515 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.515 I llm_load_print_meta: arch             = gptneox
0.00.050.516 I llm_load_print_meta: vocab type       = BPE
0.00.050.516 I llm_load_print_meta: n_vocab          = 50304
0.00.050.516 I llm_load_print_meta: n_merges         = 50009
0.00.050.516 I llm_load_print_meta: vocab_only       = 0
0.00.050.517 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.517 I llm_load_print_meta: n_embd           = 2048
0.00.050.517 I llm_load_print_meta: n_layer          = 24
0.00.050.520 I llm_load_print_meta: n_head           = 16
0.00.050.521 I llm_load_print_meta: n_head_kv        = 16
0.00.050.521 I llm_load_print_meta: n_rot            = 32
0.00.050.521 I llm_load_print_meta: n_swa            = 0
0.00.050.521 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.522 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.522 I llm_load_print_meta: n_gqa            = 1
0.00.050.523 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.524 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.524 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.525 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.525 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.525 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.526 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.526 I llm_load_print_meta: n_ff             = 8192
0.00.050.527 I llm_load_print_meta: n_expert         = 0
0.00.050.527 I llm_load_print_meta: n_expert_used    = 0
0.00.050.527 I llm_load_print_meta: causal attn      = 1
0.00.050.527 I llm_load_print_meta: pooling type     = 0
0.00.050.527 I llm_load_print_meta: rope type        = 2
0.00.050.528 I llm_load_print_meta: rope scaling     = linear
0.00.050.528 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.528 I llm_load_print_meta: freq_scale_train = 1
0.00.050.529 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.529 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.529 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.529 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.529 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.529 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.530 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.530 I llm_load_print_meta: model type       = 1.4B
0.00.050.530 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.531 I llm_load_print_meta: model params     = 1.41 B
0.00.050.532 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.532 I llm_load_print_meta: general.name     = 1.4B
0.00.050.532 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.534 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.534 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.535 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.535 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.535 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.535 I llm_load_print_meta: max token length = 1024
0.00.052.439 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.439 I llm_load_tensors: offloading output layer to GPU
0.00.052.439 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.450 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.451 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.411 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.412 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.412 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.412 I llama_new_context_with_model: n_batch       = 2048
0.00.053.412 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.413 I llama_new_context_with_model: flash_attn    = 0
0.00.053.413 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.413 I llama_new_context_with_model: freq_scale    = 1
0.00.053.414 I ggml_metal_init: allocating
0.00.053.417 I ggml_metal_init: found device: Apple M4
0.00.053.419 I ggml_metal_init: picking default device: Apple M4
0.00.054.024 I ggml_metal_init: using embedded metal library
0.00.056.366 I ggml_metal_init: GPU name:   Apple M4
0.00.056.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.368 I ggml_metal_init: simdgroup reduction   = true
0.00.056.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.369 I ggml_metal_init: has bfloat            = true
0.00.056.369 I ggml_metal_init: use bfloat            = true
0.00.056.369 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.068 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.074 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.095 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.119 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.121 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.121 I llama_new_context_with_model: graph nodes  = 967
0.00.088.121 I llama_new_context_with_model: graph splits = 2
0.00.088.137 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.440.616 I main: llama threadpool init, n_threads = 4
0.00.440.650 I 
0.00.440.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.440.684 I 
0.00.440.910 I sampler seed: 1234
0.00.440.916 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.440.978 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.440.982 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.440.983 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.119.221 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.119.222 I llama_perf_context_print:        load time =     431.47 ms
0.01.119.223 I llama_perf_context_print: prompt eval time =      35.93 ms /     7 tokens (    5.13 ms per token,   194.82 tokens per second)
0.01.119.223 I llama_perf_context_print:        eval time =     639.39 ms /    63 runs   (   10.15 ms per token,    98.53 tokens per second)
0.01.119.223 I llama_perf_context_print:       total time =     678.61 ms /    70 tokens
0.01.119.426 I ggml_metal_free: deallocating

real	0m1.135s
user	0m0.110s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.870 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.585 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.586 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.586 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.589 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.590 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.592 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.598 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.668 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.670 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.671 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.671 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.671 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.672 I llama_model_loader: - type  f32:  194 tensors
0.00.024.672 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.673 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.673 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.673 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.546 I llm_load_vocab: special tokens cache size = 25
0.00.051.332 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.335 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.335 I llm_load_print_meta: arch             = gptneox
0.00.051.336 I llm_load_print_meta: vocab type       = BPE
0.00.051.336 I llm_load_print_meta: n_vocab          = 50304
0.00.051.336 I llm_load_print_meta: n_merges         = 50009
0.00.051.336 I llm_load_print_meta: vocab_only       = 0
0.00.051.337 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.337 I llm_load_print_meta: n_embd           = 2048
0.00.051.337 I llm_load_print_meta: n_layer          = 24
0.00.051.340 I llm_load_print_meta: n_head           = 16
0.00.051.341 I llm_load_print_meta: n_head_kv        = 16
0.00.051.341 I llm_load_print_meta: n_rot            = 32
0.00.051.341 I llm_load_print_meta: n_swa            = 0
0.00.051.343 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.343 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.344 I llm_load_print_meta: n_gqa            = 1
0.00.051.345 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.346 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.346 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.347 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.347 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.347 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.347 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.348 I llm_load_print_meta: n_ff             = 8192
0.00.051.348 I llm_load_print_meta: n_expert         = 0
0.00.051.348 I llm_load_print_meta: n_expert_used    = 0
0.00.051.348 I llm_load_print_meta: causal attn      = 1
0.00.051.348 I llm_load_print_meta: pooling type     = 0
0.00.051.348 I llm_load_print_meta: rope type        = 2
0.00.051.349 I llm_load_print_meta: rope scaling     = linear
0.00.051.350 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.350 I llm_load_print_meta: freq_scale_train = 1
0.00.051.351 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.351 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.351 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.351 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.351 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.351 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.352 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.352 I llm_load_print_meta: model type       = 1.4B
0.00.051.352 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.353 I llm_load_print_meta: model params     = 1.41 B
0.00.051.353 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.353 I llm_load_print_meta: general.name     = 1.4B
0.00.051.354 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.354 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.354 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.354 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.355 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.358 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.359 I llm_load_print_meta: max token length = 1024
0.00.053.330 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.330 I llm_load_tensors: offloading output layer to GPU
0.00.053.330 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.341 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.342 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.270 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.270 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.271 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.271 I llama_new_context_with_model: n_batch       = 2048
0.00.054.271 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.271 I llama_new_context_with_model: flash_attn    = 0
0.00.054.272 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.272 I llama_new_context_with_model: freq_scale    = 1
0.00.054.272 I ggml_metal_init: allocating
0.00.054.276 I ggml_metal_init: found device: Apple M4
0.00.054.278 I ggml_metal_init: picking default device: Apple M4
0.00.054.891 I ggml_metal_init: using embedded metal library
0.00.057.222 I ggml_metal_init: GPU name:   Apple M4
0.00.057.223 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.224 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.224 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.224 I ggml_metal_init: simdgroup reduction   = true
0.00.057.224 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.224 I ggml_metal_init: has bfloat            = true
0.00.057.225 I ggml_metal_init: use bfloat            = true
0.00.057.225 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.226 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.388 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.394 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.413 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.451 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.453 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.453 I llama_new_context_with_model: graph nodes  = 967
0.00.087.453 I llama_new_context_with_model: graph splits = 2
0.00.087.467 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.542.584 I main: llama threadpool init, n_threads = 4
0.00.542.623 I 
0.00.542.653 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.542.653 I 
0.00.542.871 I sampler seed: 1234
0.00.542.876 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.542.897 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.542.897 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.542.897 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.290.924 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.290.925 I llama_perf_context_print:        load time =     533.71 ms
0.01.290.927 I llama_perf_context_print: prompt eval time =      43.48 ms /     7 tokens (    6.21 ms per token,   160.98 tokens per second)
0.01.290.927 I llama_perf_context_print:        eval time =     701.62 ms /    63 runs   (   11.14 ms per token,    89.79 tokens per second)
0.01.290.928 I llama_perf_context_print:       total time =     748.34 ms /    70 tokens
0.01.291.113 I ggml_metal_free: deallocating

real	0m1.307s
user	0m0.109s
sys	0m0.128s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.265 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.447 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.454 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.455 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.455 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.456 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.456 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.457 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.457 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.458 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.459 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.460 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.460 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.460 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.463 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.463 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.383 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.151 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.152 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.152 I llama_model_loader: - type  f32:  194 tensors
0.00.024.153 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.153 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.153 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.471 I llm_load_vocab: special tokens cache size = 25
0.00.050.416 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.418 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.419 I llm_load_print_meta: arch             = gptneox
0.00.050.419 I llm_load_print_meta: vocab type       = BPE
0.00.050.419 I llm_load_print_meta: n_vocab          = 50304
0.00.050.420 I llm_load_print_meta: n_merges         = 50009
0.00.050.420 I llm_load_print_meta: vocab_only       = 0
0.00.050.420 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.420 I llm_load_print_meta: n_embd           = 2048
0.00.050.420 I llm_load_print_meta: n_layer          = 24
0.00.050.422 I llm_load_print_meta: n_head           = 16
0.00.050.423 I llm_load_print_meta: n_head_kv        = 16
0.00.050.423 I llm_load_print_meta: n_rot            = 32
0.00.050.423 I llm_load_print_meta: n_swa            = 0
0.00.050.424 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.424 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.425 I llm_load_print_meta: n_gqa            = 1
0.00.050.426 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.427 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.427 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.428 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.428 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.428 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.428 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.430 I llm_load_print_meta: n_ff             = 8192
0.00.050.431 I llm_load_print_meta: n_expert         = 0
0.00.050.432 I llm_load_print_meta: n_expert_used    = 0
0.00.050.433 I llm_load_print_meta: causal attn      = 1
0.00.050.433 I llm_load_print_meta: pooling type     = 0
0.00.050.433 I llm_load_print_meta: rope type        = 2
0.00.050.434 I llm_load_print_meta: rope scaling     = linear
0.00.050.434 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.435 I llm_load_print_meta: freq_scale_train = 1
0.00.050.435 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.435 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.435 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.435 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.435 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.436 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.436 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.436 I llm_load_print_meta: model type       = 1.4B
0.00.050.436 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.437 I llm_load_print_meta: model params     = 1.41 B
0.00.050.437 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.438 I llm_load_print_meta: general.name     = 1.4B
0.00.050.441 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.442 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.442 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.442 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.442 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.443 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.443 I llm_load_print_meta: max token length = 1024
0.00.052.398 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.398 I llm_load_tensors: offloading output layer to GPU
0.00.052.398 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.408 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.409 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.331 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.332 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.332 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.333 I llama_new_context_with_model: n_batch       = 2048
0.00.053.333 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.333 I llama_new_context_with_model: flash_attn    = 0
0.00.053.333 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.334 I llama_new_context_with_model: freq_scale    = 1
0.00.053.334 I ggml_metal_init: allocating
0.00.053.341 I ggml_metal_init: found device: Apple M4
0.00.053.343 I ggml_metal_init: picking default device: Apple M4
0.00.053.936 I ggml_metal_init: using embedded metal library
0.00.056.233 I ggml_metal_init: GPU name:   Apple M4
0.00.056.235 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.237 I ggml_metal_init: simdgroup reduction   = true
0.00.056.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.237 I ggml_metal_init: has bfloat            = true
0.00.056.238 I ggml_metal_init: use bfloat            = true
0.00.056.238 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.239 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.264 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.273 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.293 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.291 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.293 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.293 I llama_new_context_with_model: graph nodes  = 967
0.00.086.293 I llama_new_context_with_model: graph splits = 2
0.00.086.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.882 I main: llama threadpool init, n_threads = 4
0.00.627.919 I 
0.00.627.948 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.948 I 
0.00.628.197 I sampler seed: 1234
0.00.628.201 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.628.238 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.628.240 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.628.240 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.391.934 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47811.45 tokens per second)
0.01.391.935 I llama_perf_context_print:        load time =     618.61 ms
0.01.391.935 I llama_perf_context_print: prompt eval time =      51.94 ms /     7 tokens (    7.42 ms per token,   134.78 tokens per second)
0.01.391.936 I llama_perf_context_print:        eval time =     709.15 ms /    63 runs   (   11.26 ms per token,    88.84 tokens per second)
0.01.391.937 I llama_perf_context_print:       total time =     764.05 ms /    70 tokens
0.01.392.147 I ggml_metal_free: deallocating

real	0m1.409s
user	0m0.109s
sys	0m0.147s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.012.548 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.814 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.819 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.825 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.826 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.826 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.827 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.830 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.830 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.830 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.831 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.864 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.719 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.721 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.721 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.721 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.721 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.722 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.722 I llama_model_loader: - type  f32:  194 tensors
0.00.027.723 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.723 I llama_model_loader: - type q6_K:   37 tensors
0.00.048.076 I llm_load_vocab: special tokens cache size = 25
0.00.054.179 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.182 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.183 I llm_load_print_meta: arch             = gptneox
0.00.054.183 I llm_load_print_meta: vocab type       = BPE
0.00.054.183 I llm_load_print_meta: n_vocab          = 50304
0.00.054.183 I llm_load_print_meta: n_merges         = 50009
0.00.054.183 I llm_load_print_meta: vocab_only       = 0
0.00.054.184 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.184 I llm_load_print_meta: n_embd           = 2048
0.00.054.184 I llm_load_print_meta: n_layer          = 24
0.00.054.186 I llm_load_print_meta: n_head           = 16
0.00.054.187 I llm_load_print_meta: n_head_kv        = 16
0.00.054.187 I llm_load_print_meta: n_rot            = 32
0.00.054.187 I llm_load_print_meta: n_swa            = 0
0.00.054.188 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.188 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.189 I llm_load_print_meta: n_gqa            = 1
0.00.054.189 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.190 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.190 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.191 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.191 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.191 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.191 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.192 I llm_load_print_meta: n_ff             = 8192
0.00.054.192 I llm_load_print_meta: n_expert         = 0
0.00.054.192 I llm_load_print_meta: n_expert_used    = 0
0.00.054.192 I llm_load_print_meta: causal attn      = 1
0.00.054.193 I llm_load_print_meta: pooling type     = 0
0.00.054.193 I llm_load_print_meta: rope type        = 2
0.00.054.193 I llm_load_print_meta: rope scaling     = linear
0.00.054.193 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.194 I llm_load_print_meta: freq_scale_train = 1
0.00.054.194 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.194 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.194 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.194 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.195 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.195 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.195 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.195 I llm_load_print_meta: model type       = 1.4B
0.00.054.196 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.054.196 I llm_load_print_meta: model params     = 1.41 B
0.00.054.197 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.054.197 I llm_load_print_meta: general.name     = 1.4B
0.00.054.197 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.198 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.198 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.200 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.200 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.201 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.201 I llm_load_print_meta: max token length = 1024
0.00.056.209 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.209 I llm_load_tensors: offloading output layer to GPU
0.00.056.209 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.220 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.056.221 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.057.089 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.090 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.090 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.090 I llama_new_context_with_model: n_batch       = 2048
0.00.057.090 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.090 I llama_new_context_with_model: flash_attn    = 0
0.00.057.091 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.091 I llama_new_context_with_model: freq_scale    = 1
0.00.057.092 I ggml_metal_init: allocating
0.00.057.098 I ggml_metal_init: found device: Apple M4
0.00.057.100 I ggml_metal_init: picking default device: Apple M4
0.00.057.663 I ggml_metal_init: using embedded metal library
0.00.060.030 I ggml_metal_init: GPU name:   Apple M4
0.00.060.032 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.033 I ggml_metal_init: simdgroup reduction   = true
0.00.060.033 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.033 I ggml_metal_init: has bfloat            = true
0.00.060.033 I ggml_metal_init: use bfloat            = true
0.00.060.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.034 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.269 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.275 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.291 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.281 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.283 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.283 I llama_new_context_with_model: graph nodes  = 967
0.00.091.283 I llama_new_context_with_model: graph splits = 2
0.00.091.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.753 I main: llama threadpool init, n_threads = 4
0.00.699.794 I 
0.00.699.826 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.826 I 
0.00.700.052 I sampler seed: 1234
0.00.700.057 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.112 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.117 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.117 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.550.456 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.550.457 I llama_perf_context_print:        load time =     687.20 ms
0.01.550.458 I llama_perf_context_print: prompt eval time =      51.48 ms /     7 tokens (    7.35 ms per token,   135.97 tokens per second)
0.01.550.458 I llama_perf_context_print:        eval time =     795.89 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.550.459 I llama_perf_context_print:       total time =     850.71 ms /    70 tokens
0.01.550.649 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.109s
sys	0m0.153s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.925 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.869 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.880 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.882 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.882 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.887 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.888 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.717 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.844 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.692 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.693 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.693 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.694 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.694 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.694 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.695 I llama_model_loader: - type  f32:  194 tensors
0.00.025.695 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.820 I llm_load_vocab: special tokens cache size = 25
0.00.051.645 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.648 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.648 I llm_load_print_meta: arch             = gptneox
0.00.051.649 I llm_load_print_meta: vocab type       = BPE
0.00.051.649 I llm_load_print_meta: n_vocab          = 50304
0.00.051.649 I llm_load_print_meta: n_merges         = 50009
0.00.051.649 I llm_load_print_meta: vocab_only       = 0
0.00.051.650 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.650 I llm_load_print_meta: n_embd           = 2048
0.00.051.650 I llm_load_print_meta: n_layer          = 24
0.00.051.653 I llm_load_print_meta: n_head           = 16
0.00.051.654 I llm_load_print_meta: n_head_kv        = 16
0.00.051.654 I llm_load_print_meta: n_rot            = 32
0.00.051.654 I llm_load_print_meta: n_swa            = 0
0.00.051.657 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.657 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.658 I llm_load_print_meta: n_gqa            = 1
0.00.051.658 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.659 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.660 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.661 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.661 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.662 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.662 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.662 I llm_load_print_meta: n_ff             = 8192
0.00.051.663 I llm_load_print_meta: n_expert         = 0
0.00.051.663 I llm_load_print_meta: n_expert_used    = 0
0.00.051.663 I llm_load_print_meta: causal attn      = 1
0.00.051.665 I llm_load_print_meta: pooling type     = 0
0.00.051.666 I llm_load_print_meta: rope type        = 2
0.00.051.666 I llm_load_print_meta: rope scaling     = linear
0.00.051.666 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.667 I llm_load_print_meta: freq_scale_train = 1
0.00.051.667 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.667 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.667 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.667 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.668 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.668 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.668 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.668 I llm_load_print_meta: model type       = 1.4B
0.00.051.668 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.669 I llm_load_print_meta: model params     = 1.41 B
0.00.051.673 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.673 I llm_load_print_meta: general.name     = 1.4B
0.00.051.673 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.674 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.674 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.674 I llm_load_print_meta: max token length = 1024
0.00.053.696 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.696 I llm_load_tensors: offloading output layer to GPU
0.00.053.696 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.707 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.708 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.650 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.651 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.651 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.651 I llama_new_context_with_model: n_batch       = 2048
0.00.054.651 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.651 I llama_new_context_with_model: flash_attn    = 0
0.00.054.652 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.652 I llama_new_context_with_model: freq_scale    = 1
0.00.054.653 I ggml_metal_init: allocating
0.00.054.659 I ggml_metal_init: found device: Apple M4
0.00.054.661 I ggml_metal_init: picking default device: Apple M4
0.00.055.249 I ggml_metal_init: using embedded metal library
0.00.057.563 I ggml_metal_init: GPU name:   Apple M4
0.00.057.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.566 I ggml_metal_init: simdgroup reduction   = true
0.00.057.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.567 I ggml_metal_init: has bfloat            = true
0.00.057.567 I ggml_metal_init: use bfloat            = true
0.00.057.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.783 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.788 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.805 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.879 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.881 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.881 I llama_new_context_with_model: graph nodes  = 967
0.00.088.881 I llama_new_context_with_model: graph splits = 2
0.00.088.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.799 I main: llama threadpool init, n_threads = 4
0.00.750.835 I 
0.00.750.880 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.882 I 
0.00.751.105 I sampler seed: 1234
0.00.751.109 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.119 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.120 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.120 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.630.917 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57770.55 tokens per second)
0.01.630.918 I llama_perf_context_print:        load time =     741.87 ms
0.01.630.919 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.75 tokens per second)
0.01.630.919 I llama_perf_context_print:        eval time =     822.38 ms /    63 runs   (   13.05 ms per token,    76.61 tokens per second)
0.01.630.920 I llama_perf_context_print:       total time =     880.12 ms /    70 tokens
0.01.631.111 I ggml_metal_free: deallocating

real	0m1.646s
user	0m0.108s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.549 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.754 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.563 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.587 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.590 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.591 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.591 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.592 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.595 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.289 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.076 I llama_model_loader: - type  f32:  194 tensors
0.00.055.076 I llama_model_loader: - type  f16:   98 tensors
0.00.084.270 I llm_load_vocab: special tokens cache size = 25
0.00.090.826 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.828 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.829 I llm_load_print_meta: arch             = gptneox
0.00.090.829 I llm_load_print_meta: vocab type       = BPE
0.00.090.829 I llm_load_print_meta: n_vocab          = 50304
0.00.090.829 I llm_load_print_meta: n_merges         = 50009
0.00.090.829 I llm_load_print_meta: vocab_only       = 0
0.00.090.830 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.830 I llm_load_print_meta: n_embd           = 2048
0.00.090.830 I llm_load_print_meta: n_layer          = 24
0.00.090.833 I llm_load_print_meta: n_head           = 16
0.00.090.833 I llm_load_print_meta: n_head_kv        = 16
0.00.090.834 I llm_load_print_meta: n_rot            = 32
0.00.090.834 I llm_load_print_meta: n_swa            = 0
0.00.090.834 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.834 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.835 I llm_load_print_meta: n_gqa            = 1
0.00.090.835 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.836 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.836 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.837 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.837 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.837 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.837 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.838 I llm_load_print_meta: n_ff             = 8192
0.00.090.838 I llm_load_print_meta: n_expert         = 0
0.00.090.838 I llm_load_print_meta: n_expert_used    = 0
0.00.090.838 I llm_load_print_meta: causal attn      = 1
0.00.090.838 I llm_load_print_meta: pooling type     = 0
0.00.090.838 I llm_load_print_meta: rope type        = 2
0.00.090.838 I llm_load_print_meta: rope scaling     = linear
0.00.090.839 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.839 I llm_load_print_meta: freq_scale_train = 1
0.00.090.839 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.839 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.840 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.840 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.840 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.840 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.840 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.840 I llm_load_print_meta: model type       = 1.4B
0.00.090.841 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.841 I llm_load_print_meta: model params     = 1.41 B
0.00.090.842 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.842 I llm_load_print_meta: general.name     = 1.4B
0.00.090.842 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.842 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.843 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.843 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.843 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.843 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.844 I llm_load_print_meta: max token length = 1024
0.00.093.405 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.405 I llm_load_tensors: offloading output layer to GPU
0.00.093.405 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.416 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.417 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.387 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.388 I llama_new_context_with_model: n_ctx         = 128
0.00.094.388 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.388 I llama_new_context_with_model: n_batch       = 128
0.00.094.388 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.388 I llama_new_context_with_model: flash_attn    = 0
0.00.094.389 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.389 I llama_new_context_with_model: freq_scale    = 1
0.00.094.390 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.390 I ggml_metal_init: allocating
0.00.094.397 I ggml_metal_init: found device: Apple M4
0.00.094.399 I ggml_metal_init: picking default device: Apple M4
0.00.094.973 I ggml_metal_init: using embedded metal library
0.00.097.528 I ggml_metal_init: GPU name:   Apple M4
0.00.097.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.531 I ggml_metal_init: simdgroup reduction   = true
0.00.097.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.531 I ggml_metal_init: has bfloat            = true
0.00.097.531 I ggml_metal_init: use bfloat            = true
0.00.097.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.652 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.654 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.668 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.641 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.643 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.643 I llama_new_context_with_model: graph nodes  = 967
0.00.108.643 I llama_new_context_with_model: graph splits = 2
0.00.108.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.156.005 I 
0.01.156.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.156.065 I perplexity: tokenizing the input ..
0.01.168.720 I perplexity: tokenization took 12.652 ms
0.01.168.752 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.291.673 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.293.484 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.293.530 I llama_perf_context_print:        load time =    1131.24 ms
0.01.293.531 I llama_perf_context_print: prompt eval time =     121.94 ms /   128 tokens (    0.95 ms per token,  1049.66 tokens per second)
0.01.293.533 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.293.536 I llama_perf_context_print:       total time =     137.53 ms /   129 tokens
0.01.294.268 I ggml_metal_free: deallocating

real	0m1.484s
user	0m0.124s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.124 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.429 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.670 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.688 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.690 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.212 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.426 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.426 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.427 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.427 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.427 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.428 I llama_model_loader: - type  f32:  194 tensors
0.00.032.429 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.987 I llm_load_vocab: special tokens cache size = 25
0.00.064.214 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.217 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.217 I llm_load_print_meta: arch             = gptneox
0.00.064.218 I llm_load_print_meta: vocab type       = BPE
0.00.064.218 I llm_load_print_meta: n_vocab          = 50304
0.00.064.218 I llm_load_print_meta: n_merges         = 50009
0.00.064.218 I llm_load_print_meta: vocab_only       = 0
0.00.064.218 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.219 I llm_load_print_meta: n_embd           = 2048
0.00.064.219 I llm_load_print_meta: n_layer          = 24
0.00.064.222 I llm_load_print_meta: n_head           = 16
0.00.064.223 I llm_load_print_meta: n_head_kv        = 16
0.00.064.223 I llm_load_print_meta: n_rot            = 32
0.00.064.223 I llm_load_print_meta: n_swa            = 0
0.00.064.223 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.223 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.224 I llm_load_print_meta: n_gqa            = 1
0.00.064.225 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.225 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.226 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.226 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.226 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.226 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.226 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.227 I llm_load_print_meta: n_ff             = 8192
0.00.064.227 I llm_load_print_meta: n_expert         = 0
0.00.064.227 I llm_load_print_meta: n_expert_used    = 0
0.00.064.227 I llm_load_print_meta: causal attn      = 1
0.00.064.228 I llm_load_print_meta: pooling type     = 0
0.00.064.228 I llm_load_print_meta: rope type        = 2
0.00.064.228 I llm_load_print_meta: rope scaling     = linear
0.00.064.228 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.231 I llm_load_print_meta: freq_scale_train = 1
0.00.064.231 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.231 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.231 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.231 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.231 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.232 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.232 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.232 I llm_load_print_meta: model type       = 1.4B
0.00.064.232 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.236 I llm_load_print_meta: model params     = 1.41 B
0.00.064.237 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.237 I llm_load_print_meta: general.name     = 1.4B
0.00.064.237 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.237 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.237 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.237 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.238 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.238 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.239 I llm_load_print_meta: max token length = 1024
0.00.066.556 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.556 I llm_load_tensors: offloading output layer to GPU
0.00.066.557 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.567 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.569 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.508 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.509 I llama_new_context_with_model: n_ctx         = 128
0.00.067.509 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.510 I llama_new_context_with_model: n_batch       = 128
0.00.067.510 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.510 I llama_new_context_with_model: flash_attn    = 0
0.00.067.510 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.511 I llama_new_context_with_model: freq_scale    = 1
0.00.067.511 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.512 I ggml_metal_init: allocating
0.00.067.518 I ggml_metal_init: found device: Apple M4
0.00.067.521 I ggml_metal_init: picking default device: Apple M4
0.00.068.134 I ggml_metal_init: using embedded metal library
0.00.070.503 I ggml_metal_init: GPU name:   Apple M4
0.00.070.504 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.504 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.505 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.505 I ggml_metal_init: simdgroup reduction   = true
0.00.070.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.505 I ggml_metal_init: has bfloat            = true
0.00.070.505 I ggml_metal_init: use bfloat            = true
0.00.070.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.089 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.093 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.108 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.064 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.082.065 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.082.065 I llama_new_context_with_model: graph nodes  = 967
0.00.082.065 I llama_new_context_with_model: graph splits = 2
0.00.082.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.913.379 I 
0.00.913.406 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.913.437 I perplexity: tokenizing the input ..
0.00.921.021 I perplexity: tokenization took 7.582 ms
0.00.921.035 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.044.992 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.046.227 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.046.243 I llama_perf_context_print:        load time =     901.95 ms
0.01.046.245 I llama_perf_context_print: prompt eval time =     123.73 ms /   128 tokens (    0.97 ms per token,  1034.49 tokens per second)
0.01.046.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.046.247 I llama_perf_context_print:       total time =     132.86 ms /   129 tokens
0.01.046.630 I ggml_metal_free: deallocating

real	0m1.063s
user	0m0.091s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.319 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.182 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.192 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.192 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.194 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.195 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.196 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.196 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.197 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.197 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.197 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.198 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.199 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.199 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.199 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.937 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.939 I llama_model_loader: - type  f32:  194 tensors
0.00.023.940 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.940 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.908 I llm_load_vocab: special tokens cache size = 25
0.00.049.889 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.893 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.893 I llm_load_print_meta: arch             = gptneox
0.00.049.893 I llm_load_print_meta: vocab type       = BPE
0.00.049.894 I llm_load_print_meta: n_vocab          = 50304
0.00.049.894 I llm_load_print_meta: n_merges         = 50009
0.00.049.894 I llm_load_print_meta: vocab_only       = 0
0.00.049.894 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.894 I llm_load_print_meta: n_embd           = 2048
0.00.049.895 I llm_load_print_meta: n_layer          = 24
0.00.049.897 I llm_load_print_meta: n_head           = 16
0.00.049.898 I llm_load_print_meta: n_head_kv        = 16
0.00.049.898 I llm_load_print_meta: n_rot            = 32
0.00.049.900 I llm_load_print_meta: n_swa            = 0
0.00.049.900 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.900 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.901 I llm_load_print_meta: n_gqa            = 1
0.00.049.902 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.902 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.903 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.903 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.903 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.904 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.904 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.904 I llm_load_print_meta: n_ff             = 8192
0.00.049.905 I llm_load_print_meta: n_expert         = 0
0.00.049.905 I llm_load_print_meta: n_expert_used    = 0
0.00.049.905 I llm_load_print_meta: causal attn      = 1
0.00.049.905 I llm_load_print_meta: pooling type     = 0
0.00.049.905 I llm_load_print_meta: rope type        = 2
0.00.049.905 I llm_load_print_meta: rope scaling     = linear
0.00.049.906 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.906 I llm_load_print_meta: freq_scale_train = 1
0.00.049.906 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.907 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.907 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.907 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.907 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.907 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.907 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.908 I llm_load_print_meta: model type       = 1.4B
0.00.049.908 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.908 I llm_load_print_meta: model params     = 1.41 B
0.00.049.909 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.909 I llm_load_print_meta: general.name     = 1.4B
0.00.049.909 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.909 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.910 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.910 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.910 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.910 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.911 I llm_load_print_meta: max token length = 1024
0.00.051.654 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.654 I llm_load_tensors: offloading output layer to GPU
0.00.051.654 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.660 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.660 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.683 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.684 I llama_new_context_with_model: n_ctx         = 128
0.00.052.684 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.684 I llama_new_context_with_model: n_batch       = 128
0.00.052.684 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.685 I llama_new_context_with_model: flash_attn    = 0
0.00.052.685 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.685 I llama_new_context_with_model: freq_scale    = 1
0.00.052.685 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.686 I ggml_metal_init: allocating
0.00.052.689 I ggml_metal_init: found device: Apple M4
0.00.052.691 I ggml_metal_init: picking default device: Apple M4
0.00.053.248 I ggml_metal_init: using embedded metal library
0.00.055.588 I ggml_metal_init: GPU name:   Apple M4
0.00.055.590 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.590 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.591 I ggml_metal_init: simdgroup reduction   = true
0.00.055.591 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.591 I ggml_metal_init: has bfloat            = true
0.00.055.591 I ggml_metal_init: use bfloat            = true
0.00.055.592 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.592 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.419 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.421 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.446 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.356 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.358 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.358 I llama_new_context_with_model: graph nodes  = 967
0.00.068.358 I llama_new_context_with_model: graph splits = 2
0.00.068.365 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.841 I 
0.00.655.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.887 I perplexity: tokenizing the input ..
0.00.663.428 I perplexity: tokenization took 7.539 ms
0.00.663.442 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.926 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.787.137 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.787.151 I llama_perf_context_print:        load time =     646.52 ms
0.00.787.152 I llama_perf_context_print: prompt eval time =     122.26 ms /   128 tokens (    0.96 ms per token,  1046.96 tokens per second)
0.00.787.153 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.153 I llama_perf_context_print:       total time =     131.31 ms /   129 tokens
0.00.787.533 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.077s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.551 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.347 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.348 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.349 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.350 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.351 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.194 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.335 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.335 I llama_model_loader: - type  f32:  194 tensors
0.00.023.336 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.336 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.233 I llm_load_vocab: special tokens cache size = 25
0.00.050.227 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.229 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.230 I llm_load_print_meta: arch             = gptneox
0.00.050.230 I llm_load_print_meta: vocab type       = BPE
0.00.050.230 I llm_load_print_meta: n_vocab          = 50304
0.00.050.230 I llm_load_print_meta: n_merges         = 50009
0.00.050.231 I llm_load_print_meta: vocab_only       = 0
0.00.050.231 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.231 I llm_load_print_meta: n_embd           = 2048
0.00.050.231 I llm_load_print_meta: n_layer          = 24
0.00.050.234 I llm_load_print_meta: n_head           = 16
0.00.050.235 I llm_load_print_meta: n_head_kv        = 16
0.00.050.235 I llm_load_print_meta: n_rot            = 32
0.00.050.235 I llm_load_print_meta: n_swa            = 0
0.00.050.235 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.235 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.236 I llm_load_print_meta: n_gqa            = 1
0.00.050.237 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.238 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.238 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.239 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.239 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.239 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.239 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.240 I llm_load_print_meta: n_ff             = 8192
0.00.050.240 I llm_load_print_meta: n_expert         = 0
0.00.050.241 I llm_load_print_meta: n_expert_used    = 0
0.00.050.241 I llm_load_print_meta: causal attn      = 1
0.00.050.241 I llm_load_print_meta: pooling type     = 0
0.00.050.241 I llm_load_print_meta: rope type        = 2
0.00.050.241 I llm_load_print_meta: rope scaling     = linear
0.00.050.242 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.242 I llm_load_print_meta: freq_scale_train = 1
0.00.050.242 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.242 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.244 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.244 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.244 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.244 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.245 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.245 I llm_load_print_meta: model type       = 1.4B
0.00.050.245 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.246 I llm_load_print_meta: model params     = 1.41 B
0.00.050.246 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.247 I llm_load_print_meta: general.name     = 1.4B
0.00.050.247 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.247 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.247 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.247 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.248 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: max token length = 1024
0.00.052.228 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.229 I llm_load_tensors: offloading output layer to GPU
0.00.052.229 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.239 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.240 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.138 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.138 I llama_new_context_with_model: n_ctx         = 128
0.00.053.139 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.139 I llama_new_context_with_model: n_batch       = 128
0.00.053.139 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.139 I llama_new_context_with_model: flash_attn    = 0
0.00.053.140 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.140 I llama_new_context_with_model: freq_scale    = 1
0.00.053.140 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.141 I ggml_metal_init: allocating
0.00.053.144 I ggml_metal_init: found device: Apple M4
0.00.053.146 I ggml_metal_init: picking default device: Apple M4
0.00.053.715 I ggml_metal_init: using embedded metal library
0.00.056.024 I ggml_metal_init: GPU name:   Apple M4
0.00.056.025 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.026 I ggml_metal_init: simdgroup reduction   = true
0.00.056.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.027 I ggml_metal_init: has bfloat            = true
0.00.056.027 I ggml_metal_init: use bfloat            = true
0.00.056.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.028 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.046 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.050 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.063 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.048 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.049 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.050 I llama_new_context_with_model: graph nodes  = 967
0.00.068.050 I llama_new_context_with_model: graph splits = 2
0.00.068.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.295 I 
0.00.689.326 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.335 I perplexity: tokenizing the input ..
0.00.697.165 I perplexity: tokenization took 7.828 ms
0.00.697.176 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.332 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.820.777 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.820.797 I llama_perf_context_print:        load time =     680.74 ms
0.00.820.798 I llama_perf_context_print: prompt eval time =     121.92 ms /   128 tokens (    0.95 ms per token,  1049.86 tokens per second)
0.00.820.799 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.799 I llama_perf_context_print:       total time =     131.50 ms /   129 tokens
0.00.821.152 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.080s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.019 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.761 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.767 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.768 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.768 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.769 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.769 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.770 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.770 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.770 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.771 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.772 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.774 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.546 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.548 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.549 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.549 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.550 I llama_model_loader: - type  f32:  194 tensors
0.00.024.550 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.550 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.332 I llm_load_vocab: special tokens cache size = 25
0.00.051.440 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.444 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.445 I llm_load_print_meta: arch             = gptneox
0.00.051.445 I llm_load_print_meta: vocab type       = BPE
0.00.051.445 I llm_load_print_meta: n_vocab          = 50304
0.00.051.445 I llm_load_print_meta: n_merges         = 50009
0.00.051.446 I llm_load_print_meta: vocab_only       = 0
0.00.051.446 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.446 I llm_load_print_meta: n_embd           = 2048
0.00.051.446 I llm_load_print_meta: n_layer          = 24
0.00.051.450 I llm_load_print_meta: n_head           = 16
0.00.051.451 I llm_load_print_meta: n_head_kv        = 16
0.00.051.451 I llm_load_print_meta: n_rot            = 32
0.00.051.451 I llm_load_print_meta: n_swa            = 0
0.00.051.452 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.452 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.452 I llm_load_print_meta: n_gqa            = 1
0.00.051.453 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.454 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.454 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.455 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.455 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.455 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.455 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.456 I llm_load_print_meta: n_ff             = 8192
0.00.051.456 I llm_load_print_meta: n_expert         = 0
0.00.051.456 I llm_load_print_meta: n_expert_used    = 0
0.00.051.456 I llm_load_print_meta: causal attn      = 1
0.00.051.456 I llm_load_print_meta: pooling type     = 0
0.00.051.456 I llm_load_print_meta: rope type        = 2
0.00.051.457 I llm_load_print_meta: rope scaling     = linear
0.00.051.457 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.457 I llm_load_print_meta: freq_scale_train = 1
0.00.051.457 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.458 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.458 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.458 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.458 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.458 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.458 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.458 I llm_load_print_meta: model type       = 1.4B
0.00.051.459 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.459 I llm_load_print_meta: model params     = 1.41 B
0.00.051.460 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.460 I llm_load_print_meta: general.name     = 1.4B
0.00.051.460 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.460 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.460 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.460 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.461 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.461 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.461 I llm_load_print_meta: max token length = 1024
0.00.053.491 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.491 I llm_load_tensors: offloading output layer to GPU
0.00.053.491 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.502 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.503 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.401 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.402 I llama_new_context_with_model: n_ctx         = 128
0.00.054.402 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.402 I llama_new_context_with_model: n_batch       = 128
0.00.054.402 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.402 I llama_new_context_with_model: flash_attn    = 0
0.00.054.403 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.403 I llama_new_context_with_model: freq_scale    = 1
0.00.054.404 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.405 I ggml_metal_init: allocating
0.00.054.414 I ggml_metal_init: found device: Apple M4
0.00.054.418 I ggml_metal_init: picking default device: Apple M4
0.00.055.036 I ggml_metal_init: using embedded metal library
0.00.057.423 I ggml_metal_init: GPU name:   Apple M4
0.00.057.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.426 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.426 I ggml_metal_init: simdgroup reduction   = true
0.00.057.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.427 I ggml_metal_init: has bfloat            = true
0.00.057.427 I ggml_metal_init: use bfloat            = true
0.00.057.427 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.428 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.006 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.011 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.029 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.999 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.000 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.000 I llama_new_context_with_model: graph nodes  = 967
0.00.070.000 I llama_new_context_with_model: graph splits = 2
0.00.070.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.352 I 
0.00.706.387 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.398 I perplexity: tokenizing the input ..
0.00.714.029 I perplexity: tokenization took 7.631 ms
0.00.714.040 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.090 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.849.331 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.849.353 I llama_perf_context_print:        load time =     696.33 ms
0.00.849.354 I llama_perf_context_print: prompt eval time =     133.75 ms /   128 tokens (    1.04 ms per token,   957.02 tokens per second)
0.00.849.355 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.356 I llama_perf_context_print:       total time =     143.00 ms /   129 tokens
0.00.849.761 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.079s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.775 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.103 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.109 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.109 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.110 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.111 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.111 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.111 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.112 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.114 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.114 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.116 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.117 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.070 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.888 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.889 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.889 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.889 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.890 I llama_model_loader: - type  f32:  194 tensors
0.00.022.890 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.890 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.247 I llm_load_vocab: special tokens cache size = 25
0.00.049.340 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.343 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.343 I llm_load_print_meta: arch             = gptneox
0.00.049.344 I llm_load_print_meta: vocab type       = BPE
0.00.049.344 I llm_load_print_meta: n_vocab          = 50304
0.00.049.344 I llm_load_print_meta: n_merges         = 50009
0.00.049.344 I llm_load_print_meta: vocab_only       = 0
0.00.049.344 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.344 I llm_load_print_meta: n_embd           = 2048
0.00.049.345 I llm_load_print_meta: n_layer          = 24
0.00.049.348 I llm_load_print_meta: n_head           = 16
0.00.049.349 I llm_load_print_meta: n_head_kv        = 16
0.00.049.349 I llm_load_print_meta: n_rot            = 32
0.00.049.349 I llm_load_print_meta: n_swa            = 0
0.00.049.349 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.349 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.352 I llm_load_print_meta: n_gqa            = 1
0.00.049.353 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.354 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.354 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.354 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.355 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.355 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.355 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.356 I llm_load_print_meta: n_ff             = 8192
0.00.049.356 I llm_load_print_meta: n_expert         = 0
0.00.049.358 I llm_load_print_meta: n_expert_used    = 0
0.00.049.358 I llm_load_print_meta: causal attn      = 1
0.00.049.358 I llm_load_print_meta: pooling type     = 0
0.00.049.358 I llm_load_print_meta: rope type        = 2
0.00.049.358 I llm_load_print_meta: rope scaling     = linear
0.00.049.359 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.359 I llm_load_print_meta: freq_scale_train = 1
0.00.049.359 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.359 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.360 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.360 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.360 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.360 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.360 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.360 I llm_load_print_meta: model type       = 1.4B
0.00.049.361 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.361 I llm_load_print_meta: model params     = 1.41 B
0.00.049.362 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.362 I llm_load_print_meta: general.name     = 1.4B
0.00.049.362 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.362 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.363 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.363 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.363 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.363 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.363 I llm_load_print_meta: max token length = 1024
0.00.051.227 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.227 I llm_load_tensors: offloading output layer to GPU
0.00.051.227 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.238 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.239 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.133 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.134 I llama_new_context_with_model: n_ctx         = 128
0.00.052.134 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.134 I llama_new_context_with_model: n_batch       = 128
0.00.052.134 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.135 I llama_new_context_with_model: flash_attn    = 0
0.00.052.135 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.135 I llama_new_context_with_model: freq_scale    = 1
0.00.052.136 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.136 I ggml_metal_init: allocating
0.00.052.139 I ggml_metal_init: found device: Apple M4
0.00.052.141 I ggml_metal_init: picking default device: Apple M4
0.00.052.717 I ggml_metal_init: using embedded metal library
0.00.055.057 I ggml_metal_init: GPU name:   Apple M4
0.00.055.059 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.059 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.060 I ggml_metal_init: simdgroup reduction   = true
0.00.055.060 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.060 I ggml_metal_init: has bfloat            = true
0.00.055.060 I ggml_metal_init: use bfloat            = true
0.00.055.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.061 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.425 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.430 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.445 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.363 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.364 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.364 I llama_new_context_with_model: graph nodes  = 967
0.00.066.364 I llama_new_context_with_model: graph splits = 2
0.00.066.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.857 I 
0.00.625.902 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.915 I perplexity: tokenizing the input ..
0.00.633.457 I perplexity: tokenization took 7.541 ms
0.00.633.467 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.768.283 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.769.445 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.769.462 I llama_perf_context_print:        load time =     617.08 ms
0.00.769.463 I llama_perf_context_print: prompt eval time =     134.58 ms /   128 tokens (    1.05 ms per token,   951.13 tokens per second)
0.00.769.463 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.769.464 I llama_perf_context_print:       total time =     143.60 ms /   129 tokens
0.00.769.975 I ggml_metal_free: deallocating

real	0m0.783s
user	0m0.078s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.566 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.137 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.141 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.143 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.143 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.144 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.144 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.144 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.145 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.148 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.148 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.148 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.149 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.149 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.151 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.151 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.151 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.935 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.726 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.727 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.728 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.728 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.728 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.728 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.729 I llama_model_loader: - type  f32:  194 tensors
0.00.023.729 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.729 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.730 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.685 I llm_load_vocab: special tokens cache size = 25
0.00.050.669 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.671 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.672 I llm_load_print_meta: arch             = gptneox
0.00.050.672 I llm_load_print_meta: vocab type       = BPE
0.00.050.673 I llm_load_print_meta: n_vocab          = 50304
0.00.050.673 I llm_load_print_meta: n_merges         = 50009
0.00.050.673 I llm_load_print_meta: vocab_only       = 0
0.00.050.673 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.673 I llm_load_print_meta: n_embd           = 2048
0.00.050.673 I llm_load_print_meta: n_layer          = 24
0.00.050.676 I llm_load_print_meta: n_head           = 16
0.00.050.679 I llm_load_print_meta: n_head_kv        = 16
0.00.050.680 I llm_load_print_meta: n_rot            = 32
0.00.050.680 I llm_load_print_meta: n_swa            = 0
0.00.050.680 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.680 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.681 I llm_load_print_meta: n_gqa            = 1
0.00.050.681 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.682 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.683 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.683 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.683 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.683 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.683 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.684 I llm_load_print_meta: n_ff             = 8192
0.00.050.684 I llm_load_print_meta: n_expert         = 0
0.00.050.688 I llm_load_print_meta: n_expert_used    = 0
0.00.050.689 I llm_load_print_meta: causal attn      = 1
0.00.050.689 I llm_load_print_meta: pooling type     = 0
0.00.050.689 I llm_load_print_meta: rope type        = 2
0.00.050.689 I llm_load_print_meta: rope scaling     = linear
0.00.050.690 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.690 I llm_load_print_meta: freq_scale_train = 1
0.00.050.691 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.691 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.691 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.691 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.691 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.691 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.692 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.692 I llm_load_print_meta: model type       = 1.4B
0.00.050.693 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.693 I llm_load_print_meta: model params     = 1.41 B
0.00.050.694 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.695 I llm_load_print_meta: general.name     = 1.4B
0.00.050.695 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.696 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.696 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.696 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.696 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.696 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.696 I llm_load_print_meta: max token length = 1024
0.00.052.605 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.605 I llm_load_tensors: offloading output layer to GPU
0.00.052.606 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.616 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.617 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.559 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.560 I llama_new_context_with_model: n_ctx         = 128
0.00.053.560 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.560 I llama_new_context_with_model: n_batch       = 128
0.00.053.560 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.560 I llama_new_context_with_model: flash_attn    = 0
0.00.053.561 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.561 I llama_new_context_with_model: freq_scale    = 1
0.00.053.561 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.562 I ggml_metal_init: allocating
0.00.053.568 I ggml_metal_init: found device: Apple M4
0.00.053.571 I ggml_metal_init: picking default device: Apple M4
0.00.054.134 I ggml_metal_init: using embedded metal library
0.00.056.471 I ggml_metal_init: GPU name:   Apple M4
0.00.056.472 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.473 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.473 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.473 I ggml_metal_init: simdgroup reduction   = true
0.00.056.473 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.474 I ggml_metal_init: has bfloat            = true
0.00.056.474 I ggml_metal_init: use bfloat            = true
0.00.056.474 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.475 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.966 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.969 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.984 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.844 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.844 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.845 I llama_new_context_with_model: graph nodes  = 967
0.00.067.845 I llama_new_context_with_model: graph splits = 2
0.00.067.857 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.386.617 I 
0.00.386.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.386.695 I perplexity: tokenizing the input ..
0.00.394.166 I perplexity: tokenization took 7.47 ms
0.00.394.176 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.526.049 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.527.202 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.527.219 I llama_perf_context_print:        load time =     377.04 ms
0.00.527.220 I llama_perf_context_print: prompt eval time =     131.65 ms /   128 tokens (    1.03 ms per token,   972.31 tokens per second)
0.00.527.221 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.527.222 I llama_perf_context_print:       total time =     140.61 ms /   129 tokens
0.00.527.724 I ggml_metal_free: deallocating

real	0m0.543s
user	0m0.078s
sys	0m0.069s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.567 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.259 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.264 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.265 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.266 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.266 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.272 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.272 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.273 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.224 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.004 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.005 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.006 I llama_model_loader: - type  f32:  194 tensors
0.00.023.007 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.007 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.007 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.007 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.008 I llm_load_vocab: special tokens cache size = 25
0.00.048.962 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.965 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.965 I llm_load_print_meta: arch             = gptneox
0.00.048.966 I llm_load_print_meta: vocab type       = BPE
0.00.048.966 I llm_load_print_meta: n_vocab          = 50304
0.00.048.966 I llm_load_print_meta: n_merges         = 50009
0.00.048.966 I llm_load_print_meta: vocab_only       = 0
0.00.048.966 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.967 I llm_load_print_meta: n_embd           = 2048
0.00.048.967 I llm_load_print_meta: n_layer          = 24
0.00.048.969 I llm_load_print_meta: n_head           = 16
0.00.048.976 I llm_load_print_meta: n_head_kv        = 16
0.00.048.977 I llm_load_print_meta: n_rot            = 32
0.00.048.977 I llm_load_print_meta: n_swa            = 0
0.00.048.977 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.977 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.985 I llm_load_print_meta: n_gqa            = 1
0.00.048.986 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.987 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.988 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.988 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.988 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.988 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.989 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.990 I llm_load_print_meta: n_ff             = 8192
0.00.048.990 I llm_load_print_meta: n_expert         = 0
0.00.048.990 I llm_load_print_meta: n_expert_used    = 0
0.00.048.990 I llm_load_print_meta: causal attn      = 1
0.00.048.990 I llm_load_print_meta: pooling type     = 0
0.00.048.990 I llm_load_print_meta: rope type        = 2
0.00.048.992 I llm_load_print_meta: rope scaling     = linear
0.00.048.992 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.993 I llm_load_print_meta: freq_scale_train = 1
0.00.048.993 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.993 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.993 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.993 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.994 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.995 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.995 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.995 I llm_load_print_meta: model type       = 1.4B
0.00.048.995 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.996 I llm_load_print_meta: model params     = 1.41 B
0.00.048.996 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.997 I llm_load_print_meta: general.name     = 1.4B
0.00.048.997 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.997 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.997 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.997 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.998 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.999 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.999 I llm_load_print_meta: max token length = 1024
0.00.050.733 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.733 I llm_load_tensors: offloading output layer to GPU
0.00.050.733 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.739 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.739 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.643 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.644 I llama_new_context_with_model: n_ctx         = 128
0.00.051.644 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.644 I llama_new_context_with_model: n_batch       = 128
0.00.051.645 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.645 I llama_new_context_with_model: flash_attn    = 0
0.00.051.645 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.646 I llama_new_context_with_model: freq_scale    = 1
0.00.051.646 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.646 I ggml_metal_init: allocating
0.00.051.651 I ggml_metal_init: found device: Apple M4
0.00.051.654 I ggml_metal_init: picking default device: Apple M4
0.00.052.237 I ggml_metal_init: using embedded metal library
0.00.054.835 I ggml_metal_init: GPU name:   Apple M4
0.00.054.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.837 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.838 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.838 I ggml_metal_init: simdgroup reduction   = true
0.00.054.838 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.838 I ggml_metal_init: has bfloat            = true
0.00.054.838 I ggml_metal_init: use bfloat            = true
0.00.054.839 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.840 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.471 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.473 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.487 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.398 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.399 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.400 I llama_new_context_with_model: graph nodes  = 967
0.00.066.400 I llama_new_context_with_model: graph splits = 2
0.00.066.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.497.257 I 
0.00.497.298 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.497.306 I perplexity: tokenizing the input ..
0.00.504.700 I perplexity: tokenization took 7.393 ms
0.00.504.715 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.635.922 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.637.081 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.637.098 I llama_perf_context_print:        load time =     488.68 ms
0.00.637.099 I llama_perf_context_print: prompt eval time =     130.98 ms /   128 tokens (    1.02 ms per token,   977.23 tokens per second)
0.00.637.100 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.637.100 I llama_perf_context_print:       total time =     139.84 ms /   129 tokens
0.00.637.555 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.077s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.131 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.997 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.999 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.999 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.000 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.000 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.000 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.001 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.001 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.002 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.002 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.002 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.003 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.003 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.005 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.005 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.005 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.965 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.067 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.067 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.068 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.068 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.069 I llama_model_loader: - type  f32:  194 tensors
0.00.024.069 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.069 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.070 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.869 I llm_load_vocab: special tokens cache size = 25
0.00.050.949 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.952 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.952 I llm_load_print_meta: arch             = gptneox
0.00.050.952 I llm_load_print_meta: vocab type       = BPE
0.00.050.953 I llm_load_print_meta: n_vocab          = 50304
0.00.050.953 I llm_load_print_meta: n_merges         = 50009
0.00.050.953 I llm_load_print_meta: vocab_only       = 0
0.00.050.953 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.953 I llm_load_print_meta: n_embd           = 2048
0.00.050.954 I llm_load_print_meta: n_layer          = 24
0.00.050.956 I llm_load_print_meta: n_head           = 16
0.00.050.957 I llm_load_print_meta: n_head_kv        = 16
0.00.050.957 I llm_load_print_meta: n_rot            = 32
0.00.050.957 I llm_load_print_meta: n_swa            = 0
0.00.050.958 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.958 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.959 I llm_load_print_meta: n_gqa            = 1
0.00.050.959 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.960 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.960 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.963 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.963 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.963 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.963 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.966 I llm_load_print_meta: n_ff             = 8192
0.00.050.966 I llm_load_print_meta: n_expert         = 0
0.00.050.966 I llm_load_print_meta: n_expert_used    = 0
0.00.050.966 I llm_load_print_meta: causal attn      = 1
0.00.050.967 I llm_load_print_meta: pooling type     = 0
0.00.050.967 I llm_load_print_meta: rope type        = 2
0.00.050.967 I llm_load_print_meta: rope scaling     = linear
0.00.050.967 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.968 I llm_load_print_meta: freq_scale_train = 1
0.00.050.968 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.968 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.968 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.968 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.969 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.969 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.969 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.969 I llm_load_print_meta: model type       = 1.4B
0.00.050.969 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.970 I llm_load_print_meta: model params     = 1.41 B
0.00.050.970 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.971 I llm_load_print_meta: general.name     = 1.4B
0.00.050.971 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.972 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.972 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.972 I llm_load_print_meta: max token length = 1024
0.00.052.975 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.975 I llm_load_tensors: offloading output layer to GPU
0.00.052.975 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.986 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.987 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.899 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.900 I llama_new_context_with_model: n_ctx         = 128
0.00.053.900 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.900 I llama_new_context_with_model: n_batch       = 128
0.00.053.900 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.901 I llama_new_context_with_model: flash_attn    = 0
0.00.053.901 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.901 I llama_new_context_with_model: freq_scale    = 1
0.00.053.902 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.902 I ggml_metal_init: allocating
0.00.053.907 I ggml_metal_init: found device: Apple M4
0.00.053.910 I ggml_metal_init: picking default device: Apple M4
0.00.054.474 I ggml_metal_init: using embedded metal library
0.00.056.807 I ggml_metal_init: GPU name:   Apple M4
0.00.056.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.810 I ggml_metal_init: simdgroup reduction   = true
0.00.056.810 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.810 I ggml_metal_init: has bfloat            = true
0.00.056.810 I ggml_metal_init: use bfloat            = true
0.00.056.811 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.365 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.367 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.383 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.307 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.308 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.309 I llama_new_context_with_model: graph nodes  = 967
0.00.068.309 I llama_new_context_with_model: graph splits = 2
0.00.068.321 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.633 I 
0.00.575.689 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.575.709 I perplexity: tokenizing the input ..
0.00.583.338 I perplexity: tokenization took 7.628 ms
0.00.583.348 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.716.954 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.718.149 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.718.171 I llama_perf_context_print:        load time =     566.50 ms
0.00.718.172 I llama_perf_context_print: prompt eval time =     133.37 ms /   128 tokens (    1.04 ms per token,   959.72 tokens per second)
0.00.718.173 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.718.173 I llama_perf_context_print:       total time =     142.54 ms /   129 tokens
0.00.718.722 I ggml_metal_free: deallocating

real	0m0.732s
user	0m0.079s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.510 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.083 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.087 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.089 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.090 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.090 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.091 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.092 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.092 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.092 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.093 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.094 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.094 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.095 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.097 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.098 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.989 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.727 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.728 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.729 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.729 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.730 I llama_model_loader: - type  f32:  194 tensors
0.00.023.730 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.730 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.710 I llm_load_vocab: special tokens cache size = 25
0.00.049.641 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.644 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.645 I llm_load_print_meta: arch             = gptneox
0.00.049.645 I llm_load_print_meta: vocab type       = BPE
0.00.049.645 I llm_load_print_meta: n_vocab          = 50304
0.00.049.645 I llm_load_print_meta: n_merges         = 50009
0.00.049.646 I llm_load_print_meta: vocab_only       = 0
0.00.049.646 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.646 I llm_load_print_meta: n_embd           = 2048
0.00.049.646 I llm_load_print_meta: n_layer          = 24
0.00.049.650 I llm_load_print_meta: n_head           = 16
0.00.049.651 I llm_load_print_meta: n_head_kv        = 16
0.00.049.653 I llm_load_print_meta: n_rot            = 32
0.00.049.653 I llm_load_print_meta: n_swa            = 0
0.00.049.653 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.653 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.654 I llm_load_print_meta: n_gqa            = 1
0.00.049.655 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.656 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.656 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.658 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.658 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.658 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.658 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.659 I llm_load_print_meta: n_ff             = 8192
0.00.049.659 I llm_load_print_meta: n_expert         = 0
0.00.049.659 I llm_load_print_meta: n_expert_used    = 0
0.00.049.659 I llm_load_print_meta: causal attn      = 1
0.00.049.660 I llm_load_print_meta: pooling type     = 0
0.00.049.660 I llm_load_print_meta: rope type        = 2
0.00.049.660 I llm_load_print_meta: rope scaling     = linear
0.00.049.660 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.661 I llm_load_print_meta: freq_scale_train = 1
0.00.049.661 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.661 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.661 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.661 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.661 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.661 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.662 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.665 I llm_load_print_meta: model type       = 1.4B
0.00.049.666 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.666 I llm_load_print_meta: model params     = 1.41 B
0.00.049.667 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.667 I llm_load_print_meta: general.name     = 1.4B
0.00.049.667 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.668 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.668 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.668 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.668 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.668 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.670 I llm_load_print_meta: max token length = 1024
0.00.051.687 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.687 I llm_load_tensors: offloading output layer to GPU
0.00.051.687 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.698 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.699 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.571 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.572 I llama_new_context_with_model: n_ctx         = 128
0.00.052.572 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.572 I llama_new_context_with_model: n_batch       = 128
0.00.052.572 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.573 I llama_new_context_with_model: flash_attn    = 0
0.00.052.573 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.573 I llama_new_context_with_model: freq_scale    = 1
0.00.052.574 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.574 I ggml_metal_init: allocating
0.00.052.580 I ggml_metal_init: found device: Apple M4
0.00.052.582 I ggml_metal_init: picking default device: Apple M4
0.00.053.146 I ggml_metal_init: using embedded metal library
0.00.055.532 I ggml_metal_init: GPU name:   Apple M4
0.00.055.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.534 I ggml_metal_init: simdgroup reduction   = true
0.00.055.535 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.535 I ggml_metal_init: has bfloat            = true
0.00.055.535 I ggml_metal_init: use bfloat            = true
0.00.055.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.193 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.198 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.215 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.100 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.101 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.102 I llama_new_context_with_model: graph nodes  = 967
0.00.067.102 I llama_new_context_with_model: graph splits = 2
0.00.067.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.777 I 
0.00.639.813 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.820 I perplexity: tokenizing the input ..
0.00.647.498 I perplexity: tokenization took 7.676 ms
0.00.647.514 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.725 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.789.972 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.789.988 I llama_perf_context_print:        load time =     630.26 ms
0.00.789.989 I llama_perf_context_print: prompt eval time =     140.98 ms /   128 tokens (    1.10 ms per token,   907.92 tokens per second)
0.00.789.990 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.991 I llama_perf_context_print:       total time =     150.21 ms /   129 tokens
0.00.790.429 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.077s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.329 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.099 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.103 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.105 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.105 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.107 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.112 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.112 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.113 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.113 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.114 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.114 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.115 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.115 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.115 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.118 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.118 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.119 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.900 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.768 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.769 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.770 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.770 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.770 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.771 I llama_model_loader: - type  f32:  194 tensors
0.00.023.771 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.794 I llm_load_vocab: special tokens cache size = 25
0.00.049.733 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.736 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.737 I llm_load_print_meta: arch             = gptneox
0.00.049.737 I llm_load_print_meta: vocab type       = BPE
0.00.049.737 I llm_load_print_meta: n_vocab          = 50304
0.00.049.737 I llm_load_print_meta: n_merges         = 50009
0.00.049.738 I llm_load_print_meta: vocab_only       = 0
0.00.049.738 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.738 I llm_load_print_meta: n_embd           = 2048
0.00.049.738 I llm_load_print_meta: n_layer          = 24
0.00.049.741 I llm_load_print_meta: n_head           = 16
0.00.049.742 I llm_load_print_meta: n_head_kv        = 16
0.00.049.742 I llm_load_print_meta: n_rot            = 32
0.00.049.742 I llm_load_print_meta: n_swa            = 0
0.00.049.742 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.742 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.743 I llm_load_print_meta: n_gqa            = 1
0.00.049.744 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.745 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.745 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.747 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.747 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.747 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.747 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.748 I llm_load_print_meta: n_ff             = 8192
0.00.049.748 I llm_load_print_meta: n_expert         = 0
0.00.049.748 I llm_load_print_meta: n_expert_used    = 0
0.00.049.748 I llm_load_print_meta: causal attn      = 1
0.00.049.749 I llm_load_print_meta: pooling type     = 0
0.00.049.749 I llm_load_print_meta: rope type        = 2
0.00.049.749 I llm_load_print_meta: rope scaling     = linear
0.00.049.751 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.751 I llm_load_print_meta: freq_scale_train = 1
0.00.049.752 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.752 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.752 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.752 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.752 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.752 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.753 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.753 I llm_load_print_meta: model type       = 1.4B
0.00.049.753 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.757 I llm_load_print_meta: model params     = 1.41 B
0.00.049.759 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.759 I llm_load_print_meta: general.name     = 1.4B
0.00.049.759 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.759 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.760 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.760 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.760 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.760 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.760 I llm_load_print_meta: max token length = 1024
0.00.051.814 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.814 I llm_load_tensors: offloading output layer to GPU
0.00.051.814 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.825 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.826 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.744 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.745 I llama_new_context_with_model: n_ctx         = 128
0.00.052.745 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.746 I llama_new_context_with_model: n_batch       = 128
0.00.052.746 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.746 I llama_new_context_with_model: flash_attn    = 0
0.00.052.746 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.746 I llama_new_context_with_model: freq_scale    = 1
0.00.052.747 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.747 I ggml_metal_init: allocating
0.00.052.750 I ggml_metal_init: found device: Apple M4
0.00.052.752 I ggml_metal_init: picking default device: Apple M4
0.00.053.299 I ggml_metal_init: using embedded metal library
0.00.055.616 I ggml_metal_init: GPU name:   Apple M4
0.00.055.617 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.618 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.618 I ggml_metal_init: simdgroup reduction   = true
0.00.055.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.619 I ggml_metal_init: has bfloat            = true
0.00.055.619 I ggml_metal_init: use bfloat            = true
0.00.055.619 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.620 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.367 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.369 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.383 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.353 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.354 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.355 I llama_new_context_with_model: graph nodes  = 967
0.00.067.355 I llama_new_context_with_model: graph splits = 2
0.00.067.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.443.990 I 
0.00.444.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.444.034 I perplexity: tokenizing the input ..
0.00.451.648 I perplexity: tokenization took 7.612 ms
0.00.451.662 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.591.814 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.592.975 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.592.995 I llama_perf_context_print:        load time =     434.66 ms
0.00.592.999 I llama_perf_context_print: prompt eval time =     139.93 ms /   128 tokens (    1.09 ms per token,   914.78 tokens per second)
0.00.593.000 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.593.001 I llama_perf_context_print:       total time =     149.01 ms /   129 tokens
0.00.593.430 I ggml_metal_free: deallocating

real	0m0.606s
user	0m0.077s
sys	0m0.088s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.259 I build: 4327 (ba1cb19c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.857 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.531 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.541 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.544 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.545 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.549 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.552 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.553 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.556 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.557 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.557 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.701 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.269 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.578 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.580 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.581 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.581 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.581 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.582 I llama_model_loader: - type  f32:  194 tensors
0.00.056.583 I llama_model_loader: - type  f16:   98 tensors
0.00.086.917 I llm_load_vocab: special tokens cache size = 25
0.00.093.582 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.585 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.585 I llm_load_print_meta: arch             = gptneox
0.00.093.585 I llm_load_print_meta: vocab type       = BPE
0.00.093.585 I llm_load_print_meta: n_vocab          = 50304
0.00.093.586 I llm_load_print_meta: n_merges         = 50009
0.00.093.586 I llm_load_print_meta: vocab_only       = 0
0.00.093.586 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.586 I llm_load_print_meta: n_embd           = 2048
0.00.093.586 I llm_load_print_meta: n_layer          = 24
0.00.093.589 I llm_load_print_meta: n_head           = 16
0.00.093.590 I llm_load_print_meta: n_head_kv        = 16
0.00.093.590 I llm_load_print_meta: n_rot            = 32
0.00.093.590 I llm_load_print_meta: n_swa            = 0
0.00.093.590 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.590 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.591 I llm_load_print_meta: n_gqa            = 1
0.00.093.592 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.592 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.594 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.594 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.595 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.595 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.597 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.597 I llm_load_print_meta: n_ff             = 8192
0.00.093.597 I llm_load_print_meta: n_expert         = 0
0.00.093.597 I llm_load_print_meta: n_expert_used    = 0
0.00.093.598 I llm_load_print_meta: causal attn      = 1
0.00.093.598 I llm_load_print_meta: pooling type     = 0
0.00.093.598 I llm_load_print_meta: rope type        = 2
0.00.093.598 I llm_load_print_meta: rope scaling     = linear
0.00.093.599 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.599 I llm_load_print_meta: freq_scale_train = 1
0.00.093.599 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.600 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.600 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.601 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.601 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.601 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.601 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.601 I llm_load_print_meta: model type       = 1.4B
0.00.093.602 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.093.602 I llm_load_print_meta: model params     = 1.41 B
0.00.093.602 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.093.603 I llm_load_print_meta: general.name     = 1.4B
0.00.093.603 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.607 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.607 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.607 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.609 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.093.609 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.609 I llm_load_print_meta: max token length = 1024
0.00.096.175 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.175 I llm_load_tensors: offloading output layer to GPU
0.00.096.175 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.186 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.187 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.145 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.146 I llama_new_context_with_model: n_ctx         = 128
0.00.097.146 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.097.146 I llama_new_context_with_model: n_batch       = 128
0.00.097.146 I llama_new_context_with_model: n_ubatch      = 128
0.00.097.146 I llama_new_context_with_model: flash_attn    = 0
0.00.097.147 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.147 I llama_new_context_with_model: freq_scale    = 1
0.00.097.147 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.097.148 I ggml_metal_init: allocating
0.00.097.158 I ggml_metal_init: found device: Apple M4
0.00.097.160 I ggml_metal_init: picking default device: Apple M4
0.00.097.781 I ggml_metal_init: using embedded metal library
0.00.100.410 I ggml_metal_init: GPU name:   Apple M4
0.00.100.412 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.413 I ggml_metal_init: simdgroup reduction   = true
0.00.100.413 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.413 I ggml_metal_init: has bfloat            = true
0.00.100.413 I ggml_metal_init: use bfloat            = true
0.00.100.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.414 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.844 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.851 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.865 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.760 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.111.761 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.111.761 I llama_new_context_with_model: graph nodes  = 967
0.00.111.762 I llama_new_context_with_model: graph splits = 2
0.00.111.774 I 
0.00.111.805 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.111.806 I compute_imatrix: tokenizing the input ..
0.00.118.560 I compute_imatrix: tokenization took 6.753 ms
0.00.118.562 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.601.224 I compute_imatrix: 1.48 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.603.760 I llama_perf_context_print:        load time =    1575.36 ms
0.01.603.761 I llama_perf_context_print: prompt eval time =    1481.96 ms /   128 tokens (   11.58 ms per token,    86.37 tokens per second)
0.01.603.762 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.603.762 I llama_perf_context_print:       total time =    1577.89 ms /   129 tokens
0.01.604.381 I ggml_metal_free: deallocating

real	0m1.792s
user	0m0.173s
sys	0m0.227s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4327 (ba1cb19c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125e0a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125e0a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125e0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125e0b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125e0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125e0c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125e0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125e0cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125e0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125e0d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125e0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125e0e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125e0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125e0f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125e0fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125e102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125e109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125e110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125e11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125e11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125e12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125e12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125e13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125e13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125e14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125e147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125e14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125e15a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125e15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125e16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125e166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125e169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125e17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125e17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125e17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125e17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125e18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125e18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125e18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125e19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125e195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125e19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125e19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125e1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125e1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125e1aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125e1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125e1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125e1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125e1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125e1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125e1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125e1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125e1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125e1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125e1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125e1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125e1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125e1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125e20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125e204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125e20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125e20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125e212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125e21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125e21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125e220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125e22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125e229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125e22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125e23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125e237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125e23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125e241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125e24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125e24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125e251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125e256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125e25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125e26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125e266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125e26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125e27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125e276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125e27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125e28170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125e286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125e28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125e29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125e296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125e29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125e2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125e2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125e2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125e2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125e2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125e2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125e1b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125e2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125e2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125e2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125e2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125e2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125e2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125e2e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125e2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125e2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125e2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125e2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125e2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125e30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125e307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125e30d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125e311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125e31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125e31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125e31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125e32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125e328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125e32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125e33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125e336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125e33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125e33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125e34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125e34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125e34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125e35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125e35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125e36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125e364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125e36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125e36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125e372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125e37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125e37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125e380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125e38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125e389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125e38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125e39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125e397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125e39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125e3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125e3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125e3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125e3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125e3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125e3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125e3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125e3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125e3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125e3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125e3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125e3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125e3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125e3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125e3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125e3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125e3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125e3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125e3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125e3f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125e3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125e40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125e406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125e40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125e41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125e414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125e41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125e41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125e42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125e42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125e42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125e43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125e43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125e439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125e43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125e442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125e44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125e44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125e450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125e45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125e45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125e45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125e46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125e467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125e46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125e47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125e475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125e47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125e47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125e48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125e489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125e48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125e49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125e49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125e49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125e4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125e4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125e4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125e4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125e4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125e4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125e4c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125e4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125e4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125e4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125e4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125e4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125e4e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125e4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125e4f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125e4f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125e4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125e50210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125e50760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125e50cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125e51200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125e51750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125e51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125e521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125e52740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125e52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125e531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125e53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125e53c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125e541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125e54720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125e54c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125e551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125e55710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125e55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125e561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125e56700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125e56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125e571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125e576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125e57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125e58190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125e586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125e58c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125e59180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125e596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125e59c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125e5a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125e5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125e5ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125e5b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125e5b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125e5bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125e5c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125e5c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125e5cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125e5d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125e5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125e5dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125e5e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125e5e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125e5ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125e5f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125e5f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125e5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125e60110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125e60660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125e60bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125e61050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125e614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125e61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125e61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125e622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125e62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125e62c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125e630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125e63550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125e639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125e63e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125e64330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125e647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125e64c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125e65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125e65660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125e65d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125e664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125e66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125e672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125e675a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125e67d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125e68050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125e68660 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.147.430 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117804b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117804f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117805400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117805870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117805ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117806150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1178065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117806a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117806ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117807310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117807780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117807e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117808990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117809140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117809950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11780a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11780a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11780aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11780b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11780bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11780c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11780cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11780d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11780d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11780e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11780e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11780e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11780ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11780ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11780f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11780f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11780fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117810180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117810440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1178108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117810d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117811190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117811600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117811a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117811ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117812350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1178127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117812c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1178130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117813510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117813980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117813df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117814260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1178146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117814b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117814fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117815420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117815890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117815d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117816170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1178165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117816b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117817050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1178174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117817930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117817da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117818210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117818680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117818af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117818f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1178193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117819840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117819cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11781a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11781a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11781aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11781ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x115f04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x115f046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x115f04b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x115f04fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x115f05440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x115f058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x115f05d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x115f06190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x115f06600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x115f06a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x115f06ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x115f07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x115f077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x115f07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x115f080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x115f08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x115f08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x115f08df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x115f09260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x115f096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x115f09b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x115f09fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x115f0a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x115f0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x115f0ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x115f0b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x115f0b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x115f0ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x115f0bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x115f0c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x115f0c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x115f0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x115f0d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x115f0d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x115f0d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x115f0ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x115f0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x115f0e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x115f0eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x115f0ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x115f0f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115f0f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115f0fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115f10150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115f105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115f10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115f10ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x115f11310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x115f11780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x115f11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115f12060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115f124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x115f12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115f12db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x115f13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x115f13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115f13b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115f13f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115f143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115f14850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115f14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115f15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115f155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115f15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x115f15e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115f162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115f16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115f16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115f17040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115f174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115f17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115f17d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115f18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115f18670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115f18ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115f18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115f193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115f19830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115f19ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115f1a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x115f1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115f1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x115f1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115f1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115f1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115f1bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115f1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x115f1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115f1c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115f1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115f1d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x115f1d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x115f1dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x115f1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x115f1e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x115f1e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x115f1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x115f1f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x115f1f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x115f1f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x115f1fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x115f202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x115f20720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x115f20b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x115f21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x115f21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x115f218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x115f21d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x115f221c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x115f22630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x115f22aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x115f22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x115f23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115f237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115f23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115f240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115f24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115f249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115f24e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115f25290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115f25700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115f25b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115f25fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115f26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115f268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115f26d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115f271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115f27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x115f27a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x115f27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x115f28360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115f287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115f28c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x115f290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115f29520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115f29990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x115f29e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115f2a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115f2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115f2b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115f2b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115f2b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115f2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x115f2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115f2c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x115f2ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115f2ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115f2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115f2d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115f2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115f2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115f2e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115f2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115f2ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115f2f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115f2f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x115f2fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x115f2ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x115f303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115f30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115f30c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x115f31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115f31570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115f319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x115f31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115f322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x115f32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115f32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115f33010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115f33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x115f338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115f33d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115f341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x115f34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x115f34ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115f34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115f35390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x115f35800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115f35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x115f360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115f36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x115f369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115f36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115f372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x115f37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115f37b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x115f37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115f38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x115f388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115f38d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115f391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x115f39620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115f39a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115f39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115f3a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x115f3a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x115f3ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115f3b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115f3b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115f3b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115f3be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115f3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115f3c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115f3cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115f3cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115f3d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115f3d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115f3dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115f3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115f3e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115f3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115f3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115f3fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115f40320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115f40a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115f40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x115f41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115f41770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115f41d80 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125e24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125e251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125e25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125e25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125e25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125e263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125e26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125e26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125e270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125e27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125e279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125e27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125e288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125e29020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125e29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125e29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125e2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125e2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125e2b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125e2bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125e2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125e2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125e2d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125e2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125e2dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125e2e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125e2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125e2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125e2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125e2f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125e2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125e2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125e30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125e30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125e30aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125e30f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125e31380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125e317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125e31c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125e320d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125e32540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125e329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125e32e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125e33290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125e33700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125e33b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125e33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125e34450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125e348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125e34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125e351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125e35610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125e35a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125e35ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125e36360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125e367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125e36c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125e370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125e37520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125e37990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125e37e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125e38270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125e386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125e38b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125e38fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125e39430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125e398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125e39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125e3a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125e3a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125e3aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125e3aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125e3b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125e3b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125e3bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125e3c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125e3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125e3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125e3cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125e3d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125e3d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125e3db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125e3dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125e3e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125e3e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125e3ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125e3f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125e3f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125e3fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125e3feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125e40320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125e40790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125e40c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125e41070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125e414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125e41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125e42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125e426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125e42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125e42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125e433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125e43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125e43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125e44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125e445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125e44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125e44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125e45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125e45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125e45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125e46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125e464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125e46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125e46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125e47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125e47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125e47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125e47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125e483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125e48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125e48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125e49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125e49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125e49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125e49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125e4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125e4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125e4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125e4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125e4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125e4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125e4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125e4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125e4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125e4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125e4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125e4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125e4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125e4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125e4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125e4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125e4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125e4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125e4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125e4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125e4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125e50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125e50480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125e508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125e50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125e511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125e51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125e51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125e51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125e52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125e52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125e52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125e530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125e53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125e539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125e53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125e542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125e54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125e54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125e54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125e55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125e558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125e55d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125e561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125e56620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125e56a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125e56f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125e57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125e577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125e57c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125e580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125e58530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125e589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125e58e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125e59280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125e596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125e59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125e59fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125e5a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125e5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125e5ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125e5b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125e5b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125e5ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125e5bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125e5c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125e5c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125e5cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125e5d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125e5d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125e5d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125e5ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125e5e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125e5e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125e5eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125e5efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125e5f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125e5f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125e5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125e60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125e605e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125e60a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125e60ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125e61330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125e61ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125e61f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125e62390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125e62800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125e62c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125e630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125e63550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125e639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125e63e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125e642a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125e64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125e64b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125e64ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125e65460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125e658d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125e65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125e661b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125e66620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125e66a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125e66f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125e67370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125e677e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125e67c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125e680c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125e68530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125e0b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125e0ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125e09900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125e0a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125e17910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125e17d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125e181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125e18660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125e18ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125e18f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125e193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125e19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125e19c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125e1a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125e1a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125e1a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125e1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125e1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125e1b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125e1bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125e1c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125e1c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125e1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125e1cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125e1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125e1d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125e1dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125e1df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125e1e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125e1e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125e1ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125e1f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125e1f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125e1f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125e1fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125e202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125e20710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125e20b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125e20ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125e21460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125e218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125e21d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125e221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125e22620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125e22a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125e22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125e23370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125e237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125e23ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125e245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125e163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125e16a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125e16f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125e0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125e0dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125e0df50 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m2.121s
user	0m0.294s
sys	0m0.303s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4327 (ba1cb19c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140e0e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140e0ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140e0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140e0f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140e0fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140e10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140e108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140e10e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140e11420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140e11920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140e11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140e12320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140e12e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140e135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140e13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140e14520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140e14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140e15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140e15a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140e16250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140e16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140e17090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140e177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140e18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140e18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140e18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140e19040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140e19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140e1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140e1a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140e1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140e1ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140e1b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140e1b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140e1bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140e1c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140e1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140e1ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140e1cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140e1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140e1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140e1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140e1e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140e1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140e1e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140e1ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140e1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140e1fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140e20450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140e20a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140e21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140e21680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140e21c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140e222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140e22a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140e22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140e233d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140e23690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140e23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140e24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140e24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140e24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140e25090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140e25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140e259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140e25e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140e26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140e267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140e26c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140e270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140e27590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140e27a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140e27ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x140e28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x140e28970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x140e28ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x140e29410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x140e29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140e29eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140e2a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140e2a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140e2aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x140e2b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140e2b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140e2be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140e2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140e2c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140e2ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140e2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140e2d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140e2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140e2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140e2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140e2ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140e2f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140e2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140e2fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140e1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140e302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140e30a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140e30fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140e31510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140e31a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140e31fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140e32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140e32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140e32fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140e334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140e33a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140e33f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140e344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140e34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140e34f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140e35420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140e358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140e35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140e36200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140e366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140e36b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140e36fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140e37480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140e37920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140e37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140e38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140e38700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140e38ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140e39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140e394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140e39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140e39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140e3a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140e3a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140e3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140e3b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140e3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140e3b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140e3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140e3c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140e3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140e3cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140e3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140e3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140e3da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140e3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140e3e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140e3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140e3ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140e3f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140e3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140e3faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140e3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140e403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140e40880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140e40d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140e411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140e41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140e41b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140e41fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140e42440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140e428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140e42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140e43220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140e436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140e43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140e44000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140e444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140e44940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140e44de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140e45280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140e45720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140e45bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140e46060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140e46500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140e469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140e46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140e472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140e47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140e47c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140e480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140e48560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140e48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140e48ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140e49340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140e497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140e49c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140e4a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140e4a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140e4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140e4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140e4b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140e4b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140e4bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140e4c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140e4c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140e4cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140e4d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140e4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140e4d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140e4df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140e4e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140e4ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140e4f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140e4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140e4fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140e50110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140e50720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140e50f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140e513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140e51850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140e51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140e524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140e529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140e52f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140e53490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140e539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140e53f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140e54480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140e549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140e54f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140e55470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140e559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140e55f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140e56460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140e569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140e56f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140e57450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140e579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140e57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140e58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140e58990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140e58ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140e59430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140e59980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140e59ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140e5a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140e5a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140e5aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140e5b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140e5b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140e5beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140e5c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140e5c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140e5cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140e5d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140e5d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140e5de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140e5e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140e5e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140e5ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140e5f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140e5f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140e5fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140e603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140e60910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140e60e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140e613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140e61900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140e61e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140e623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140e628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140e62e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140e63390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140e638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140e63e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140e64380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140e648d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140e64e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140e652c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140e65760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140e65c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140e660a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140e66540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140e669e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140e66e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140e67320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140e677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140e67c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140e68100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140e685a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140e68a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140e68ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140e69380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140e698d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140e69ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140e6a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140e6ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140e6b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140e6b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140e6c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140e6c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140e6c8d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.091.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1420053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1420069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1420072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1420090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14200a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14200a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14200ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14200b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14200bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14200c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14200cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14200d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14200d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14200e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14200e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14200e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14200eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14200ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14200f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14200f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14200fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1420101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1420111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1420123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1420130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1420139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1420142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1420158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1420161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1420170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1420186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14201a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14201a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14201aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14201aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14201b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14201b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14201bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14201c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14201c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14201c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14201cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14201d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14201d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14201db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14201df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14201e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14201e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14201ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14201f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14201f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14201fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14201fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1420214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1420233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1420245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1420252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1420264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1420283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1420299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14202a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14202a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14202abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14202b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14202b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14202b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14202bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14202c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14202c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14202cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14202cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14202d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14202d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14202dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14202e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14202e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14202e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14202ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14202f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14202f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14202fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1420308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1420311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1420327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1420330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1420339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142035450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1420358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142035d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1420361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142036610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142036a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142036ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142037360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1420377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142037c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1420380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142038520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142038990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142038e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142039270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1420396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142039b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142039fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14203a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14203a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14203ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14203b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14203b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14203ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14203bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14203c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14203c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14203cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14203d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14203d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14203d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14203dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14203e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14203e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14203eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14203efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14203f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14203f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14203fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142040160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1420405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142040b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142040fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142041440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142041f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142042250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142042510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142042980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142042df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142043260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1420436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142043b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142043fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142044420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142044890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142044d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142045170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1420455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142045a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142045ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142046330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1420467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142046c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142047080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1420474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142047960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142047dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142048240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1420486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142048b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142048f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142049400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142049870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142049ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14204a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14204a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14204aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14204aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14204b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14204b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14204bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14204c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14204c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14204c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14204cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14204d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14204d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14204db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14204df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14204e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14204e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14204ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14204f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14204f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14204fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14204fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1420502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142050760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142050bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142051040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1420514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142051920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142051d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142052200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142052670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142052ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142052f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1420533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142053830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142053ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142054110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142054580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1420549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142054e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1420552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142055740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142055bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142056620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142056d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142057460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142057b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142057e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1420582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1420588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142058ec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140e28fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140e29450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140e298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140e29d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140e2a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140e2a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140e2aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140e2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140e2b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140e2b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140e2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140e2c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140e2cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140e2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140e2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140e2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140e2e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140e2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140e2f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140e2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140e306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140e30d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140e31480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140e31b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140e32260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140e326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140e32b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140e32fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140e33420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140e33890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140e33d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140e34170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140e345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140e348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140e34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140e35180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140e355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140e35a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140e35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140e36340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140e367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140e36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140e37090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140e37500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140e37970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140e37de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140e38250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140e386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140e38b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140e38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140e39410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140e39880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140e39cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140e3a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140e3a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140e3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140e3aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140e3b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140e3b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140e3bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140e3c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140e3c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140e3c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140e3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140e3d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140e3d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140e3db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140e3df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140e3e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140e3e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140e3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140e3f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140e3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x140e3fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x140e3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x140e40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x140e40770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x140e40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140e41050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140e414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140e41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140e41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x140e42210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140e42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140e42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140e42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140e433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140e43840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140e43cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140e44120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140e44590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140e44a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140e44e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140e452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140e45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140e45bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140e46030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140e464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140e46910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140e46d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140e471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140e47660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140e47ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140e47f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140e483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140e48820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140e48c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140e49100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140e49570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140e499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140e49e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140e4a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140e4a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140e4aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140e4b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140e4b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140e4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140e4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140e4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140e4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140e4cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140e4cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140e4d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140e4d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140e4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140e4e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140e4e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140e4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140e4ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140e4f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140e4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140e4fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140e4fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140e50460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140e508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140e50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140e511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140e51620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140e51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140e51f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140e52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140e527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140e52c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140e530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140e53530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140e539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140e53e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140e54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140e546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140e54b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140e54fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140e55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140e558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140e55d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140e56190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140e56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140e56a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140e56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140e57350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140e577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140e57c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140e580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140e58510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140e58980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140e58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140e59260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140e596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140e59b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140e59fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140e5a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140e5a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140e5ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140e5b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140e5b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140e5ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140e5bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140e5c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140e5c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140e5cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140e5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140e5d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140e5d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140e5ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140e5e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140e5e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140e5eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140e5ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140e5f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140e5f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140e5fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140e60150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140e605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140e60a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140e60ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140e61310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140e61780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140e61bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140e62060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140e624d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140e62940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140e62db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140e63220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140e63690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140e63b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140e63f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140e643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140e64850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140e64cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140e65130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140e655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140e65d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140e66190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140e66600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140e66a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140e66ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140e67350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140e677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140e67c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140e680a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140e68510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140e68980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140e68df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140e69260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140e696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140e69b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140e69fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140e6a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140e6a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140e6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140e6b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140e6b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140e6ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140e6bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140e6c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140e6c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140e0f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140e0f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140e0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140e0e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140e1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140e1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140e1c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140e1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140e1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140e1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140e1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140e1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140e1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140e1e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140e1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140e1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140e1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140e1f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140e1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140e1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140e20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140e206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140e20b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140e20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140e21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140e218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140e21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140e22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140e22600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140e22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140e22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140e23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140e237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140e23c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140e240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140e24510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x140e24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140e24df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140e25260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140e256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140e25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140e25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140e26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140e26890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140e26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140e27170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140e275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140e27a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140e28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140e28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140e1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140e1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140e1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140e118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140e11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140e121c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.935s
user	0m0.243s
sys	0m0.147s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
