### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.19 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.21 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.01 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.30 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.47 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.76 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.01 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.53 sec*proc (28 tests)

Total Test time (real) = 222.54 sec

real	3m42.534s
user	7m47.250s
sys	0m6.404s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.33 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.32 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.08 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.32 sec*proc (28 tests)

Total Test time (real) =  51.33 sec

real	0m51.339s
user	1m11.834s
sys	0m5.642s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.152 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.940 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.947 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.950 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.951 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.952 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.953 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.954 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.955 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.956 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.956 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.957 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.960 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.961 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.961 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.962 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.962 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.963 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.964 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.042 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.525 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.527 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.528 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.528 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.529 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.028.529 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.530 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.028.531 I llama_model_loader: - type  f32:  124 tensors
0.00.028.531 I llama_model_loader: - type  f16:   73 tensors
0.00.033.218 I llm_load_vocab: special tokens cache size = 5
0.00.035.555 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.035.559 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.035.559 I llm_load_print_meta: arch             = bert
0.00.035.560 I llm_load_print_meta: vocab type       = WPM
0.00.035.560 I llm_load_print_meta: n_vocab          = 30522
0.00.035.560 I llm_load_print_meta: n_merges         = 0
0.00.035.561 I llm_load_print_meta: vocab_only       = 0
0.00.035.561 I llm_load_print_meta: n_ctx_train      = 512
0.00.035.561 I llm_load_print_meta: n_embd           = 384
0.00.035.561 I llm_load_print_meta: n_layer          = 12
0.00.035.565 I llm_load_print_meta: n_head           = 12
0.00.035.566 I llm_load_print_meta: n_head_kv        = 12
0.00.035.566 I llm_load_print_meta: n_rot            = 32
0.00.035.566 I llm_load_print_meta: n_swa            = 0
0.00.035.567 I llm_load_print_meta: n_embd_head_k    = 32
0.00.035.570 I llm_load_print_meta: n_embd_head_v    = 32
0.00.035.570 I llm_load_print_meta: n_gqa            = 1
0.00.035.571 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.035.574 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.035.575 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.035.575 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.035.576 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.035.576 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.035.576 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.035.577 I llm_load_print_meta: n_ff             = 1536
0.00.035.577 I llm_load_print_meta: n_expert         = 0
0.00.035.577 I llm_load_print_meta: n_expert_used    = 0
0.00.035.578 I llm_load_print_meta: causal attn      = 0
0.00.035.578 I llm_load_print_meta: pooling type     = 2
0.00.035.578 I llm_load_print_meta: rope type        = 2
0.00.035.579 I llm_load_print_meta: rope scaling     = linear
0.00.035.579 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.035.580 I llm_load_print_meta: freq_scale_train = 1
0.00.035.580 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.035.581 I llm_load_print_meta: rope_finetuned   = unknown
0.00.035.581 I llm_load_print_meta: ssm_d_conv       = 0
0.00.035.581 I llm_load_print_meta: ssm_d_inner      = 0
0.00.035.581 I llm_load_print_meta: ssm_d_state      = 0
0.00.035.581 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.035.581 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.035.582 I llm_load_print_meta: model type       = 33M
0.00.035.584 I llm_load_print_meta: model ftype      = F16
0.00.035.584 I llm_load_print_meta: model params     = 33.21 M
0.00.035.585 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.035.586 I llm_load_print_meta: general.name     = Bge Small
0.00.035.586 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.035.586 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.035.587 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.035.587 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.035.587 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.035.587 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.035.588 I llm_load_print_meta: max token length = 21
0.00.037.709 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.037.710 I llm_load_tensors: offloading output layer to GPU
0.00.037.711 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.037.739 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.740 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.038.326 I llama_new_context_with_model: n_seq_max     = 1
0.00.038.327 I llama_new_context_with_model: n_ctx         = 512
0.00.038.327 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.038.328 I llama_new_context_with_model: n_batch       = 2048
0.00.038.328 I llama_new_context_with_model: n_ubatch      = 2048
0.00.038.328 I llama_new_context_with_model: flash_attn    = 0
0.00.038.329 I llama_new_context_with_model: freq_base     = 10000.0
0.00.038.329 I llama_new_context_with_model: freq_scale    = 1
0.00.038.330 I ggml_metal_init: allocating
0.00.038.334 I ggml_metal_init: found device: Apple M4
0.00.038.337 I ggml_metal_init: picking default device: Apple M4
0.00.039.217 I ggml_metal_init: using embedded metal library
0.00.043.578 I ggml_metal_init: GPU name:   Apple M4
0.00.043.581 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.043.582 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.043.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.043.583 I ggml_metal_init: simdgroup reduction   = true
0.00.043.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.043.583 I ggml_metal_init: has bfloat            = true
0.00.043.583 I ggml_metal_init: use bfloat            = true
0.00.043.584 I ggml_metal_init: hasUnifiedMemory      = true
0.00.043.585 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.056.142 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.056.871 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.056.873 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.056.874 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.057.676 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.057.678 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.057.678 I llama_new_context_with_model: graph nodes  = 429
0.00.057.678 I llama_new_context_with_model: graph splits = 2
0.00.057.680 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.057.680 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.064.314 I 
0.00.064.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.065.027 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.069.750 I llama_perf_context_print:        load time =      46.61 ms
0.00.069.752 I llama_perf_context_print: prompt eval time =       4.58 ms /     9 tokens (    0.51 ms per token,  1966.78 tokens per second)
0.00.069.753 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.069.754 I llama_perf_context_print:       total time =       5.44 ms /    10 tokens
0.00.069.930 I ggml_metal_free: deallocating

real	0m0.251s
user	0m0.051s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.040 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.355 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.388 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.393 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.394 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.394 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.394 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.395 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.395 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.396 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.396 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.398 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.400 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.400 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.401 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.402 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.405 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.406 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.407 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.898 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.609 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.610 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.610 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.611 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.611 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.611 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.611 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.612 I llama_model_loader: - type  f32:  124 tensors
0.00.014.612 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.031 I llm_load_vocab: special tokens cache size = 5
0.00.018.327 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.330 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.330 I llm_load_print_meta: arch             = bert
0.00.018.331 I llm_load_print_meta: vocab type       = WPM
0.00.018.331 I llm_load_print_meta: n_vocab          = 30522
0.00.018.331 I llm_load_print_meta: n_merges         = 0
0.00.018.331 I llm_load_print_meta: vocab_only       = 0
0.00.018.331 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.331 I llm_load_print_meta: n_embd           = 384
0.00.018.332 I llm_load_print_meta: n_layer          = 12
0.00.018.334 I llm_load_print_meta: n_head           = 12
0.00.018.335 I llm_load_print_meta: n_head_kv        = 12
0.00.018.335 I llm_load_print_meta: n_rot            = 32
0.00.018.336 I llm_load_print_meta: n_swa            = 0
0.00.018.336 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.336 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.336 I llm_load_print_meta: n_gqa            = 1
0.00.018.337 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.337 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.338 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.338 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.338 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.338 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.338 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.339 I llm_load_print_meta: n_ff             = 1536
0.00.018.339 I llm_load_print_meta: n_expert         = 0
0.00.018.339 I llm_load_print_meta: n_expert_used    = 0
0.00.018.339 I llm_load_print_meta: causal attn      = 0
0.00.018.339 I llm_load_print_meta: pooling type     = 2
0.00.018.340 I llm_load_print_meta: rope type        = 2
0.00.018.340 I llm_load_print_meta: rope scaling     = linear
0.00.018.340 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.340 I llm_load_print_meta: freq_scale_train = 1
0.00.018.340 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.341 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.341 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.341 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.341 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.341 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.341 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.342 I llm_load_print_meta: model type       = 33M
0.00.018.342 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.342 I llm_load_print_meta: model params     = 33.21 M
0.00.018.343 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.343 I llm_load_print_meta: general.name     = Bge Small
0.00.018.343 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.343 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.344 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.344 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.344 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.344 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.346 I llm_load_print_meta: max token length = 21
0.00.019.632 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.632 I llm_load_tensors: offloading output layer to GPU
0.00.019.632 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.640 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.641 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.992 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.993 I llama_new_context_with_model: n_ctx         = 512
0.00.019.993 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.993 I llama_new_context_with_model: n_batch       = 2048
0.00.019.993 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.994 I llama_new_context_with_model: flash_attn    = 0
0.00.019.994 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.994 I llama_new_context_with_model: freq_scale    = 1
0.00.019.995 I ggml_metal_init: allocating
0.00.019.997 I ggml_metal_init: found device: Apple M4
0.00.019.999 I ggml_metal_init: picking default device: Apple M4
0.00.020.615 I ggml_metal_init: using embedded metal library
0.00.023.069 I ggml_metal_init: GPU name:   Apple M4
0.00.023.071 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.071 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.071 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.072 I ggml_metal_init: simdgroup reduction   = true
0.00.023.072 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.072 I ggml_metal_init: has bfloat            = true
0.00.023.072 I ggml_metal_init: use bfloat            = true
0.00.023.073 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.073 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.481 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.962 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.964 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.967 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.631 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.632 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.633 I llama_new_context_with_model: graph nodes  = 429
0.00.034.633 I llama_new_context_with_model: graph splits = 2
0.00.034.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.731 I 
0.00.039.749 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.262 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.734 I llama_perf_context_print:        load time =      30.37 ms
0.00.044.735 I llama_perf_context_print: prompt eval time =       4.35 ms /     9 tokens (    0.48 ms per token,  2069.92 tokens per second)
0.00.044.736 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.737 I llama_perf_context_print:       total time =       5.00 ms /    10 tokens
0.00.044.907 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.205 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.421 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.259 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.264 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.266 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.266 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.274 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.275 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.276 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.277 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.277 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.278 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.278 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.279 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.282 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.282 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.283 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.284 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.039.902 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.121 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.640 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.046.642 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.642 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.046.643 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.046.643 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.046.643 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.046.644 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.046.644 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.046.644 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.046.645 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.046.645 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.046.645 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.046.646 I llama_model_loader: - type  f32:   40 tensors
0.00.046.646 I llama_model_loader: - type  f16:   30 tensors
0.00.064.646 W llm_load_vocab: empty token at index 5
0.00.069.173 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.070.516 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.546 I llm_load_vocab: special tokens cache size = 5
0.00.323.637 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.323.645 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.323.645 I llm_load_print_meta: arch             = jina-bert-v2
0.00.323.646 I llm_load_print_meta: vocab type       = BPE
0.00.323.646 I llm_load_print_meta: n_vocab          = 61056
0.00.323.646 I llm_load_print_meta: n_merges         = 39382
0.00.323.646 I llm_load_print_meta: vocab_only       = 0
0.00.323.647 I llm_load_print_meta: n_ctx_train      = 8192
0.00.323.647 I llm_load_print_meta: n_embd           = 384
0.00.323.650 I llm_load_print_meta: n_layer          = 4
0.00.323.654 I llm_load_print_meta: n_head           = 12
0.00.323.655 I llm_load_print_meta: n_head_kv        = 12
0.00.323.655 I llm_load_print_meta: n_rot            = 32
0.00.323.655 I llm_load_print_meta: n_swa            = 0
0.00.323.656 I llm_load_print_meta: n_embd_head_k    = 32
0.00.323.656 I llm_load_print_meta: n_embd_head_v    = 32
0.00.323.656 I llm_load_print_meta: n_gqa            = 1
0.00.323.668 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.323.674 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.323.675 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.323.676 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.323.676 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.323.678 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.323.678 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.323.679 I llm_load_print_meta: n_ff             = 1536
0.00.323.679 I llm_load_print_meta: n_expert         = 0
0.00.323.679 I llm_load_print_meta: n_expert_used    = 0
0.00.323.680 I llm_load_print_meta: causal attn      = 0
0.00.323.680 I llm_load_print_meta: pooling type     = -1
0.00.323.680 I llm_load_print_meta: rope type        = -1
0.00.323.680 I llm_load_print_meta: rope scaling     = linear
0.00.323.681 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.323.681 I llm_load_print_meta: freq_scale_train = 1
0.00.323.681 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.323.686 I llm_load_print_meta: rope_finetuned   = unknown
0.00.323.686 I llm_load_print_meta: ssm_d_conv       = 0
0.00.323.686 I llm_load_print_meta: ssm_d_inner      = 0
0.00.323.686 I llm_load_print_meta: ssm_d_state      = 0
0.00.323.687 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.323.687 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.323.689 I llm_load_print_meta: model type       = 33M
0.00.323.689 I llm_load_print_meta: model ftype      = F16
0.00.323.690 I llm_load_print_meta: model params     = 32.90 M
0.00.323.690 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.323.694 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.323.695 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.323.695 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.323.696 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.323.696 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.323.697 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.323.697 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.323.697 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.323.697 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.323.698 I llm_load_print_meta: max token length = 45
0.00.324.764 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.324.764 I llm_load_tensors: offloading output layer to GPU
0.00.324.764 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.324.788 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.789 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.325.574 I llama_new_context_with_model: n_seq_max     = 1
0.00.325.575 I llama_new_context_with_model: n_ctx         = 8192
0.00.325.575 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.325.575 I llama_new_context_with_model: n_batch       = 2048
0.00.325.575 I llama_new_context_with_model: n_ubatch      = 2048
0.00.325.576 I llama_new_context_with_model: flash_attn    = 0
0.00.325.576 I llama_new_context_with_model: freq_base     = 10000.0
0.00.325.576 I llama_new_context_with_model: freq_scale    = 1
0.00.325.577 I ggml_metal_init: allocating
0.00.325.580 I ggml_metal_init: found device: Apple M4
0.00.325.582 I ggml_metal_init: picking default device: Apple M4
0.00.326.389 I ggml_metal_init: using embedded metal library
0.00.329.114 I ggml_metal_init: GPU name:   Apple M4
0.00.329.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.329.116 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.329.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.329.116 I ggml_metal_init: simdgroup reduction   = true
0.00.329.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.329.117 I ggml_metal_init: has bfloat            = true
0.00.329.117 I ggml_metal_init: use bfloat            = true
0.00.329.117 I ggml_metal_init: hasUnifiedMemory      = true
0.00.329.119 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.338.678 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.341.219 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.341.222 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.341.223 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.341.723 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.341.724 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.341.724 I llama_new_context_with_model: graph nodes  = 154
0.00.341.724 I llama_new_context_with_model: graph splits = 2
0.00.341.726 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.341.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.353.639 I 
0.00.353.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.353.832 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.353.833 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.353.836 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.353.836 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.353.840 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.353.840 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.354.370 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.358.051 I llama_perf_context_print:        load time =     331.21 ms
0.00.358.052 I llama_perf_context_print: prompt eval time =       3.67 ms /    62 tokens (    0.06 ms per token, 16879.93 tokens per second)
0.00.358.052 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.358.053 I llama_perf_context_print:       total time =       4.41 ms /    63 tokens
0.00.358.304 I ggml_metal_free: deallocating

real	0m1.078s
user	0m0.330s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.189 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.306 I main: llama backend init
0.00.000.312 I main: load the model and apply lora adapter, if any
0.00.078.446 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.089.376 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.089.388 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.089.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.089.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.089.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.089.393 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.089.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.089.396 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.089.396 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.089.397 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.089.406 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.089.406 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.089.407 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.089.408 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.089.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.089.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.089.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.096.313 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.098.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.105.555 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.105.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.105.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.105.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.105.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.105.564 I llama_model_loader: - type  f32:  194 tensors
0.00.105.564 I llama_model_loader: - type  f16:   98 tensors
0.00.144.544 I llm_load_vocab: special tokens cache size = 25
0.00.152.012 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.152.015 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.152.015 I llm_load_print_meta: arch             = gptneox
0.00.152.016 I llm_load_print_meta: vocab type       = BPE
0.00.152.016 I llm_load_print_meta: n_vocab          = 50304
0.00.152.016 I llm_load_print_meta: n_merges         = 50009
0.00.152.016 I llm_load_print_meta: vocab_only       = 0
0.00.152.017 I llm_load_print_meta: n_ctx_train      = 2048
0.00.152.017 I llm_load_print_meta: n_embd           = 2048
0.00.152.017 I llm_load_print_meta: n_layer          = 24
0.00.152.022 I llm_load_print_meta: n_head           = 16
0.00.152.023 I llm_load_print_meta: n_head_kv        = 16
0.00.152.023 I llm_load_print_meta: n_rot            = 32
0.00.152.023 I llm_load_print_meta: n_swa            = 0
0.00.152.023 I llm_load_print_meta: n_embd_head_k    = 128
0.00.152.023 I llm_load_print_meta: n_embd_head_v    = 128
0.00.152.026 I llm_load_print_meta: n_gqa            = 1
0.00.152.026 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.152.027 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.152.028 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.152.028 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.152.028 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.152.028 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.152.029 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.152.029 I llm_load_print_meta: n_ff             = 8192
0.00.152.029 I llm_load_print_meta: n_expert         = 0
0.00.152.030 I llm_load_print_meta: n_expert_used    = 0
0.00.152.030 I llm_load_print_meta: causal attn      = 1
0.00.152.030 I llm_load_print_meta: pooling type     = 0
0.00.152.030 I llm_load_print_meta: rope type        = 2
0.00.152.030 I llm_load_print_meta: rope scaling     = linear
0.00.152.031 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.152.031 I llm_load_print_meta: freq_scale_train = 1
0.00.152.032 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.152.032 I llm_load_print_meta: rope_finetuned   = unknown
0.00.152.032 I llm_load_print_meta: ssm_d_conv       = 0
0.00.152.032 I llm_load_print_meta: ssm_d_inner      = 0
0.00.152.032 I llm_load_print_meta: ssm_d_state      = 0
0.00.152.033 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.152.033 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.152.033 I llm_load_print_meta: model type       = 1.4B
0.00.152.034 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.152.034 I llm_load_print_meta: model params     = 1.41 B
0.00.152.036 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.152.036 I llm_load_print_meta: general.name     = 1.4B
0.00.152.036 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.152.037 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.152.037 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.152.037 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.152.037 I llm_load_print_meta: LF token         = 128 ''
0.00.152.038 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.152.038 I llm_load_print_meta: max token length = 1024
0.00.154.717 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.154.717 I llm_load_tensors: offloading output layer to GPU
0.00.154.718 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.154.736 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.154.738 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.155.731 I llama_new_context_with_model: n_seq_max     = 1
0.00.155.732 I llama_new_context_with_model: n_ctx         = 2048
0.00.155.732 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.155.732 I llama_new_context_with_model: n_batch       = 2048
0.00.155.732 I llama_new_context_with_model: n_ubatch      = 512
0.00.155.733 I llama_new_context_with_model: flash_attn    = 0
0.00.155.733 I llama_new_context_with_model: freq_base     = 10000.0
0.00.155.733 I llama_new_context_with_model: freq_scale    = 1
0.00.155.734 I ggml_metal_init: allocating
0.00.155.737 I ggml_metal_init: found device: Apple M4
0.00.155.739 I ggml_metal_init: picking default device: Apple M4
0.00.156.415 I ggml_metal_init: using embedded metal library
0.00.237.869 I ggml_metal_init: GPU name:   Apple M4
0.00.237.875 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.237.875 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.237.876 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.237.876 I ggml_metal_init: simdgroup reduction   = true
0.00.237.876 I ggml_metal_init: simdgroup matrix mul. = true
0.00.237.876 I ggml_metal_init: has bfloat            = true
0.00.237.876 I ggml_metal_init: use bfloat            = true
0.00.237.877 I ggml_metal_init: hasUnifiedMemory      = true
0.00.237.879 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.324.035 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.346.202 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.346.208 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.346.228 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.347.227 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.347.228 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.347.229 I llama_new_context_with_model: graph nodes  = 967
0.00.347.229 I llama_new_context_with_model: graph splits = 2
0.00.347.232 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.347.360 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.347.360 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.426.795 I main: llama threadpool init, n_threads = 4
0.00.426.838 I 
0.00.426.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.426.860 I 
0.00.426.930 I sampler seed: 1234
0.00.426.936 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.426.977 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.426.979 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.426.979 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.266.478 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54322.88 tokens per second)
0.02.266.478 I llama_perf_context_print:        load time =     348.34 ms
0.02.266.480 I llama_perf_context_print: prompt eval time =      43.80 ms /     7 tokens (    6.26 ms per token,   159.80 tokens per second)
0.02.266.480 I llama_perf_context_print:        eval time =    1792.62 ms /    63 runs   (   28.45 ms per token,    35.14 tokens per second)
0.02.266.481 I llama_perf_context_print:       total time =    1839.69 ms /    70 tokens
0.02.266.735 I ggml_metal_free: deallocating

real	0m2.631s
user	0m0.163s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.882 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.733 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.867 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.876 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.879 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.880 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.880 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.881 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.882 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.883 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.883 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.884 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.885 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.890 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.891 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.894 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.895 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.824 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.849 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.850 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.850 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.850 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.851 I llama_model_loader: - type  f32:  194 tensors
0.00.045.851 I llama_model_loader: - type  f16:   98 tensors
0.00.069.611 I llm_load_vocab: special tokens cache size = 25
0.00.075.809 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.814 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.815 I llm_load_print_meta: arch             = gptneox
0.00.075.815 I llm_load_print_meta: vocab type       = BPE
0.00.075.815 I llm_load_print_meta: n_vocab          = 50304
0.00.075.815 I llm_load_print_meta: n_merges         = 50009
0.00.075.817 I llm_load_print_meta: vocab_only       = 0
0.00.075.817 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.817 I llm_load_print_meta: n_embd           = 2048
0.00.075.817 I llm_load_print_meta: n_layer          = 24
0.00.075.822 I llm_load_print_meta: n_head           = 16
0.00.075.823 I llm_load_print_meta: n_head_kv        = 16
0.00.075.823 I llm_load_print_meta: n_rot            = 32
0.00.075.823 I llm_load_print_meta: n_swa            = 0
0.00.075.824 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.824 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.825 I llm_load_print_meta: n_gqa            = 1
0.00.075.825 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.826 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.827 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.828 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.828 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.828 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.828 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.829 I llm_load_print_meta: n_ff             = 8192
0.00.075.829 I llm_load_print_meta: n_expert         = 0
0.00.075.829 I llm_load_print_meta: n_expert_used    = 0
0.00.075.829 I llm_load_print_meta: causal attn      = 1
0.00.075.829 I llm_load_print_meta: pooling type     = 0
0.00.075.829 I llm_load_print_meta: rope type        = 2
0.00.075.830 I llm_load_print_meta: rope scaling     = linear
0.00.075.830 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.830 I llm_load_print_meta: freq_scale_train = 1
0.00.075.831 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.831 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.832 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.832 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.833 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.833 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.833 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.833 I llm_load_print_meta: model type       = 1.4B
0.00.075.834 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.075.834 I llm_load_print_meta: model params     = 1.41 B
0.00.075.834 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.075.835 I llm_load_print_meta: general.name     = 1.4B
0.00.075.835 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.835 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.835 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.836 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.836 I llm_load_print_meta: LF token         = 128 ''
0.00.075.836 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.836 I llm_load_print_meta: max token length = 1024
0.00.078.170 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.078.170 I llm_load_tensors: offloading output layer to GPU
0.00.078.170 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.078.181 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.078.183 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.079.057 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.057 I llama_new_context_with_model: n_ctx         = 128
0.00.079.058 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.079.058 I llama_new_context_with_model: n_batch       = 128
0.00.079.058 I llama_new_context_with_model: n_ubatch      = 128
0.00.079.058 I llama_new_context_with_model: flash_attn    = 0
0.00.079.059 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.059 I llama_new_context_with_model: freq_scale    = 1
0.00.079.059 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.079.060 I ggml_metal_init: allocating
0.00.079.063 I ggml_metal_init: found device: Apple M4
0.00.079.065 I ggml_metal_init: picking default device: Apple M4
0.00.079.692 I ggml_metal_init: using embedded metal library
0.00.082.134 I ggml_metal_init: GPU name:   Apple M4
0.00.082.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.137 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.137 I ggml_metal_init: simdgroup reduction   = true
0.00.082.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.137 I ggml_metal_init: has bfloat            = true
0.00.082.137 I ggml_metal_init: use bfloat            = true
0.00.082.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.139 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.722 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.093.010 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.093.024 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.879 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.093.880 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.093.880 I llama_new_context_with_model: graph nodes  = 967
0.00.093.881 I llama_new_context_with_model: graph splits = 2
0.00.093.882 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.093.882 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.181.527 I 
0.01.181.583 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.181.634 I perplexity: tokenizing the input ..
0.01.195.582 I perplexity: tokenization took 13.947 ms
0.01.195.589 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.318.387 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.320.255 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.320.289 I llama_perf_context_print:        load time =    1160.79 ms
0.01.320.291 I llama_perf_context_print: prompt eval time =     121.86 ms /   128 tokens (    0.95 ms per token,  1050.39 tokens per second)
0.01.320.292 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.320.293 I llama_perf_context_print:       total time =     138.76 ms /   129 tokens
0.01.321.256 I ggml_metal_free: deallocating

real	0m1.511s
user	0m0.113s
sys	0m0.221s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.623 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.717 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.719 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.720 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.722 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.726 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.727 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.728 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.728 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.728 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.729 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.729 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.730 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.732 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.732 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.641 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.643 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.643 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.643 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.644 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.644 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.645 I llama_model_loader: - type  f32:  194 tensors
0.00.036.645 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.140 I llm_load_vocab: special tokens cache size = 25
0.00.066.257 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.261 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.262 I llm_load_print_meta: arch             = gptneox
0.00.066.262 I llm_load_print_meta: vocab type       = BPE
0.00.066.263 I llm_load_print_meta: n_vocab          = 50304
0.00.066.263 I llm_load_print_meta: n_merges         = 50009
0.00.066.263 I llm_load_print_meta: vocab_only       = 0
0.00.066.265 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.266 I llm_load_print_meta: n_embd           = 2048
0.00.066.266 I llm_load_print_meta: n_layer          = 24
0.00.066.270 I llm_load_print_meta: n_head           = 16
0.00.066.271 I llm_load_print_meta: n_head_kv        = 16
0.00.066.272 I llm_load_print_meta: n_rot            = 32
0.00.066.272 I llm_load_print_meta: n_swa            = 0
0.00.066.272 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.272 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.273 I llm_load_print_meta: n_gqa            = 1
0.00.066.273 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.274 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.275 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.275 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.275 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.275 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.275 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.276 I llm_load_print_meta: n_ff             = 8192
0.00.066.276 I llm_load_print_meta: n_expert         = 0
0.00.066.277 I llm_load_print_meta: n_expert_used    = 0
0.00.066.277 I llm_load_print_meta: causal attn      = 1
0.00.066.277 I llm_load_print_meta: pooling type     = 0
0.00.066.277 I llm_load_print_meta: rope type        = 2
0.00.066.277 I llm_load_print_meta: rope scaling     = linear
0.00.066.278 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.278 I llm_load_print_meta: freq_scale_train = 1
0.00.066.278 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.279 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.279 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.279 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.279 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.280 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.281 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.281 I llm_load_print_meta: model type       = 1.4B
0.00.066.281 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.282 I llm_load_print_meta: model params     = 1.41 B
0.00.066.283 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.283 I llm_load_print_meta: general.name     = 1.4B
0.00.066.284 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.284 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.284 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.284 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.284 I llm_load_print_meta: LF token         = 128 ''
0.00.066.285 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.285 I llm_load_print_meta: max token length = 1024
0.00.068.724 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.724 I llm_load_tensors: offloading output layer to GPU
0.00.068.724 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.736 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.737 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.695 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.695 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.696 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.696 I llama_new_context_with_model: n_batch       = 2048
0.00.069.696 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.696 I llama_new_context_with_model: flash_attn    = 0
0.00.069.697 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.697 I llama_new_context_with_model: freq_scale    = 1
0.00.069.697 I ggml_metal_init: allocating
0.00.069.701 I ggml_metal_init: found device: Apple M4
0.00.069.703 I ggml_metal_init: picking default device: Apple M4
0.00.070.447 I ggml_metal_init: using embedded metal library
0.00.073.161 I ggml_metal_init: GPU name:   Apple M4
0.00.073.163 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.163 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.164 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.164 I ggml_metal_init: simdgroup reduction   = true
0.00.073.164 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.164 I ggml_metal_init: has bfloat            = true
0.00.073.165 I ggml_metal_init: use bfloat            = true
0.00.073.165 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.166 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.598 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.494 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.108.505 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.108.530 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.644 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.647 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.647 I llama_new_context_with_model: graph nodes  = 967
0.00.109.647 I llama_new_context_with_model: graph splits = 2
0.00.109.652 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.782 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.782 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.549.015 I main: llama threadpool init, n_threads = 4
0.01.549.092 I 
0.01.549.133 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.549.135 I 
0.01.549.547 I sampler seed: 1234
0.01.549.552 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.549.591 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.549.593 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.549.593 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.649.681 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51374.82 tokens per second)
0.02.649.681 I llama_perf_context_print:        load time =    1539.38 ms
0.02.649.682 I llama_perf_context_print: prompt eval time =      49.53 ms /     7 tokens (    7.08 ms per token,   141.32 tokens per second)
0.02.649.683 I llama_perf_context_print:        eval time =    1047.58 ms /    63 runs   (   16.63 ms per token,    60.14 tokens per second)
0.02.649.684 I llama_perf_context_print:       total time =    1100.67 ms /    70 tokens
0.02.649.926 I ggml_metal_free: deallocating

real	0m2.668s
user	0m0.122s
sys	0m0.248s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.161 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.514 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.383 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.399 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.404 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.404 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.405 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.405 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.406 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.406 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.407 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.409 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.410 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.581 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.583 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.584 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.585 I llama_model_loader: - type  f32:  194 tensors
0.00.037.585 I llama_model_loader: - type q8_0:   98 tensors
0.00.064.489 I llm_load_vocab: special tokens cache size = 25
0.00.071.239 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.242 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.242 I llm_load_print_meta: arch             = gptneox
0.00.071.243 I llm_load_print_meta: vocab type       = BPE
0.00.071.243 I llm_load_print_meta: n_vocab          = 50304
0.00.071.243 I llm_load_print_meta: n_merges         = 50009
0.00.071.243 I llm_load_print_meta: vocab_only       = 0
0.00.071.243 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.243 I llm_load_print_meta: n_embd           = 2048
0.00.071.243 I llm_load_print_meta: n_layer          = 24
0.00.071.247 I llm_load_print_meta: n_head           = 16
0.00.071.250 I llm_load_print_meta: n_head_kv        = 16
0.00.071.251 I llm_load_print_meta: n_rot            = 32
0.00.071.251 I llm_load_print_meta: n_swa            = 0
0.00.071.251 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.251 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.252 I llm_load_print_meta: n_gqa            = 1
0.00.071.252 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.253 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.253 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.254 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.254 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.254 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.254 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.255 I llm_load_print_meta: n_ff             = 8192
0.00.071.255 I llm_load_print_meta: n_expert         = 0
0.00.071.255 I llm_load_print_meta: n_expert_used    = 0
0.00.071.255 I llm_load_print_meta: causal attn      = 1
0.00.071.255 I llm_load_print_meta: pooling type     = 0
0.00.071.255 I llm_load_print_meta: rope type        = 2
0.00.071.256 I llm_load_print_meta: rope scaling     = linear
0.00.071.256 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.256 I llm_load_print_meta: freq_scale_train = 1
0.00.071.256 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.257 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.257 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.257 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.257 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.257 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.257 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.258 I llm_load_print_meta: model type       = 1.4B
0.00.071.258 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.258 I llm_load_print_meta: model params     = 1.41 B
0.00.071.259 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.259 I llm_load_print_meta: general.name     = 1.4B
0.00.071.262 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.263 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.263 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.263 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.263 I llm_load_print_meta: LF token         = 128 ''
0.00.071.264 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.264 I llm_load_print_meta: max token length = 1024
0.00.073.747 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.747 I llm_load_tensors: offloading output layer to GPU
0.00.073.747 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.759 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.760 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.074.725 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.726 I llama_new_context_with_model: n_ctx         = 128
0.00.074.726 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.074.726 I llama_new_context_with_model: n_batch       = 128
0.00.074.726 I llama_new_context_with_model: n_ubatch      = 128
0.00.074.726 I llama_new_context_with_model: flash_attn    = 0
0.00.074.727 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.727 I llama_new_context_with_model: freq_scale    = 1
0.00.074.728 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.074.728 I ggml_metal_init: allocating
0.00.074.735 I ggml_metal_init: found device: Apple M4
0.00.074.737 I ggml_metal_init: picking default device: Apple M4
0.00.075.457 I ggml_metal_init: using embedded metal library
0.00.078.174 I ggml_metal_init: GPU name:   Apple M4
0.00.078.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.177 I ggml_metal_init: simdgroup reduction   = true
0.00.078.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.177 I ggml_metal_init: has bfloat            = true
0.00.078.178 I ggml_metal_init: use bfloat            = true
0.00.078.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.510 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.890 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.089.894 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.089.910 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.775 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.090.776 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.090.776 I llama_new_context_with_model: graph nodes  = 967
0.00.090.777 I llama_new_context_with_model: graph splits = 2
0.00.090.778 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.090.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.800 I 
0.00.879.822 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.834 I perplexity: tokenizing the input ..
0.00.887.427 I perplexity: tokenization took 7.592 ms
0.00.887.431 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.011.725 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.012.894 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.012.913 I llama_perf_context_print:        load time =     866.28 ms
0.01.012.914 I llama_perf_context_print: prompt eval time =     124.05 ms /   128 tokens (    0.97 ms per token,  1031.87 tokens per second)
0.01.012.914 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.012.915 I llama_perf_context_print:       total time =     133.11 ms /   129 tokens
0.01.013.375 I ggml_metal_free: deallocating

real	0m1.034s
user	0m0.100s
sys	0m0.139s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.015.448 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.916 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.028.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.923 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.924 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.924 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.924 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.926 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.934 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.934 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.106 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.460 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.461 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.462 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.462 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.462 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.463 I llama_model_loader: - type  f32:  194 tensors
0.00.038.463 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.464 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.351 I llm_load_vocab: special tokens cache size = 25
0.00.072.254 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.257 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.258 I llm_load_print_meta: arch             = gptneox
0.00.072.259 I llm_load_print_meta: vocab type       = BPE
0.00.072.259 I llm_load_print_meta: n_vocab          = 50304
0.00.072.259 I llm_load_print_meta: n_merges         = 50009
0.00.072.259 I llm_load_print_meta: vocab_only       = 0
0.00.072.260 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.260 I llm_load_print_meta: n_embd           = 2048
0.00.072.260 I llm_load_print_meta: n_layer          = 24
0.00.072.264 I llm_load_print_meta: n_head           = 16
0.00.072.265 I llm_load_print_meta: n_head_kv        = 16
0.00.072.266 I llm_load_print_meta: n_rot            = 32
0.00.072.266 I llm_load_print_meta: n_swa            = 0
0.00.072.266 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.266 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.267 I llm_load_print_meta: n_gqa            = 1
0.00.072.268 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.269 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.270 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.270 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.271 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.271 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.271 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.272 I llm_load_print_meta: n_ff             = 8192
0.00.072.272 I llm_load_print_meta: n_expert         = 0
0.00.072.272 I llm_load_print_meta: n_expert_used    = 0
0.00.072.273 I llm_load_print_meta: causal attn      = 1
0.00.072.273 I llm_load_print_meta: pooling type     = 0
0.00.072.273 I llm_load_print_meta: rope type        = 2
0.00.072.273 I llm_load_print_meta: rope scaling     = linear
0.00.072.274 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.274 I llm_load_print_meta: freq_scale_train = 1
0.00.072.274 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.275 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.276 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.276 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.276 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.276 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.279 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.279 I llm_load_print_meta: model type       = 1.4B
0.00.072.279 I llm_load_print_meta: model ftype      = Q4_0
0.00.072.280 I llm_load_print_meta: model params     = 1.41 B
0.00.072.280 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.072.282 I llm_load_print_meta: general.name     = 1.4B
0.00.072.283 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.283 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.283 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.283 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.284 I llm_load_print_meta: LF token         = 128 ''
0.00.072.285 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.285 I llm_load_print_meta: max token length = 1024
0.00.075.251 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.251 I llm_load_tensors: offloading output layer to GPU
0.00.075.251 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.264 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.075.266 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.076.676 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.677 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.677 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.678 I llama_new_context_with_model: n_batch       = 2048
0.00.076.678 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.678 I llama_new_context_with_model: flash_attn    = 0
0.00.076.679 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.679 I llama_new_context_with_model: freq_scale    = 1
0.00.076.680 I ggml_metal_init: allocating
0.00.076.684 I ggml_metal_init: found device: Apple M4
0.00.076.687 I ggml_metal_init: picking default device: Apple M4
0.00.077.653 I ggml_metal_init: using embedded metal library
0.00.081.805 I ggml_metal_init: GPU name:   Apple M4
0.00.081.807 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.808 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.808 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.809 I ggml_metal_init: simdgroup reduction   = true
0.00.081.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.809 I ggml_metal_init: has bfloat            = true
0.00.081.809 I ggml_metal_init: use bfloat            = true
0.00.081.810 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.907 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.120.482 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.495 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.525 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.684 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.121.686 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.121.687 I llama_new_context_with_model: graph nodes  = 967
0.00.121.687 I llama_new_context_with_model: graph splits = 2
0.00.121.694 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.121.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.121.825 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.898 I main: llama threadpool init, n_threads = 4
0.00.726.936 I 
0.00.726.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.959 I 
0.00.727.191 I sampler seed: 1234
0.00.727.195 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.727.211 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.727.211 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.727.211 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.411.392 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.411.398 I llama_perf_context_print:        load time =     711.45 ms
0.01.411.399 I llama_perf_context_print: prompt eval time =      39.75 ms /     7 tokens (    5.68 ms per token,   176.08 tokens per second)
0.01.411.399 I llama_perf_context_print:        eval time =     641.41 ms /    63 runs   (   10.18 ms per token,    98.22 tokens per second)
0.01.411.400 I llama_perf_context_print:       total time =     684.50 ms /    70 tokens
0.01.411.640 I ggml_metal_free: deallocating

real	0m1.436s
user	0m0.126s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.002 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.702 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.709 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.709 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.711 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.711 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.711 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.712 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.712 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.712 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.713 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.714 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.715 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.715 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.475 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.476 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.476 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.476 I llama_model_loader: - type  f32:  194 tensors
0.00.024.477 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.477 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.207 I llm_load_vocab: special tokens cache size = 25
0.00.051.348 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.351 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.352 I llm_load_print_meta: arch             = gptneox
0.00.051.352 I llm_load_print_meta: vocab type       = BPE
0.00.051.352 I llm_load_print_meta: n_vocab          = 50304
0.00.051.352 I llm_load_print_meta: n_merges         = 50009
0.00.051.353 I llm_load_print_meta: vocab_only       = 0
0.00.051.353 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.353 I llm_load_print_meta: n_embd           = 2048
0.00.051.353 I llm_load_print_meta: n_layer          = 24
0.00.051.356 I llm_load_print_meta: n_head           = 16
0.00.051.357 I llm_load_print_meta: n_head_kv        = 16
0.00.051.357 I llm_load_print_meta: n_rot            = 32
0.00.051.358 I llm_load_print_meta: n_swa            = 0
0.00.051.358 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.358 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.359 I llm_load_print_meta: n_gqa            = 1
0.00.051.359 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.360 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.360 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.361 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.361 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.361 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.361 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.362 I llm_load_print_meta: n_ff             = 8192
0.00.051.362 I llm_load_print_meta: n_expert         = 0
0.00.051.363 I llm_load_print_meta: n_expert_used    = 0
0.00.051.363 I llm_load_print_meta: causal attn      = 1
0.00.051.363 I llm_load_print_meta: pooling type     = 0
0.00.051.363 I llm_load_print_meta: rope type        = 2
0.00.051.363 I llm_load_print_meta: rope scaling     = linear
0.00.051.366 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.366 I llm_load_print_meta: freq_scale_train = 1
0.00.051.366 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.366 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.367 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.367 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.367 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.367 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.367 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.368 I llm_load_print_meta: model type       = 1.4B
0.00.051.368 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.368 I llm_load_print_meta: model params     = 1.41 B
0.00.051.373 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.373 I llm_load_print_meta: general.name     = 1.4B
0.00.051.373 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.373 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.374 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.374 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.374 I llm_load_print_meta: LF token         = 128 ''
0.00.051.374 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.374 I llm_load_print_meta: max token length = 1024
0.00.053.359 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.359 I llm_load_tensors: offloading output layer to GPU
0.00.053.360 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.370 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.371 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.328 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.328 I llama_new_context_with_model: n_ctx         = 128
0.00.054.328 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.329 I llama_new_context_with_model: n_batch       = 128
0.00.054.329 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.329 I llama_new_context_with_model: flash_attn    = 0
0.00.054.329 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.330 I llama_new_context_with_model: freq_scale    = 1
0.00.054.330 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.331 I ggml_metal_init: allocating
0.00.054.334 I ggml_metal_init: found device: Apple M4
0.00.054.336 I ggml_metal_init: picking default device: Apple M4
0.00.054.907 I ggml_metal_init: using embedded metal library
0.00.057.284 I ggml_metal_init: GPU name:   Apple M4
0.00.057.286 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.286 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.287 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.287 I ggml_metal_init: simdgroup reduction   = true
0.00.057.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.287 I ggml_metal_init: has bfloat            = true
0.00.057.287 I ggml_metal_init: use bfloat            = true
0.00.057.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.288 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.386 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.659 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.664 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.679 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.528 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.529 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.530 I llama_new_context_with_model: graph nodes  = 967
0.00.069.530 I llama_new_context_with_model: graph splits = 2
0.00.069.531 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.531 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.185 I 
0.00.638.233 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.247 I perplexity: tokenizing the input ..
0.00.645.917 I perplexity: tokenization took 7.667 ms
0.00.645.921 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.768.659 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.769.819 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.769.831 I llama_perf_context_print:        load time =     628.18 ms
0.00.769.832 I llama_perf_context_print: prompt eval time =     122.51 ms /   128 tokens (    0.96 ms per token,  1044.80 tokens per second)
0.00.769.833 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.769.833 I llama_perf_context_print:       total time =     131.65 ms /   129 tokens
0.00.770.175 I ggml_metal_free: deallocating

real	0m0.785s
user	0m0.079s
sys	0m0.108s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.777 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.058 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.062 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.068 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.069 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.070 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.071 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.071 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.071 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.072 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.072 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.074 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.077 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.077 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.078 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.843 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.724 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.725 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.726 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.726 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.727 I llama_model_loader: - type  f32:  194 tensors
0.00.024.727 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.727 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.434 I llm_load_vocab: special tokens cache size = 25
0.00.051.382 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.384 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.385 I llm_load_print_meta: arch             = gptneox
0.00.051.385 I llm_load_print_meta: vocab type       = BPE
0.00.051.385 I llm_load_print_meta: n_vocab          = 50304
0.00.051.385 I llm_load_print_meta: n_merges         = 50009
0.00.051.386 I llm_load_print_meta: vocab_only       = 0
0.00.051.386 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.386 I llm_load_print_meta: n_embd           = 2048
0.00.051.386 I llm_load_print_meta: n_layer          = 24
0.00.051.389 I llm_load_print_meta: n_head           = 16
0.00.051.390 I llm_load_print_meta: n_head_kv        = 16
0.00.051.390 I llm_load_print_meta: n_rot            = 32
0.00.051.391 I llm_load_print_meta: n_swa            = 0
0.00.051.391 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.391 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.392 I llm_load_print_meta: n_gqa            = 1
0.00.051.393 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.393 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.394 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.394 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.394 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.394 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.395 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.395 I llm_load_print_meta: n_ff             = 8192
0.00.051.396 I llm_load_print_meta: n_expert         = 0
0.00.051.396 I llm_load_print_meta: n_expert_used    = 0
0.00.051.398 I llm_load_print_meta: causal attn      = 1
0.00.051.399 I llm_load_print_meta: pooling type     = 0
0.00.051.399 I llm_load_print_meta: rope type        = 2
0.00.051.399 I llm_load_print_meta: rope scaling     = linear
0.00.051.400 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.400 I llm_load_print_meta: freq_scale_train = 1
0.00.051.400 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.400 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.400 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.401 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.401 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.401 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.401 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.401 I llm_load_print_meta: model type       = 1.4B
0.00.051.402 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.402 I llm_load_print_meta: model params     = 1.41 B
0.00.051.403 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.404 I llm_load_print_meta: general.name     = 1.4B
0.00.051.404 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.404 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.404 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.405 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.405 I llm_load_print_meta: LF token         = 128 ''
0.00.051.405 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.409 I llm_load_print_meta: max token length = 1024
0.00.053.423 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.424 I llm_load_tensors: offloading output layer to GPU
0.00.053.424 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.435 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.436 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.373 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.374 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.374 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.374 I llama_new_context_with_model: n_batch       = 2048
0.00.054.374 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.375 I llama_new_context_with_model: flash_attn    = 0
0.00.054.375 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.375 I llama_new_context_with_model: freq_scale    = 1
0.00.054.376 I ggml_metal_init: allocating
0.00.054.379 I ggml_metal_init: found device: Apple M4
0.00.054.381 I ggml_metal_init: picking default device: Apple M4
0.00.054.983 I ggml_metal_init: using embedded metal library
0.00.057.363 I ggml_metal_init: GPU name:   Apple M4
0.00.057.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.365 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.366 I ggml_metal_init: simdgroup reduction   = true
0.00.057.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.366 I ggml_metal_init: has bfloat            = true
0.00.057.368 I ggml_metal_init: use bfloat            = true
0.00.057.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.581 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.923 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.932 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.955 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.893 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.896 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.896 I llama_new_context_with_model: graph nodes  = 967
0.00.088.896 I llama_new_context_with_model: graph splits = 2
0.00.088.902 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.035 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.036 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.855 I main: llama threadpool init, n_threads = 4
0.00.732.899 I 
0.00.732.924 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.926 I 
0.00.733.173 I sampler seed: 1234
0.00.733.177 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.733.211 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.733.212 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.733.212 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.451.964 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63223.51 tokens per second)
0.01.451.965 I llama_perf_context_print:        load time =     724.07 ms
0.01.451.966 I llama_perf_context_print: prompt eval time =      43.64 ms /     7 tokens (    6.23 ms per token,   160.40 tokens per second)
0.01.451.966 I llama_perf_context_print:        eval time =     672.26 ms /    63 runs   (   10.67 ms per token,    93.71 tokens per second)
0.01.451.967 I llama_perf_context_print:       total time =     719.11 ms /    70 tokens
0.01.452.196 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.110s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.784 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.283 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.289 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.298 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.298 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.299 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.301 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.301 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.113 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.931 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.933 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.934 I llama_model_loader: - type  f32:  194 tensors
0.00.022.934 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.934 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.709 I llm_load_vocab: special tokens cache size = 25
0.00.048.607 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.609 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.610 I llm_load_print_meta: arch             = gptneox
0.00.048.610 I llm_load_print_meta: vocab type       = BPE
0.00.048.610 I llm_load_print_meta: n_vocab          = 50304
0.00.048.610 I llm_load_print_meta: n_merges         = 50009
0.00.048.610 I llm_load_print_meta: vocab_only       = 0
0.00.048.611 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.611 I llm_load_print_meta: n_embd           = 2048
0.00.048.611 I llm_load_print_meta: n_layer          = 24
0.00.048.614 I llm_load_print_meta: n_head           = 16
0.00.048.615 I llm_load_print_meta: n_head_kv        = 16
0.00.048.615 I llm_load_print_meta: n_rot            = 32
0.00.048.615 I llm_load_print_meta: n_swa            = 0
0.00.048.618 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.618 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.619 I llm_load_print_meta: n_gqa            = 1
0.00.048.620 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.620 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.621 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.621 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.621 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.622 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.622 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.623 I llm_load_print_meta: n_ff             = 8192
0.00.048.623 I llm_load_print_meta: n_expert         = 0
0.00.048.623 I llm_load_print_meta: n_expert_used    = 0
0.00.048.623 I llm_load_print_meta: causal attn      = 1
0.00.048.623 I llm_load_print_meta: pooling type     = 0
0.00.048.623 I llm_load_print_meta: rope type        = 2
0.00.048.624 I llm_load_print_meta: rope scaling     = linear
0.00.048.624 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.624 I llm_load_print_meta: freq_scale_train = 1
0.00.048.624 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.625 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.625 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.625 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.625 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.625 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.625 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.626 I llm_load_print_meta: model type       = 1.4B
0.00.048.627 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.628 I llm_load_print_meta: model params     = 1.41 B
0.00.048.628 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.628 I llm_load_print_meta: general.name     = 1.4B
0.00.048.629 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.629 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.629 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.629 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.630 I llm_load_print_meta: LF token         = 128 ''
0.00.048.630 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.630 I llm_load_print_meta: max token length = 1024
0.00.050.522 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.522 I llm_load_tensors: offloading output layer to GPU
0.00.050.523 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.533 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.534 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.424 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.425 I llama_new_context_with_model: n_ctx         = 128
0.00.051.425 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.425 I llama_new_context_with_model: n_batch       = 128
0.00.051.425 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.426 I llama_new_context_with_model: flash_attn    = 0
0.00.051.426 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.426 I llama_new_context_with_model: freq_scale    = 1
0.00.051.427 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.427 I ggml_metal_init: allocating
0.00.051.433 I ggml_metal_init: found device: Apple M4
0.00.051.435 I ggml_metal_init: picking default device: Apple M4
0.00.051.981 I ggml_metal_init: using embedded metal library
0.00.054.319 I ggml_metal_init: GPU name:   Apple M4
0.00.054.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.322 I ggml_metal_init: simdgroup reduction   = true
0.00.054.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.322 I ggml_metal_init: has bfloat            = true
0.00.054.322 I ggml_metal_init: use bfloat            = true
0.00.054.323 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.323 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.529 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.801 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.804 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.820 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.690 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.691 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.692 I llama_new_context_with_model: graph nodes  = 967
0.00.065.692 I llama_new_context_with_model: graph splits = 2
0.00.065.693 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.578 I 
0.00.645.623 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.638 I perplexity: tokenizing the input ..
0.00.653.389 I perplexity: tokenization took 7.749 ms
0.00.653.392 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.776.247 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.777.470 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.777.483 I llama_perf_context_print:        load time =     636.78 ms
0.00.777.484 I llama_perf_context_print: prompt eval time =     122.63 ms /   128 tokens (    0.96 ms per token,  1043.82 tokens per second)
0.00.777.485 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.485 I llama_perf_context_print:       total time =     131.91 ms /   129 tokens
0.00.777.917 I ggml_metal_free: deallocating

real	0m0.791s
user	0m0.076s
sys	0m0.092s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.844 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.847 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.852 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.854 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.854 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.856 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.856 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.857 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.857 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.857 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.858 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.858 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.858 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.859 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.863 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.863 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.863 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.695 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.794 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.581 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.583 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.584 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.584 I llama_model_loader: - type  f32:  194 tensors
0.00.025.585 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.585 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.363 I llm_load_vocab: special tokens cache size = 25
0.00.052.062 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.064 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.065 I llm_load_print_meta: arch             = gptneox
0.00.052.065 I llm_load_print_meta: vocab type       = BPE
0.00.052.065 I llm_load_print_meta: n_vocab          = 50304
0.00.052.065 I llm_load_print_meta: n_merges         = 50009
0.00.052.066 I llm_load_print_meta: vocab_only       = 0
0.00.052.066 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.066 I llm_load_print_meta: n_embd           = 2048
0.00.052.066 I llm_load_print_meta: n_layer          = 24
0.00.052.069 I llm_load_print_meta: n_head           = 16
0.00.052.070 I llm_load_print_meta: n_head_kv        = 16
0.00.052.070 I llm_load_print_meta: n_rot            = 32
0.00.052.072 I llm_load_print_meta: n_swa            = 0
0.00.052.073 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.073 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.074 I llm_load_print_meta: n_gqa            = 1
0.00.052.074 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.075 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.076 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.076 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.076 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.077 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.079 I llm_load_print_meta: n_ff             = 8192
0.00.052.079 I llm_load_print_meta: n_expert         = 0
0.00.052.079 I llm_load_print_meta: n_expert_used    = 0
0.00.052.080 I llm_load_print_meta: causal attn      = 1
0.00.052.081 I llm_load_print_meta: pooling type     = 0
0.00.052.081 I llm_load_print_meta: rope type        = 2
0.00.052.081 I llm_load_print_meta: rope scaling     = linear
0.00.052.081 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.082 I llm_load_print_meta: freq_scale_train = 1
0.00.052.082 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.082 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.086 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.086 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.086 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.087 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.087 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.088 I llm_load_print_meta: model type       = 1.4B
0.00.052.088 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.088 I llm_load_print_meta: model params     = 1.41 B
0.00.052.089 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.089 I llm_load_print_meta: general.name     = 1.4B
0.00.052.089 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: LF token         = 128 ''
0.00.052.090 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.091 I llm_load_print_meta: max token length = 1024
0.00.054.119 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.119 I llm_load_tensors: offloading output layer to GPU
0.00.054.119 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.130 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.131 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.054 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.055 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.055 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.055 I llama_new_context_with_model: n_batch       = 2048
0.00.055.055 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.055 I llama_new_context_with_model: flash_attn    = 0
0.00.055.056 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.056 I llama_new_context_with_model: freq_scale    = 1
0.00.055.057 I ggml_metal_init: allocating
0.00.055.060 I ggml_metal_init: found device: Apple M4
0.00.055.062 I ggml_metal_init: picking default device: Apple M4
0.00.055.657 I ggml_metal_init: using embedded metal library
0.00.057.995 I ggml_metal_init: GPU name:   Apple M4
0.00.057.996 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.997 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.997 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.997 I ggml_metal_init: simdgroup reduction   = true
0.00.057.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.998 I ggml_metal_init: has bfloat            = true
0.00.057.998 I ggml_metal_init: use bfloat            = true
0.00.057.998 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.899 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.864 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.869 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.885 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.970 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.972 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.972 I llama_new_context_with_model: graph nodes  = 967
0.00.087.972 I llama_new_context_with_model: graph splits = 2
0.00.087.975 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.114 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.058 I main: llama threadpool init, n_threads = 4
0.00.759.099 I 
0.00.759.121 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.121 I 
0.00.759.361 I sampler seed: 1234
0.00.759.365 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.409 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.411 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.411 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.545.260 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.545.261 I llama_perf_context_print:        load time =     748.21 ms
0.01.545.262 I llama_perf_context_print: prompt eval time =      46.38 ms /     7 tokens (    6.63 ms per token,   150.94 tokens per second)
0.01.545.263 I llama_perf_context_print:        eval time =     736.46 ms /    63 runs   (   11.69 ms per token,    85.54 tokens per second)
0.01.545.263 I llama_perf_context_print:       total time =     786.20 ms /    70 tokens
0.01.545.476 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.110s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.686 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.222 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.226 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.227 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.228 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.228 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.229 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.230 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.231 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.231 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.231 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.232 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.235 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.236 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.236 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.027 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.871 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.873 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.873 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.874 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.874 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.874 I llama_model_loader: - type  f32:  194 tensors
0.00.023.875 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.875 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.516 I llm_load_vocab: special tokens cache size = 25
0.00.050.431 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.434 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.434 I llm_load_print_meta: arch             = gptneox
0.00.050.435 I llm_load_print_meta: vocab type       = BPE
0.00.050.435 I llm_load_print_meta: n_vocab          = 50304
0.00.050.435 I llm_load_print_meta: n_merges         = 50009
0.00.050.435 I llm_load_print_meta: vocab_only       = 0
0.00.050.435 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.436 I llm_load_print_meta: n_embd           = 2048
0.00.050.436 I llm_load_print_meta: n_layer          = 24
0.00.050.438 I llm_load_print_meta: n_head           = 16
0.00.050.439 I llm_load_print_meta: n_head_kv        = 16
0.00.050.440 I llm_load_print_meta: n_rot            = 32
0.00.050.440 I llm_load_print_meta: n_swa            = 0
0.00.050.440 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.440 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.441 I llm_load_print_meta: n_gqa            = 1
0.00.050.442 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.444 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.445 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.445 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.445 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.446 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.446 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.446 I llm_load_print_meta: n_ff             = 8192
0.00.050.447 I llm_load_print_meta: n_expert         = 0
0.00.050.447 I llm_load_print_meta: n_expert_used    = 0
0.00.050.447 I llm_load_print_meta: causal attn      = 1
0.00.050.447 I llm_load_print_meta: pooling type     = 0
0.00.050.447 I llm_load_print_meta: rope type        = 2
0.00.050.448 I llm_load_print_meta: rope scaling     = linear
0.00.050.448 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.450 I llm_load_print_meta: freq_scale_train = 1
0.00.050.450 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.450 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.450 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.450 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.450 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.451 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.451 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.451 I llm_load_print_meta: model type       = 1.4B
0.00.050.451 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.452 I llm_load_print_meta: model params     = 1.41 B
0.00.050.456 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.457 I llm_load_print_meta: general.name     = 1.4B
0.00.050.457 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.457 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.457 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.457 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.458 I llm_load_print_meta: LF token         = 128 ''
0.00.050.458 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.458 I llm_load_print_meta: max token length = 1024
0.00.052.472 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.472 I llm_load_tensors: offloading output layer to GPU
0.00.052.473 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.483 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.485 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.451 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.452 I llama_new_context_with_model: n_ctx         = 128
0.00.053.452 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.453 I llama_new_context_with_model: n_batch       = 128
0.00.053.453 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.453 I llama_new_context_with_model: flash_attn    = 0
0.00.053.453 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.454 I llama_new_context_with_model: freq_scale    = 1
0.00.053.454 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.454 I ggml_metal_init: allocating
0.00.053.458 I ggml_metal_init: found device: Apple M4
0.00.053.460 I ggml_metal_init: picking default device: Apple M4
0.00.054.067 I ggml_metal_init: using embedded metal library
0.00.056.373 I ggml_metal_init: GPU name:   Apple M4
0.00.056.375 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.375 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.375 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.376 I ggml_metal_init: simdgroup reduction   = true
0.00.056.376 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.376 I ggml_metal_init: has bfloat            = true
0.00.056.376 I ggml_metal_init: use bfloat            = true
0.00.056.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.377 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.039 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.421 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.423 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.439 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.432 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.433 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.434 I llama_new_context_with_model: graph nodes  = 967
0.00.068.434 I llama_new_context_with_model: graph splits = 2
0.00.068.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.929 I 
0.00.729.957 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.986 I perplexity: tokenizing the input ..
0.00.737.658 I perplexity: tokenization took 7.67 ms
0.00.737.662 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.872.686 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.873.861 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.873.879 I llama_perf_context_print:        load time =     720.24 ms
0.00.873.880 I llama_perf_context_print: prompt eval time =     134.80 ms /   128 tokens (    1.05 ms per token,   949.58 tokens per second)
0.00.873.881 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.873.881 I llama_perf_context_print:       total time =     143.95 ms /   129 tokens
0.00.874.372 I ggml_metal_free: deallocating

real	0m0.889s
user	0m0.078s
sys	0m0.107s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.618 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.043 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.048 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.048 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.049 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.049 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.050 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.051 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.051 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.051 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.055 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.055 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.056 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.814 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.862 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.576 I llama_model_loader: - type  f32:  194 tensors
0.00.024.576 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.577 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.582 I llm_load_vocab: special tokens cache size = 25
0.00.050.585 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.587 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.588 I llm_load_print_meta: arch             = gptneox
0.00.050.588 I llm_load_print_meta: vocab type       = BPE
0.00.050.588 I llm_load_print_meta: n_vocab          = 50304
0.00.050.588 I llm_load_print_meta: n_merges         = 50009
0.00.050.589 I llm_load_print_meta: vocab_only       = 0
0.00.050.589 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.589 I llm_load_print_meta: n_embd           = 2048
0.00.050.589 I llm_load_print_meta: n_layer          = 24
0.00.050.592 I llm_load_print_meta: n_head           = 16
0.00.050.593 I llm_load_print_meta: n_head_kv        = 16
0.00.050.593 I llm_load_print_meta: n_rot            = 32
0.00.050.593 I llm_load_print_meta: n_swa            = 0
0.00.050.594 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.595 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.595 I llm_load_print_meta: n_gqa            = 1
0.00.050.596 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.598 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.599 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.599 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.600 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.600 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.600 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.600 I llm_load_print_meta: n_ff             = 8192
0.00.050.601 I llm_load_print_meta: n_expert         = 0
0.00.050.602 I llm_load_print_meta: n_expert_used    = 0
0.00.050.604 I llm_load_print_meta: causal attn      = 1
0.00.050.604 I llm_load_print_meta: pooling type     = 0
0.00.050.604 I llm_load_print_meta: rope type        = 2
0.00.050.604 I llm_load_print_meta: rope scaling     = linear
0.00.050.604 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.605 I llm_load_print_meta: freq_scale_train = 1
0.00.050.605 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.605 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.605 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.605 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.606 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.606 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.606 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.606 I llm_load_print_meta: model type       = 1.4B
0.00.050.607 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.611 I llm_load_print_meta: model params     = 1.41 B
0.00.050.611 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.612 I llm_load_print_meta: general.name     = 1.4B
0.00.050.612 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.612 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.612 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.612 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.613 I llm_load_print_meta: LF token         = 128 ''
0.00.050.613 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.613 I llm_load_print_meta: max token length = 1024
0.00.052.687 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.687 I llm_load_tensors: offloading output layer to GPU
0.00.052.687 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.698 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.700 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.642 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.642 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.643 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.643 I llama_new_context_with_model: n_batch       = 2048
0.00.053.643 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.643 I llama_new_context_with_model: flash_attn    = 0
0.00.053.644 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.644 I llama_new_context_with_model: freq_scale    = 1
0.00.053.644 I ggml_metal_init: allocating
0.00.053.647 I ggml_metal_init: found device: Apple M4
0.00.053.649 I ggml_metal_init: picking default device: Apple M4
0.00.054.244 I ggml_metal_init: using embedded metal library
0.00.056.532 I ggml_metal_init: GPU name:   Apple M4
0.00.056.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.535 I ggml_metal_init: simdgroup reduction   = true
0.00.056.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.536 I ggml_metal_init: has bfloat            = true
0.00.056.536 I ggml_metal_init: use bfloat            = true
0.00.056.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.538 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.134 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.135 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.145 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.170 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.146 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.147 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.148 I llama_new_context_with_model: graph nodes  = 967
0.00.086.148 I llama_new_context_with_model: graph splits = 2
0.00.086.150 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.280 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.489 I main: llama threadpool init, n_threads = 4
0.00.699.534 I 
0.00.699.563 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.564 I 
0.00.699.793 I sampler seed: 1234
0.00.699.797 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.840 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.840 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.840 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.541.451 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.541.452 I llama_perf_context_print:        load time =     690.87 ms
0.01.541.453 I llama_perf_context_print: prompt eval time =      46.18 ms /     7 tokens (    6.60 ms per token,   151.57 tokens per second)
0.01.541.454 I llama_perf_context_print:        eval time =     792.45 ms /    63 runs   (   12.58 ms per token,    79.50 tokens per second)
0.01.541.454 I llama_perf_context_print:       total time =     841.96 ms /    70 tokens
0.01.541.656 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.108s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.777 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.575 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.581 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.584 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.585 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.587 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.345 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.163 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.164 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.164 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.165 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.165 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.165 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.166 I llama_model_loader: - type  f32:  194 tensors
0.00.023.166 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.166 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.942 I llm_load_vocab: special tokens cache size = 25
0.00.048.967 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.969 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.970 I llm_load_print_meta: arch             = gptneox
0.00.048.970 I llm_load_print_meta: vocab type       = BPE
0.00.048.970 I llm_load_print_meta: n_vocab          = 50304
0.00.048.970 I llm_load_print_meta: n_merges         = 50009
0.00.048.970 I llm_load_print_meta: vocab_only       = 0
0.00.048.971 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.971 I llm_load_print_meta: n_embd           = 2048
0.00.048.971 I llm_load_print_meta: n_layer          = 24
0.00.048.974 I llm_load_print_meta: n_head           = 16
0.00.048.975 I llm_load_print_meta: n_head_kv        = 16
0.00.048.975 I llm_load_print_meta: n_rot            = 32
0.00.048.977 I llm_load_print_meta: n_swa            = 0
0.00.048.978 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.978 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.978 I llm_load_print_meta: n_gqa            = 1
0.00.048.979 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.980 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.980 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.980 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.981 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.981 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.981 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.982 I llm_load_print_meta: n_ff             = 8192
0.00.048.982 I llm_load_print_meta: n_expert         = 0
0.00.048.982 I llm_load_print_meta: n_expert_used    = 0
0.00.048.982 I llm_load_print_meta: causal attn      = 1
0.00.048.982 I llm_load_print_meta: pooling type     = 0
0.00.048.982 I llm_load_print_meta: rope type        = 2
0.00.048.983 I llm_load_print_meta: rope scaling     = linear
0.00.048.985 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.985 I llm_load_print_meta: freq_scale_train = 1
0.00.048.985 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.985 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.986 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.986 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.986 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.986 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.986 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.986 I llm_load_print_meta: model type       = 1.4B
0.00.048.987 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.987 I llm_load_print_meta: model params     = 1.41 B
0.00.048.989 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.989 I llm_load_print_meta: general.name     = 1.4B
0.00.048.989 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.989 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.989 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.990 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.990 I llm_load_print_meta: LF token         = 128 ''
0.00.048.990 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.990 I llm_load_print_meta: max token length = 1024
0.00.050.947 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.947 I llm_load_tensors: offloading output layer to GPU
0.00.050.947 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.958 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.959 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.859 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.860 I llama_new_context_with_model: n_ctx         = 128
0.00.051.860 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.860 I llama_new_context_with_model: n_batch       = 128
0.00.051.860 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.861 I llama_new_context_with_model: flash_attn    = 0
0.00.051.861 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.861 I llama_new_context_with_model: freq_scale    = 1
0.00.051.862 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.862 I ggml_metal_init: allocating
0.00.051.865 I ggml_metal_init: found device: Apple M4
0.00.051.867 I ggml_metal_init: picking default device: Apple M4
0.00.052.415 I ggml_metal_init: using embedded metal library
0.00.054.744 I ggml_metal_init: GPU name:   Apple M4
0.00.054.746 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.746 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.747 I ggml_metal_init: simdgroup reduction   = true
0.00.054.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.747 I ggml_metal_init: has bfloat            = true
0.00.054.747 I ggml_metal_init: use bfloat            = true
0.00.054.747 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.748 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.158 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.445 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.448 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.465 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.323 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.324 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.324 I llama_new_context_with_model: graph nodes  = 967
0.00.066.324 I llama_new_context_with_model: graph splits = 2
0.00.066.325 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.326 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.084 I 
0.00.784.112 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.125 I perplexity: tokenizing the input ..
0.00.791.646 I perplexity: tokenization took 7.52 ms
0.00.791.650 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.926.354 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.927.504 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.927.521 I llama_perf_context_print:        load time =     775.30 ms
0.00.927.522 I llama_perf_context_print: prompt eval time =     134.48 ms /   128 tokens (    1.05 ms per token,   951.81 tokens per second)
0.00.927.523 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.927.524 I llama_perf_context_print:       total time =     143.44 ms /   129 tokens
0.00.927.986 I ggml_metal_free: deallocating

real	0m0.942s
user	0m0.077s
sys	0m0.136s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.664 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.976 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.980 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.982 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.982 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.983 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.983 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.983 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.987 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.992 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.994 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.159 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.143 I llama_model_loader: - type  f32:  194 tensors
0.00.025.144 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.144 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.144 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.900 I llm_load_vocab: special tokens cache size = 25
0.00.051.906 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.908 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.909 I llm_load_print_meta: arch             = gptneox
0.00.051.909 I llm_load_print_meta: vocab type       = BPE
0.00.051.909 I llm_load_print_meta: n_vocab          = 50304
0.00.051.909 I llm_load_print_meta: n_merges         = 50009
0.00.051.910 I llm_load_print_meta: vocab_only       = 0
0.00.051.910 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.910 I llm_load_print_meta: n_embd           = 2048
0.00.051.910 I llm_load_print_meta: n_layer          = 24
0.00.051.913 I llm_load_print_meta: n_head           = 16
0.00.051.913 I llm_load_print_meta: n_head_kv        = 16
0.00.051.914 I llm_load_print_meta: n_rot            = 32
0.00.051.914 I llm_load_print_meta: n_swa            = 0
0.00.051.914 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.914 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.916 I llm_load_print_meta: n_gqa            = 1
0.00.051.917 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.917 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.918 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.918 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.918 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.919 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.919 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.919 I llm_load_print_meta: n_ff             = 8192
0.00.051.920 I llm_load_print_meta: n_expert         = 0
0.00.051.921 I llm_load_print_meta: n_expert_used    = 0
0.00.051.921 I llm_load_print_meta: causal attn      = 1
0.00.051.922 I llm_load_print_meta: pooling type     = 0
0.00.051.922 I llm_load_print_meta: rope type        = 2
0.00.051.922 I llm_load_print_meta: rope scaling     = linear
0.00.051.922 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.923 I llm_load_print_meta: freq_scale_train = 1
0.00.051.923 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.923 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.923 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.923 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.923 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.924 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.924 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.924 I llm_load_print_meta: model type       = 1.4B
0.00.051.924 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.925 I llm_load_print_meta: model params     = 1.41 B
0.00.051.925 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.926 I llm_load_print_meta: general.name     = 1.4B
0.00.051.926 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.931 I llm_load_print_meta: LF token         = 128 ''
0.00.051.932 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.932 I llm_load_print_meta: max token length = 1024
0.00.053.820 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.821 I llm_load_tensors: offloading output layer to GPU
0.00.053.821 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.831 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.832 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.717 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.718 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.718 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.718 I llama_new_context_with_model: n_batch       = 2048
0.00.054.718 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.718 I llama_new_context_with_model: flash_attn    = 0
0.00.054.719 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.719 I llama_new_context_with_model: freq_scale    = 1
0.00.054.720 I ggml_metal_init: allocating
0.00.054.723 I ggml_metal_init: found device: Apple M4
0.00.054.725 I ggml_metal_init: picking default device: Apple M4
0.00.055.301 I ggml_metal_init: using embedded metal library
0.00.057.638 I ggml_metal_init: GPU name:   Apple M4
0.00.057.639 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.640 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.640 I ggml_metal_init: simdgroup reduction   = true
0.00.057.640 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.640 I ggml_metal_init: has bfloat            = true
0.00.057.641 I ggml_metal_init: use bfloat            = true
0.00.057.641 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.642 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.291 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.385 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.392 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.410 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.531 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.533 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.533 I llama_new_context_with_model: graph nodes  = 967
0.00.089.534 I llama_new_context_with_model: graph splits = 2
0.00.089.536 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.691 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.691 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.443.515 I main: llama threadpool init, n_threads = 4
0.00.443.555 I 
0.00.443.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.443.589 I 
0.00.443.833 I sampler seed: 1234
0.00.443.837 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.443.852 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.443.853 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.443.854 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.120.791 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.01.120.791 I llama_perf_context_print:        load time =     432.85 ms
0.01.120.792 I llama_perf_context_print: prompt eval time =      39.69 ms /     7 tokens (    5.67 ms per token,   176.37 tokens per second)
0.01.120.794 I llama_perf_context_print:        eval time =     634.72 ms /    63 runs   (   10.07 ms per token,    99.26 tokens per second)
0.01.120.795 I llama_perf_context_print:       total time =     677.28 ms /    70 tokens
0.01.121.097 I ggml_metal_free: deallocating

real	0m1.142s
user	0m0.111s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.834 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.208 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.219 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.220 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.220 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.221 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.222 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.223 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.223 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.223 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.224 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.224 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.226 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.227 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.954 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.968 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.832 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.833 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.833 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.833 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.834 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.834 I llama_model_loader: - type  f32:  194 tensors
0.00.023.835 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.835 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.835 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.499 I llm_load_vocab: special tokens cache size = 25
0.00.049.408 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.411 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.411 I llm_load_print_meta: arch             = gptneox
0.00.049.411 I llm_load_print_meta: vocab type       = BPE
0.00.049.411 I llm_load_print_meta: n_vocab          = 50304
0.00.049.412 I llm_load_print_meta: n_merges         = 50009
0.00.049.412 I llm_load_print_meta: vocab_only       = 0
0.00.049.412 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.412 I llm_load_print_meta: n_embd           = 2048
0.00.049.412 I llm_load_print_meta: n_layer          = 24
0.00.049.415 I llm_load_print_meta: n_head           = 16
0.00.049.416 I llm_load_print_meta: n_head_kv        = 16
0.00.049.416 I llm_load_print_meta: n_rot            = 32
0.00.049.416 I llm_load_print_meta: n_swa            = 0
0.00.049.416 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.416 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.417 I llm_load_print_meta: n_gqa            = 1
0.00.049.418 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.418 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.419 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.419 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.419 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.420 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.420 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.420 I llm_load_print_meta: n_ff             = 8192
0.00.049.421 I llm_load_print_meta: n_expert         = 0
0.00.049.421 I llm_load_print_meta: n_expert_used    = 0
0.00.049.421 I llm_load_print_meta: causal attn      = 1
0.00.049.421 I llm_load_print_meta: pooling type     = 0
0.00.049.421 I llm_load_print_meta: rope type        = 2
0.00.049.423 I llm_load_print_meta: rope scaling     = linear
0.00.049.423 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.424 I llm_load_print_meta: freq_scale_train = 1
0.00.049.424 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.424 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.424 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.424 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.424 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.424 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.425 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.425 I llm_load_print_meta: model type       = 1.4B
0.00.049.425 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.426 I llm_load_print_meta: model params     = 1.41 B
0.00.049.426 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.426 I llm_load_print_meta: general.name     = 1.4B
0.00.049.427 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.428 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.433 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.433 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.433 I llm_load_print_meta: LF token         = 128 ''
0.00.049.434 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.434 I llm_load_print_meta: max token length = 1024
0.00.051.316 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.317 I llm_load_tensors: offloading output layer to GPU
0.00.051.317 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.327 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.328 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.224 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.225 I llama_new_context_with_model: n_ctx         = 128
0.00.052.225 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.225 I llama_new_context_with_model: n_batch       = 128
0.00.052.226 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.226 I llama_new_context_with_model: flash_attn    = 0
0.00.052.226 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.227 I llama_new_context_with_model: freq_scale    = 1
0.00.052.227 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.227 I ggml_metal_init: allocating
0.00.052.233 I ggml_metal_init: found device: Apple M4
0.00.052.235 I ggml_metal_init: picking default device: Apple M4
0.00.052.780 I ggml_metal_init: using embedded metal library
0.00.055.126 I ggml_metal_init: GPU name:   Apple M4
0.00.055.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.128 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.128 I ggml_metal_init: simdgroup reduction   = true
0.00.055.128 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.128 I ggml_metal_init: has bfloat            = true
0.00.055.129 I ggml_metal_init: use bfloat            = true
0.00.055.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.130 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.511 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.826 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.828 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.841 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.697 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.698 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.698 I llama_new_context_with_model: graph nodes  = 967
0.00.066.699 I llama_new_context_with_model: graph splits = 2
0.00.066.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.420.035 I 
0.00.420.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.420.087 I perplexity: tokenizing the input ..
0.00.427.815 I perplexity: tokenization took 7.726 ms
0.00.427.823 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.559.964 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.120 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.138 I llama_perf_context_print:        load time =     410.19 ms
0.00.561.139 I llama_perf_context_print: prompt eval time =     131.92 ms /   128 tokens (    1.03 ms per token,   970.31 tokens per second)
0.00.561.140 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.140 I llama_perf_context_print:       total time =     141.11 ms /   129 tokens
0.00.561.635 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.077s
sys	0m0.072s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.641 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.172 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.173 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.173 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.178 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.179 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.179 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.180 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.180 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.181 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.077 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.014 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.015 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.015 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.015 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.016 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.016 I llama_model_loader: - type  f32:  194 tensors
0.00.023.017 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.017 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.017 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.017 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.778 I llm_load_vocab: special tokens cache size = 25
0.00.050.954 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.959 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.959 I llm_load_print_meta: arch             = gptneox
0.00.050.960 I llm_load_print_meta: vocab type       = BPE
0.00.050.960 I llm_load_print_meta: n_vocab          = 50304
0.00.050.962 I llm_load_print_meta: n_merges         = 50009
0.00.050.963 I llm_load_print_meta: vocab_only       = 0
0.00.050.963 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.963 I llm_load_print_meta: n_embd           = 2048
0.00.050.963 I llm_load_print_meta: n_layer          = 24
0.00.050.968 I llm_load_print_meta: n_head           = 16
0.00.050.968 I llm_load_print_meta: n_head_kv        = 16
0.00.050.968 I llm_load_print_meta: n_rot            = 32
0.00.050.969 I llm_load_print_meta: n_swa            = 0
0.00.050.969 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.969 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.969 I llm_load_print_meta: n_gqa            = 1
0.00.050.970 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.971 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.971 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.972 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.973 I llm_load_print_meta: n_ff             = 8192
0.00.050.973 I llm_load_print_meta: n_expert         = 0
0.00.050.974 I llm_load_print_meta: n_expert_used    = 0
0.00.050.974 I llm_load_print_meta: causal attn      = 1
0.00.050.974 I llm_load_print_meta: pooling type     = 0
0.00.050.974 I llm_load_print_meta: rope type        = 2
0.00.050.975 I llm_load_print_meta: rope scaling     = linear
0.00.050.975 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.975 I llm_load_print_meta: freq_scale_train = 1
0.00.050.976 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.977 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.977 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.977 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.977 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.977 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.977 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.977 I llm_load_print_meta: model type       = 1.4B
0.00.050.978 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.978 I llm_load_print_meta: model params     = 1.41 B
0.00.050.979 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.979 I llm_load_print_meta: general.name     = 1.4B
0.00.050.979 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.979 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.979 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.979 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.980 I llm_load_print_meta: LF token         = 128 ''
0.00.050.980 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.980 I llm_load_print_meta: max token length = 1024
0.00.052.869 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.869 I llm_load_tensors: offloading output layer to GPU
0.00.052.869 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.880 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.882 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.800 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.801 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.801 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.801 I llama_new_context_with_model: n_batch       = 2048
0.00.053.802 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.802 I llama_new_context_with_model: flash_attn    = 0
0.00.053.802 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.803 I llama_new_context_with_model: freq_scale    = 1
0.00.053.804 I ggml_metal_init: allocating
0.00.053.811 I ggml_metal_init: found device: Apple M4
0.00.053.814 I ggml_metal_init: picking default device: Apple M4
0.00.054.438 I ggml_metal_init: using embedded metal library
0.00.056.868 I ggml_metal_init: GPU name:   Apple M4
0.00.056.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.870 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.871 I ggml_metal_init: simdgroup reduction   = true
0.00.056.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.871 I ggml_metal_init: has bfloat            = true
0.00.056.871 I ggml_metal_init: use bfloat            = true
0.00.056.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.110 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.376 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.382 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.406 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.345 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.346 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.347 I llama_new_context_with_model: graph nodes  = 967
0.00.086.347 I llama_new_context_with_model: graph splits = 2
0.00.086.350 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.491 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.492 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.530 I main: llama threadpool init, n_threads = 4
0.00.513.571 I 
0.00.513.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.597 I 
0.00.513.824 I sampler seed: 1234
0.00.513.830 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.513.845 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.513.847 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.513.847 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.260.230 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.01.260.230 I llama_perf_context_print:        load time =     504.88 ms
0.01.260.231 I llama_perf_context_print: prompt eval time =      44.23 ms /     7 tokens (    6.32 ms per token,   158.27 tokens per second)
0.01.260.232 I llama_perf_context_print:        eval time =     699.21 ms /    63 runs   (   11.10 ms per token,    90.10 tokens per second)
0.01.260.232 I llama_perf_context_print:       total time =     746.70 ms /    70 tokens
0.01.260.458 I ggml_metal_free: deallocating

real	0m1.278s
user	0m0.110s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.039 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.046 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.051 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.051 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.051 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.052 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.052 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.055 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.058 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.756 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.765 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.534 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.534 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.535 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.535 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.535 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.536 I llama_model_loader: - type  f32:  194 tensors
0.00.022.536 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.536 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.537 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.537 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.233 I llm_load_vocab: special tokens cache size = 25
0.00.048.019 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.022 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.022 I llm_load_print_meta: arch             = gptneox
0.00.048.022 I llm_load_print_meta: vocab type       = BPE
0.00.048.023 I llm_load_print_meta: n_vocab          = 50304
0.00.048.023 I llm_load_print_meta: n_merges         = 50009
0.00.048.023 I llm_load_print_meta: vocab_only       = 0
0.00.048.023 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.023 I llm_load_print_meta: n_embd           = 2048
0.00.048.023 I llm_load_print_meta: n_layer          = 24
0.00.048.026 I llm_load_print_meta: n_head           = 16
0.00.048.027 I llm_load_print_meta: n_head_kv        = 16
0.00.048.027 I llm_load_print_meta: n_rot            = 32
0.00.048.027 I llm_load_print_meta: n_swa            = 0
0.00.048.028 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.028 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.028 I llm_load_print_meta: n_gqa            = 1
0.00.048.029 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.032 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.032 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.033 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.034 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.034 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.034 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.035 I llm_load_print_meta: n_ff             = 8192
0.00.048.035 I llm_load_print_meta: n_expert         = 0
0.00.048.035 I llm_load_print_meta: n_expert_used    = 0
0.00.048.035 I llm_load_print_meta: causal attn      = 1
0.00.048.036 I llm_load_print_meta: pooling type     = 0
0.00.048.036 I llm_load_print_meta: rope type        = 2
0.00.048.036 I llm_load_print_meta: rope scaling     = linear
0.00.048.036 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.037 I llm_load_print_meta: freq_scale_train = 1
0.00.048.038 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.038 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.038 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.038 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.038 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.040 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.040 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.040 I llm_load_print_meta: model type       = 1.4B
0.00.048.040 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.041 I llm_load_print_meta: model params     = 1.41 B
0.00.048.041 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.042 I llm_load_print_meta: general.name     = 1.4B
0.00.048.042 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.042 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.042 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.042 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.043 I llm_load_print_meta: LF token         = 128 ''
0.00.048.043 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.043 I llm_load_print_meta: max token length = 1024
0.00.049.628 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.049.629 I llm_load_tensors: offloading output layer to GPU
0.00.049.629 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.049.639 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.049.640 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.050.485 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.486 I llama_new_context_with_model: n_ctx         = 128
0.00.050.486 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.486 I llama_new_context_with_model: n_batch       = 128
0.00.050.486 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.487 I llama_new_context_with_model: flash_attn    = 0
0.00.050.487 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.487 I llama_new_context_with_model: freq_scale    = 1
0.00.050.488 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.488 I ggml_metal_init: allocating
0.00.050.491 I ggml_metal_init: found device: Apple M4
0.00.050.493 I ggml_metal_init: picking default device: Apple M4
0.00.051.047 I ggml_metal_init: using embedded metal library
0.00.053.388 I ggml_metal_init: GPU name:   Apple M4
0.00.053.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.391 I ggml_metal_init: simdgroup reduction   = true
0.00.053.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.391 I ggml_metal_init: has bfloat            = true
0.00.053.391 I ggml_metal_init: use bfloat            = true
0.00.053.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.392 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.768 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.005 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.007 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.023 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.959 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.960 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.960 I llama_new_context_with_model: graph nodes  = 967
0.00.064.960 I llama_new_context_with_model: graph splits = 2
0.00.064.961 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.154 I 
0.00.532.196 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.210 I perplexity: tokenizing the input ..
0.00.539.725 I perplexity: tokenization took 7.514 ms
0.00.539.729 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.671.659 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.672.855 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.672.877 I llama_perf_context_print:        load time =     523.48 ms
0.00.672.878 I llama_perf_context_print: prompt eval time =     131.70 ms /   128 tokens (    1.03 ms per token,   971.88 tokens per second)
0.00.672.879 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.672.879 I llama_perf_context_print:       total time =     140.73 ms /   129 tokens
0.00.673.419 I ggml_metal_free: deallocating

real	0m0.687s
user	0m0.077s
sys	0m0.093s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.618 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.713 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.717 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.719 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.720 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.720 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.722 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.723 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.724 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.725 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.726 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.592 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.659 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.488 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.490 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.490 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.490 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.491 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.491 I llama_model_loader: - type  f32:  194 tensors
0.00.023.491 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.492 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.492 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.293 I llm_load_vocab: special tokens cache size = 25
0.00.050.269 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.272 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.272 I llm_load_print_meta: arch             = gptneox
0.00.050.273 I llm_load_print_meta: vocab type       = BPE
0.00.050.273 I llm_load_print_meta: n_vocab          = 50304
0.00.050.273 I llm_load_print_meta: n_merges         = 50009
0.00.050.273 I llm_load_print_meta: vocab_only       = 0
0.00.050.273 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.273 I llm_load_print_meta: n_embd           = 2048
0.00.050.274 I llm_load_print_meta: n_layer          = 24
0.00.050.276 I llm_load_print_meta: n_head           = 16
0.00.050.277 I llm_load_print_meta: n_head_kv        = 16
0.00.050.279 I llm_load_print_meta: n_rot            = 32
0.00.050.279 I llm_load_print_meta: n_swa            = 0
0.00.050.280 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.280 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.280 I llm_load_print_meta: n_gqa            = 1
0.00.050.281 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.282 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.287 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.287 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.288 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.288 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.288 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.289 I llm_load_print_meta: n_ff             = 8192
0.00.050.289 I llm_load_print_meta: n_expert         = 0
0.00.050.289 I llm_load_print_meta: n_expert_used    = 0
0.00.050.289 I llm_load_print_meta: causal attn      = 1
0.00.050.289 I llm_load_print_meta: pooling type     = 0
0.00.050.290 I llm_load_print_meta: rope type        = 2
0.00.050.290 I llm_load_print_meta: rope scaling     = linear
0.00.050.290 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.290 I llm_load_print_meta: freq_scale_train = 1
0.00.050.291 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.291 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.291 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.291 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.291 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.291 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.292 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.292 I llm_load_print_meta: model type       = 1.4B
0.00.050.293 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.293 I llm_load_print_meta: model params     = 1.41 B
0.00.050.294 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.294 I llm_load_print_meta: general.name     = 1.4B
0.00.050.294 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.294 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.295 I llm_load_print_meta: LF token         = 128 ''
0.00.050.295 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.297 I llm_load_print_meta: max token length = 1024
0.00.052.259 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.259 I llm_load_tensors: offloading output layer to GPU
0.00.052.260 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.270 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.271 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.162 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.162 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.162 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.163 I llama_new_context_with_model: n_batch       = 2048
0.00.053.163 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.163 I llama_new_context_with_model: flash_attn    = 0
0.00.053.163 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.164 I llama_new_context_with_model: freq_scale    = 1
0.00.053.164 I ggml_metal_init: allocating
0.00.053.167 I ggml_metal_init: found device: Apple M4
0.00.053.169 I ggml_metal_init: picking default device: Apple M4
0.00.053.759 I ggml_metal_init: using embedded metal library
0.00.056.100 I ggml_metal_init: GPU name:   Apple M4
0.00.056.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.102 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.102 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.102 I ggml_metal_init: simdgroup reduction   = true
0.00.056.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.103 I ggml_metal_init: has bfloat            = true
0.00.056.103 I ggml_metal_init: use bfloat            = true
0.00.056.103 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.104 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.989 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.630 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.640 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.678 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.601 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.602 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.603 I llama_new_context_with_model: graph nodes  = 967
0.00.087.603 I llama_new_context_with_model: graph splits = 2
0.00.087.606 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.616 I main: llama threadpool init, n_threads = 4
0.00.735.660 I 
0.00.735.685 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.685 I 
0.00.735.928 I sampler seed: 1234
0.00.735.933 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.980 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.984 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.984 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.495.275 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.01.495.276 I llama_perf_context_print:        load time =     726.99 ms
0.01.495.277 I llama_perf_context_print: prompt eval time =      47.05 ms /     7 tokens (    6.72 ms per token,   148.79 tokens per second)
0.01.495.277 I llama_perf_context_print:        eval time =     709.03 ms /    63 runs   (   11.25 ms per token,    88.85 tokens per second)
0.01.495.278 I llama_perf_context_print:       total time =     759.66 ms /    70 tokens
0.01.495.483 I ggml_metal_free: deallocating

real	0m1.511s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.721 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.091 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.098 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.098 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.100 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.101 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.101 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.102 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.102 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.106 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.106 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.875 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.773 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.774 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.774 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.774 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.775 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.775 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.775 I llama_model_loader: - type  f32:  194 tensors
0.00.022.776 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.776 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.776 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.509 I llm_load_vocab: special tokens cache size = 25
0.00.048.355 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.357 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.358 I llm_load_print_meta: arch             = gptneox
0.00.048.358 I llm_load_print_meta: vocab type       = BPE
0.00.048.358 I llm_load_print_meta: n_vocab          = 50304
0.00.048.358 I llm_load_print_meta: n_merges         = 50009
0.00.048.359 I llm_load_print_meta: vocab_only       = 0
0.00.048.359 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.359 I llm_load_print_meta: n_embd           = 2048
0.00.048.359 I llm_load_print_meta: n_layer          = 24
0.00.048.362 I llm_load_print_meta: n_head           = 16
0.00.048.363 I llm_load_print_meta: n_head_kv        = 16
0.00.048.363 I llm_load_print_meta: n_rot            = 32
0.00.048.363 I llm_load_print_meta: n_swa            = 0
0.00.048.363 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.363 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.364 I llm_load_print_meta: n_gqa            = 1
0.00.048.365 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.365 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.367 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.368 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.368 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.368 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.371 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.371 I llm_load_print_meta: n_ff             = 8192
0.00.048.371 I llm_load_print_meta: n_expert         = 0
0.00.048.372 I llm_load_print_meta: n_expert_used    = 0
0.00.048.372 I llm_load_print_meta: causal attn      = 1
0.00.048.372 I llm_load_print_meta: pooling type     = 0
0.00.048.372 I llm_load_print_meta: rope type        = 2
0.00.048.372 I llm_load_print_meta: rope scaling     = linear
0.00.048.373 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.373 I llm_load_print_meta: freq_scale_train = 1
0.00.048.373 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.373 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.373 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.374 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.374 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.374 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.378 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.378 I llm_load_print_meta: model type       = 1.4B
0.00.048.378 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.379 I llm_load_print_meta: model params     = 1.41 B
0.00.048.379 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.380 I llm_load_print_meta: general.name     = 1.4B
0.00.048.380 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.380 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.380 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.381 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.381 I llm_load_print_meta: LF token         = 128 ''
0.00.048.381 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.381 I llm_load_print_meta: max token length = 1024
0.00.050.335 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.336 I llm_load_tensors: offloading output layer to GPU
0.00.050.336 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.346 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.347 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.189 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.190 I llama_new_context_with_model: n_ctx         = 128
0.00.051.190 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.190 I llama_new_context_with_model: n_batch       = 128
0.00.051.190 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.191 I llama_new_context_with_model: flash_attn    = 0
0.00.051.191 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.191 I llama_new_context_with_model: freq_scale    = 1
0.00.051.192 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.192 I ggml_metal_init: allocating
0.00.051.199 I ggml_metal_init: found device: Apple M4
0.00.051.202 I ggml_metal_init: picking default device: Apple M4
0.00.051.766 I ggml_metal_init: using embedded metal library
0.00.054.082 I ggml_metal_init: GPU name:   Apple M4
0.00.054.083 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.084 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.084 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.084 I ggml_metal_init: simdgroup reduction   = true
0.00.054.085 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.085 I ggml_metal_init: has bfloat            = true
0.00.054.085 I ggml_metal_init: use bfloat            = true
0.00.054.085 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.086 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.535 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.801 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.803 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.816 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.724 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.725 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.725 I llama_new_context_with_model: graph nodes  = 967
0.00.065.725 I llama_new_context_with_model: graph splits = 2
0.00.065.726 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.555.156 I 
0.00.555.195 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.555.209 I perplexity: tokenizing the input ..
0.00.562.669 I perplexity: tokenization took 7.459 ms
0.00.562.672 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.696.746 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.697.984 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.697.995 I llama_perf_context_print:        load time =     546.43 ms
0.00.697.996 I llama_perf_context_print: prompt eval time =     133.85 ms /   128 tokens (    1.05 ms per token,   956.31 tokens per second)
0.00.697.996 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.697.997 I llama_perf_context_print:       total time =     142.84 ms /   129 tokens
0.00.698.300 I ggml_metal_free: deallocating

real	0m0.712s
user	0m0.076s
sys	0m0.096s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.026.872 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.603 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.033.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.608 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.608 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.614 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.615 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.615 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.615 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.616 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.618 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.618 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.619 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.734 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.269 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.270 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.270 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.271 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.043.271 I llama_model_loader: - type  f32:  194 tensors
0.00.043.271 I llama_model_loader: - type q5_K:   61 tensors
0.00.043.272 I llama_model_loader: - type q6_K:   37 tensors
0.00.069.148 I llm_load_vocab: special tokens cache size = 25
0.00.078.172 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.176 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.176 I llm_load_print_meta: arch             = gptneox
0.00.078.177 I llm_load_print_meta: vocab type       = BPE
0.00.078.177 I llm_load_print_meta: n_vocab          = 50304
0.00.078.177 I llm_load_print_meta: n_merges         = 50009
0.00.078.177 I llm_load_print_meta: vocab_only       = 0
0.00.078.177 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.178 I llm_load_print_meta: n_embd           = 2048
0.00.078.178 I llm_load_print_meta: n_layer          = 24
0.00.078.182 I llm_load_print_meta: n_head           = 16
0.00.078.183 I llm_load_print_meta: n_head_kv        = 16
0.00.078.183 I llm_load_print_meta: n_rot            = 32
0.00.078.183 I llm_load_print_meta: n_swa            = 0
0.00.078.183 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.183 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.184 I llm_load_print_meta: n_gqa            = 1
0.00.078.185 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.186 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.187 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.187 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.187 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.188 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.188 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.189 I llm_load_print_meta: n_ff             = 8192
0.00.078.189 I llm_load_print_meta: n_expert         = 0
0.00.078.189 I llm_load_print_meta: n_expert_used    = 0
0.00.078.191 I llm_load_print_meta: causal attn      = 1
0.00.078.193 I llm_load_print_meta: pooling type     = 0
0.00.078.193 I llm_load_print_meta: rope type        = 2
0.00.078.193 I llm_load_print_meta: rope scaling     = linear
0.00.078.194 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.194 I llm_load_print_meta: freq_scale_train = 1
0.00.078.194 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.195 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.195 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.195 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.195 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.195 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.195 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.196 I llm_load_print_meta: model type       = 1.4B
0.00.078.201 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.078.202 I llm_load_print_meta: model params     = 1.41 B
0.00.078.203 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.078.203 I llm_load_print_meta: general.name     = 1.4B
0.00.078.203 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.203 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.204 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.204 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.204 I llm_load_print_meta: LF token         = 128 ''
0.00.078.205 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.205 I llm_load_print_meta: max token length = 1024
0.00.080.617 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.618 I llm_load_tensors: offloading output layer to GPU
0.00.080.618 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.624 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.080.625 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.081.809 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.811 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.811 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.811 I llama_new_context_with_model: n_batch       = 2048
0.00.081.811 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.811 I llama_new_context_with_model: flash_attn    = 0
0.00.081.812 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.812 I llama_new_context_with_model: freq_scale    = 1
0.00.081.813 I ggml_metal_init: allocating
0.00.081.816 I ggml_metal_init: found device: Apple M4
0.00.081.818 I ggml_metal_init: picking default device: Apple M4
0.00.082.528 I ggml_metal_init: using embedded metal library
0.00.085.638 I ggml_metal_init: GPU name:   Apple M4
0.00.085.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.643 I ggml_metal_init: simdgroup reduction   = true
0.00.085.643 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.643 I ggml_metal_init: has bfloat            = true
0.00.085.644 I ggml_metal_init: use bfloat            = true
0.00.085.644 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.102 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.118.600 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.118.606 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.118.626 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.640 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.119.642 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.119.642 I llama_new_context_with_model: graph nodes  = 967
0.00.119.642 I llama_new_context_with_model: graph splits = 2
0.00.119.645 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.119.786 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.119.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.553 I main: llama threadpool init, n_threads = 4
0.00.815.596 I 
0.00.815.629 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.630 I 
0.00.815.857 I sampler seed: 1234
0.00.815.863 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.879 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.881 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.881 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.666.840 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.666.841 I llama_perf_context_print:        load time =     788.68 ms
0.01.666.843 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.77 tokens per second)
0.01.666.844 I llama_perf_context_print:        eval time =     796.53 ms /    63 runs   (   12.64 ms per token,    79.09 tokens per second)
0.01.666.846 I llama_perf_context_print:       total time =     851.29 ms /    70 tokens
0.01.667.093 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.123s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.171 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.002 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.007 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.007 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.008 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.008 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.009 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.010 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.012 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.015 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.015 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.679 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.679 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.679 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.680 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.680 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.680 I llama_model_loader: - type  f32:  194 tensors
0.00.023.681 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.681 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.442 I llm_load_vocab: special tokens cache size = 25
0.00.049.401 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.406 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.406 I llm_load_print_meta: arch             = gptneox
0.00.049.407 I llm_load_print_meta: vocab type       = BPE
0.00.049.408 I llm_load_print_meta: n_vocab          = 50304
0.00.049.408 I llm_load_print_meta: n_merges         = 50009
0.00.049.409 I llm_load_print_meta: vocab_only       = 0
0.00.049.409 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.409 I llm_load_print_meta: n_embd           = 2048
0.00.049.409 I llm_load_print_meta: n_layer          = 24
0.00.049.412 I llm_load_print_meta: n_head           = 16
0.00.049.413 I llm_load_print_meta: n_head_kv        = 16
0.00.049.415 I llm_load_print_meta: n_rot            = 32
0.00.049.415 I llm_load_print_meta: n_swa            = 0
0.00.049.415 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.415 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.416 I llm_load_print_meta: n_gqa            = 1
0.00.049.417 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.418 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.419 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.420 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.420 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.420 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.420 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.421 I llm_load_print_meta: n_ff             = 8192
0.00.049.421 I llm_load_print_meta: n_expert         = 0
0.00.049.421 I llm_load_print_meta: n_expert_used    = 0
0.00.049.421 I llm_load_print_meta: causal attn      = 1
0.00.049.422 I llm_load_print_meta: pooling type     = 0
0.00.049.422 I llm_load_print_meta: rope type        = 2
0.00.049.422 I llm_load_print_meta: rope scaling     = linear
0.00.049.422 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.426 I llm_load_print_meta: freq_scale_train = 1
0.00.049.426 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.427 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.427 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.427 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.427 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.427 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.427 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.428 I llm_load_print_meta: model type       = 1.4B
0.00.049.428 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.428 I llm_load_print_meta: model params     = 1.41 B
0.00.049.429 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.429 I llm_load_print_meta: general.name     = 1.4B
0.00.049.429 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: LF token         = 128 ''
0.00.049.430 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: max token length = 1024
0.00.051.447 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.447 I llm_load_tensors: offloading output layer to GPU
0.00.051.448 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.458 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.459 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.431 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.432 I llama_new_context_with_model: n_ctx         = 128
0.00.052.432 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.432 I llama_new_context_with_model: n_batch       = 128
0.00.052.432 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.432 I llama_new_context_with_model: flash_attn    = 0
0.00.052.433 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.433 I llama_new_context_with_model: freq_scale    = 1
0.00.052.433 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.434 I ggml_metal_init: allocating
0.00.052.439 I ggml_metal_init: found device: Apple M4
0.00.052.441 I ggml_metal_init: picking default device: Apple M4
0.00.053.007 I ggml_metal_init: using embedded metal library
0.00.055.351 I ggml_metal_init: GPU name:   Apple M4
0.00.055.353 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.354 I ggml_metal_init: simdgroup reduction   = true
0.00.055.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.354 I ggml_metal_init: has bfloat            = true
0.00.055.354 I ggml_metal_init: use bfloat            = true
0.00.055.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.829 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.059 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.063 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.077 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.948 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.949 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.949 I llama_new_context_with_model: graph nodes  = 967
0.00.066.949 I llama_new_context_with_model: graph splits = 2
0.00.066.951 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.683 I 
0.00.699.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.721 I perplexity: tokenizing the input ..
0.00.707.357 I perplexity: tokenization took 7.634 ms
0.00.707.366 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.057 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.849.220 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.849.238 I llama_perf_context_print:        load time =     690.51 ms
0.00.849.239 I llama_perf_context_print: prompt eval time =     140.46 ms /   128 tokens (    1.10 ms per token,   911.28 tokens per second)
0.00.849.240 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.240 I llama_perf_context_print:       total time =     149.55 ms /   129 tokens
0.00.849.730 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.077s
sys	0m0.121s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.729 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.772 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.021.776 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.777 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.777 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.778 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.778 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.778 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.779 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.779 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.779 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.780 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.780 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.780 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.781 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.783 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.784 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.784 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.802 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.864 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.825 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.826 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.826 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.826 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.827 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.827 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.030.827 I llama_model_loader: - type  f32:  194 tensors
0.00.030.828 I llama_model_loader: - type q6_K:   98 tensors
0.00.054.049 I llm_load_vocab: special tokens cache size = 25
0.00.060.151 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.154 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.154 I llm_load_print_meta: arch             = gptneox
0.00.060.154 I llm_load_print_meta: vocab type       = BPE
0.00.060.154 I llm_load_print_meta: n_vocab          = 50304
0.00.060.154 I llm_load_print_meta: n_merges         = 50009
0.00.060.155 I llm_load_print_meta: vocab_only       = 0
0.00.060.155 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.155 I llm_load_print_meta: n_embd           = 2048
0.00.060.155 I llm_load_print_meta: n_layer          = 24
0.00.060.158 I llm_load_print_meta: n_head           = 16
0.00.060.158 I llm_load_print_meta: n_head_kv        = 16
0.00.060.159 I llm_load_print_meta: n_rot            = 32
0.00.060.159 I llm_load_print_meta: n_swa            = 0
0.00.060.159 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.159 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.160 I llm_load_print_meta: n_gqa            = 1
0.00.060.161 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.161 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.162 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.163 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.164 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.166 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.166 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.167 I llm_load_print_meta: n_ff             = 8192
0.00.060.167 I llm_load_print_meta: n_expert         = 0
0.00.060.167 I llm_load_print_meta: n_expert_used    = 0
0.00.060.167 I llm_load_print_meta: causal attn      = 1
0.00.060.168 I llm_load_print_meta: pooling type     = 0
0.00.060.168 I llm_load_print_meta: rope type        = 2
0.00.060.168 I llm_load_print_meta: rope scaling     = linear
0.00.060.168 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.169 I llm_load_print_meta: freq_scale_train = 1
0.00.060.169 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.169 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.169 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.169 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.169 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.170 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.170 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.170 I llm_load_print_meta: model type       = 1.4B
0.00.060.170 I llm_load_print_meta: model ftype      = Q6_K
0.00.060.174 I llm_load_print_meta: model params     = 1.41 B
0.00.060.174 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.060.174 I llm_load_print_meta: general.name     = 1.4B
0.00.060.175 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.175 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.175 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.175 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.177 I llm_load_print_meta: LF token         = 128 ''
0.00.060.177 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.177 I llm_load_print_meta: max token length = 1024
0.00.062.265 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.265 I llm_load_tensors: offloading output layer to GPU
0.00.062.266 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.276 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.062.278 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.063.208 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.208 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.209 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.209 I llama_new_context_with_model: n_batch       = 2048
0.00.063.209 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.209 I llama_new_context_with_model: flash_attn    = 0
0.00.063.210 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.210 I llama_new_context_with_model: freq_scale    = 1
0.00.063.210 I ggml_metal_init: allocating
0.00.063.217 I ggml_metal_init: found device: Apple M4
0.00.063.220 I ggml_metal_init: picking default device: Apple M4
0.00.063.856 I ggml_metal_init: using embedded metal library
0.00.066.327 I ggml_metal_init: GPU name:   Apple M4
0.00.066.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.329 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.329 I ggml_metal_init: simdgroup reduction   = true
0.00.066.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.329 I ggml_metal_init: has bfloat            = true
0.00.066.330 I ggml_metal_init: use bfloat            = true
0.00.066.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.587 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.878 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.889 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.910 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.903 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.904 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.905 I llama_new_context_with_model: graph nodes  = 967
0.00.099.905 I llama_new_context_with_model: graph splits = 2
0.00.099.908 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.049 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.050 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.907.908 I main: llama threadpool init, n_threads = 4
0.00.907.942 I 
0.00.907.965 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.907.965 I 
0.00.908.211 I sampler seed: 1234
0.00.908.216 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.908.262 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.908.266 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.908.266 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.783.080 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.01.783.080 I llama_perf_context_print:        load time =     899.18 ms
0.01.783.089 I llama_perf_context_print: prompt eval time =      54.41 ms /     7 tokens (    7.77 ms per token,   128.65 tokens per second)
0.01.783.092 I llama_perf_context_print:        eval time =     817.45 ms /    63 runs   (   12.98 ms per token,    77.07 tokens per second)
0.01.783.093 I llama_perf_context_print:       total time =     875.17 ms /    70 tokens
0.01.783.356 I ggml_metal_free: deallocating

real	0m1.805s
user	0m0.113s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4417 (9394bbd4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.810 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.480 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.484 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.485 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.486 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.486 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.486 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.487 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.487 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.488 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.488 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.488 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.489 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.489 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.490 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.492 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.492 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.380 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.252 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.253 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.253 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.254 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.254 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.254 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.254 I llama_model_loader: - type  f32:  194 tensors
0.00.023.255 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.782 I llm_load_vocab: special tokens cache size = 25
0.00.049.657 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.660 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.660 I llm_load_print_meta: arch             = gptneox
0.00.049.660 I llm_load_print_meta: vocab type       = BPE
0.00.049.661 I llm_load_print_meta: n_vocab          = 50304
0.00.049.661 I llm_load_print_meta: n_merges         = 50009
0.00.049.661 I llm_load_print_meta: vocab_only       = 0
0.00.049.661 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.661 I llm_load_print_meta: n_embd           = 2048
0.00.049.661 I llm_load_print_meta: n_layer          = 24
0.00.049.664 I llm_load_print_meta: n_head           = 16
0.00.049.665 I llm_load_print_meta: n_head_kv        = 16
0.00.049.665 I llm_load_print_meta: n_rot            = 32
0.00.049.666 I llm_load_print_meta: n_swa            = 0
0.00.049.666 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.668 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.669 I llm_load_print_meta: n_gqa            = 1
0.00.049.670 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.670 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.671 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.672 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.673 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.673 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.673 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.674 I llm_load_print_meta: n_ff             = 8192
0.00.049.674 I llm_load_print_meta: n_expert         = 0
0.00.049.674 I llm_load_print_meta: n_expert_used    = 0
0.00.049.674 I llm_load_print_meta: causal attn      = 1
0.00.049.674 I llm_load_print_meta: pooling type     = 0
0.00.049.674 I llm_load_print_meta: rope type        = 2
0.00.049.675 I llm_load_print_meta: rope scaling     = linear
0.00.049.675 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.675 I llm_load_print_meta: freq_scale_train = 1
0.00.049.676 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.676 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.676 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.676 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.676 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.677 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.677 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.678 I llm_load_print_meta: model type       = 1.4B
0.00.049.678 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.678 I llm_load_print_meta: model params     = 1.41 B
0.00.049.684 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.684 I llm_load_print_meta: general.name     = 1.4B
0.00.049.684 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: LF token         = 128 ''
0.00.049.687 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.687 I llm_load_print_meta: max token length = 1024
0.00.051.715 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.716 I llm_load_tensors: offloading output layer to GPU
0.00.051.716 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.726 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.728 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.635 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.635 I llama_new_context_with_model: n_ctx         = 128
0.00.052.636 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.636 I llama_new_context_with_model: n_batch       = 128
0.00.052.636 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.636 I llama_new_context_with_model: flash_attn    = 0
0.00.052.637 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.637 I llama_new_context_with_model: freq_scale    = 1
0.00.052.637 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.638 I ggml_metal_init: allocating
0.00.052.643 I ggml_metal_init: found device: Apple M4
0.00.052.645 I ggml_metal_init: picking default device: Apple M4
0.00.053.181 I ggml_metal_init: using embedded metal library
0.00.055.504 I ggml_metal_init: GPU name:   Apple M4
0.00.055.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.506 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.506 I ggml_metal_init: simdgroup reduction   = true
0.00.055.506 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.507 I ggml_metal_init: has bfloat            = true
0.00.055.507 I ggml_metal_init: use bfloat            = true
0.00.055.507 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.434 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.699 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.701 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.715 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.569 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.570 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.570 I llama_new_context_with_model: graph nodes  = 967
0.00.067.571 I llama_new_context_with_model: graph splits = 2
0.00.067.572 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.467.053 I 
0.00.467.093 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.467.111 I perplexity: tokenizing the input ..
0.00.474.869 I perplexity: tokenization took 7.754 ms
0.00.474.873 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.614.213 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.615.764 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.615.789 I llama_perf_context_print:        load time =     458.24 ms
0.00.615.789 I llama_perf_context_print: prompt eval time =     139.09 ms /   128 tokens (    1.09 ms per token,   920.24 tokens per second)
0.00.615.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.790 I llama_perf_context_print:       total time =     148.74 ms /   129 tokens
0.00.616.181 I ggml_metal_free: deallocating

real	0m0.632s
user	0m0.077s
sys	0m0.083s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4417 (9394bbd4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134207740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134207e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134208400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1342089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134208f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134209510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134209ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13420a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13420a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13420ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13420b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13420b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13420c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13420c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13420d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13420d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13420de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13420e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13420ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13420f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13420fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134210290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1342109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134211250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134211970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134211c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134212240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134212eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1342133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1342136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134213b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134213e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1342146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134214be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134214ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134215340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1342157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134215c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134216120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1342165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134216a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134216f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1342173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134217840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134217b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134218110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134218720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134219040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134219650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134219c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13421a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13421a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13421ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13421b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13421bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13421c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13421c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13421c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13421cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13421d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13421d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13421ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13421e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13421e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13421ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13421f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13421f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13421f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13421fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1342202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134220790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134220c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1342210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134221620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134221b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1342220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134222610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134222b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1342230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134223600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134223b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1342240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1342245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134224b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134225090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1342255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134225b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134226080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1342265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134226b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134227070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1342275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134227b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134228060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1342285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134228b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134229050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134218d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1342294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134229c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13422a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13422a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13422ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13422b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13422b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13422bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13422c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13422c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13422cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13422d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13422d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13422dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13422e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13422e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13422eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13422ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13422f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13422f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13422fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1342301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134230680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134230b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134230fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134231460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134231900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134231da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134232240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1342326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134232b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134233020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1342334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134233960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134233e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1342342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134234740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134234be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134235080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134235520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1342359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134235e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134236300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1342367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134236c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1342370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134237580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134237a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134237ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134238360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134238800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134238ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134239140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1342395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134239a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134239f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13423a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13423a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13423ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13423b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13423b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13423bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13423bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13423c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13423c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13423cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13423d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13423d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13423db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13423dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13423e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13423e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13423edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13423f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13423f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13423fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134240040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1342404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134240980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134240e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1342412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134241760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134241c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1342420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134242540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1342429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134242e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134243320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1342437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134243c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134244100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1342445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134244a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134244ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134245380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1342458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134245e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134246370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1342468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134246b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134247190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1342477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134247db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1342485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134248a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134248d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134249310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134249920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13424a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13424a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13424aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13424aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13424b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13424bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13424c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13424c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13424cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13424d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13424d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13424dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13424e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13424e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13424ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13424f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13424f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13424fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134250100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134250650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134250ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1342510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134251640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134251b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1342520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134252630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134252b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1342530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134253620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134253b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1342540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134254610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134254b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1342550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134255600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134255b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1342560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1342565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134256b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134257090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1342575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134257b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134258080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1342585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134258b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134259070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1342595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134259b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13425a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13425a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13425ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13425b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13425b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13425baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13425c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13425c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13425cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13425d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13425d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13425dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13425e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13425e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13425e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13425ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13425f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13425f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13425fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134260080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134260520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1342609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134260e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134261300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1342617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134261c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1342620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134262580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134262ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1342631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134263910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134264030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134264750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134264a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134265200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1342654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134265ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.137.020 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.137.024 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116308500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116308970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1163091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116309740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116309c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11630a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11630a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11630ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11630b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11630b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11630ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11630bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11630c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11630cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11630d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11630ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11630e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11630ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11630f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11630fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x116310420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116310b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x116311260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x116311980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1163120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116312360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x116312970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116312f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116313590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x116313d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116314220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1163144e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116314d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1163152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116315570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116315a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116315eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116316350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1163167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116316c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116317130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1163175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116317a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116317f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1163181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1163187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116318df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116319400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116319a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11631a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11631a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11631ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11631b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11631b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11631c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11631c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11631c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11631cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11631d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11631da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11631def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11631e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11631e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11631ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11631f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11631f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11631fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11631ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1163203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x116320890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x116320d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1163211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x116321670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x116321bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x116322110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x116322660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x116322bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x116323100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x116323650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x116323ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1163240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x116324640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x116324b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1163250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x116325630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x116325b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1163260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116326620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x116326b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1163270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116327610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116327b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1163280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116328600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116328b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1163290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1163295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116329b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11632a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11632a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11632ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11632b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11632b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11632bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11632c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11632c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11632cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11632d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11632d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11632db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11632e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11632e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11632eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11632ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11632f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11632f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11632fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116330210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1163306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116330b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116330ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116331490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116331930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116331dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116332270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116332710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116332bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x116333050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1163334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x116333990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x116333e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1163342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116334770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x116334c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1163350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x116335550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1163359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x116335e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116336330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1163367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x116336c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x116337110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1163375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x116337a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x116337ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x116338390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x116338830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x116338cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116339170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116339610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116339ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116339f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11633a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11633a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11633ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11633b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11633b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11633bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11633bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11633c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11633c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11633cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11633d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11633d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11633db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11633e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11633e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11633e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11633edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11633f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11633f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11633fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116340070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116340510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1163409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116340e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1163412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116341790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116341c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1163420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116342570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116342a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116342eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116343350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1163437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116343c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x116344130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1163445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116344a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x116344f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1163453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x116345850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116345cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x116346240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116346790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116346ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116347230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1163474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x116347b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116348110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116348720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x116348f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1163493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116349670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116349c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11634a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11634aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11634af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11634b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11634b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11634c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11634c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11634cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11634d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11634d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11634daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11634dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11634e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11634ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11634efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11634f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11634fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11634ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116350520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116350a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116350fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116351510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116351a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116351fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116352500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116352a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116352fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1163534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x116353a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x116353f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1163544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x116354a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x116354f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1163554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x116355a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x116355f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1163564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x116356a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x116356f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1163574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x116357a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x116357f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1163584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1163589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x116358f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116359490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1163599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116359f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11635a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11635a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11635af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11635b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11635b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11635bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11635c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11635c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11635cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11635d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11635d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11635def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11635e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11635e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11635ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11635f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11635f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11635fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1163600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116360550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1163609f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116360e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116361330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1163617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116361c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x116362110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1163625b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x116362a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116362ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x116363440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116363b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116364280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1163649a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1163650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116365380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116365b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x116365e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116366440 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116312c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116349930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116347dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1163660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1163477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1163483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11631af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11631a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11631cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116349f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116312620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1163190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116319cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116313240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116318490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11631b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11631a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11631d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x116365640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1163147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x116314a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116313850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11634a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1163489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x116308c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116308ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1163668a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116366b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116366e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1163670e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1163673a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116367660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116367920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116367be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116367ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116368160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116368420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1163686e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1163689a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116368c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116368f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1163691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1163694a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116369760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116369a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116369ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116369fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11636a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11636a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11636a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11636aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11636ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11636b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11636b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11636b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11636b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11636bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11636bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11636c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11636c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11636c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11636c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11636cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11636ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11636d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11636d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11636d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11636d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11636dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11636dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11636e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11636e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11636e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11636e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11636eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11636ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11636f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11636f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11636f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11636fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11636fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11636ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1163702a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x116370560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x116370820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x116370ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x116370da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116371060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x116371320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1163715e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1163718a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116371b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116371e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1163720e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1163723a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116372660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116372920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116372be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116372ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116373160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116373420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1163736e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1163739a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116373c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116373f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1163741e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1163744a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116374760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116374a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116374ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116374fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116375260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116375520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1163757e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116375aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116375d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116376020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1163762e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1163765a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116376860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116376b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116376de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1163770a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116377360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116377620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1163778e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116377ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x116377e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x116378120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1163783e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1163786a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x116378960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116378c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x116378ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1163791a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x116379460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x116379720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1163799e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116379ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x116379f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11637a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11637a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11637a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11637aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11637ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11637afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11637b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11637b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11637b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11637bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11637bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11637c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11637c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11637c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11637c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11637cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11637ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11637d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11637d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11637d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11637d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11637dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11637dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11637e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11637e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11637e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11637e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11637ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11637ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11637f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11637f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11637f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11637fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11637fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11637ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116380260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116380520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1163807e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116380aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116380d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116381020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1163812e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1163815a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116381860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116381b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116381de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1163820a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x116382360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116382620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1163828e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x116382ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x116382e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116383120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1163833e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1163836a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116383960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116383c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116383ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1163841a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116384460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116384720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1163849e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116384ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116384f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116385220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1163854e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1163857a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116385a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116385d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116385fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1163865b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116386870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116386b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116386df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1163870b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116387370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116387630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1163878f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116387bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116387e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116388130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1163883f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1163886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116388970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116388c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116388ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1163891b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116389470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116389730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1163899f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116389cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116389f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11638a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11638a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11638a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11638aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11638ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11638aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11638b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11638b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11638b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11638baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11638bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11638c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11638c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11638c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11638cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11638d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11638d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11638ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11638e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11638e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11638edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11638f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11638f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11638fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1163902f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116390840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116390d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1163912e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116391830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x116391d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1163922d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116392820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116392d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1163932c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116393810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116393ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116393d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116394050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1163944c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116394930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116394da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116395210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116395680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116395af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116395f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1163963d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x116396840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x116396cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x116397120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116397590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x116397a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116397e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116398b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116399280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1163999a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116399c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11639a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11639a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11639ace0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.828s
user	0m0.294s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4417 (9394bbd4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e70d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e70d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e70df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e70e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e70ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e70f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e70f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e70fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e710120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e710620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e710b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e711020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e711b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e7122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e712b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e713220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e713940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e714060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e714780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e714f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e715670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e715d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e7164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e716d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e717470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e717730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e717d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e7189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e718ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e7191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e719650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e719910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e71a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e71a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e71a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e71ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e71b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e71b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e71bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e71c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e71c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e71ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e71cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e71d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e71d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e71dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e71e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e71eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e71f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e71f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e71fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e720380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e720990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e720fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e721790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e721c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e7220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e722390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e7229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e723190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e723450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e7238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e723d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e724230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e7246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e725010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e7254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e725950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e725df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e726290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e726730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e726bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e727120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e727670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e727bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e728110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e728660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e728bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e729100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e729650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e729ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e72a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e72a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e72ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e72b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e72b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e72bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e72c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e72c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e72cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e72d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e72d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e72db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e72e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e72e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e72eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e71e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e72efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e72f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e72fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e730210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e730760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e730cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e731200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e731750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e731ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e7321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e732740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e732c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e7331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e733730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e733c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e734120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e7345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e734a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e734f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e7353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e735840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e735ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e736180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e736620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e736ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e736f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e737400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e7378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e737d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e7381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e738b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e738fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e739460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e739900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e739da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e73a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e73a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e73ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e73b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e73b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e73b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e73be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e73c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e73c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e73cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e73d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e73d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e73d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e73de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e73e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e73e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e73ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e73f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e73f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e73fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e73fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e740360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e740800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e740ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e741140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e7415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e741a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e741f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e7423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e742860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e742d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e7431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e743640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e743ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e743f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e744420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e7448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e744d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e745200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e7456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e745b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e745fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e746480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e746920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e746dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e747260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e747700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e747ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e748040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e7484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e748980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e748e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e7492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e749760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e749c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e74a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e74a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e74a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e74ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e74b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e74b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e74be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e74c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e74c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e74cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e74d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e74d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e74e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e74e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e74e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e74ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e74f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e74fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e7500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e750550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e7509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e7511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e7516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e751c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e752190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e7526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e752c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e753180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e7536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e753c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e754170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e7546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e754c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e755160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e7556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e755c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e756150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e7566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e756bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e757140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e757690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e757be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e758130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e758680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e758bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e759120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e759670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e759bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e75a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e75a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e75abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e75b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e75b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e75bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e75c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e75c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e75cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e75d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e75d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e75db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e75e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e75e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e75eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e75f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e75f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e75fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e7600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e760600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e760b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e7610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e7615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e761b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e762090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e7625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e762b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e763080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e7635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e763b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e763fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e764460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e764900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e764da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e765240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e7656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e765b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e766020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e7664c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e766960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e766e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e7672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e767740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e767be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e768080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e7685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e768cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e769410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e769b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e76a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e76a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e76ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e76afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e76b5d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.112.110 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.112.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e608f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e6093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e609850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e609cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e60a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e60a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e60aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e60ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e60b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e60b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e60bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e60c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e60ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e60d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e60ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e60e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e60ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e60f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e60fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e610210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e610930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e611050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e611770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e611e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e6125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e612870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e612b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e612fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e613410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e613880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e613d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e614290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e6149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e6152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e615800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e615d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e616700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e617100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e617600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e618000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e618470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e6188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e618d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e6191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e619630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e619f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e61a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e61a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e61ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e61b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e61b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e61bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e61c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e61c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e61ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e61d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e61d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e61dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e61e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e61e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e61e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e61ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e61f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e61f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e61fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e620110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e6205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e620b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e621050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e6215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e621af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e622040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e622590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e622ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e623030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e623580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e623ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e624020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e624570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e624ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e625010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e625560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e625ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e626000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e626550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e626aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e627540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e627a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e627fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e628530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e628fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e629520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e629a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e629fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e62a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e62aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e62afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e62b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e62ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e62bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e62c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e62ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e62cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e62d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e62da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e62ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e62e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e62e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e62ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e62f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e62f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e62fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e62ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e6303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e630870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e630d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e6311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e631650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e631af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e631f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e632430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e6328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e632d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e633210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e6336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e633b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e633ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e634490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e634930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e635270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e635710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e635bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e6364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e636990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e636e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e6372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e637770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e637c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e6380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e638550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e6389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e638e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e639330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e6397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e639c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e63a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e63a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e63aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e63aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e63b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e63b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e63bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e63c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e63c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e63cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e63cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e63d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e63d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e63dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e63e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e63e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e63eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e63efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e63f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e63f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e63fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e640230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e6406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e640b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e641010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e6414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e641950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e641df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e642290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e642730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e642bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e643070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e643510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e6439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e643e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e6442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e644790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e644c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e645180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e6456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e645c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e646170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e646430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e646a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e647050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e647660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e647e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e6482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e6485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e648bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e6491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e6499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e649e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e64a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e64a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e64af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e64b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e64b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e64bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e64c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e64c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e64cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e64d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e64d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e64df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e64e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e64e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e64ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e64f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e64f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e64ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e650450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e6509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e650ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e651440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e651990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e651ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e652430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e652980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e652ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e653420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e653970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e653ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e654410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e654960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e654eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e655400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e655950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e655ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e6563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e656940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e656e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e6573e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e657930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e657e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e6583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e658920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e658e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e6593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e659910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e659e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e65a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e65a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e65ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e65b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e65b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e65be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e65c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e65c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e65ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e65d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e65d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e65dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e65e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e65e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e65eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e65eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e65f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e65f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e65fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e660270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e660710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e660bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e661050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e6614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e661990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e661e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e662380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e662aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e6631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e6638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e664000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e6642c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e664ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e664d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e665380 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e665030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e646d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e6466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e647310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e61be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e648e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e60c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e60bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e608b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e61c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e664580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e61af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e647920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e665af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e666120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e6663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e6666a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e666960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e666c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e666ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e6671a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e667460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e667720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e6679e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e667ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e667f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e668220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e6684e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e6687a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e668a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e668d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e668fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e6692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e669560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e669820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e669ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e669da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e66a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e66a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e66a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e66a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e66ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e66ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e66b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e66b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e66b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e66b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e66bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e66bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e66c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e66c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e66c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e66c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e66cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e66cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e66d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e66d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e66d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e66da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e66dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e66dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e66e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e66e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e66e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e66eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e66ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e66f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e66f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e66f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e66f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e66fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e66fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e6700a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e670360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e670620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e6708e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e670ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e670e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e671120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e6713e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e6716a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e671960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e671c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e671ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e6721a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e672460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e672720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e6729e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e672ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e672f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e673220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e6734e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e6737a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e673a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e673d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e673fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e6742a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e674560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e674820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e674ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e674da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e675060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e675320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e6755e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e6758a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e675b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e675e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e6760e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e6763a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e676660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e676920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e676be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e676ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e677160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e677420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e6776e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e6779a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e677c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e677f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e6781e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e6784a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e678760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e678a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e678ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e678fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e679260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e679520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e6797e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e679aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e679d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e67a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e67a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e67a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e67a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e67ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e67ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e67b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e67b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e67b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e67b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e67bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e67be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e67c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e67c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e67c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e67c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e67cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e67cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e67d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e67d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e67d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e67d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e67dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e67df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e67e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e67e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e67e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e67ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e67ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e67efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e67f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e67f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e67f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e67fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e67fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e680060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e680320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e6805e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e6808a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e680b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e680e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e6810e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e6813a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e681660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e681920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e681be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e681ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e682160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e682420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e6826e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e6829a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e682c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e682f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e6831e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e6834a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e683760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e683a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e683ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e683fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e684260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e684520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e6847e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e684aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e684d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e685020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e6852e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e6855a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e685860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e685b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e685de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e6860a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e686360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e686620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e6868e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e686ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e686e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e687120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e6873e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e6876a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e687960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e687f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e6881f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e6884b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e688770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e688a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e688cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e688fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e689270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e689530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e6897f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e689ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e689d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e68a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e68a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e68a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e68a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e68ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e68adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e68b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e68b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e68b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e68b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e68bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e68be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e68c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e68c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e68c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e68c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e68cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e68cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e68d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e68d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e68d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e68d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e68dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e68df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e68e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e68e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e68e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e68ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e68ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e68eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e68f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e68f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e68f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e68faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e68fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e690070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e690330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e690880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e690dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e691320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e691870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e691dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e692310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e692860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e692db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e693070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e693330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e693830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e693d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e694230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e694730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e694c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e695130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e695630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e695b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e696030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e696530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e696a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e696f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e697430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e697930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e698340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e698a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e699180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e6998a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e699b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e69a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e69a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e69ac20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.937s
user	0m0.246s
sys	0m0.138s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.16 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.53 real         0.14 user         0.04 sys
```
