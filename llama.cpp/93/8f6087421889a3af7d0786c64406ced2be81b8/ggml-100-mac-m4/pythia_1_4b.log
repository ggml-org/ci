Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:42 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:104 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.599s
user	0m0.693s
sys	0m0.951s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Built target sha256
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target build_info
[  6%] Built target sha1
[  6%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 11%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX shared library libllama.dylib
[ 21%] Built target llama-gguf
[ 21%] Built target llama-gguf-hash
[ 21%] Built target llama
[ 22%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 22%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 22%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 22%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Linking C executable ../bin/test-c
[ 25%] Linking CXX executable ../../bin/llama-quantize-stats
[ 25%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 27%] Linking CXX executable ../../bin/llama-simple
[ 28%] Linking CXX executable ../../bin/llama-run
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 28%] Built target llava
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target llama-simple
[ 32%] Built target llama-quantize-stats
[ 32%] Built target test-c
[ 32%] Built target llama-run
[ 32%] Built target llama-simple-chat
[ 32%] Linking CXX static library libcommon.a
[ 32%] Built target llava_static
[ 32%] Built target common
[ 32%] Built target llava_shared
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-0
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-arg-parser
[ 44%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-chat-template
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-chat-template
[ 48%] Built target test-arg-parser
[ 48%] Built target test-log
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../../bin/llama-batched-bench
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../../bin/llama-batched
[ 62%] Built target test-quantize-perf
[ 62%] Built target llama-batched-bench
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Built target test-barrier
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-json-schema-to-grammar
[ 62%] Built target test-rope
[ 62%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 62%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Built target llama-batched
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-lookahead
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-lookup
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-lookahead
[ 72%] Built target llama-infill
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 72%] Built target llama-bench
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Built target llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Generating completion.js.hpp
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-parallel
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-perplexity
[ 83%] Built target llama-passkey
[ 84%] Generating deps_daisyui.min.css.hpp
[ 84%] Generating deps_markdown-it.js.hpp
[ 84%] Built target llama-quantize
[ 84%] Built target llama-retrieval
[ 85%] Generating deps_tailwindcss.js.hpp
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 86%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-export-lora
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative-simple
[ 94%] Generating deps_vue.esm-browser.js.hpp
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Built target llama-tokenize
[ 94%] Built target llama-cvector-generator
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Generating index.html.hpp
[ 96%] Built target llama-export-lora
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.642s
user	0m5.660s
sys	0m8.357s

main: quantize time =  4253.66 ms
main:    total time =  4253.66 ms

main: quantize time =  1870.57 ms
main:    total time =  1870.57 ms

main: quantize time =  1967.42 ms
main:    total time =  1967.42 ms

main: quantize time =  2103.40 ms
main:    total time =  2103.40 ms

main: quantize time =  2670.02 ms
main:    total time =  2670.02 ms

main: quantize time =  5285.54 ms
main:    total time =  5285.54 ms

main: quantize time =  5899.67 ms
main:    total time =  5899.67 ms

main: quantize time =  6874.87 ms
main:    total time =  6874.87 ms

main: quantize time =  5907.93 ms
main:    total time =  5907.93 ms

main: quantize time =  4527.45 ms
main:    total time =  4527.45 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.138 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.249 I main: llama backend init
0.00.000.267 I main: load the model and apply lora adapter, if any
0.00.027.324 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.380 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.394 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.398 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.400 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.400 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.403 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.403 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.404 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.405 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.405 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.406 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.407 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.637 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.746 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.019 I llama_model_loader: - type  f32:  194 tensors
0.00.057.020 I llama_model_loader: - type  f16:   98 tensors
0.00.087.261 I llm_load_vocab: special tokens cache size = 25
0.00.094.062 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.064 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.065 I llm_load_print_meta: arch             = gptneox
0.00.094.065 I llm_load_print_meta: vocab type       = BPE
0.00.094.065 I llm_load_print_meta: n_vocab          = 50304
0.00.094.066 I llm_load_print_meta: n_merges         = 50009
0.00.094.066 I llm_load_print_meta: vocab_only       = 0
0.00.094.066 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.066 I llm_load_print_meta: n_embd           = 2048
0.00.094.066 I llm_load_print_meta: n_layer          = 24
0.00.094.069 I llm_load_print_meta: n_head           = 16
0.00.094.070 I llm_load_print_meta: n_head_kv        = 16
0.00.094.070 I llm_load_print_meta: n_rot            = 32
0.00.094.073 I llm_load_print_meta: n_swa            = 0
0.00.094.073 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.073 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.074 I llm_load_print_meta: n_gqa            = 1
0.00.094.074 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.075 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.075 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.076 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.076 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.076 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.076 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.078 I llm_load_print_meta: n_ff             = 8192
0.00.094.078 I llm_load_print_meta: n_expert         = 0
0.00.094.078 I llm_load_print_meta: n_expert_used    = 0
0.00.094.079 I llm_load_print_meta: causal attn      = 1
0.00.094.079 I llm_load_print_meta: pooling type     = 0
0.00.094.079 I llm_load_print_meta: rope type        = 2
0.00.094.079 I llm_load_print_meta: rope scaling     = linear
0.00.094.079 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.080 I llm_load_print_meta: freq_scale_train = 1
0.00.094.080 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.080 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.080 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.080 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.080 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.080 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.080 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.092 I llm_load_print_meta: model type       = 1.4B
0.00.094.092 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.094.093 I llm_load_print_meta: model params     = 1.41 B
0.00.094.093 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.094.093 I llm_load_print_meta: general.name     = 1.4B
0.00.094.093 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.094 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.094 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.094 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.094 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.094.095 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.095 I llm_load_print_meta: max token length = 1024
0.00.095.829 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.829 I llm_load_tensors: offloading output layer to GPU
0.00.095.830 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.846 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.847 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.096.737 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.738 I llama_new_context_with_model: n_ctx         = 2048
0.00.096.738 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.096.738 I llama_new_context_with_model: n_batch       = 2048
0.00.096.739 I llama_new_context_with_model: n_ubatch      = 512
0.00.096.739 I llama_new_context_with_model: flash_attn    = 0
0.00.096.739 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.740 I llama_new_context_with_model: freq_scale    = 1
0.00.096.740 I ggml_metal_init: allocating
0.00.096.748 I ggml_metal_init: found device: Apple M4
0.00.096.751 I ggml_metal_init: picking default device: Apple M4
0.00.097.393 I ggml_metal_init: using embedded metal library
0.00.106.448 I ggml_metal_init: GPU name:   Apple M4
0.00.106.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.106.451 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.106.451 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.106.451 I ggml_metal_init: simdgroup reduction   = true
0.00.106.451 I ggml_metal_init: simdgroup matrix mul. = true
0.00.106.451 I ggml_metal_init: has bfloat            = true
0.00.106.452 I ggml_metal_init: use bfloat            = true
0.00.106.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.106.452 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.141.219 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.141.223 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.141.241 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.142.175 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.142.176 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.142.176 I llama_new_context_with_model: graph nodes  = 967
0.00.142.177 I llama_new_context_with_model: graph splits = 2
0.00.142.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.222.258 I main: llama threadpool init, n_threads = 4
0.00.222.290 I 
0.00.222.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.222.327 I 
0.00.222.401 I sampler seed: 1234
0.00.222.406 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.222.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.222.432 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.222.432 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.068.520 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55252.92 tokens per second)
0.02.068.521 I llama_perf_context_print:        load time =     194.92 ms
0.02.068.521 I llama_perf_context_print: prompt eval time =      39.13 ms /     7 tokens (    5.59 ms per token,   178.90 tokens per second)
0.02.068.522 I llama_perf_context_print:        eval time =    1804.01 ms /    63 runs   (   28.64 ms per token,    34.92 tokens per second)
0.02.068.522 I llama_perf_context_print:       total time =    1846.26 ms /    70 tokens
0.02.068.689 I ggml_metal_free: deallocating

real	0m2.379s
user	0m0.143s
sys	0m0.095s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.700 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.592 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.597 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.599 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.599 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.600 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.600 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.600 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.602 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.602 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.602 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.603 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.605 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.606 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.606 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.607 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.607 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.608 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.497 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.497 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.497 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.498 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.498 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.499 I llama_model_loader: - type  f32:  194 tensors
0.00.027.500 I llama_model_loader: - type q8_0:   98 tensors
0.00.048.530 I llm_load_vocab: special tokens cache size = 25
0.00.054.598 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.602 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.602 I llm_load_print_meta: arch             = gptneox
0.00.054.603 I llm_load_print_meta: vocab type       = BPE
0.00.054.603 I llm_load_print_meta: n_vocab          = 50304
0.00.054.603 I llm_load_print_meta: n_merges         = 50009
0.00.054.603 I llm_load_print_meta: vocab_only       = 0
0.00.054.604 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.604 I llm_load_print_meta: n_embd           = 2048
0.00.054.604 I llm_load_print_meta: n_layer          = 24
0.00.054.610 I llm_load_print_meta: n_head           = 16
0.00.054.611 I llm_load_print_meta: n_head_kv        = 16
0.00.054.612 I llm_load_print_meta: n_rot            = 32
0.00.054.612 I llm_load_print_meta: n_swa            = 0
0.00.054.612 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.612 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.613 I llm_load_print_meta: n_gqa            = 1
0.00.054.614 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.615 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.616 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.616 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.617 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.617 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.617 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.618 I llm_load_print_meta: n_ff             = 8192
0.00.054.618 I llm_load_print_meta: n_expert         = 0
0.00.054.618 I llm_load_print_meta: n_expert_used    = 0
0.00.054.618 I llm_load_print_meta: causal attn      = 1
0.00.054.620 I llm_load_print_meta: pooling type     = 0
0.00.054.620 I llm_load_print_meta: rope type        = 2
0.00.054.620 I llm_load_print_meta: rope scaling     = linear
0.00.054.621 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.621 I llm_load_print_meta: freq_scale_train = 1
0.00.054.621 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.621 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.622 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.622 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.622 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.622 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.622 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.636 I llm_load_print_meta: model type       = 1.4B
0.00.054.637 I llm_load_print_meta: model ftype      = Q8_0
0.00.054.637 I llm_load_print_meta: model params     = 1.41 B
0.00.054.638 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.054.638 I llm_load_print_meta: general.name     = 1.4B
0.00.054.638 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.638 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.638 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.639 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.639 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.639 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.639 I llm_load_print_meta: max token length = 1024
0.00.056.993 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.993 I llm_load_tensors: offloading output layer to GPU
0.00.056.993 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.004 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.005 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.057.987 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.987 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.988 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.988 I llama_new_context_with_model: n_batch       = 2048
0.00.057.988 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.988 I llama_new_context_with_model: flash_attn    = 0
0.00.057.989 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.989 I llama_new_context_with_model: freq_scale    = 1
0.00.057.990 I ggml_metal_init: allocating
0.00.057.997 I ggml_metal_init: found device: Apple M4
0.00.058.000 I ggml_metal_init: picking default device: Apple M4
0.00.058.705 I ggml_metal_init: using embedded metal library
0.00.060.869 I ggml_metal_init: GPU name:   Apple M4
0.00.060.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.871 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.871 I ggml_metal_init: simdgroup reduction   = true
0.00.060.872 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.872 I ggml_metal_init: has bfloat            = true
0.00.060.872 I ggml_metal_init: use bfloat            = true
0.00.060.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.737 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.751 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.776 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.842 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.844 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.844 I llama_new_context_with_model: graph nodes  = 967
0.00.094.845 I llama_new_context_with_model: graph splits = 2
0.00.094.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.332.989 I main: llama threadpool init, n_threads = 4
0.01.333.077 I 
0.01.333.142 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.333.143 I 
0.01.333.652 I sampler seed: 1234
0.01.333.659 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.333.727 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.333.733 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.333.733 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.429.838 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.02.429.839 I llama_perf_context_print:        load time =    1323.28 ms
0.02.429.839 I llama_perf_context_print: prompt eval time =      42.81 ms /     7 tokens (    6.12 ms per token,   163.50 tokens per second)
0.02.429.840 I llama_perf_context_print:        eval time =    1050.14 ms /    63 runs   (   16.67 ms per token,    59.99 tokens per second)
0.02.429.841 I llama_perf_context_print:       total time =    1096.86 ms /    70 tokens
0.02.430.026 I ggml_metal_free: deallocating

real	0m2.449s
user	0m0.121s
sys	0m0.253s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.016.070 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.214 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.023.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.231 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.231 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.231 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.231 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.235 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.237 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.238 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.238 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.559 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.032.639 I llama_model_loader: - type  f32:  194 tensors
0.00.032.639 I llama_model_loader: - type q4_0:   97 tensors
0.00.032.639 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.191 I llm_load_vocab: special tokens cache size = 25
0.00.065.521 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.524 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.524 I llm_load_print_meta: arch             = gptneox
0.00.065.525 I llm_load_print_meta: vocab type       = BPE
0.00.065.525 I llm_load_print_meta: n_vocab          = 50304
0.00.065.525 I llm_load_print_meta: n_merges         = 50009
0.00.065.525 I llm_load_print_meta: vocab_only       = 0
0.00.065.525 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.526 I llm_load_print_meta: n_embd           = 2048
0.00.065.526 I llm_load_print_meta: n_layer          = 24
0.00.065.530 I llm_load_print_meta: n_head           = 16
0.00.065.530 I llm_load_print_meta: n_head_kv        = 16
0.00.065.531 I llm_load_print_meta: n_rot            = 32
0.00.065.531 I llm_load_print_meta: n_swa            = 0
0.00.065.531 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.531 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.533 I llm_load_print_meta: n_gqa            = 1
0.00.065.534 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.535 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.535 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.536 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.536 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.536 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.536 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.537 I llm_load_print_meta: n_ff             = 8192
0.00.065.537 I llm_load_print_meta: n_expert         = 0
0.00.065.537 I llm_load_print_meta: n_expert_used    = 0
0.00.065.537 I llm_load_print_meta: causal attn      = 1
0.00.065.537 I llm_load_print_meta: pooling type     = 0
0.00.065.544 I llm_load_print_meta: rope type        = 2
0.00.065.546 I llm_load_print_meta: rope scaling     = linear
0.00.065.547 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.547 I llm_load_print_meta: freq_scale_train = 1
0.00.065.547 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.548 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.548 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.548 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.548 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.548 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.548 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.565 I llm_load_print_meta: model type       = 1.4B
0.00.065.566 I llm_load_print_meta: model ftype      = Q4_0
0.00.065.566 I llm_load_print_meta: model params     = 1.41 B
0.00.065.566 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.065.567 I llm_load_print_meta: general.name     = 1.4B
0.00.065.567 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.567 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.567 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.567 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.568 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.568 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.568 I llm_load_print_meta: max token length = 1024
0.00.068.077 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.077 I llm_load_tensors: offloading output layer to GPU
0.00.068.077 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.088 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.068.089 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.069.248 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.249 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.249 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.249 I llama_new_context_with_model: n_batch       = 2048
0.00.069.250 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.250 I llama_new_context_with_model: flash_attn    = 0
0.00.069.250 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.251 I llama_new_context_with_model: freq_scale    = 1
0.00.069.251 I ggml_metal_init: allocating
0.00.069.254 I ggml_metal_init: found device: Apple M4
0.00.069.256 I ggml_metal_init: picking default device: Apple M4
0.00.070.052 I ggml_metal_init: using embedded metal library
0.00.072.635 I ggml_metal_init: GPU name:   Apple M4
0.00.072.637 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.637 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.638 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.638 I ggml_metal_init: simdgroup reduction   = true
0.00.072.638 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.638 I ggml_metal_init: has bfloat            = true
0.00.072.639 I ggml_metal_init: use bfloat            = true
0.00.072.639 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.018 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.026 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.050 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.109 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.112 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.112 I llama_new_context_with_model: graph nodes  = 967
0.00.107.112 I llama_new_context_with_model: graph splits = 2
0.00.107.137 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.291 I main: llama threadpool init, n_threads = 4
0.00.767.331 I 
0.00.767.358 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.767.360 I 
0.00.767.586 I sampler seed: 1234
0.00.767.590 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.621 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.625 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.625 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.445.600 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.01.445.600 I llama_perf_context_print:        load time =     751.21 ms
0.01.445.601 I llama_perf_context_print: prompt eval time =      38.44 ms /     7 tokens (    5.49 ms per token,   182.13 tokens per second)
0.01.445.602 I llama_perf_context_print:        eval time =     636.64 ms /    63 runs   (   10.11 ms per token,    98.96 tokens per second)
0.01.445.603 I llama_perf_context_print:       total time =     678.31 ms /    70 tokens
0.01.445.783 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.119s
sys	0m0.165s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.305 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.952 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.956 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.965 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.968 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.969 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.875 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.916 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.801 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.802 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.804 I llama_model_loader: - type  f32:  194 tensors
0.00.024.805 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.805 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.620 I llm_load_vocab: special tokens cache size = 25
0.00.051.702 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.705 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.705 I llm_load_print_meta: arch             = gptneox
0.00.051.705 I llm_load_print_meta: vocab type       = BPE
0.00.051.706 I llm_load_print_meta: n_vocab          = 50304
0.00.051.706 I llm_load_print_meta: n_merges         = 50009
0.00.051.706 I llm_load_print_meta: vocab_only       = 0
0.00.051.706 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.706 I llm_load_print_meta: n_embd           = 2048
0.00.051.706 I llm_load_print_meta: n_layer          = 24
0.00.051.709 I llm_load_print_meta: n_head           = 16
0.00.051.710 I llm_load_print_meta: n_head_kv        = 16
0.00.051.710 I llm_load_print_meta: n_rot            = 32
0.00.051.710 I llm_load_print_meta: n_swa            = 0
0.00.051.711 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.711 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.712 I llm_load_print_meta: n_gqa            = 1
0.00.051.712 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.713 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.713 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.714 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.714 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.714 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.715 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.715 I llm_load_print_meta: n_ff             = 8192
0.00.051.715 I llm_load_print_meta: n_expert         = 0
0.00.051.716 I llm_load_print_meta: n_expert_used    = 0
0.00.051.716 I llm_load_print_meta: causal attn      = 1
0.00.051.716 I llm_load_print_meta: pooling type     = 0
0.00.051.716 I llm_load_print_meta: rope type        = 2
0.00.051.716 I llm_load_print_meta: rope scaling     = linear
0.00.051.717 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.717 I llm_load_print_meta: freq_scale_train = 1
0.00.051.717 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.718 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.718 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.718 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.718 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.718 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.718 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.730 I llm_load_print_meta: model type       = 1.4B
0.00.051.730 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.730 I llm_load_print_meta: model params     = 1.41 B
0.00.051.731 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.731 I llm_load_print_meta: general.name     = 1.4B
0.00.051.731 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.731 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.731 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.731 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.732 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.732 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.732 I llm_load_print_meta: max token length = 1024
0.00.053.246 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.246 I llm_load_tensors: offloading output layer to GPU
0.00.053.247 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.256 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.257 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.082 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.082 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.083 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.083 I llama_new_context_with_model: n_batch       = 2048
0.00.054.083 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.083 I llama_new_context_with_model: flash_attn    = 0
0.00.054.084 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.084 I llama_new_context_with_model: freq_scale    = 1
0.00.054.084 I ggml_metal_init: allocating
0.00.054.088 I ggml_metal_init: found device: Apple M4
0.00.054.090 I ggml_metal_init: picking default device: Apple M4
0.00.054.646 I ggml_metal_init: using embedded metal library
0.00.056.620 I ggml_metal_init: GPU name:   Apple M4
0.00.056.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.622 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.622 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.623 I ggml_metal_init: simdgroup reduction   = true
0.00.056.623 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.623 I ggml_metal_init: has bfloat            = true
0.00.056.623 I ggml_metal_init: use bfloat            = true
0.00.056.623 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.621 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.626 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.643 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.693 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.694 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.694 I llama_new_context_with_model: graph nodes  = 967
0.00.084.694 I llama_new_context_with_model: graph splits = 2
0.00.084.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.627 I main: llama threadpool init, n_threads = 4
0.00.648.660 I 
0.00.648.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.648.687 I 
0.00.648.915 I sampler seed: 1234
0.00.648.919 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.648.957 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.648.957 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.648.957 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.369.451 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.369.452 I llama_perf_context_print:        load time =     639.32 ms
0.01.369.453 I llama_perf_context_print: prompt eval time =      36.68 ms /     7 tokens (    5.24 ms per token,   190.87 tokens per second)
0.01.369.453 I llama_perf_context_print:        eval time =     680.83 ms /    63 runs   (   10.81 ms per token,    92.53 tokens per second)
0.01.369.454 I llama_perf_context_print:       total time =     720.83 ms /    70 tokens
0.01.369.634 I ggml_metal_free: deallocating

real	0m1.388s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.644 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.338 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.342 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.349 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.349 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.351 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.357 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.358 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.358 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.900 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.903 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.903 I llama_model_loader: - type  f32:  194 tensors
0.00.023.904 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.904 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.790 I llm_load_vocab: special tokens cache size = 25
0.00.049.772 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.775 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.775 I llm_load_print_meta: arch             = gptneox
0.00.049.776 I llm_load_print_meta: vocab type       = BPE
0.00.049.776 I llm_load_print_meta: n_vocab          = 50304
0.00.049.776 I llm_load_print_meta: n_merges         = 50009
0.00.049.776 I llm_load_print_meta: vocab_only       = 0
0.00.049.776 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.776 I llm_load_print_meta: n_embd           = 2048
0.00.049.777 I llm_load_print_meta: n_layer          = 24
0.00.049.780 I llm_load_print_meta: n_head           = 16
0.00.049.780 I llm_load_print_meta: n_head_kv        = 16
0.00.049.781 I llm_load_print_meta: n_rot            = 32
0.00.049.781 I llm_load_print_meta: n_swa            = 0
0.00.049.781 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.781 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.782 I llm_load_print_meta: n_gqa            = 1
0.00.049.783 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.783 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.784 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.785 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.787 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.787 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.787 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.788 I llm_load_print_meta: n_ff             = 8192
0.00.049.788 I llm_load_print_meta: n_expert         = 0
0.00.049.788 I llm_load_print_meta: n_expert_used    = 0
0.00.049.789 I llm_load_print_meta: causal attn      = 1
0.00.049.789 I llm_load_print_meta: pooling type     = 0
0.00.049.789 I llm_load_print_meta: rope type        = 2
0.00.049.790 I llm_load_print_meta: rope scaling     = linear
0.00.049.790 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.790 I llm_load_print_meta: freq_scale_train = 1
0.00.049.791 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.791 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.791 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.791 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.791 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.792 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.792 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.803 I llm_load_print_meta: model type       = 1.4B
0.00.049.803 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.803 I llm_load_print_meta: model params     = 1.41 B
0.00.049.804 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.804 I llm_load_print_meta: general.name     = 1.4B
0.00.049.804 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.804 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.806 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.806 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.806 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.806 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.806 I llm_load_print_meta: max token length = 1024
0.00.051.356 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.356 I llm_load_tensors: offloading output layer to GPU
0.00.051.356 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.366 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.367 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.214 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.215 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.215 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.215 I llama_new_context_with_model: n_batch       = 2048
0.00.052.216 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.216 I llama_new_context_with_model: flash_attn    = 0
0.00.052.216 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.216 I llama_new_context_with_model: freq_scale    = 1
0.00.052.217 I ggml_metal_init: allocating
0.00.052.223 I ggml_metal_init: found device: Apple M4
0.00.052.227 I ggml_metal_init: picking default device: Apple M4
0.00.052.772 I ggml_metal_init: using embedded metal library
0.00.054.723 I ggml_metal_init: GPU name:   Apple M4
0.00.054.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.725 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.725 I ggml_metal_init: simdgroup reduction   = true
0.00.054.725 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.726 I ggml_metal_init: has bfloat            = true
0.00.054.726 I ggml_metal_init: use bfloat            = true
0.00.054.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.405 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.411 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.438 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.360 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.361 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.362 I llama_new_context_with_model: graph nodes  = 967
0.00.083.362 I llama_new_context_with_model: graph splits = 2
0.00.083.384 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.954 I main: llama threadpool init, n_threads = 4
0.00.700.994 I 
0.00.701.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.701.037 I 
0.00.701.269 I sampler seed: 1234
0.00.701.273 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.701.284 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.701.285 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.701.285 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.482.988 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.482.989 I llama_perf_context_print:        load time =     692.30 ms
0.01.482.989 I llama_perf_context_print: prompt eval time =      36.62 ms /     7 tokens (    5.23 ms per token,   191.17 tokens per second)
0.01.482.990 I llama_perf_context_print:        eval time =     742.05 ms /    63 runs   (   11.78 ms per token,    84.90 tokens per second)
0.01.482.990 I llama_perf_context_print:       total time =     782.04 ms /    70 tokens
0.01.483.166 I ggml_metal_free: deallocating

real	0m1.497s
user	0m0.108s
sys	0m0.153s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.628 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.404 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.409 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.411 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.411 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.413 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.413 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.418 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.032 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.033 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.033 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.034 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.034 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.035 I llama_model_loader: - type  f32:  194 tensors
0.00.025.035 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.035 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.945 I llm_load_vocab: special tokens cache size = 25
0.00.050.958 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.961 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.961 I llm_load_print_meta: arch             = gptneox
0.00.050.961 I llm_load_print_meta: vocab type       = BPE
0.00.050.961 I llm_load_print_meta: n_vocab          = 50304
0.00.050.962 I llm_load_print_meta: n_merges         = 50009
0.00.050.962 I llm_load_print_meta: vocab_only       = 0
0.00.050.962 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.962 I llm_load_print_meta: n_embd           = 2048
0.00.050.962 I llm_load_print_meta: n_layer          = 24
0.00.050.965 I llm_load_print_meta: n_head           = 16
0.00.050.966 I llm_load_print_meta: n_head_kv        = 16
0.00.050.966 I llm_load_print_meta: n_rot            = 32
0.00.050.966 I llm_load_print_meta: n_swa            = 0
0.00.050.966 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.966 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.967 I llm_load_print_meta: n_gqa            = 1
0.00.050.968 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.970 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.970 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.970 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.973 I llm_load_print_meta: n_ff             = 8192
0.00.050.973 I llm_load_print_meta: n_expert         = 0
0.00.050.973 I llm_load_print_meta: n_expert_used    = 0
0.00.050.974 I llm_load_print_meta: causal attn      = 1
0.00.050.975 I llm_load_print_meta: pooling type     = 0
0.00.050.976 I llm_load_print_meta: rope type        = 2
0.00.050.976 I llm_load_print_meta: rope scaling     = linear
0.00.050.976 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.977 I llm_load_print_meta: freq_scale_train = 1
0.00.050.977 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.977 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.977 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.977 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.977 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.978 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.978 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.989 I llm_load_print_meta: model type       = 1.4B
0.00.050.990 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.991 I llm_load_print_meta: model params     = 1.41 B
0.00.050.991 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.991 I llm_load_print_meta: general.name     = 1.4B
0.00.050.992 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.992 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.992 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.992 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.993 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: max token length = 1024
0.00.052.539 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.539 I llm_load_tensors: offloading output layer to GPU
0.00.052.539 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.549 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.550 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.407 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.408 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.408 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.408 I llama_new_context_with_model: n_batch       = 2048
0.00.053.408 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.409 I llama_new_context_with_model: flash_attn    = 0
0.00.053.409 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.409 I llama_new_context_with_model: freq_scale    = 1
0.00.053.410 I ggml_metal_init: allocating
0.00.053.415 I ggml_metal_init: found device: Apple M4
0.00.053.417 I ggml_metal_init: picking default device: Apple M4
0.00.053.960 I ggml_metal_init: using embedded metal library
0.00.055.886 I ggml_metal_init: GPU name:   Apple M4
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.889 I ggml_metal_init: simdgroup reduction   = true
0.00.055.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.890 I ggml_metal_init: has bfloat            = true
0.00.055.890 I ggml_metal_init: use bfloat            = true
0.00.055.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.696 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.701 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.720 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.699 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.700 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.701 I llama_new_context_with_model: graph nodes  = 967
0.00.083.701 I llama_new_context_with_model: graph splits = 2
0.00.083.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.945 I main: llama threadpool init, n_threads = 4
0.00.768.979 I 
0.00.769.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.769.025 I 
0.00.769.253 I sampler seed: 1234
0.00.769.258 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.294 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.295 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.296 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.606.547 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.606.548 I llama_perf_context_print:        load time =     759.31 ms
0.01.606.549 I llama_perf_context_print: prompt eval time =      36.53 ms /     7 tokens (    5.22 ms per token,   191.62 tokens per second)
0.01.606.549 I llama_perf_context_print:        eval time =     797.65 ms /    63 runs   (   12.66 ms per token,    78.98 tokens per second)
0.01.606.550 I llama_perf_context_print:       total time =     837.60 ms /    70 tokens
0.01.606.715 I ggml_metal_free: deallocating

real	0m1.625s
user	0m0.108s
sys	0m0.165s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.230 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.960 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.964 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.966 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.967 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.969 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.972 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.973 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.973 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.953 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.822 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.823 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.824 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.825 I llama_model_loader: - type  f32:  194 tensors
0.00.023.825 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.825 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.825 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.712 I llm_load_vocab: special tokens cache size = 25
0.00.050.705 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.708 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.708 I llm_load_print_meta: arch             = gptneox
0.00.050.709 I llm_load_print_meta: vocab type       = BPE
0.00.050.709 I llm_load_print_meta: n_vocab          = 50304
0.00.050.709 I llm_load_print_meta: n_merges         = 50009
0.00.050.709 I llm_load_print_meta: vocab_only       = 0
0.00.050.709 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.709 I llm_load_print_meta: n_embd           = 2048
0.00.050.710 I llm_load_print_meta: n_layer          = 24
0.00.050.713 I llm_load_print_meta: n_head           = 16
0.00.050.713 I llm_load_print_meta: n_head_kv        = 16
0.00.050.713 I llm_load_print_meta: n_rot            = 32
0.00.050.714 I llm_load_print_meta: n_swa            = 0
0.00.050.714 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.714 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.715 I llm_load_print_meta: n_gqa            = 1
0.00.050.716 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.716 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.717 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.717 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.717 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.720 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.720 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.721 I llm_load_print_meta: n_ff             = 8192
0.00.050.721 I llm_load_print_meta: n_expert         = 0
0.00.050.721 I llm_load_print_meta: n_expert_used    = 0
0.00.050.721 I llm_load_print_meta: causal attn      = 1
0.00.050.721 I llm_load_print_meta: pooling type     = 0
0.00.050.722 I llm_load_print_meta: rope type        = 2
0.00.050.722 I llm_load_print_meta: rope scaling     = linear
0.00.050.722 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.722 I llm_load_print_meta: freq_scale_train = 1
0.00.050.724 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.724 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.724 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.724 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.725 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.725 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.725 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.737 I llm_load_print_meta: model type       = 1.4B
0.00.050.737 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.738 I llm_load_print_meta: model params     = 1.41 B
0.00.050.738 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.738 I llm_load_print_meta: general.name     = 1.4B
0.00.050.739 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.741 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.741 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.741 I llm_load_print_meta: max token length = 1024
0.00.052.685 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.685 I llm_load_tensors: offloading output layer to GPU
0.00.052.685 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.695 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.697 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.618 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.619 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.619 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.619 I llama_new_context_with_model: n_batch       = 2048
0.00.053.620 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.620 I llama_new_context_with_model: flash_attn    = 0
0.00.053.620 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.620 I llama_new_context_with_model: freq_scale    = 1
0.00.053.621 I ggml_metal_init: allocating
0.00.053.624 I ggml_metal_init: found device: Apple M4
0.00.053.626 I ggml_metal_init: picking default device: Apple M4
0.00.054.176 I ggml_metal_init: using embedded metal library
0.00.056.131 I ggml_metal_init: GPU name:   Apple M4
0.00.056.133 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.134 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.134 I ggml_metal_init: simdgroup reduction   = true
0.00.056.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.134 I ggml_metal_init: has bfloat            = true
0.00.056.134 I ggml_metal_init: use bfloat            = true
0.00.056.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.982 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.991 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.015 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.112 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.114 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.114 I llama_new_context_with_model: graph nodes  = 967
0.00.086.115 I llama_new_context_with_model: graph splits = 2
0.00.086.137 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.488.265 I main: llama threadpool init, n_threads = 4
0.00.488.309 I 
0.00.488.347 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.488.347 I 
0.00.488.592 I sampler seed: 1234
0.00.488.598 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.488.628 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.488.629 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.488.629 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.172.861 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62117.24 tokens per second)
0.01.172.861 I llama_perf_context_print:        load time =     479.03 ms
0.01.172.862 I llama_perf_context_print: prompt eval time =      39.67 ms /     7 tokens (    5.67 ms per token,   176.46 tokens per second)
0.01.172.863 I llama_perf_context_print:        eval time =     641.59 ms /    63 runs   (   10.18 ms per token,    98.19 tokens per second)
0.01.172.864 I llama_perf_context_print:       total time =     684.60 ms /    70 tokens
0.01.173.024 I ggml_metal_free: deallocating

real	0m1.192s
user	0m0.110s
sys	0m0.116s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.557 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.934 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.935 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.937 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.939 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.939 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.879 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.702 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.702 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.703 I llama_model_loader: - type  f32:  194 tensors
0.00.023.703 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.703 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.703 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.704 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.803 I llm_load_vocab: special tokens cache size = 25
0.00.049.871 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.874 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.875 I llm_load_print_meta: arch             = gptneox
0.00.049.875 I llm_load_print_meta: vocab type       = BPE
0.00.049.875 I llm_load_print_meta: n_vocab          = 50304
0.00.049.876 I llm_load_print_meta: n_merges         = 50009
0.00.049.876 I llm_load_print_meta: vocab_only       = 0
0.00.049.876 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.876 I llm_load_print_meta: n_embd           = 2048
0.00.049.876 I llm_load_print_meta: n_layer          = 24
0.00.049.879 I llm_load_print_meta: n_head           = 16
0.00.049.879 I llm_load_print_meta: n_head_kv        = 16
0.00.049.880 I llm_load_print_meta: n_rot            = 32
0.00.049.880 I llm_load_print_meta: n_swa            = 0
0.00.049.880 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.880 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.881 I llm_load_print_meta: n_gqa            = 1
0.00.049.882 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.882 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.883 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.883 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.883 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.884 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.884 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.885 I llm_load_print_meta: n_ff             = 8192
0.00.049.886 I llm_load_print_meta: n_expert         = 0
0.00.049.888 I llm_load_print_meta: n_expert_used    = 0
0.00.049.888 I llm_load_print_meta: causal attn      = 1
0.00.049.888 I llm_load_print_meta: pooling type     = 0
0.00.049.888 I llm_load_print_meta: rope type        = 2
0.00.049.888 I llm_load_print_meta: rope scaling     = linear
0.00.049.889 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.889 I llm_load_print_meta: freq_scale_train = 1
0.00.049.891 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.891 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.891 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.891 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.891 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.891 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.891 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.903 I llm_load_print_meta: model type       = 1.4B
0.00.049.903 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.904 I llm_load_print_meta: model params     = 1.41 B
0.00.049.904 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.905 I llm_load_print_meta: general.name     = 1.4B
0.00.049.906 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.907 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.907 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.907 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.907 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.907 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.908 I llm_load_print_meta: max token length = 1024
0.00.051.699 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.699 I llm_load_tensors: offloading output layer to GPU
0.00.051.699 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.703 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.704 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.613 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.614 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.614 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.614 I llama_new_context_with_model: n_batch       = 2048
0.00.052.614 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.615 I llama_new_context_with_model: flash_attn    = 0
0.00.052.615 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.615 I llama_new_context_with_model: freq_scale    = 1
0.00.052.615 I ggml_metal_init: allocating
0.00.052.619 I ggml_metal_init: found device: Apple M4
0.00.052.620 I ggml_metal_init: picking default device: Apple M4
0.00.053.168 I ggml_metal_init: using embedded metal library
0.00.055.071 I ggml_metal_init: GPU name:   Apple M4
0.00.055.074 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.075 I ggml_metal_init: simdgroup reduction   = true
0.00.055.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.075 I ggml_metal_init: has bfloat            = true
0.00.055.075 I ggml_metal_init: use bfloat            = true
0.00.055.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.590 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.599 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.618 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.532 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.533 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.533 I llama_new_context_with_model: graph nodes  = 967
0.00.082.534 I llama_new_context_with_model: graph splits = 2
0.00.082.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.544.518 I main: llama threadpool init, n_threads = 4
0.00.544.557 I 
0.00.544.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.544.595 I 
0.00.544.826 I sampler seed: 1234
0.00.544.830 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.544.841 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.544.841 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.544.841 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.289.076 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.289.077 I llama_perf_context_print:        load time =     535.96 ms
0.01.289.077 I llama_perf_context_print: prompt eval time =      39.54 ms /     7 tokens (    5.65 ms per token,   177.02 tokens per second)
0.01.289.078 I llama_perf_context_print:        eval time =     701.67 ms /    63 runs   (   11.14 ms per token,    89.79 tokens per second)
0.01.289.078 I llama_perf_context_print:       total time =     744.56 ms /    70 tokens
0.01.289.236 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.109s
sys	0m0.131s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.677 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.128 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.133 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.134 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.137 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.138 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.140 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.994 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.038 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.875 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.876 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.876 I llama_model_loader: - type  f32:  194 tensors
0.00.024.877 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.877 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.877 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.023 I llm_load_vocab: special tokens cache size = 25
0.00.051.052 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.055 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.055 I llm_load_print_meta: arch             = gptneox
0.00.051.056 I llm_load_print_meta: vocab type       = BPE
0.00.051.056 I llm_load_print_meta: n_vocab          = 50304
0.00.051.056 I llm_load_print_meta: n_merges         = 50009
0.00.051.056 I llm_load_print_meta: vocab_only       = 0
0.00.051.057 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.057 I llm_load_print_meta: n_embd           = 2048
0.00.051.057 I llm_load_print_meta: n_layer          = 24
0.00.051.060 I llm_load_print_meta: n_head           = 16
0.00.051.061 I llm_load_print_meta: n_head_kv        = 16
0.00.051.061 I llm_load_print_meta: n_rot            = 32
0.00.051.061 I llm_load_print_meta: n_swa            = 0
0.00.051.064 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.064 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.065 I llm_load_print_meta: n_gqa            = 1
0.00.051.065 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.066 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.067 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.068 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.068 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.069 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.069 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.069 I llm_load_print_meta: n_ff             = 8192
0.00.051.070 I llm_load_print_meta: n_expert         = 0
0.00.051.071 I llm_load_print_meta: n_expert_used    = 0
0.00.051.072 I llm_load_print_meta: causal attn      = 1
0.00.051.072 I llm_load_print_meta: pooling type     = 0
0.00.051.072 I llm_load_print_meta: rope type        = 2
0.00.051.073 I llm_load_print_meta: rope scaling     = linear
0.00.051.073 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.073 I llm_load_print_meta: freq_scale_train = 1
0.00.051.074 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.074 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.074 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.074 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.074 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.074 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.074 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.081 I llm_load_print_meta: model type       = 1.4B
0.00.051.082 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.083 I llm_load_print_meta: model params     = 1.41 B
0.00.051.083 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.084 I llm_load_print_meta: general.name     = 1.4B
0.00.051.084 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.084 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.084 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.084 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.085 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.085 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.085 I llm_load_print_meta: max token length = 1024
0.00.052.837 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.837 I llm_load_tensors: offloading output layer to GPU
0.00.052.837 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.842 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.842 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.771 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.772 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.772 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.772 I llama_new_context_with_model: n_batch       = 2048
0.00.053.773 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.773 I llama_new_context_with_model: flash_attn    = 0
0.00.053.773 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.773 I llama_new_context_with_model: freq_scale    = 1
0.00.053.774 I ggml_metal_init: allocating
0.00.053.779 I ggml_metal_init: found device: Apple M4
0.00.053.781 I ggml_metal_init: picking default device: Apple M4
0.00.054.331 I ggml_metal_init: using embedded metal library
0.00.056.237 I ggml_metal_init: GPU name:   Apple M4
0.00.056.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.241 I ggml_metal_init: simdgroup reduction   = true
0.00.056.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.241 I ggml_metal_init: has bfloat            = true
0.00.056.241 I ggml_metal_init: use bfloat            = true
0.00.056.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.506 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.517 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.536 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.456 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.457 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.457 I llama_new_context_with_model: graph nodes  = 967
0.00.084.458 I llama_new_context_with_model: graph splits = 2
0.00.084.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.914 I main: llama threadpool init, n_threads = 4
0.00.639.952 I 
0.00.639.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.639.980 I 
0.00.640.214 I sampler seed: 1234
0.00.640.219 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.640.258 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.640.259 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.640.259 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.395.889 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.395.889 I llama_perf_context_print:        load time =     630.23 ms
0.01.395.890 I llama_perf_context_print: prompt eval time =      42.66 ms /     7 tokens (    6.09 ms per token,   164.07 tokens per second)
0.01.395.891 I llama_perf_context_print:        eval time =     709.84 ms /    63 runs   (   11.27 ms per token,    88.75 tokens per second)
0.01.395.892 I llama_perf_context_print:       total time =     755.98 ms /    70 tokens
0.01.396.060 I ggml_metal_free: deallocating

real	0m1.415s
user	0m0.107s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.683 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.457 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.462 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.468 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.468 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.468 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.469 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.471 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.472 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.472 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.472 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.473 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.476 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.478 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.479 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.479 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.292 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.060 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.061 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.062 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.062 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.062 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.063 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.063 I llama_model_loader: - type  f32:  194 tensors
0.00.024.064 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.064 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.015 I llm_load_vocab: special tokens cache size = 25
0.00.049.910 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.913 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.913 I llm_load_print_meta: arch             = gptneox
0.00.049.914 I llm_load_print_meta: vocab type       = BPE
0.00.049.914 I llm_load_print_meta: n_vocab          = 50304
0.00.049.914 I llm_load_print_meta: n_merges         = 50009
0.00.049.914 I llm_load_print_meta: vocab_only       = 0
0.00.049.914 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.914 I llm_load_print_meta: n_embd           = 2048
0.00.049.915 I llm_load_print_meta: n_layer          = 24
0.00.049.917 I llm_load_print_meta: n_head           = 16
0.00.049.918 I llm_load_print_meta: n_head_kv        = 16
0.00.049.918 I llm_load_print_meta: n_rot            = 32
0.00.049.919 I llm_load_print_meta: n_swa            = 0
0.00.049.919 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.919 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.922 I llm_load_print_meta: n_gqa            = 1
0.00.049.922 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.923 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.924 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.924 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.925 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.926 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.926 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.927 I llm_load_print_meta: n_ff             = 8192
0.00.049.927 I llm_load_print_meta: n_expert         = 0
0.00.049.927 I llm_load_print_meta: n_expert_used    = 0
0.00.049.927 I llm_load_print_meta: causal attn      = 1
0.00.049.927 I llm_load_print_meta: pooling type     = 0
0.00.049.927 I llm_load_print_meta: rope type        = 2
0.00.049.928 I llm_load_print_meta: rope scaling     = linear
0.00.049.928 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.928 I llm_load_print_meta: freq_scale_train = 1
0.00.049.929 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.929 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.929 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.929 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.929 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.929 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.930 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.941 I llm_load_print_meta: model type       = 1.4B
0.00.049.942 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.942 I llm_load_print_meta: model params     = 1.41 B
0.00.049.943 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.943 I llm_load_print_meta: general.name     = 1.4B
0.00.049.943 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.943 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.944 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.944 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.944 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.944 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.944 I llm_load_print_meta: max token length = 1024
0.00.051.960 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.960 I llm_load_tensors: offloading output layer to GPU
0.00.051.961 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.971 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.972 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.962 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.963 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.963 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.964 I llama_new_context_with_model: n_batch       = 2048
0.00.052.964 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.964 I llama_new_context_with_model: flash_attn    = 0
0.00.052.964 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.965 I llama_new_context_with_model: freq_scale    = 1
0.00.052.965 I ggml_metal_init: allocating
0.00.052.972 I ggml_metal_init: found device: Apple M4
0.00.052.974 I ggml_metal_init: picking default device: Apple M4
0.00.053.506 I ggml_metal_init: using embedded metal library
0.00.055.461 I ggml_metal_init: GPU name:   Apple M4
0.00.055.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.463 I ggml_metal_init: simdgroup reduction   = true
0.00.055.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.464 I ggml_metal_init: has bfloat            = true
0.00.055.464 I ggml_metal_init: use bfloat            = true
0.00.055.464 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.263 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.269 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.287 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.288 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.289 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.289 I llama_new_context_with_model: graph nodes  = 967
0.00.084.290 I llama_new_context_with_model: graph splits = 2
0.00.084.313 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.299 I main: llama threadpool init, n_threads = 4
0.00.703.333 I 
0.00.703.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.703.377 I 
0.00.703.508 I sampler seed: 1234
0.00.703.512 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.523 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.524 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.524 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.545.023 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.01.545.023 I llama_perf_context_print:        load time =     694.61 ms
0.01.545.024 I llama_perf_context_print: prompt eval time =      38.48 ms /     7 tokens (    5.50 ms per token,   181.92 tokens per second)
0.01.545.025 I llama_perf_context_print:        eval time =     800.06 ms /    63 runs   (   12.70 ms per token,    78.74 tokens per second)
0.01.545.025 I llama_perf_context_print:       total time =     841.73 ms /    70 tokens
0.01.545.220 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.107s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.721 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.547 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.554 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.555 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.557 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.560 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.560 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.560 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.435 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.535 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.390 I llama_model_loader: - type  f32:  194 tensors
0.00.025.390 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.595 I llm_load_vocab: special tokens cache size = 25
0.00.051.508 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.511 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.512 I llm_load_print_meta: arch             = gptneox
0.00.051.512 I llm_load_print_meta: vocab type       = BPE
0.00.051.512 I llm_load_print_meta: n_vocab          = 50304
0.00.051.512 I llm_load_print_meta: n_merges         = 50009
0.00.051.513 I llm_load_print_meta: vocab_only       = 0
0.00.051.513 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.513 I llm_load_print_meta: n_embd           = 2048
0.00.051.513 I llm_load_print_meta: n_layer          = 24
0.00.051.516 I llm_load_print_meta: n_head           = 16
0.00.051.516 I llm_load_print_meta: n_head_kv        = 16
0.00.051.517 I llm_load_print_meta: n_rot            = 32
0.00.051.517 I llm_load_print_meta: n_swa            = 0
0.00.051.517 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.517 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.518 I llm_load_print_meta: n_gqa            = 1
0.00.051.519 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.519 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.520 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.520 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.520 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.521 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.521 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.521 I llm_load_print_meta: n_ff             = 8192
0.00.051.522 I llm_load_print_meta: n_expert         = 0
0.00.051.522 I llm_load_print_meta: n_expert_used    = 0
0.00.051.522 I llm_load_print_meta: causal attn      = 1
0.00.051.523 I llm_load_print_meta: pooling type     = 0
0.00.051.525 I llm_load_print_meta: rope type        = 2
0.00.051.526 I llm_load_print_meta: rope scaling     = linear
0.00.051.526 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.526 I llm_load_print_meta: freq_scale_train = 1
0.00.051.526 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.527 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.527 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.527 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.527 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.527 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.528 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.540 I llm_load_print_meta: model type       = 1.4B
0.00.051.540 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.540 I llm_load_print_meta: model params     = 1.41 B
0.00.051.541 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.541 I llm_load_print_meta: general.name     = 1.4B
0.00.051.541 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.541 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.541 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.542 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.542 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.542 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.542 I llm_load_print_meta: max token length = 1024
0.00.053.561 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.561 I llm_load_tensors: offloading output layer to GPU
0.00.053.561 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.571 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.572 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.484 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.485 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.485 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.485 I llama_new_context_with_model: n_batch       = 2048
0.00.054.485 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.485 I llama_new_context_with_model: flash_attn    = 0
0.00.054.486 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.486 I llama_new_context_with_model: freq_scale    = 1
0.00.054.486 I ggml_metal_init: allocating
0.00.054.489 I ggml_metal_init: found device: Apple M4
0.00.054.491 I ggml_metal_init: picking default device: Apple M4
0.00.055.061 I ggml_metal_init: using embedded metal library
0.00.056.960 I ggml_metal_init: GPU name:   Apple M4
0.00.056.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.962 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.962 I ggml_metal_init: simdgroup reduction   = true
0.00.056.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.964 I ggml_metal_init: has bfloat            = true
0.00.056.964 I ggml_metal_init: use bfloat            = true
0.00.056.964 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.965 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.563 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.567 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.586 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.509 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.510 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.511 I llama_new_context_with_model: graph nodes  = 967
0.00.084.511 I llama_new_context_with_model: graph splits = 2
0.00.084.524 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.544 I main: llama threadpool init, n_threads = 4
0.00.769.580 I 
0.00.769.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.769.610 I 
0.00.769.877 I sampler seed: 1234
0.00.769.882 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.921 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.922 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.922 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.633.247 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.633.248 I llama_perf_context_print:        load time =     759.82 ms
0.01.633.248 I llama_perf_context_print: prompt eval time =      38.42 ms /     7 tokens (    5.49 ms per token,   182.20 tokens per second)
0.01.633.249 I llama_perf_context_print:        eval time =     821.85 ms /    63 runs   (   13.05 ms per token,    76.66 tokens per second)
0.01.633.250 I llama_perf_context_print:       total time =     863.70 ms /    70 tokens
0.01.633.438 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.108s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.617 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.477 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.622 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.630 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.632 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.633 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.633 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.635 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.635 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.635 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.636 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.636 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.637 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.637 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.639 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.639 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.640 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.597 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.026 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.614 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.617 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.617 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.618 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.619 I llama_model_loader: - type  f32:  194 tensors
0.00.056.620 I llama_model_loader: - type  f16:   98 tensors
0.00.087.062 I llm_load_vocab: special tokens cache size = 25
0.00.094.089 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.092 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.092 I llm_load_print_meta: arch             = gptneox
0.00.094.092 I llm_load_print_meta: vocab type       = BPE
0.00.094.093 I llm_load_print_meta: n_vocab          = 50304
0.00.094.093 I llm_load_print_meta: n_merges         = 50009
0.00.094.093 I llm_load_print_meta: vocab_only       = 0
0.00.094.093 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.093 I llm_load_print_meta: n_embd           = 2048
0.00.094.093 I llm_load_print_meta: n_layer          = 24
0.00.094.096 I llm_load_print_meta: n_head           = 16
0.00.094.097 I llm_load_print_meta: n_head_kv        = 16
0.00.094.097 I llm_load_print_meta: n_rot            = 32
0.00.094.097 I llm_load_print_meta: n_swa            = 0
0.00.094.097 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.097 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.098 I llm_load_print_meta: n_gqa            = 1
0.00.094.099 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.099 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.100 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.100 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.100 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.100 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.100 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.101 I llm_load_print_meta: n_ff             = 8192
0.00.094.101 I llm_load_print_meta: n_expert         = 0
0.00.094.101 I llm_load_print_meta: n_expert_used    = 0
0.00.094.102 I llm_load_print_meta: causal attn      = 1
0.00.094.102 I llm_load_print_meta: pooling type     = 0
0.00.094.102 I llm_load_print_meta: rope type        = 2
0.00.094.102 I llm_load_print_meta: rope scaling     = linear
0.00.094.102 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.103 I llm_load_print_meta: freq_scale_train = 1
0.00.094.103 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.103 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.103 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.103 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.103 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.104 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.104 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.115 I llm_load_print_meta: model type       = 1.4B
0.00.094.116 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.094.116 I llm_load_print_meta: model params     = 1.41 B
0.00.094.117 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.094.117 I llm_load_print_meta: general.name     = 1.4B
0.00.094.117 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.117 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.117 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.118 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.118 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.094.118 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.118 I llm_load_print_meta: max token length = 1024
0.00.096.679 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.680 I llm_load_tensors: offloading output layer to GPU
0.00.096.680 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.690 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.691 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.627 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.628 I llama_new_context_with_model: n_ctx         = 128
0.00.097.628 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.097.628 I llama_new_context_with_model: n_batch       = 128
0.00.097.628 I llama_new_context_with_model: n_ubatch      = 128
0.00.097.628 I llama_new_context_with_model: flash_attn    = 0
0.00.097.629 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.629 I llama_new_context_with_model: freq_scale    = 1
0.00.097.629 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.097.630 I ggml_metal_init: allocating
0.00.097.632 I ggml_metal_init: found device: Apple M4
0.00.097.634 I ggml_metal_init: picking default device: Apple M4
0.00.098.198 I ggml_metal_init: using embedded metal library
0.00.100.351 I ggml_metal_init: GPU name:   Apple M4
0.00.100.353 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.354 I ggml_metal_init: simdgroup reduction   = true
0.00.100.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.354 I ggml_metal_init: has bfloat            = true
0.00.100.354 I ggml_metal_init: use bfloat            = true
0.00.100.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.355 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.228 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.231 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.252 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.148 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.111.149 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.111.149 I llama_new_context_with_model: graph nodes  = 967
0.00.111.149 I llama_new_context_with_model: graph splits = 2
0.00.111.162 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.962.665 I 
0.00.962.760 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.962.773 I perplexity: tokenizing the input ..
0.00.976.184 I perplexity: tokenization took 13.41 ms
0.00.976.229 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.097.428 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.099.267 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.099.284 I llama_perf_context_print:        load time =     937.16 ms
0.01.099.285 I llama_perf_context_print: prompt eval time =     120.31 ms /   128 tokens (    0.94 ms per token,  1063.93 tokens per second)
0.01.099.287 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.099.287 I llama_perf_context_print:       total time =     136.64 ms /   129 tokens
0.01.099.722 I ggml_metal_free: deallocating

real	0m1.307s
user	0m0.127s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.122 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.563 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.674 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.679 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.768 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.563 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.563 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.564 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.564 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.564 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.565 I llama_model_loader: - type  f32:  194 tensors
0.00.028.565 I llama_model_loader: - type q8_0:   98 tensors
0.00.051.492 I llm_load_vocab: special tokens cache size = 25
0.00.057.563 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.565 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.566 I llm_load_print_meta: arch             = gptneox
0.00.057.566 I llm_load_print_meta: vocab type       = BPE
0.00.057.566 I llm_load_print_meta: n_vocab          = 50304
0.00.057.566 I llm_load_print_meta: n_merges         = 50009
0.00.057.567 I llm_load_print_meta: vocab_only       = 0
0.00.057.567 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.567 I llm_load_print_meta: n_embd           = 2048
0.00.057.567 I llm_load_print_meta: n_layer          = 24
0.00.057.571 I llm_load_print_meta: n_head           = 16
0.00.057.572 I llm_load_print_meta: n_head_kv        = 16
0.00.057.572 I llm_load_print_meta: n_rot            = 32
0.00.057.572 I llm_load_print_meta: n_swa            = 0
0.00.057.572 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.572 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.573 I llm_load_print_meta: n_gqa            = 1
0.00.057.574 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.574 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.575 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.575 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.575 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.576 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.576 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.576 I llm_load_print_meta: n_ff             = 8192
0.00.057.577 I llm_load_print_meta: n_expert         = 0
0.00.057.577 I llm_load_print_meta: n_expert_used    = 0
0.00.057.577 I llm_load_print_meta: causal attn      = 1
0.00.057.577 I llm_load_print_meta: pooling type     = 0
0.00.057.577 I llm_load_print_meta: rope type        = 2
0.00.057.577 I llm_load_print_meta: rope scaling     = linear
0.00.057.578 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.578 I llm_load_print_meta: freq_scale_train = 1
0.00.057.578 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.579 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.579 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.579 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.579 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.579 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.580 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.592 I llm_load_print_meta: model type       = 1.4B
0.00.057.592 I llm_load_print_meta: model ftype      = Q8_0
0.00.057.595 I llm_load_print_meta: model params     = 1.41 B
0.00.057.595 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.057.595 I llm_load_print_meta: general.name     = 1.4B
0.00.057.595 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.595 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.596 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.596 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.596 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.057.596 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.596 I llm_load_print_meta: max token length = 1024
0.00.059.698 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.698 I llm_load_tensors: offloading output layer to GPU
0.00.059.699 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.709 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.059.710 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.060.774 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.774 I llama_new_context_with_model: n_ctx         = 128
0.00.060.775 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.060.775 I llama_new_context_with_model: n_batch       = 128
0.00.060.775 I llama_new_context_with_model: n_ubatch      = 128
0.00.060.775 I llama_new_context_with_model: flash_attn    = 0
0.00.060.775 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.776 I llama_new_context_with_model: freq_scale    = 1
0.00.060.776 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.060.776 I ggml_metal_init: allocating
0.00.060.780 I ggml_metal_init: found device: Apple M4
0.00.060.782 I ggml_metal_init: picking default device: Apple M4
0.00.061.338 I ggml_metal_init: using embedded metal library
0.00.063.225 I ggml_metal_init: GPU name:   Apple M4
0.00.063.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.227 I ggml_metal_init: simdgroup reduction   = true
0.00.063.227 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.227 I ggml_metal_init: has bfloat            = true
0.00.063.227 I ggml_metal_init: use bfloat            = true
0.00.063.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.135 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.137 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.151 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.049 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.050 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.050 I llama_new_context_with_model: graph nodes  = 967
0.00.073.050 I llama_new_context_with_model: graph splits = 2
0.00.073.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.876.447 I 
0.00.876.471 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.876.474 I perplexity: tokenizing the input ..
0.00.884.403 I perplexity: tokenization took 7.927 ms
0.00.884.414 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.005.801 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.007.764 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.007.775 I llama_perf_context_print:        load time =     865.88 ms
0.01.007.776 I llama_perf_context_print: prompt eval time =     121.16 ms /   128 tokens (    0.95 ms per token,  1056.46 tokens per second)
0.01.007.777 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.007.778 I llama_perf_context_print:       total time =     131.33 ms /   129 tokens
0.01.008.215 I ggml_metal_free: deallocating

real	0m1.025s
user	0m0.084s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.538 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.213 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.213 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.214 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.214 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.215 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.215 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.215 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.216 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.217 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.219 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.219 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.982 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.019 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.810 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.811 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.812 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.812 I llama_model_loader: - type  f32:  194 tensors
0.00.023.812 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.813 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.676 I llm_load_vocab: special tokens cache size = 25
0.00.049.544 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.546 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.547 I llm_load_print_meta: arch             = gptneox
0.00.049.547 I llm_load_print_meta: vocab type       = BPE
0.00.049.547 I llm_load_print_meta: n_vocab          = 50304
0.00.049.547 I llm_load_print_meta: n_merges         = 50009
0.00.049.548 I llm_load_print_meta: vocab_only       = 0
0.00.049.548 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.548 I llm_load_print_meta: n_embd           = 2048
0.00.049.548 I llm_load_print_meta: n_layer          = 24
0.00.049.551 I llm_load_print_meta: n_head           = 16
0.00.049.551 I llm_load_print_meta: n_head_kv        = 16
0.00.049.552 I llm_load_print_meta: n_rot            = 32
0.00.049.552 I llm_load_print_meta: n_swa            = 0
0.00.049.552 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.552 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.553 I llm_load_print_meta: n_gqa            = 1
0.00.049.554 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.554 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.555 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.555 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.555 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.556 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.556 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.556 I llm_load_print_meta: n_ff             = 8192
0.00.049.557 I llm_load_print_meta: n_expert         = 0
0.00.049.557 I llm_load_print_meta: n_expert_used    = 0
0.00.049.557 I llm_load_print_meta: causal attn      = 1
0.00.049.557 I llm_load_print_meta: pooling type     = 0
0.00.049.557 I llm_load_print_meta: rope type        = 2
0.00.049.557 I llm_load_print_meta: rope scaling     = linear
0.00.049.558 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.558 I llm_load_print_meta: freq_scale_train = 1
0.00.049.558 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.559 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.559 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.559 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.561 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.561 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.561 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.573 I llm_load_print_meta: model type       = 1.4B
0.00.049.573 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.573 I llm_load_print_meta: model params     = 1.41 B
0.00.049.575 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.575 I llm_load_print_meta: general.name     = 1.4B
0.00.049.575 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.575 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.575 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.575 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.576 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.576 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.576 I llm_load_print_meta: max token length = 1024
0.00.051.068 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.068 I llm_load_tensors: offloading output layer to GPU
0.00.051.069 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.078 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.079 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.051.896 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.897 I llama_new_context_with_model: n_ctx         = 128
0.00.051.897 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.898 I llama_new_context_with_model: n_batch       = 128
0.00.051.898 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.898 I llama_new_context_with_model: flash_attn    = 0
0.00.051.899 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.899 I llama_new_context_with_model: freq_scale    = 1
0.00.051.899 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.899 I ggml_metal_init: allocating
0.00.051.902 I ggml_metal_init: found device: Apple M4
0.00.051.904 I ggml_metal_init: picking default device: Apple M4
0.00.052.482 I ggml_metal_init: using embedded metal library
0.00.054.396 I ggml_metal_init: GPU name:   Apple M4
0.00.054.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.399 I ggml_metal_init: simdgroup reduction   = true
0.00.054.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.399 I ggml_metal_init: has bfloat            = true
0.00.054.399 I ggml_metal_init: use bfloat            = true
0.00.054.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.390 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.392 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.405 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.311 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.312 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.312 I llama_new_context_with_model: graph nodes  = 967
0.00.064.313 I llama_new_context_with_model: graph splits = 2
0.00.064.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.551.908 I 
0.00.551.945 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.551.948 I perplexity: tokenizing the input ..
0.00.559.770 I perplexity: tokenization took 7.818 ms
0.00.559.782 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.682.576 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.683.903 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.683.918 I llama_perf_context_print:        load time =     542.36 ms
0.00.683.919 I llama_perf_context_print: prompt eval time =     122.57 ms /   128 tokens (    0.96 ms per token,  1044.32 tokens per second)
0.00.683.920 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.683.921 I llama_perf_context_print:       total time =     132.01 ms /   129 tokens
0.00.684.379 I ggml_metal_free: deallocating

real	0m0.700s
user	0m0.077s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.814 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.669 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.676 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.682 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.683 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.683 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.684 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.684 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.688 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.477 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.409 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.409 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.409 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.409 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.410 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.410 I llama_model_loader: - type  f32:  194 tensors
0.00.023.410 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.411 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.417 I llm_load_vocab: special tokens cache size = 25
0.00.050.351 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.354 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.354 I llm_load_print_meta: arch             = gptneox
0.00.050.355 I llm_load_print_meta: vocab type       = BPE
0.00.050.355 I llm_load_print_meta: n_vocab          = 50304
0.00.050.355 I llm_load_print_meta: n_merges         = 50009
0.00.050.355 I llm_load_print_meta: vocab_only       = 0
0.00.050.355 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.356 I llm_load_print_meta: n_embd           = 2048
0.00.050.356 I llm_load_print_meta: n_layer          = 24
0.00.050.359 I llm_load_print_meta: n_head           = 16
0.00.050.360 I llm_load_print_meta: n_head_kv        = 16
0.00.050.360 I llm_load_print_meta: n_rot            = 32
0.00.050.360 I llm_load_print_meta: n_swa            = 0
0.00.050.360 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.360 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.361 I llm_load_print_meta: n_gqa            = 1
0.00.050.362 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.363 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.363 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.364 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.365 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.366 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.366 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.366 I llm_load_print_meta: n_ff             = 8192
0.00.050.367 I llm_load_print_meta: n_expert         = 0
0.00.050.367 I llm_load_print_meta: n_expert_used    = 0
0.00.050.367 I llm_load_print_meta: causal attn      = 1
0.00.050.367 I llm_load_print_meta: pooling type     = 0
0.00.050.367 I llm_load_print_meta: rope type        = 2
0.00.050.368 I llm_load_print_meta: rope scaling     = linear
0.00.050.377 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.379 I llm_load_print_meta: freq_scale_train = 1
0.00.050.379 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.379 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.380 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.380 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.380 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.380 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.380 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.392 I llm_load_print_meta: model type       = 1.4B
0.00.050.392 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.393 I llm_load_print_meta: model params     = 1.41 B
0.00.050.393 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.393 I llm_load_print_meta: general.name     = 1.4B
0.00.050.394 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.394 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.394 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.394 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.394 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.396 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.396 I llm_load_print_meta: max token length = 1024
0.00.051.943 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.943 I llm_load_tensors: offloading output layer to GPU
0.00.051.943 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.953 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.954 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.742 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.743 I llama_new_context_with_model: n_ctx         = 128
0.00.052.743 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.743 I llama_new_context_with_model: n_batch       = 128
0.00.052.743 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.744 I llama_new_context_with_model: flash_attn    = 0
0.00.052.744 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.744 I llama_new_context_with_model: freq_scale    = 1
0.00.052.744 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.745 I ggml_metal_init: allocating
0.00.052.748 I ggml_metal_init: found device: Apple M4
0.00.052.749 I ggml_metal_init: picking default device: Apple M4
0.00.053.269 I ggml_metal_init: using embedded metal library
0.00.055.184 I ggml_metal_init: GPU name:   Apple M4
0.00.055.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.186 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.187 I ggml_metal_init: simdgroup reduction   = true
0.00.055.187 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.187 I ggml_metal_init: has bfloat            = true
0.00.055.187 I ggml_metal_init: use bfloat            = true
0.00.055.188 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.188 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.056 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.058 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.071 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.919 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.920 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.920 I llama_new_context_with_model: graph nodes  = 967
0.00.064.920 I llama_new_context_with_model: graph splits = 2
0.00.064.932 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.278 I 
0.00.587.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.587.311 I perplexity: tokenizing the input ..
0.00.595.531 I perplexity: tokenization took 8.219 ms
0.00.595.546 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.718.252 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.719.546 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.719.560 I llama_perf_context_print:        load time =     578.46 ms
0.00.719.561 I llama_perf_context_print: prompt eval time =     122.48 ms /   128 tokens (    0.96 ms per token,  1045.08 tokens per second)
0.00.719.562 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.719.562 I llama_perf_context_print:       total time =     132.28 ms /   129 tokens
0.00.719.838 I ggml_metal_free: deallocating

real	0m0.733s
user	0m0.079s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.934 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.558 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.564 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.565 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.566 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.567 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.567 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.567 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.570 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.571 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.385 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.477 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.241 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.242 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.242 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.243 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.243 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.243 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.244 I llama_model_loader: - type  f32:  194 tensors
0.00.024.244 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.244 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.235 I llm_load_vocab: special tokens cache size = 25
0.00.050.089 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.091 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.092 I llm_load_print_meta: arch             = gptneox
0.00.050.092 I llm_load_print_meta: vocab type       = BPE
0.00.050.092 I llm_load_print_meta: n_vocab          = 50304
0.00.050.092 I llm_load_print_meta: n_merges         = 50009
0.00.050.093 I llm_load_print_meta: vocab_only       = 0
0.00.050.093 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.093 I llm_load_print_meta: n_embd           = 2048
0.00.050.093 I llm_load_print_meta: n_layer          = 24
0.00.050.096 I llm_load_print_meta: n_head           = 16
0.00.050.097 I llm_load_print_meta: n_head_kv        = 16
0.00.050.097 I llm_load_print_meta: n_rot            = 32
0.00.050.097 I llm_load_print_meta: n_swa            = 0
0.00.050.097 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.097 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.098 I llm_load_print_meta: n_gqa            = 1
0.00.050.099 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.102 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.103 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.103 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.103 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.103 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.104 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.104 I llm_load_print_meta: n_ff             = 8192
0.00.050.111 I llm_load_print_meta: n_expert         = 0
0.00.050.113 I llm_load_print_meta: n_expert_used    = 0
0.00.050.113 I llm_load_print_meta: causal attn      = 1
0.00.050.113 I llm_load_print_meta: pooling type     = 0
0.00.050.113 I llm_load_print_meta: rope type        = 2
0.00.050.114 I llm_load_print_meta: rope scaling     = linear
0.00.050.115 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.116 I llm_load_print_meta: freq_scale_train = 1
0.00.050.116 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.116 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.117 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.117 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.117 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.117 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.117 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.129 I llm_load_print_meta: model type       = 1.4B
0.00.050.129 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.129 I llm_load_print_meta: model params     = 1.41 B
0.00.050.130 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.130 I llm_load_print_meta: general.name     = 1.4B
0.00.050.131 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.131 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.131 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.131 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.131 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.132 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.133 I llm_load_print_meta: max token length = 1024
0.00.051.699 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.699 I llm_load_tensors: offloading output layer to GPU
0.00.051.699 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.709 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.710 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.514 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.515 I llama_new_context_with_model: n_ctx         = 128
0.00.052.515 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.515 I llama_new_context_with_model: n_batch       = 128
0.00.052.516 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.516 I llama_new_context_with_model: flash_attn    = 0
0.00.052.516 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.516 I llama_new_context_with_model: freq_scale    = 1
0.00.052.517 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.517 I ggml_metal_init: allocating
0.00.052.521 I ggml_metal_init: found device: Apple M4
0.00.052.523 I ggml_metal_init: picking default device: Apple M4
0.00.053.056 I ggml_metal_init: using embedded metal library
0.00.055.026 I ggml_metal_init: GPU name:   Apple M4
0.00.055.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.028 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.028 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.028 I ggml_metal_init: simdgroup reduction   = true
0.00.055.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.029 I ggml_metal_init: has bfloat            = true
0.00.055.029 I ggml_metal_init: use bfloat            = true
0.00.055.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.098 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.100 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.114 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.004 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.005 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.006 I llama_new_context_with_model: graph nodes  = 967
0.00.065.006 I llama_new_context_with_model: graph splits = 2
0.00.065.018 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.197 I 
0.00.650.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.650.232 I perplexity: tokenizing the input ..
0.00.657.944 I perplexity: tokenization took 7.71 ms
0.00.657.954 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.065 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.794.420 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.794.444 I llama_perf_context_print:        load time =     640.26 ms
0.00.794.445 I llama_perf_context_print: prompt eval time =     134.88 ms /   128 tokens (    1.05 ms per token,   948.99 tokens per second)
0.00.794.448 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.448 I llama_perf_context_print:       total time =     144.25 ms /   129 tokens
0.00.795.000 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.077s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.792 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.633 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.637 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.639 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.639 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.639 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.640 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.640 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.641 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.641 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.642 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.642 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.642 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.643 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.643 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.646 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.646 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.647 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.252 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.253 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.253 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.253 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.254 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.254 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.255 I llama_model_loader: - type  f32:  194 tensors
0.00.023.255 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.255 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.035 I llm_load_vocab: special tokens cache size = 25
0.00.049.062 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.065 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.065 I llm_load_print_meta: arch             = gptneox
0.00.049.065 I llm_load_print_meta: vocab type       = BPE
0.00.049.066 I llm_load_print_meta: n_vocab          = 50304
0.00.049.066 I llm_load_print_meta: n_merges         = 50009
0.00.049.066 I llm_load_print_meta: vocab_only       = 0
0.00.049.066 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.066 I llm_load_print_meta: n_embd           = 2048
0.00.049.067 I llm_load_print_meta: n_layer          = 24
0.00.049.069 I llm_load_print_meta: n_head           = 16
0.00.049.070 I llm_load_print_meta: n_head_kv        = 16
0.00.049.070 I llm_load_print_meta: n_rot            = 32
0.00.049.070 I llm_load_print_meta: n_swa            = 0
0.00.049.070 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.071 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.071 I llm_load_print_meta: n_gqa            = 1
0.00.049.072 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.073 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.073 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.074 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.074 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.074 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.074 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.075 I llm_load_print_meta: n_ff             = 8192
0.00.049.075 I llm_load_print_meta: n_expert         = 0
0.00.049.075 I llm_load_print_meta: n_expert_used    = 0
0.00.049.075 I llm_load_print_meta: causal attn      = 1
0.00.049.075 I llm_load_print_meta: pooling type     = 0
0.00.049.075 I llm_load_print_meta: rope type        = 2
0.00.049.076 I llm_load_print_meta: rope scaling     = linear
0.00.049.076 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.079 I llm_load_print_meta: freq_scale_train = 1
0.00.049.079 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.079 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.079 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.079 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.080 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.080 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.080 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.095 I llm_load_print_meta: model type       = 1.4B
0.00.049.096 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.096 I llm_load_print_meta: model params     = 1.41 B
0.00.049.096 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.098 I llm_load_print_meta: general.name     = 1.4B
0.00.049.098 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.098 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.098 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.098 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.099 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.099 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.099 I llm_load_print_meta: max token length = 1024
0.00.050.607 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.607 I llm_load_tensors: offloading output layer to GPU
0.00.050.607 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.617 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.618 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.440 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.440 I llama_new_context_with_model: n_ctx         = 128
0.00.051.440 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.441 I llama_new_context_with_model: n_batch       = 128
0.00.051.441 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.441 I llama_new_context_with_model: flash_attn    = 0
0.00.051.441 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.442 I llama_new_context_with_model: freq_scale    = 1
0.00.051.442 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.442 I ggml_metal_init: allocating
0.00.051.445 I ggml_metal_init: found device: Apple M4
0.00.051.447 I ggml_metal_init: picking default device: Apple M4
0.00.051.974 I ggml_metal_init: using embedded metal library
0.00.053.922 I ggml_metal_init: GPU name:   Apple M4
0.00.053.923 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.924 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.924 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.924 I ggml_metal_init: simdgroup reduction   = true
0.00.053.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.925 I ggml_metal_init: has bfloat            = true
0.00.053.925 I ggml_metal_init: use bfloat            = true
0.00.053.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.971 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.972 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.987 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.867 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.868 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.869 I llama_new_context_with_model: graph nodes  = 967
0.00.063.869 I llama_new_context_with_model: graph splits = 2
0.00.063.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.359 I 
0.00.714.397 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.714.400 I perplexity: tokenizing the input ..
0.00.722.395 I perplexity: tokenization took 7.992 ms
0.00.722.406 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.034 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.858.351 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.858.372 I llama_perf_context_print:        load time =     705.56 ms
0.00.858.373 I llama_perf_context_print: prompt eval time =     134.40 ms /   128 tokens (    1.05 ms per token,   952.39 tokens per second)
0.00.858.374 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.858.375 I llama_perf_context_print:       total time =     144.02 ms /   129 tokens
0.00.858.841 I ggml_metal_free: deallocating

real	0m0.872s
user	0m0.077s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.721 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.077 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.081 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.083 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.085 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.085 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.085 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.088 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.090 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.090 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.090 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.811 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.869 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.549 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.550 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.550 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.551 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.551 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.551 I llama_model_loader: - type  f32:  194 tensors
0.00.023.552 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.552 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.552 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.256 I llm_load_vocab: special tokens cache size = 25
0.00.049.336 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.338 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.339 I llm_load_print_meta: arch             = gptneox
0.00.049.339 I llm_load_print_meta: vocab type       = BPE
0.00.049.339 I llm_load_print_meta: n_vocab          = 50304
0.00.049.339 I llm_load_print_meta: n_merges         = 50009
0.00.049.340 I llm_load_print_meta: vocab_only       = 0
0.00.049.340 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.340 I llm_load_print_meta: n_embd           = 2048
0.00.049.340 I llm_load_print_meta: n_layer          = 24
0.00.049.343 I llm_load_print_meta: n_head           = 16
0.00.049.344 I llm_load_print_meta: n_head_kv        = 16
0.00.049.344 I llm_load_print_meta: n_rot            = 32
0.00.049.344 I llm_load_print_meta: n_swa            = 0
0.00.049.344 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.345 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.345 I llm_load_print_meta: n_gqa            = 1
0.00.049.346 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.347 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.348 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.348 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.348 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.348 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.349 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.350 I llm_load_print_meta: n_ff             = 8192
0.00.049.350 I llm_load_print_meta: n_expert         = 0
0.00.049.350 I llm_load_print_meta: n_expert_used    = 0
0.00.049.350 I llm_load_print_meta: causal attn      = 1
0.00.049.351 I llm_load_print_meta: pooling type     = 0
0.00.049.351 I llm_load_print_meta: rope type        = 2
0.00.049.351 I llm_load_print_meta: rope scaling     = linear
0.00.049.353 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.354 I llm_load_print_meta: freq_scale_train = 1
0.00.049.354 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.354 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.354 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.354 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.354 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.354 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.355 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.366 I llm_load_print_meta: model type       = 1.4B
0.00.049.366 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.367 I llm_load_print_meta: model params     = 1.41 B
0.00.049.367 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.367 I llm_load_print_meta: general.name     = 1.4B
0.00.049.367 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.368 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.368 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.368 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.369 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.369 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.370 I llm_load_print_meta: max token length = 1024
0.00.050.889 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.889 I llm_load_tensors: offloading output layer to GPU
0.00.050.890 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.899 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.900 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.735 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.735 I llama_new_context_with_model: n_ctx         = 128
0.00.051.735 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.736 I llama_new_context_with_model: n_batch       = 128
0.00.051.736 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.736 I llama_new_context_with_model: flash_attn    = 0
0.00.051.736 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.737 I llama_new_context_with_model: freq_scale    = 1
0.00.051.737 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.737 I ggml_metal_init: allocating
0.00.051.740 I ggml_metal_init: found device: Apple M4
0.00.051.742 I ggml_metal_init: picking default device: Apple M4
0.00.052.271 I ggml_metal_init: using embedded metal library
0.00.054.166 I ggml_metal_init: GPU name:   Apple M4
0.00.054.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.168 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.169 I ggml_metal_init: simdgroup reduction   = true
0.00.054.169 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.169 I ggml_metal_init: has bfloat            = true
0.00.054.169 I ggml_metal_init: use bfloat            = true
0.00.054.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.170 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.291 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.293 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.306 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.164 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.165 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.165 I llama_new_context_with_model: graph nodes  = 967
0.00.064.166 I llama_new_context_with_model: graph splits = 2
0.00.064.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.267 I 
0.00.437.302 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.437.308 I perplexity: tokenizing the input ..
0.00.445.334 I perplexity: tokenization took 8.024 ms
0.00.445.348 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.733 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.579.141 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.579.159 I llama_perf_context_print:        load time =     427.54 ms
0.00.579.160 I llama_perf_context_print: prompt eval time =     132.15 ms /   128 tokens (    1.03 ms per token,   968.63 tokens per second)
0.00.579.161 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.162 I llama_perf_context_print:       total time =     141.89 ms /   129 tokens
0.00.579.547 I ggml_metal_free: deallocating

real	0m0.596s
user	0m0.077s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.703 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.527 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.532 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.534 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.535 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.535 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.535 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.536 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.537 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.540 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.540 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.540 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.541 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.543 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.544 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.544 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.053 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.054 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.055 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.056 I llama_model_loader: - type  f32:  194 tensors
0.00.023.056 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.057 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.057 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.057 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.993 I llm_load_vocab: special tokens cache size = 25
0.00.048.969 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.971 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.971 I llm_load_print_meta: arch             = gptneox
0.00.048.972 I llm_load_print_meta: vocab type       = BPE
0.00.048.972 I llm_load_print_meta: n_vocab          = 50304
0.00.048.972 I llm_load_print_meta: n_merges         = 50009
0.00.048.973 I llm_load_print_meta: vocab_only       = 0
0.00.048.973 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.973 I llm_load_print_meta: n_embd           = 2048
0.00.048.973 I llm_load_print_meta: n_layer          = 24
0.00.048.976 I llm_load_print_meta: n_head           = 16
0.00.048.977 I llm_load_print_meta: n_head_kv        = 16
0.00.048.977 I llm_load_print_meta: n_rot            = 32
0.00.048.977 I llm_load_print_meta: n_swa            = 0
0.00.048.977 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.977 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.978 I llm_load_print_meta: n_gqa            = 1
0.00.048.979 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.980 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.980 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.981 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.982 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.982 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.982 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.983 I llm_load_print_meta: n_ff             = 8192
0.00.048.983 I llm_load_print_meta: n_expert         = 0
0.00.048.983 I llm_load_print_meta: n_expert_used    = 0
0.00.048.983 I llm_load_print_meta: causal attn      = 1
0.00.048.983 I llm_load_print_meta: pooling type     = 0
0.00.048.985 I llm_load_print_meta: rope type        = 2
0.00.048.986 I llm_load_print_meta: rope scaling     = linear
0.00.048.986 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.987 I llm_load_print_meta: freq_scale_train = 1
0.00.048.987 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.987 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.987 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.987 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.987 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.988 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.988 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.999 I llm_load_print_meta: model type       = 1.4B
0.00.049.000 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.000 I llm_load_print_meta: model params     = 1.41 B
0.00.049.000 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.000 I llm_load_print_meta: general.name     = 1.4B
0.00.049.001 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.001 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.001 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.001 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.001 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.002 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.002 I llm_load_print_meta: max token length = 1024
0.00.051.181 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.181 I llm_load_tensors: offloading output layer to GPU
0.00.051.182 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.191 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.192 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.196 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.197 I llama_new_context_with_model: n_ctx         = 128
0.00.052.197 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.198 I llama_new_context_with_model: n_batch       = 128
0.00.052.198 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.198 I llama_new_context_with_model: flash_attn    = 0
0.00.052.198 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.199 I llama_new_context_with_model: freq_scale    = 1
0.00.052.199 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.199 I ggml_metal_init: allocating
0.00.052.205 I ggml_metal_init: found device: Apple M4
0.00.052.207 I ggml_metal_init: picking default device: Apple M4
0.00.052.756 I ggml_metal_init: using embedded metal library
0.00.054.699 I ggml_metal_init: GPU name:   Apple M4
0.00.054.700 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.700 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.701 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.702 I ggml_metal_init: simdgroup reduction   = true
0.00.054.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.703 I ggml_metal_init: has bfloat            = true
0.00.054.703 I ggml_metal_init: use bfloat            = true
0.00.054.703 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.704 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.749 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.761 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.784 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.663 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.664 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.665 I llama_new_context_with_model: graph nodes  = 967
0.00.064.665 I llama_new_context_with_model: graph splits = 2
0.00.064.677 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.674 I 
0.00.493.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.493.710 I perplexity: tokenizing the input ..
0.00.501.508 I perplexity: tokenization took 7.796 ms
0.00.501.519 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.633.717 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.635.056 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.635.070 I llama_perf_context_print:        load time =     484.97 ms
0.00.635.071 I llama_perf_context_print: prompt eval time =     131.97 ms /   128 tokens (    1.03 ms per token,   969.92 tokens per second)
0.00.635.072 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.635.072 I llama_perf_context_print:       total time =     141.40 ms /   129 tokens
0.00.635.567 I ggml_metal_free: deallocating

real	0m0.649s
user	0m0.077s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.216 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.871 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.876 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.878 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.878 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.879 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.879 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.880 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.880 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.880 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.881 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.883 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.606 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.641 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.449 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.450 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.450 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.450 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.451 I llama_model_loader: - type  f32:  194 tensors
0.00.024.451 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.451 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.451 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.233 I llm_load_vocab: special tokens cache size = 25
0.00.050.198 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.200 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.201 I llm_load_print_meta: arch             = gptneox
0.00.050.201 I llm_load_print_meta: vocab type       = BPE
0.00.050.201 I llm_load_print_meta: n_vocab          = 50304
0.00.050.201 I llm_load_print_meta: n_merges         = 50009
0.00.050.201 I llm_load_print_meta: vocab_only       = 0
0.00.050.202 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.202 I llm_load_print_meta: n_embd           = 2048
0.00.050.202 I llm_load_print_meta: n_layer          = 24
0.00.050.205 I llm_load_print_meta: n_head           = 16
0.00.050.205 I llm_load_print_meta: n_head_kv        = 16
0.00.050.206 I llm_load_print_meta: n_rot            = 32
0.00.050.206 I llm_load_print_meta: n_swa            = 0
0.00.050.206 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.206 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.207 I llm_load_print_meta: n_gqa            = 1
0.00.050.207 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.211 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.211 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.211 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.212 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.212 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.212 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.213 I llm_load_print_meta: n_ff             = 8192
0.00.050.214 I llm_load_print_meta: n_expert         = 0
0.00.050.214 I llm_load_print_meta: n_expert_used    = 0
0.00.050.214 I llm_load_print_meta: causal attn      = 1
0.00.050.214 I llm_load_print_meta: pooling type     = 0
0.00.050.214 I llm_load_print_meta: rope type        = 2
0.00.050.215 I llm_load_print_meta: rope scaling     = linear
0.00.050.215 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.215 I llm_load_print_meta: freq_scale_train = 1
0.00.050.216 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.216 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.216 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.216 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.216 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.218 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.218 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.229 I llm_load_print_meta: model type       = 1.4B
0.00.050.230 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.230 I llm_load_print_meta: model params     = 1.41 B
0.00.050.230 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.231 I llm_load_print_meta: general.name     = 1.4B
0.00.050.231 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.231 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.231 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.231 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.232 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.232 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.232 I llm_load_print_meta: max token length = 1024
0.00.052.153 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.153 I llm_load_tensors: offloading output layer to GPU
0.00.052.154 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.164 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.165 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.073 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.073 I llama_new_context_with_model: n_ctx         = 128
0.00.053.073 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.073 I llama_new_context_with_model: n_batch       = 128
0.00.053.074 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.074 I llama_new_context_with_model: flash_attn    = 0
0.00.053.074 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.075 I llama_new_context_with_model: freq_scale    = 1
0.00.053.075 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.075 I ggml_metal_init: allocating
0.00.053.081 I ggml_metal_init: found device: Apple M4
0.00.053.083 I ggml_metal_init: picking default device: Apple M4
0.00.053.617 I ggml_metal_init: using embedded metal library
0.00.055.609 I ggml_metal_init: GPU name:   Apple M4
0.00.055.610 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.611 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.611 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.611 I ggml_metal_init: simdgroup reduction   = true
0.00.055.611 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.611 I ggml_metal_init: has bfloat            = true
0.00.055.612 I ggml_metal_init: use bfloat            = true
0.00.055.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.613 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.535 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.538 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.552 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.417 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.418 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.419 I llama_new_context_with_model: graph nodes  = 967
0.00.065.419 I llama_new_context_with_model: graph splits = 2
0.00.065.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.471 I 
0.00.585.538 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.585.548 I perplexity: tokenizing the input ..
0.00.593.640 I perplexity: tokenization took 8.09 ms
0.00.593.653 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.728.030 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.729.370 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.729.388 I llama_perf_context_print:        load time =     575.25 ms
0.00.729.390 I llama_perf_context_print: prompt eval time =     134.15 ms /   128 tokens (    1.05 ms per token,   954.16 tokens per second)
0.00.729.390 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.729.391 I llama_perf_context_print:       total time =     143.92 ms /   129 tokens
0.00.729.777 I ggml_metal_free: deallocating

real	0m0.746s
user	0m0.077s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.813 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.644 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.649 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.650 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.651 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.651 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.651 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.652 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.653 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.653 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.654 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.654 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.654 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.655 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.658 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.520 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.607 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.451 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.452 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.453 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.453 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.453 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.454 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.454 I llama_model_loader: - type  f32:  194 tensors
0.00.023.455 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.455 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.206 I llm_load_vocab: special tokens cache size = 25
0.00.050.332 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.335 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.335 I llm_load_print_meta: arch             = gptneox
0.00.050.335 I llm_load_print_meta: vocab type       = BPE
0.00.050.336 I llm_load_print_meta: n_vocab          = 50304
0.00.050.336 I llm_load_print_meta: n_merges         = 50009
0.00.050.336 I llm_load_print_meta: vocab_only       = 0
0.00.050.336 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.336 I llm_load_print_meta: n_embd           = 2048
0.00.050.336 I llm_load_print_meta: n_layer          = 24
0.00.050.339 I llm_load_print_meta: n_head           = 16
0.00.050.340 I llm_load_print_meta: n_head_kv        = 16
0.00.050.340 I llm_load_print_meta: n_rot            = 32
0.00.050.340 I llm_load_print_meta: n_swa            = 0
0.00.050.340 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.341 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.341 I llm_load_print_meta: n_gqa            = 1
0.00.050.342 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.343 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.344 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.345 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.345 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.345 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.345 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.346 I llm_load_print_meta: n_ff             = 8192
0.00.050.346 I llm_load_print_meta: n_expert         = 0
0.00.050.346 I llm_load_print_meta: n_expert_used    = 0
0.00.050.347 I llm_load_print_meta: causal attn      = 1
0.00.050.347 I llm_load_print_meta: pooling type     = 0
0.00.050.349 I llm_load_print_meta: rope type        = 2
0.00.050.349 I llm_load_print_meta: rope scaling     = linear
0.00.050.349 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.349 I llm_load_print_meta: freq_scale_train = 1
0.00.050.350 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.350 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.350 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.350 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.350 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.350 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.350 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.362 I llm_load_print_meta: model type       = 1.4B
0.00.050.363 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.363 I llm_load_print_meta: model params     = 1.41 B
0.00.050.364 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.364 I llm_load_print_meta: general.name     = 1.4B
0.00.050.364 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.364 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.367 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.367 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.367 I llm_load_print_meta: max token length = 1024
0.00.052.388 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.388 I llm_load_tensors: offloading output layer to GPU
0.00.052.388 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.398 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.399 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.296 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.296 I llama_new_context_with_model: n_ctx         = 128
0.00.053.297 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.297 I llama_new_context_with_model: n_batch       = 128
0.00.053.297 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.297 I llama_new_context_with_model: flash_attn    = 0
0.00.053.297 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.298 I llama_new_context_with_model: freq_scale    = 1
0.00.053.298 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.298 I ggml_metal_init: allocating
0.00.053.301 I ggml_metal_init: found device: Apple M4
0.00.053.303 I ggml_metal_init: picking default device: Apple M4
0.00.053.842 I ggml_metal_init: using embedded metal library
0.00.055.771 I ggml_metal_init: GPU name:   Apple M4
0.00.055.772 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.773 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.773 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.773 I ggml_metal_init: simdgroup reduction   = true
0.00.055.773 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.774 I ggml_metal_init: has bfloat            = true
0.00.055.774 I ggml_metal_init: use bfloat            = true
0.00.055.774 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.978 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.983 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.999 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.887 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.888 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.888 I llama_new_context_with_model: graph nodes  = 967
0.00.065.889 I llama_new_context_with_model: graph splits = 2
0.00.065.900 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.154 I 
0.00.663.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.663.209 I perplexity: tokenizing the input ..
0.00.671.324 I perplexity: tokenization took 8.113 ms
0.00.671.338 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.936 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.813.265 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.813.284 I llama_perf_context_print:        load time =     654.33 ms
0.00.813.285 I llama_perf_context_print: prompt eval time =     140.37 ms /   128 tokens (    1.10 ms per token,   911.88 tokens per second)
0.00.813.286 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.286 I llama_perf_context_print:       total time =     150.14 ms /   129 tokens
0.00.813.690 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.079s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.893 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.504 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.509 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.514 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.514 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.514 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.515 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.516 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.516 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.518 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.519 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.519 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.520 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.521 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.521 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.322 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.349 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.090 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.091 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.091 I llama_model_loader: - type  f32:  194 tensors
0.00.024.092 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.756 I llm_load_vocab: special tokens cache size = 25
0.00.049.720 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.722 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.723 I llm_load_print_meta: arch             = gptneox
0.00.049.723 I llm_load_print_meta: vocab type       = BPE
0.00.049.723 I llm_load_print_meta: n_vocab          = 50304
0.00.049.724 I llm_load_print_meta: n_merges         = 50009
0.00.049.724 I llm_load_print_meta: vocab_only       = 0
0.00.049.724 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.724 I llm_load_print_meta: n_embd           = 2048
0.00.049.724 I llm_load_print_meta: n_layer          = 24
0.00.049.727 I llm_load_print_meta: n_head           = 16
0.00.049.728 I llm_load_print_meta: n_head_kv        = 16
0.00.049.728 I llm_load_print_meta: n_rot            = 32
0.00.049.728 I llm_load_print_meta: n_swa            = 0
0.00.049.728 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.729 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.729 I llm_load_print_meta: n_gqa            = 1
0.00.049.730 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.731 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.731 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.731 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.732 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.732 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.732 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.733 I llm_load_print_meta: n_ff             = 8192
0.00.049.733 I llm_load_print_meta: n_expert         = 0
0.00.049.733 I llm_load_print_meta: n_expert_used    = 0
0.00.049.733 I llm_load_print_meta: causal attn      = 1
0.00.049.733 I llm_load_print_meta: pooling type     = 0
0.00.049.733 I llm_load_print_meta: rope type        = 2
0.00.049.734 I llm_load_print_meta: rope scaling     = linear
0.00.049.734 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.735 I llm_load_print_meta: freq_scale_train = 1
0.00.049.737 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.737 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.737 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.737 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.737 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.737 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.738 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.749 I llm_load_print_meta: model type       = 1.4B
0.00.049.749 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.749 I llm_load_print_meta: model params     = 1.41 B
0.00.049.750 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.751 I llm_load_print_meta: general.name     = 1.4B
0.00.049.753 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.753 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.753 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.753 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.753 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.754 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.754 I llm_load_print_meta: max token length = 1024
0.00.051.295 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.295 I llm_load_tensors: offloading output layer to GPU
0.00.051.296 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.305 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.306 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.161 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.162 I llama_new_context_with_model: n_ctx         = 128
0.00.052.162 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.162 I llama_new_context_with_model: n_batch       = 128
0.00.052.162 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.162 I llama_new_context_with_model: flash_attn    = 0
0.00.052.163 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.163 I llama_new_context_with_model: freq_scale    = 1
0.00.052.163 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.164 I ggml_metal_init: allocating
0.00.052.170 I ggml_metal_init: found device: Apple M4
0.00.052.172 I ggml_metal_init: picking default device: Apple M4
0.00.052.716 I ggml_metal_init: using embedded metal library
0.00.054.631 I ggml_metal_init: GPU name:   Apple M4
0.00.054.633 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.633 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.633 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.634 I ggml_metal_init: simdgroup reduction   = true
0.00.054.634 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.634 I ggml_metal_init: has bfloat            = true
0.00.054.634 I ggml_metal_init: use bfloat            = true
0.00.054.635 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.767 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.771 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.787 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.673 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.674 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.674 I llama_new_context_with_model: graph nodes  = 967
0.00.064.674 I llama_new_context_with_model: graph splits = 2
0.00.064.687 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.220.853 I 
0.00.220.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.220.880 I perplexity: tokenizing the input ..
0.00.228.405 I perplexity: tokenization took 7.524 ms
0.00.228.419 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.369.157 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.370.562 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.370.579 I llama_perf_context_print:        load time =     210.96 ms
0.00.370.580 I llama_perf_context_print: prompt eval time =     140.44 ms /   128 tokens (    1.10 ms per token,   911.41 tokens per second)
0.00.370.581 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.370.581 I llama_perf_context_print:       total time =     149.73 ms /   129 tokens
0.00.370.986 I ggml_metal_free: deallocating

real	0m0.387s
user	0m0.077s
sys	0m0.052s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.255 I build: 4218 (938f6087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.885 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.877 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.884 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.886 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.887 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.890 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.891 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.892 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.993 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.607 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.610 I llama_model_loader: - type  f32:  194 tensors
0.00.053.610 I llama_model_loader: - type  f16:   98 tensors
0.00.080.889 I llm_load_vocab: special tokens cache size = 25
0.00.087.597 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.600 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.600 I llm_load_print_meta: arch             = gptneox
0.00.087.600 I llm_load_print_meta: vocab type       = BPE
0.00.087.600 I llm_load_print_meta: n_vocab          = 50304
0.00.087.601 I llm_load_print_meta: n_merges         = 50009
0.00.087.601 I llm_load_print_meta: vocab_only       = 0
0.00.087.601 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.601 I llm_load_print_meta: n_embd           = 2048
0.00.087.601 I llm_load_print_meta: n_layer          = 24
0.00.087.604 I llm_load_print_meta: n_head           = 16
0.00.087.605 I llm_load_print_meta: n_head_kv        = 16
0.00.087.605 I llm_load_print_meta: n_rot            = 32
0.00.087.605 I llm_load_print_meta: n_swa            = 0
0.00.087.605 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.605 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.606 I llm_load_print_meta: n_gqa            = 1
0.00.087.606 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.607 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.607 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.608 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.608 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.608 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.608 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.609 I llm_load_print_meta: n_ff             = 8192
0.00.087.610 I llm_load_print_meta: n_expert         = 0
0.00.087.610 I llm_load_print_meta: n_expert_used    = 0
0.00.087.610 I llm_load_print_meta: causal attn      = 1
0.00.087.610 I llm_load_print_meta: pooling type     = 0
0.00.087.610 I llm_load_print_meta: rope type        = 2
0.00.087.610 I llm_load_print_meta: rope scaling     = linear
0.00.087.611 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.611 I llm_load_print_meta: freq_scale_train = 1
0.00.087.611 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.613 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.613 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.613 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.613 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.613 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.614 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.624 I llm_load_print_meta: model type       = 1.4B
0.00.087.625 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.625 I llm_load_print_meta: model params     = 1.41 B
0.00.087.626 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.626 I llm_load_print_meta: general.name     = 1.4B
0.00.087.626 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.626 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.626 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.627 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.627 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.627 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.627 I llm_load_print_meta: max token length = 1024
0.00.089.252 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.253 I llm_load_tensors: offloading output layer to GPU
0.00.089.253 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.262 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.263 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.151 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.152 I llama_new_context_with_model: n_ctx         = 128
0.00.090.152 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.152 I llama_new_context_with_model: n_batch       = 128
0.00.090.152 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.152 I llama_new_context_with_model: flash_attn    = 0
0.00.090.153 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.153 I llama_new_context_with_model: freq_scale    = 1
0.00.090.153 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.154 I ggml_metal_init: allocating
0.00.090.156 I ggml_metal_init: found device: Apple M4
0.00.090.158 I ggml_metal_init: picking default device: Apple M4
0.00.090.701 I ggml_metal_init: using embedded metal library
0.00.092.769 I ggml_metal_init: GPU name:   Apple M4
0.00.092.773 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.773 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.774 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.774 I ggml_metal_init: simdgroup reduction   = true
0.00.092.774 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.774 I ggml_metal_init: has bfloat            = true
0.00.092.774 I ggml_metal_init: use bfloat            = true
0.00.092.775 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.295 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.297 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.310 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.154 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.155 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.155 I llama_new_context_with_model: graph nodes  = 967
0.00.102.156 I llama_new_context_with_model: graph splits = 2
0.00.102.167 I 
0.00.102.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.102.207 I compute_imatrix: tokenizing the input ..
0.00.108.872 I compute_imatrix: tokenization took 6.665 ms
0.00.108.874 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.596.440 I compute_imatrix: 1.49 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.599.298 I llama_perf_context_print:        load time =    1575.56 ms
0.01.599.300 I llama_perf_context_print: prompt eval time =    1487.07 ms /   128 tokens (   11.62 ms per token,    86.08 tokens per second)
0.01.599.301 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.599.302 I llama_perf_context_print:       total time =    1578.40 ms /   129 tokens
0.01.600.283 I ggml_metal_free: deallocating

real	0m1.785s
user	0m0.170s
sys	0m0.242s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4218 (938f6087)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13720b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13720b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13720be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13720c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13720c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13720cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13720d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13720da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13720e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13720e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13720ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13720ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13720fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1372101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137210a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137211120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137211840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137211f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137212680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137212e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137213570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137213c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1372143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137214c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137215370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137215630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137215c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1372168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137216df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1372170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137217550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137217810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1372180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1372185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1372188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137218d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1372191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137219680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137219b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137219fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13721a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13721a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13721ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13721b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13721b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13721bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13721c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13721ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13721d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13721d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13721dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13721e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13721e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13721eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13721f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13721fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13721ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137220290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1372208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137221090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137221350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1372217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137221c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137222130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1372225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137222a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137222f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1372233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137223850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137223cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137224190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137224630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137224ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137224f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137225410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1372258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137225d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1372261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137226690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137226b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137226fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137227470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137227910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137227db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137228250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1372286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137228b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137229030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1372294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137229970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137229e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13722a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13722a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13722abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13722b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13722b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13722b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13721c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13722c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13722c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13722c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13722ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13722d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13722d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13722dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13722e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13722e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13722e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13722ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13722f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13722f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13722fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1372300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137230580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137230a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137230ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137231360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137231800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137231ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137232140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1372325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137232a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137232f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1372333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137233860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137233d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1372341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137234640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137234ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137234f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137235420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1372358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137235d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137236200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1372366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137236b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137236fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137237480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137237920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137237dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137238260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137238700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137238ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137239040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1372394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137239980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137239e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13723a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13723a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13723ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13723b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13723b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13723b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13723bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13723c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13723c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13723cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13723d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13723d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13723de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13723e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13723ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13723f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13723f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13723fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137240160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137240600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137240db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137241300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137241850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137241da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1372422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137242840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137242d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1372432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137243830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137243d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1372442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137244820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137244d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1372452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137245810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137245d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1372462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137246800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137246d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1372472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1372477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137247d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137248290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1372487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137248d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137249280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1372497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137249d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13724a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13724a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13724ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13724b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13724b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13724bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13724c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13724c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13724ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13724d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13724d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13724dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13724e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13724e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13724ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13724f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13724f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13724fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137250210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137250760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137250cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137251200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137251750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137251ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1372521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137252740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137252c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1372531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137253730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137253bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137254070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137254510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1372549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137254e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1372552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137255790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137255c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1372560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137256570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137256a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137256eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137257350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1372578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137257fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1372586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137258e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137259520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1372597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137259df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13725a400 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.133.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137305530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1373059a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137305e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137306280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1373066f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137306b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137306fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137307440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1373078b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137307d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137308190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137308870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137309390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137309b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13730a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13730aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13730b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13730b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13730bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13730c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13730cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13730d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13730dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13730e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13730eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13730ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13730f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13730f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13730f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13730fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137310280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1373107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137310c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137310ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137311350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1373117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137311c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1373120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137312510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137312980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137312df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137313260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1373136d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137313b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137313fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137314420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137314890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137314d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137315170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1373155e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137315a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137315ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137316330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1373167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137316c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137317080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1373175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137317af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137317f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1373183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137318840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137318cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137319120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137319590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137319a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137319e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13731a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13731a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13731abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13731b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13731b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13731b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13731bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13731c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13731c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13731cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13731cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13731d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13731d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13731dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13731e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13731e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13731e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13731ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13731f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13731f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13731fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137320010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137320480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1373208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137320d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1373211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137321640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137321ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137321f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137322390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137322800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137322c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1373230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137323550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1373239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137323e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1373242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137324710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137324b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137324ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137325460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1373258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137325d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1373261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137326620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137326a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137326f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137327370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1373277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137327c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1373280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137328530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1373289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137328e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137329280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1373296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137329b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137329fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13732a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13732a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13732ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13732b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13732b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13732ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13732bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13732c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13732c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13732cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13732d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13732d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13732d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13732ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13732e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13732e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13732eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13732efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13732f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13732f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13732fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137330170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1373305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137330a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137330ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137331330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1373317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137331c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137332080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1373324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137332960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137332dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137333240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1373336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137333b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137333f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137334400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137334870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137334ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137335150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1373355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137335a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137335ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137336a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137336cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137336fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137337420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137337890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137337d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137338170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1373385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137338a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137338ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137339330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1373397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137339c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13733a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13733a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13733a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13733add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13733b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13733b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13733bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13733bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13733c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13733c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13733cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13733d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13733d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13733da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13733dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13733e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13733e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13733ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13733f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13733f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13733f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13733fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137340220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137340690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137340b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137340f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1373413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137341850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137341cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137342130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1373425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137342a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137342e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1373432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137343760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137343bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137344040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1373444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137344920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137344d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137345200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137345670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137345ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137345f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1373463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137346830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137346ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137347110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137347580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1373479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137347e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1373482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137348740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137348bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137349020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137349490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137349900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137349d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13734a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13734afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13734b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13734be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13734c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13734c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13734c800 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137104d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1371051e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137105650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137105ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137105f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1371063a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137106810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137106c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1371070f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137107560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1371079d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137108090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137108bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137109360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137109b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13710a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13710a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13710b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13710b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13710bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13710c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13710ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13710d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13710dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13710e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13710e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13710e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13710ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13710f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13710f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13710faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13710ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137110440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137110700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137110b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137110fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137111450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1371118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137111d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1371121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137112610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137112a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137112ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137113360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1371137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137113c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1371140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137114520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137114990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137114e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137115270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1371156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137115b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137115fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137116430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1371168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137116e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137117310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137117780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137117bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137118060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1371184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137118940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137118db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137119220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137119690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137119b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137119f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13711a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13711a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13711acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13711b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13711b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13711ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13711be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13711c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13711c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13711cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13711d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13711d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13711d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13711dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13711e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13711e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13711eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13711ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13711f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13711f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13711fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137120110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137120580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1371209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137120e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1371212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137121740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137121bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137122020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137122490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137122900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137122d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1371231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137123650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137123ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137123f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1371243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137124810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137124c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1371250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137125560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1371259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137125e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1371262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137126720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137126b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137127000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137127470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1371278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137127d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1371281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137128630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137128aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137128f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137129380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1371297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137129c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13712a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13712a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13712a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13712ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13712b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13712b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13712bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13712bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13712c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13712c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13712cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13712d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13712d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13712da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13712def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13712e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13712e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13712ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13712f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13712f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13712f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13712fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137130270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1371306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137130b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137130fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137131430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1371318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137131d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137132180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1371325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137132a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137132ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137133340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1371337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137133c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137134090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137134500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137134970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137134de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137135250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1371356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137136250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137136510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1371367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137136c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1371370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137137520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137137990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137137e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137138270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1371386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137138b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137138fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137139430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1371398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137139d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13713a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13713a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13713aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13713aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13713b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13713b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13713bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13713c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13713c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13713c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13713cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13713d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13713d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13713db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13713dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13713e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13713e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13713ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13713f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13713f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13713fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13713feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137140320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137140790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137140c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137141070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1371414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137141950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137141dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137142230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1371426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137142b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137142f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1371433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137143860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137143cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137144140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1371445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137144a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137144e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137145300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137145770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137145be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137146050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1371464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137146930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137146da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137147210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137147680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137147af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137147f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1371483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137148840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137148cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137149120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137149590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13714a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13714a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13714af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13714b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13714b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13714bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13714c020 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.713s
user	0m0.285s
sys	0m0.296s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4218 (938f6087)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b710240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b710970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b710f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b7114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b711a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b712030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b7125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b712b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b713140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b713640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b713b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b714040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b714b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b715310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b715b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b716240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b716960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b717080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b7177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b717f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b718690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b718db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b7194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b719d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b71a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b71a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b71ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b71b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b71bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b71c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b71c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b71c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b71d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b71d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b71d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b71de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b71e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b71e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b71ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b71f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b71f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b71fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b71fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b720360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b720620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b720c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b721240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b721b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b722170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b722780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b722d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b7233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b7239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b723fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b7247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b724c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b7250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b7253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b7259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b7261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b726470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b726910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b726db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b727250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b7276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b727b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b728030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b7284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b728970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b728e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b7292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b729750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b729bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b72a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b72a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b72a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b72ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b72b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b72b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b72bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b72c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b72c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b72ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b72ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b72d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b72d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b72dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b72e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b72e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b72ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b72ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b72f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b72f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b72fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b7301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b730650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b730af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b721850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b731140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b7315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b731a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b731f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b7323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b732860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b732d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b7331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b733640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b733ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b734420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b7348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b734d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b735200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b7356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b735b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b735fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b736480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b736920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b736dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b737260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b737700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b737ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b738040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b7384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b738980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b738e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b7392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b739760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b739c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b73a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b73a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b73a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b73ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b73b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b73b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b73bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b73c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b73c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b73ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b73cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b73d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b73d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b73dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b73e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b73e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b73eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b73ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b73f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b73f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b73fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b7401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b740660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b740b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b741050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b7415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b741af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b742040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b742300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b742910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b742f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b743530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b744150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b744940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b744de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b745280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b745720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b745ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b746420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b746970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b746ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b747410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b747960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b747eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b748400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b748950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b748ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b7493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b749940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b749e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b74a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b74a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b74ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b74b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b74b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b74be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b74c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b74c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b74ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b74d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b74d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b74de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b74e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b74e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b74ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b74f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b74f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b74fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b750380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b7508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b750e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b751370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b7518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b751e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b752360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b7528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b752e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b753350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b7538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b753df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b754340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b754890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b754de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b755330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b755880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b755dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b756320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b756870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b756dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b757310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b757860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b757db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b758300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b758850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b758cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b759190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b759630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b759ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b759f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b75a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b75a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b75ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b75b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b75b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b75bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b75bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b75c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b75c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b75d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b75d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b75df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b75e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b75e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b75ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b75f520 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.083.081 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128904bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128905040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1289054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128905920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128905d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128906200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128906670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128906ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128906f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1289073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128907830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128907f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128908a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1289091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128909a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12890a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12890a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12890af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12890b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12890bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12890c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12890cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12890d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12890da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12890e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12890e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12890e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12890eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12890efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12890f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12890f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12890fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128910230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1289104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128910960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128910dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128911240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1289116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128911b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128911f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128912400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128912870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128912ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128913150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1289135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128913a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128913ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128914310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128914780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128914bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128915060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1289154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128915940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128915db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128916220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128916690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128916c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128917100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128917570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1289179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128917e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1289182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128918730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128918ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128919010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x128919480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1289198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128919d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12891a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12891a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12891aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12891af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12891b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12891b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12891bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12891c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12891c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12891c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12891ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12891d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12891d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12891db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12891dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12891e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12891e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12891ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12891f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12891f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12891fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12891ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128920370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1289207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128920c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1289210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128921530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1289219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128921e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128922280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1289226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128922b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128922fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128923440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1289238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128923d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128924190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128924600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128924a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128924ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128925350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1289257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128925c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1289260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128926510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128926980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128926df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128927260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1289276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128927b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128927fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128928420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128928890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128928d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128929170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1289295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128929a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128929ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12892a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12892a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12892ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12892b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12892b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12892b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12892bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12892c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12892c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12892cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12892cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12892d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12892d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12892dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12892e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12892e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12892ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12892eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12892f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12892f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12892fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x128930060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1289304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128930940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128930db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128931220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128931690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128931b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128931f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1289323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128932850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128932cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128933130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1289335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128933a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128933e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1289342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128934760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128934bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128935040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1289354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128936040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128936300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1289365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128936a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128936ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128937310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128937780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128937bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128938060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1289384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x128938940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128938db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128939220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128939690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128939b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128939f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12893a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12893a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12893acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12893b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12893b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12893ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12893be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12893c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12893c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12893cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12893d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12893d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12893d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12893dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12893e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12893e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12893eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12893ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12893f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12893f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12893fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128940110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x128940580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1289409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128940e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1289412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128941740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128941bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128942020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128942490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128942900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128942d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1289431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128943650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128943ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128943f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1289443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128944810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128944c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1289450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128945560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1289459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128945e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1289462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128946720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128946b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128947000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128947470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1289478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x128947d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1289481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x128948630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128948aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128948f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128949380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x128949ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12894a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12894ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12894b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12894b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12894b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12894be10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128904ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128904f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1289053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128905830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128905ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128906110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128906580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1289069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128906e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1289072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128907740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128907d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128908610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128908d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128909570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128909c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12890a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12890aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12890b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12890bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12890c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12890c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12890cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12890d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12890dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12890e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12890e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12890eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12890ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12890f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12890f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12890fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1289100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1289103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128910810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128910c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1289110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128911560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1289119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128911e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1289122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128912720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128912b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128913000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128913470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1289138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128913d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1289141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128914630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128914aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128914f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128915380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1289157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128915c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1289160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128916540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1289169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128916e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128917290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128917700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128917b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128917fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128918450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1289188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128918d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1289191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128919610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128919a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x128919ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12891a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12891a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12891ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12891b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12891b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12891b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12891be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12891c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12891c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12891cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12891cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12891d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12891d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12891dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12891e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12891e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12891ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12891eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12891f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12891f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12891fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x128920090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x128920500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x128920970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x128920de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x128921250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1289216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x128921b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128921fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128922410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x128922880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128922cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128923160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1289235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128923a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128923eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128924320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128924790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128924c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128925070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1289254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128925950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128925dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128926230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1289266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128926b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128926f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1289273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128927860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128927cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128928140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1289285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128928a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128928e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128929300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128929770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128929be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12892a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12892a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12892a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12892ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12892b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12892b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12892baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12892bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12892c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12892c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12892ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12892d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12892d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12892da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12892de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12892e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12892e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12892ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12892f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12892f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12892f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12892fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1289301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x128930660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x128930ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128930f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1289313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128931820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128931c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x128932100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128932570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1289329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128932e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1289332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128933730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128933ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128934010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128934480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1289348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128934d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1289351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128935950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128935dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128936230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1289366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128936b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128936f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1289373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128937860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128937cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128938140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1289385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128938a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128938e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128939300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128939770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128939be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12893a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12893a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12893a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12893ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12893b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12893b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12893baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12893bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12893c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12893c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12893ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12893d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12893d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12893da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12893de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12893e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12893e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12893ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12893f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12893f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12893f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12893fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1289401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128940660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128940ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128940f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1289413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128941820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128941c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128942100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128942570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1289429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128942e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1289432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128943730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128943ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128944010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128944480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1289448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128944d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1289451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128945640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128945ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128945f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128946390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128946800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128946c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1289470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128947550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1289479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x128947e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1289482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x128948710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x128948b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x128948ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1289496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x128949dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12894a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12894abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12894b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12894b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12894b900 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.914s
user	0m0.240s
sys	0m0.134s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
