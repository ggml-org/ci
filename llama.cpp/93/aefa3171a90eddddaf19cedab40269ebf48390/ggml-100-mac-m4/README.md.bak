### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.80 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.69 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.46 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.00 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.21 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.41 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.47 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.60 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.88 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.69 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.35 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 223.53 sec*proc (28 tests)

Total Test time (real) = 223.54 sec

real	3m43.643s
user	7m35.421s
sys	0m6.256s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.94 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.33 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.74 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.38 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.28 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.06 sec*proc (28 tests)

Total Test time (real) =  52.08 sec

real	0m52.089s
user	1m12.208s
sys	0m5.733s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.132 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.196 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.218 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.226 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.229 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.231 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.231 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.232 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.233 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.234 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.235 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.241 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.242 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.243 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.246 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.247 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.248 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.249 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.249 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.250 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.251 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.532 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.672 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.675 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.675 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.676 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.677 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.026.677 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.677 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.026.678 I llama_model_loader: - type  f32:  124 tensors
0.00.026.679 I llama_model_loader: - type  f16:   73 tensors
0.00.031.477 I llm_load_vocab: special tokens cache size = 5
0.00.033.754 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.033.759 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.033.759 I llm_load_print_meta: arch             = bert
0.00.033.760 I llm_load_print_meta: vocab type       = WPM
0.00.033.760 I llm_load_print_meta: n_vocab          = 30522
0.00.033.761 I llm_load_print_meta: n_merges         = 0
0.00.033.761 I llm_load_print_meta: vocab_only       = 0
0.00.033.761 I llm_load_print_meta: n_ctx_train      = 512
0.00.033.762 I llm_load_print_meta: n_embd           = 384
0.00.033.762 I llm_load_print_meta: n_layer          = 12
0.00.033.766 I llm_load_print_meta: n_head           = 12
0.00.033.768 I llm_load_print_meta: n_head_kv        = 12
0.00.033.768 I llm_load_print_meta: n_rot            = 32
0.00.033.768 I llm_load_print_meta: n_swa            = 0
0.00.033.769 I llm_load_print_meta: n_embd_head_k    = 32
0.00.033.769 I llm_load_print_meta: n_embd_head_v    = 32
0.00.033.773 I llm_load_print_meta: n_gqa            = 1
0.00.033.774 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.033.775 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.033.776 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.033.776 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.033.777 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.033.780 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.033.780 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.033.782 I llm_load_print_meta: n_ff             = 1536
0.00.033.782 I llm_load_print_meta: n_expert         = 0
0.00.033.782 I llm_load_print_meta: n_expert_used    = 0
0.00.033.782 I llm_load_print_meta: causal attn      = 0
0.00.033.783 I llm_load_print_meta: pooling type     = 2
0.00.033.794 I llm_load_print_meta: rope type        = 2
0.00.033.796 I llm_load_print_meta: rope scaling     = linear
0.00.033.797 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.033.798 I llm_load_print_meta: freq_scale_train = 1
0.00.033.801 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.033.801 I llm_load_print_meta: rope_finetuned   = unknown
0.00.033.802 I llm_load_print_meta: ssm_d_conv       = 0
0.00.033.802 I llm_load_print_meta: ssm_d_inner      = 0
0.00.033.802 I llm_load_print_meta: ssm_d_state      = 0
0.00.033.802 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.033.802 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.033.803 I llm_load_print_meta: model type       = 33M
0.00.033.817 I llm_load_print_meta: model ftype      = F16
0.00.033.818 I llm_load_print_meta: model params     = 33.21 M
0.00.033.819 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.033.822 I llm_load_print_meta: general.name     = Bge Small
0.00.033.822 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.033.823 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.033.823 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.033.823 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.033.824 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.033.824 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.033.824 I llm_load_print_meta: max token length = 21
0.00.036.025 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.036.025 I llm_load_tensors: offloading output layer to GPU
0.00.036.026 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.036.053 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.054 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.036.684 I llama_new_context_with_model: n_seq_max     = 1
0.00.036.686 I llama_new_context_with_model: n_ctx         = 512
0.00.036.686 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.036.686 I llama_new_context_with_model: n_batch       = 2048
0.00.036.687 I llama_new_context_with_model: n_ubatch      = 2048
0.00.036.687 I llama_new_context_with_model: flash_attn    = 0
0.00.036.687 I llama_new_context_with_model: freq_base     = 10000.0
0.00.036.688 I llama_new_context_with_model: freq_scale    = 1
0.00.036.689 I ggml_metal_init: allocating
0.00.036.694 I ggml_metal_init: found device: Apple M4
0.00.036.697 I ggml_metal_init: picking default device: Apple M4
0.00.037.590 I ggml_metal_init: using embedded metal library
0.00.041.720 I ggml_metal_init: GPU name:   Apple M4
0.00.041.723 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.724 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.724 I ggml_metal_init: simdgroup reduction   = true
0.00.041.725 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.725 I ggml_metal_init: has bfloat            = true
0.00.041.725 I ggml_metal_init: use bfloat            = true
0.00.041.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.727 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.893 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.054.465 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.054.467 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.469 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.055.237 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.055.239 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.055.239 I llama_new_context_with_model: graph nodes  = 429
0.00.055.239 I llama_new_context_with_model: graph splits = 2
0.00.055.260 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.055.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.323 I 
0.00.061.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.060 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.066.505 I llama_perf_context_print:        load time =      42.12 ms
0.00.066.506 I llama_perf_context_print: prompt eval time =       4.31 ms /     9 tokens (    0.48 ms per token,  2087.68 tokens per second)
0.00.066.507 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.066.507 I llama_perf_context_print:       total time =       5.18 ms /    10 tokens
0.00.066.706 I ggml_metal_free: deallocating

real	0m0.247s
user	0m0.047s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.941 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.010.705 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.010.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.010.710 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.010.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.010.711 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.010.711 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.010.711 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.010.712 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.010.713 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.010.713 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.010.713 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.010.714 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.010.716 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.010.716 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.010.717 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.010.717 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.010.717 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.010.718 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.010.718 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.012.955 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.561 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.562 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.562 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.562 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.563 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.013.563 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.563 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.013.564 I llama_model_loader: - type  f32:  124 tensors
0.00.013.564 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.851 I llm_load_vocab: special tokens cache size = 5
0.00.017.112 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.114 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.115 I llm_load_print_meta: arch             = bert
0.00.017.115 I llm_load_print_meta: vocab type       = WPM
0.00.017.116 I llm_load_print_meta: n_vocab          = 30522
0.00.017.116 I llm_load_print_meta: n_merges         = 0
0.00.017.116 I llm_load_print_meta: vocab_only       = 0
0.00.017.116 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.116 I llm_load_print_meta: n_embd           = 384
0.00.017.116 I llm_load_print_meta: n_layer          = 12
0.00.017.120 I llm_load_print_meta: n_head           = 12
0.00.017.120 I llm_load_print_meta: n_head_kv        = 12
0.00.017.121 I llm_load_print_meta: n_rot            = 32
0.00.017.121 I llm_load_print_meta: n_swa            = 0
0.00.017.121 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.121 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.122 I llm_load_print_meta: n_gqa            = 1
0.00.017.123 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.123 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.124 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.124 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.124 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.124 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.124 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.125 I llm_load_print_meta: n_ff             = 1536
0.00.017.125 I llm_load_print_meta: n_expert         = 0
0.00.017.126 I llm_load_print_meta: n_expert_used    = 0
0.00.017.126 I llm_load_print_meta: causal attn      = 0
0.00.017.126 I llm_load_print_meta: pooling type     = 2
0.00.017.126 I llm_load_print_meta: rope type        = 2
0.00.017.126 I llm_load_print_meta: rope scaling     = linear
0.00.017.127 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.127 I llm_load_print_meta: freq_scale_train = 1
0.00.017.127 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.127 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.127 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.127 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.127 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.128 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.130 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.130 I llm_load_print_meta: model type       = 33M
0.00.017.137 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.137 I llm_load_print_meta: model params     = 33.21 M
0.00.017.138 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.138 I llm_load_print_meta: general.name     = Bge Small
0.00.017.138 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.138 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.139 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.139 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.139 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.139 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.139 I llm_load_print_meta: max token length = 21
0.00.018.380 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.018.380 I llm_load_tensors: offloading output layer to GPU
0.00.018.382 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.018.390 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.391 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.018.739 I llama_new_context_with_model: n_seq_max     = 1
0.00.018.740 I llama_new_context_with_model: n_ctx         = 512
0.00.018.740 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.018.740 I llama_new_context_with_model: n_batch       = 2048
0.00.018.740 I llama_new_context_with_model: n_ubatch      = 2048
0.00.018.740 I llama_new_context_with_model: flash_attn    = 0
0.00.018.741 I llama_new_context_with_model: freq_base     = 10000.0
0.00.018.741 I llama_new_context_with_model: freq_scale    = 1
0.00.018.741 I ggml_metal_init: allocating
0.00.018.745 I ggml_metal_init: found device: Apple M4
0.00.018.747 I ggml_metal_init: picking default device: Apple M4
0.00.019.363 I ggml_metal_init: using embedded metal library
0.00.021.797 I ggml_metal_init: GPU name:   Apple M4
0.00.021.799 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.799 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.800 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.800 I ggml_metal_init: simdgroup reduction   = true
0.00.021.800 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.800 I ggml_metal_init: has bfloat            = true
0.00.021.800 I ggml_metal_init: use bfloat            = true
0.00.021.801 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.802 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.074 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.032.593 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.595 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.597 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.236 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.237 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.237 I llama_new_context_with_model: graph nodes  = 429
0.00.033.238 I llama_new_context_with_model: graph splits = 2
0.00.033.251 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.252 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.600 I 
0.00.038.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.181 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.668 I llama_perf_context_print:        load time =      29.65 ms
0.00.043.669 I llama_perf_context_print: prompt eval time =       4.36 ms /     9 tokens (    0.48 ms per token,  2064.22 tokens per second)
0.00.043.670 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.670 I llama_perf_context_print:       total time =       5.07 ms /    10 tokens
0.00.043.847 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.029s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.141 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.460 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.859 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.866 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.872 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.873 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.873 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.875 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.876 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.876 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.877 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.878 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.881 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.882 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.883 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.884 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.129 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.129 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.130 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.130 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.130 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.131 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.131 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.131 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.132 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.132 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.133 I llama_model_loader: - type  f32:   40 tensors
0.00.048.133 I llama_model_loader: - type  f16:   30 tensors
0.00.065.992 W llm_load_vocab: empty token at index 5
0.00.070.633 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.898 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.928 I llm_load_vocab: special tokens cache size = 5
0.00.339.891 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.339.896 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.339.897 I llm_load_print_meta: arch             = jina-bert-v2
0.00.339.898 I llm_load_print_meta: vocab type       = BPE
0.00.339.898 I llm_load_print_meta: n_vocab          = 61056
0.00.339.898 I llm_load_print_meta: n_merges         = 39382
0.00.339.898 I llm_load_print_meta: vocab_only       = 0
0.00.339.899 I llm_load_print_meta: n_ctx_train      = 8192
0.00.339.899 I llm_load_print_meta: n_embd           = 384
0.00.339.899 I llm_load_print_meta: n_layer          = 4
0.00.339.906 I llm_load_print_meta: n_head           = 12
0.00.339.906 I llm_load_print_meta: n_head_kv        = 12
0.00.339.906 I llm_load_print_meta: n_rot            = 32
0.00.339.907 I llm_load_print_meta: n_swa            = 0
0.00.339.907 I llm_load_print_meta: n_embd_head_k    = 32
0.00.339.907 I llm_load_print_meta: n_embd_head_v    = 32
0.00.339.908 I llm_load_print_meta: n_gqa            = 1
0.00.339.909 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.339.909 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.339.910 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.339.910 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.339.911 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.339.911 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.339.911 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.339.913 I llm_load_print_meta: n_ff             = 1536
0.00.339.915 I llm_load_print_meta: n_expert         = 0
0.00.339.916 I llm_load_print_meta: n_expert_used    = 0
0.00.339.916 I llm_load_print_meta: causal attn      = 0
0.00.339.916 I llm_load_print_meta: pooling type     = -1
0.00.339.916 I llm_load_print_meta: rope type        = -1
0.00.339.916 I llm_load_print_meta: rope scaling     = linear
0.00.339.917 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.339.917 I llm_load_print_meta: freq_scale_train = 1
0.00.339.917 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.339.917 I llm_load_print_meta: rope_finetuned   = unknown
0.00.339.917 I llm_load_print_meta: ssm_d_conv       = 0
0.00.339.918 I llm_load_print_meta: ssm_d_inner      = 0
0.00.339.923 I llm_load_print_meta: ssm_d_state      = 0
0.00.339.923 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.339.923 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.339.924 I llm_load_print_meta: model type       = 33M
0.00.339.925 I llm_load_print_meta: model ftype      = F16
0.00.339.925 I llm_load_print_meta: model params     = 32.90 M
0.00.339.926 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.339.927 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.339.928 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.339.929 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.339.929 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.339.929 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.339.929 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.339.929 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.339.930 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.339.930 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.339.930 I llm_load_print_meta: max token length = 45
0.00.341.140 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.341.140 I llm_load_tensors: offloading output layer to GPU
0.00.341.141 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.341.164 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.341.165 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.342.157 I llama_new_context_with_model: n_seq_max     = 1
0.00.342.158 I llama_new_context_with_model: n_ctx         = 8192
0.00.342.158 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.342.159 I llama_new_context_with_model: n_batch       = 2048
0.00.342.159 I llama_new_context_with_model: n_ubatch      = 2048
0.00.342.159 I llama_new_context_with_model: flash_attn    = 0
0.00.342.159 I llama_new_context_with_model: freq_base     = 10000.0
0.00.342.160 I llama_new_context_with_model: freq_scale    = 1
0.00.342.160 I ggml_metal_init: allocating
0.00.342.168 I ggml_metal_init: found device: Apple M4
0.00.342.170 I ggml_metal_init: picking default device: Apple M4
0.00.343.126 I ggml_metal_init: using embedded metal library
0.00.346.289 I ggml_metal_init: GPU name:   Apple M4
0.00.346.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.292 I ggml_metal_init: simdgroup reduction   = true
0.00.346.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.292 I ggml_metal_init: has bfloat            = true
0.00.346.293 I ggml_metal_init: use bfloat            = true
0.00.346.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.294 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.355.916 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.358.404 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.358.407 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.358.408 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.359.009 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.359.010 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.359.011 I llama_new_context_with_model: graph nodes  = 154
0.00.359.011 I llama_new_context_with_model: graph splits = 2
0.00.359.029 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.359.029 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.372.001 I 
0.00.372.035 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.372.261 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.372.262 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.372.266 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.372.266 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.372.276 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.372.276 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.372.833 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.376.580 I llama_perf_context_print:        load time =     349.53 ms
0.00.376.581 I llama_perf_context_print: prompt eval time =       3.74 ms /    62 tokens (    0.06 ms per token, 16586.41 tokens per second)
0.00.376.582 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.376.583 I llama_perf_context_print:       total time =       4.58 ms /    63 tokens
0.00.376.790 I ggml_metal_free: deallocating

real	0m1.118s
user	0m0.348s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.109 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.224 I main: llama backend init
0.00.000.230 I main: load the model and apply lora adapter, if any
0.00.032.527 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.234 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.254 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.255 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.256 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.257 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.260 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.261 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.262 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.262 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.263 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.264 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.269 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.270 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.658 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.063.281 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.282 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.282 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.283 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.284 I llama_model_loader: - type  f32:  194 tensors
0.00.063.284 I llama_model_loader: - type  f16:   98 tensors
0.00.092.972 I llm_load_vocab: special tokens cache size = 25
0.00.099.650 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.652 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.653 I llm_load_print_meta: arch             = gptneox
0.00.099.653 I llm_load_print_meta: vocab type       = BPE
0.00.099.653 I llm_load_print_meta: n_vocab          = 50304
0.00.099.653 I llm_load_print_meta: n_merges         = 50009
0.00.099.654 I llm_load_print_meta: vocab_only       = 0
0.00.099.654 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.654 I llm_load_print_meta: n_embd           = 2048
0.00.099.654 I llm_load_print_meta: n_layer          = 24
0.00.099.657 I llm_load_print_meta: n_head           = 16
0.00.099.658 I llm_load_print_meta: n_head_kv        = 16
0.00.099.658 I llm_load_print_meta: n_rot            = 32
0.00.099.658 I llm_load_print_meta: n_swa            = 0
0.00.099.658 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.658 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.659 I llm_load_print_meta: n_gqa            = 1
0.00.099.660 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.661 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.661 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.662 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.662 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.662 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.662 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.663 I llm_load_print_meta: n_ff             = 8192
0.00.099.663 I llm_load_print_meta: n_expert         = 0
0.00.099.663 I llm_load_print_meta: n_expert_used    = 0
0.00.099.663 I llm_load_print_meta: causal attn      = 1
0.00.099.663 I llm_load_print_meta: pooling type     = 0
0.00.099.665 I llm_load_print_meta: rope type        = 2
0.00.099.665 I llm_load_print_meta: rope scaling     = linear
0.00.099.665 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.666 I llm_load_print_meta: freq_scale_train = 1
0.00.099.666 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.666 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.666 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.667 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.667 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.667 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.667 I llm_load_print_meta: model type       = 1.4B
0.00.099.668 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.668 I llm_load_print_meta: model params     = 1.41 B
0.00.099.669 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.669 I llm_load_print_meta: general.name     = 1.4B
0.00.099.669 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.669 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.669 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.669 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.670 I llm_load_print_meta: LF token         = 128 ''
0.00.099.670 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.670 I llm_load_print_meta: max token length = 1024
0.00.102.272 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.102.272 I llm_load_tensors: offloading output layer to GPU
0.00.102.272 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.102.290 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.102.291 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.103.228 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.229 I llama_new_context_with_model: n_ctx         = 2048
0.00.103.229 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.103.230 I llama_new_context_with_model: n_batch       = 2048
0.00.103.230 I llama_new_context_with_model: n_ubatch      = 512
0.00.103.230 I llama_new_context_with_model: flash_attn    = 0
0.00.103.230 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.231 I llama_new_context_with_model: freq_scale    = 1
0.00.103.231 I ggml_metal_init: allocating
0.00.103.240 I ggml_metal_init: found device: Apple M4
0.00.103.243 I ggml_metal_init: picking default device: Apple M4
0.00.103.933 I ggml_metal_init: using embedded metal library
0.00.113.240 I ggml_metal_init: GPU name:   Apple M4
0.00.113.242 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.242 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.242 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.243 I ggml_metal_init: simdgroup reduction   = true
0.00.113.243 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.243 I ggml_metal_init: has bfloat            = true
0.00.113.243 I ggml_metal_init: use bfloat            = true
0.00.113.244 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.528 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.156.353 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.156.360 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.156.378 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.157.342 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.157.344 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.157.344 I llama_new_context_with_model: graph nodes  = 967
0.00.157.344 I llama_new_context_with_model: graph splits = 2
0.00.157.385 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.157.524 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.157.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.237.280 I main: llama threadpool init, n_threads = 4
0.00.237.317 I 
0.00.237.353 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.237.354 I 
0.00.237.412 I sampler seed: 1234
0.00.237.416 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.237.442 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.237.444 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.237.444 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.080.668 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.02.080.669 I llama_perf_context_print:        load time =     204.74 ms
0.02.080.670 I llama_perf_context_print: prompt eval time =      43.84 ms /     7 tokens (    6.26 ms per token,   159.68 tokens per second)
0.02.080.670 I llama_perf_context_print:        eval time =    1796.41 ms /    63 runs   (   28.51 ms per token,    35.07 tokens per second)
0.02.080.671 I llama_perf_context_print:       total time =    1843.39 ms /    70 tokens
0.02.080.865 I ggml_metal_free: deallocating

real	0m2.409s
user	0m0.143s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.611 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.855 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.296 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.300 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.302 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.303 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.303 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.304 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.304 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.306 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.306 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.309 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.309 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.310 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.312 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.313 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.313 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.389 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.007 I llama_model_loader: - type  f32:  194 tensors
0.00.054.007 I llama_model_loader: - type  f16:   98 tensors
0.00.083.003 I llm_load_vocab: special tokens cache size = 25
0.00.089.392 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.395 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.395 I llm_load_print_meta: arch             = gptneox
0.00.089.395 I llm_load_print_meta: vocab type       = BPE
0.00.089.396 I llm_load_print_meta: n_vocab          = 50304
0.00.089.396 I llm_load_print_meta: n_merges         = 50009
0.00.089.396 I llm_load_print_meta: vocab_only       = 0
0.00.089.396 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.396 I llm_load_print_meta: n_embd           = 2048
0.00.089.396 I llm_load_print_meta: n_layer          = 24
0.00.089.400 I llm_load_print_meta: n_head           = 16
0.00.089.400 I llm_load_print_meta: n_head_kv        = 16
0.00.089.401 I llm_load_print_meta: n_rot            = 32
0.00.089.401 I llm_load_print_meta: n_swa            = 0
0.00.089.401 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.401 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.402 I llm_load_print_meta: n_gqa            = 1
0.00.089.403 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.403 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.403 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.404 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.404 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.404 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.404 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.405 I llm_load_print_meta: n_ff             = 8192
0.00.089.405 I llm_load_print_meta: n_expert         = 0
0.00.089.405 I llm_load_print_meta: n_expert_used    = 0
0.00.089.406 I llm_load_print_meta: causal attn      = 1
0.00.089.406 I llm_load_print_meta: pooling type     = 0
0.00.089.406 I llm_load_print_meta: rope type        = 2
0.00.089.406 I llm_load_print_meta: rope scaling     = linear
0.00.089.406 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.407 I llm_load_print_meta: freq_scale_train = 1
0.00.089.407 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.407 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.407 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.407 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.407 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.407 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.408 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.408 I llm_load_print_meta: model type       = 1.4B
0.00.089.408 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.411 I llm_load_print_meta: model params     = 1.41 B
0.00.089.411 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.411 I llm_load_print_meta: general.name     = 1.4B
0.00.089.411 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.411 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.412 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.412 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.412 I llm_load_print_meta: LF token         = 128 ''
0.00.089.413 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.413 I llm_load_print_meta: max token length = 1024
0.00.092.007 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.007 I llm_load_tensors: offloading output layer to GPU
0.00.092.007 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.018 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.019 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.947 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.948 I llama_new_context_with_model: n_ctx         = 128
0.00.092.948 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.948 I llama_new_context_with_model: n_batch       = 128
0.00.092.948 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.949 I llama_new_context_with_model: flash_attn    = 0
0.00.092.949 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.949 I llama_new_context_with_model: freq_scale    = 1
0.00.092.950 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.950 I ggml_metal_init: allocating
0.00.092.953 I ggml_metal_init: found device: Apple M4
0.00.092.956 I ggml_metal_init: picking default device: Apple M4
0.00.093.582 I ggml_metal_init: using embedded metal library
0.00.096.099 I ggml_metal_init: GPU name:   Apple M4
0.00.096.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.101 I ggml_metal_init: simdgroup reduction   = true
0.00.096.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.102 I ggml_metal_init: has bfloat            = true
0.00.096.102 I ggml_metal_init: use bfloat            = true
0.00.096.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.993 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.107.569 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.571 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.585 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.556 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.558 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.558 I llama_new_context_with_model: graph nodes  = 967
0.00.108.558 I llama_new_context_with_model: graph splits = 2
0.00.108.571 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.118.970 I 
0.01.119.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.119.059 I perplexity: tokenizing the input ..
0.01.133.070 I perplexity: tokenization took 14.01 ms
0.01.133.076 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.256.261 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.258.080 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.258.108 I llama_perf_context_print:        load time =    1096.10 ms
0.01.258.109 I llama_perf_context_print: prompt eval time =     122.12 ms /   128 tokens (    0.95 ms per token,  1048.15 tokens per second)
0.01.258.110 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.258.111 I llama_perf_context_print:       total time =     139.14 ms /   129 tokens
0.01.258.866 I ggml_metal_free: deallocating

real	0m1.449s
user	0m0.125s
sys	0m0.203s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.845 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.713 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.718 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.720 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.721 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.726 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.726 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.727 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.728 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.728 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.729 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.731 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.732 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.732 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.792 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.920 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.920 I llama_model_loader: - type  f32:  194 tensors
0.00.033.921 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.646 I llm_load_vocab: special tokens cache size = 25
0.00.062.559 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.563 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.564 I llm_load_print_meta: arch             = gptneox
0.00.062.564 I llm_load_print_meta: vocab type       = BPE
0.00.062.564 I llm_load_print_meta: n_vocab          = 50304
0.00.062.565 I llm_load_print_meta: n_merges         = 50009
0.00.062.565 I llm_load_print_meta: vocab_only       = 0
0.00.062.565 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.565 I llm_load_print_meta: n_embd           = 2048
0.00.062.565 I llm_load_print_meta: n_layer          = 24
0.00.062.572 I llm_load_print_meta: n_head           = 16
0.00.062.573 I llm_load_print_meta: n_head_kv        = 16
0.00.062.573 I llm_load_print_meta: n_rot            = 32
0.00.062.574 I llm_load_print_meta: n_swa            = 0
0.00.062.574 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.574 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.575 I llm_load_print_meta: n_gqa            = 1
0.00.062.576 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.576 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.577 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.577 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.578 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.578 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.578 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.579 I llm_load_print_meta: n_ff             = 8192
0.00.062.579 I llm_load_print_meta: n_expert         = 0
0.00.062.579 I llm_load_print_meta: n_expert_used    = 0
0.00.062.579 I llm_load_print_meta: causal attn      = 1
0.00.062.579 I llm_load_print_meta: pooling type     = 0
0.00.062.580 I llm_load_print_meta: rope type        = 2
0.00.062.580 I llm_load_print_meta: rope scaling     = linear
0.00.062.580 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.581 I llm_load_print_meta: freq_scale_train = 1
0.00.062.581 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.581 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.581 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.581 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.582 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.582 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.582 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.582 I llm_load_print_meta: model type       = 1.4B
0.00.062.583 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.583 I llm_load_print_meta: model params     = 1.41 B
0.00.062.584 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.584 I llm_load_print_meta: general.name     = 1.4B
0.00.062.584 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.584 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.585 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.585 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.585 I llm_load_print_meta: LF token         = 128 ''
0.00.062.585 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.586 I llm_load_print_meta: max token length = 1024
0.00.064.525 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.525 I llm_load_tensors: offloading output layer to GPU
0.00.064.525 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.536 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.537 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.439 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.439 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.440 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.440 I llama_new_context_with_model: n_batch       = 2048
0.00.065.440 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.440 I llama_new_context_with_model: flash_attn    = 0
0.00.065.441 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.441 I llama_new_context_with_model: freq_scale    = 1
0.00.065.442 I ggml_metal_init: allocating
0.00.065.446 I ggml_metal_init: found device: Apple M4
0.00.065.449 I ggml_metal_init: picking default device: Apple M4
0.00.066.199 I ggml_metal_init: using embedded metal library
0.00.068.704 I ggml_metal_init: GPU name:   Apple M4
0.00.068.706 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.706 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.707 I ggml_metal_init: simdgroup reduction   = true
0.00.068.707 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.707 I ggml_metal_init: has bfloat            = true
0.00.068.708 I ggml_metal_init: use bfloat            = true
0.00.068.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.220 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.174 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.186 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.208 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.343 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.344 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.344 I llama_new_context_with_model: graph nodes  = 967
0.00.105.345 I llama_new_context_with_model: graph splits = 2
0.00.105.372 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.502 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.363.844 I main: llama threadpool init, n_threads = 4
0.01.363.924 I 
0.01.363.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.363.999 I 
0.01.364.543 I sampler seed: 1234
0.01.364.550 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.364.618 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.364.622 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.364.622 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.471.122 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47459.89 tokens per second)
0.02.471.123 I llama_perf_context_print:        load time =    1353.99 ms
0.02.471.123 I llama_perf_context_print: prompt eval time =      50.77 ms /     7 tokens (    7.25 ms per token,   137.89 tokens per second)
0.02.471.124 I llama_perf_context_print:        eval time =    1053.08 ms /    63 runs   (   16.72 ms per token,    59.82 tokens per second)
0.02.471.124 I llama_perf_context_print:       total time =    1107.28 ms /    70 tokens
0.02.471.340 I ggml_metal_free: deallocating

real	0m2.492s
user	0m0.125s
sys	0m0.261s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.136 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.193 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.615 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.625 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.626 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.628 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.629 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.636 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.636 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.639 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.640 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.709 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.688 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.007 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.008 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.009 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.009 I llama_model_loader: - type  f32:  194 tensors
0.00.040.010 I llama_model_loader: - type q8_0:   98 tensors
0.00.069.643 I llm_load_vocab: special tokens cache size = 25
0.00.077.634 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.637 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.638 I llm_load_print_meta: arch             = gptneox
0.00.077.638 I llm_load_print_meta: vocab type       = BPE
0.00.077.639 I llm_load_print_meta: n_vocab          = 50304
0.00.077.639 I llm_load_print_meta: n_merges         = 50009
0.00.077.639 I llm_load_print_meta: vocab_only       = 0
0.00.077.639 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.639 I llm_load_print_meta: n_embd           = 2048
0.00.077.640 I llm_load_print_meta: n_layer          = 24
0.00.077.643 I llm_load_print_meta: n_head           = 16
0.00.077.644 I llm_load_print_meta: n_head_kv        = 16
0.00.077.644 I llm_load_print_meta: n_rot            = 32
0.00.077.644 I llm_load_print_meta: n_swa            = 0
0.00.077.644 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.644 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.645 I llm_load_print_meta: n_gqa            = 1
0.00.077.646 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.647 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.647 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.648 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.648 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.648 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.651 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.652 I llm_load_print_meta: n_ff             = 8192
0.00.077.652 I llm_load_print_meta: n_expert         = 0
0.00.077.652 I llm_load_print_meta: n_expert_used    = 0
0.00.077.652 I llm_load_print_meta: causal attn      = 1
0.00.077.653 I llm_load_print_meta: pooling type     = 0
0.00.077.653 I llm_load_print_meta: rope type        = 2
0.00.077.653 I llm_load_print_meta: rope scaling     = linear
0.00.077.653 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.654 I llm_load_print_meta: freq_scale_train = 1
0.00.077.654 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.654 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.654 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.654 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.654 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.654 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.656 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.656 I llm_load_print_meta: model type       = 1.4B
0.00.077.657 I llm_load_print_meta: model ftype      = Q8_0
0.00.077.657 I llm_load_print_meta: model params     = 1.41 B
0.00.077.658 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.077.658 I llm_load_print_meta: general.name     = 1.4B
0.00.077.658 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.658 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.659 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.659 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.659 I llm_load_print_meta: LF token         = 128 ''
0.00.077.659 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.660 I llm_load_print_meta: max token length = 1024
0.00.079.981 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.981 I llm_load_tensors: offloading output layer to GPU
0.00.079.982 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.988 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.079.988 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.081.128 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.129 I llama_new_context_with_model: n_ctx         = 128
0.00.081.129 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.081.129 I llama_new_context_with_model: n_batch       = 128
0.00.081.130 I llama_new_context_with_model: n_ubatch      = 128
0.00.081.130 I llama_new_context_with_model: flash_attn    = 0
0.00.081.130 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.131 I llama_new_context_with_model: freq_scale    = 1
0.00.081.131 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.081.131 I ggml_metal_init: allocating
0.00.081.135 I ggml_metal_init: found device: Apple M4
0.00.081.137 I ggml_metal_init: picking default device: Apple M4
0.00.081.834 I ggml_metal_init: using embedded metal library
0.00.084.896 I ggml_metal_init: GPU name:   Apple M4
0.00.084.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.084.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.084.899 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.084.899 I ggml_metal_init: simdgroup reduction   = true
0.00.084.899 I ggml_metal_init: simdgroup matrix mul. = true
0.00.084.900 I ggml_metal_init: has bfloat            = true
0.00.084.900 I ggml_metal_init: use bfloat            = true
0.00.084.900 I ggml_metal_init: hasUnifiedMemory      = true
0.00.084.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.663 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.096.116 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.096.122 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.096.139 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.081 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.097.083 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.097.083 I llama_new_context_with_model: graph nodes  = 967
0.00.097.083 I llama_new_context_with_model: graph splits = 2
0.00.097.091 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.097.092 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.070.347 I 
0.01.070.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.070.405 I perplexity: tokenizing the input ..
0.01.079.517 I perplexity: tokenization took 9.11 ms
0.01.079.524 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.204.644 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.205.901 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.205.913 I llama_perf_context_print:        load time =    1056.15 ms
0.01.205.917 I llama_perf_context_print: prompt eval time =     124.86 ms /   128 tokens (    0.98 ms per token,  1025.16 tokens per second)
0.01.205.918 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.205.918 I llama_perf_context_print:       total time =     135.57 ms /   129 tokens
0.01.206.313 I ggml_metal_free: deallocating

real	0m1.229s
user	0m0.105s
sys	0m0.155s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.011.809 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.463 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.466 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.467 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.468 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.468 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.471 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.471 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.568 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.596 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.597 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.597 I llama_model_loader: - type  f32:  194 tensors
0.00.029.598 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.598 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.568 I llm_load_vocab: special tokens cache size = 25
0.00.056.435 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.438 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.439 I llm_load_print_meta: arch             = gptneox
0.00.056.439 I llm_load_print_meta: vocab type       = BPE
0.00.056.440 I llm_load_print_meta: n_vocab          = 50304
0.00.056.440 I llm_load_print_meta: n_merges         = 50009
0.00.056.440 I llm_load_print_meta: vocab_only       = 0
0.00.056.440 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.440 I llm_load_print_meta: n_embd           = 2048
0.00.056.441 I llm_load_print_meta: n_layer          = 24
0.00.056.444 I llm_load_print_meta: n_head           = 16
0.00.056.445 I llm_load_print_meta: n_head_kv        = 16
0.00.056.445 I llm_load_print_meta: n_rot            = 32
0.00.056.445 I llm_load_print_meta: n_swa            = 0
0.00.056.445 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.446 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.446 I llm_load_print_meta: n_gqa            = 1
0.00.056.447 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.448 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.449 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.449 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.449 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.449 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.449 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.450 I llm_load_print_meta: n_ff             = 8192
0.00.056.450 I llm_load_print_meta: n_expert         = 0
0.00.056.451 I llm_load_print_meta: n_expert_used    = 0
0.00.056.451 I llm_load_print_meta: causal attn      = 1
0.00.056.451 I llm_load_print_meta: pooling type     = 0
0.00.056.451 I llm_load_print_meta: rope type        = 2
0.00.056.451 I llm_load_print_meta: rope scaling     = linear
0.00.056.452 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.452 I llm_load_print_meta: freq_scale_train = 1
0.00.056.453 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.453 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.453 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.453 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.453 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.453 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.453 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.454 I llm_load_print_meta: model type       = 1.4B
0.00.056.454 I llm_load_print_meta: model ftype      = Q4_0
0.00.056.455 I llm_load_print_meta: model params     = 1.41 B
0.00.056.455 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.056.455 I llm_load_print_meta: general.name     = 1.4B
0.00.056.456 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.456 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.456 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.456 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.457 I llm_load_print_meta: LF token         = 128 ''
0.00.056.457 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.457 I llm_load_print_meta: max token length = 1024
0.00.058.152 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.152 I llm_load_tensors: offloading output layer to GPU
0.00.058.152 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.163 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.058.164 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.059.080 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.081 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.081 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.081 I llama_new_context_with_model: n_batch       = 2048
0.00.059.081 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.081 I llama_new_context_with_model: flash_attn    = 0
0.00.059.082 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.082 I llama_new_context_with_model: freq_scale    = 1
0.00.059.082 I ggml_metal_init: allocating
0.00.059.086 I ggml_metal_init: found device: Apple M4
0.00.059.088 I ggml_metal_init: picking default device: Apple M4
0.00.059.735 I ggml_metal_init: using embedded metal library
0.00.062.201 I ggml_metal_init: GPU name:   Apple M4
0.00.062.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.203 I ggml_metal_init: simdgroup reduction   = true
0.00.062.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.203 I ggml_metal_init: has bfloat            = true
0.00.062.204 I ggml_metal_init: use bfloat            = true
0.00.062.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.205 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.995 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.094.336 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.343 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.362 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.487 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.488 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.489 I llama_new_context_with_model: graph nodes  = 967
0.00.095.489 I llama_new_context_with_model: graph splits = 2
0.00.095.514 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.668 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.668 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.726 I main: llama threadpool init, n_threads = 4
0.00.737.770 I 
0.00.737.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.806 I 
0.00.738.036 I sampler seed: 1234
0.00.738.040 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.081 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.081 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.081 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.423.032 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.423.032 I llama_perf_context_print:        load time =     725.91 ms
0.01.423.033 I llama_perf_context_print: prompt eval time =      43.41 ms /     7 tokens (    6.20 ms per token,   161.25 tokens per second)
0.01.423.034 I llama_perf_context_print:        eval time =     638.44 ms /    63 runs   (   10.13 ms per token,    98.68 tokens per second)
0.01.423.034 I llama_perf_context_print:       total time =     685.31 ms /    70 tokens
0.01.423.227 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.112s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.835 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.183 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.026.187 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.193 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.193 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.193 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.194 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.194 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.201 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.292 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.293 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.295 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.295 I llama_model_loader: - type  f32:  194 tensors
0.00.035.296 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.296 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.500 I llm_load_vocab: special tokens cache size = 25
0.00.068.072 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.076 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.076 I llm_load_print_meta: arch             = gptneox
0.00.068.077 I llm_load_print_meta: vocab type       = BPE
0.00.068.077 I llm_load_print_meta: n_vocab          = 50304
0.00.068.077 I llm_load_print_meta: n_merges         = 50009
0.00.068.077 I llm_load_print_meta: vocab_only       = 0
0.00.068.082 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.083 I llm_load_print_meta: n_embd           = 2048
0.00.068.083 I llm_load_print_meta: n_layer          = 24
0.00.068.087 I llm_load_print_meta: n_head           = 16
0.00.068.087 I llm_load_print_meta: n_head_kv        = 16
0.00.068.087 I llm_load_print_meta: n_rot            = 32
0.00.068.088 I llm_load_print_meta: n_swa            = 0
0.00.068.089 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.089 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.090 I llm_load_print_meta: n_gqa            = 1
0.00.068.091 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.091 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.092 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.092 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.092 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.092 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.093 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.093 I llm_load_print_meta: n_ff             = 8192
0.00.068.094 I llm_load_print_meta: n_expert         = 0
0.00.068.095 I llm_load_print_meta: n_expert_used    = 0
0.00.068.095 I llm_load_print_meta: causal attn      = 1
0.00.068.095 I llm_load_print_meta: pooling type     = 0
0.00.068.095 I llm_load_print_meta: rope type        = 2
0.00.068.095 I llm_load_print_meta: rope scaling     = linear
0.00.068.097 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.098 I llm_load_print_meta: freq_scale_train = 1
0.00.068.098 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.099 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.099 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.099 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.099 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.099 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.099 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.100 I llm_load_print_meta: model type       = 1.4B
0.00.068.100 I llm_load_print_meta: model ftype      = Q4_0
0.00.068.100 I llm_load_print_meta: model params     = 1.41 B
0.00.068.101 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.068.101 I llm_load_print_meta: general.name     = 1.4B
0.00.068.101 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.101 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.102 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.102 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.102 I llm_load_print_meta: LF token         = 128 ''
0.00.068.102 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.103 I llm_load_print_meta: max token length = 1024
0.00.070.312 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.312 I llm_load_tensors: offloading output layer to GPU
0.00.070.313 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.324 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.070.325 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.071.486 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.487 I llama_new_context_with_model: n_ctx         = 128
0.00.071.487 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.071.487 I llama_new_context_with_model: n_batch       = 128
0.00.071.487 I llama_new_context_with_model: n_ubatch      = 128
0.00.071.488 I llama_new_context_with_model: flash_attn    = 0
0.00.071.488 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.488 I llama_new_context_with_model: freq_scale    = 1
0.00.071.489 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.489 I ggml_metal_init: allocating
0.00.071.492 I ggml_metal_init: found device: Apple M4
0.00.071.494 I ggml_metal_init: picking default device: Apple M4
0.00.072.150 I ggml_metal_init: using embedded metal library
0.00.075.033 I ggml_metal_init: GPU name:   Apple M4
0.00.075.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.036 I ggml_metal_init: simdgroup reduction   = true
0.00.075.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.036 I ggml_metal_init: has bfloat            = true
0.00.075.036 I ggml_metal_init: use bfloat            = true
0.00.075.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.037 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.627 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.056 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.087.059 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.087.072 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.168 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.088.170 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.088.170 I llama_new_context_with_model: graph nodes  = 967
0.00.088.171 I llama_new_context_with_model: graph splits = 2
0.00.088.183 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.088.184 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.288 I 
0.00.629.323 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.337 I perplexity: tokenizing the input ..
0.00.637.608 I perplexity: tokenization took 8.269 ms
0.00.637.612 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.326 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.761.489 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.761.507 I llama_perf_context_print:        load time =     612.45 ms
0.00.761.508 I llama_perf_context_print: prompt eval time =     122.49 ms /   128 tokens (    0.96 ms per token,  1045.01 tokens per second)
0.00.761.508 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.509 I llama_perf_context_print:       total time =     132.22 ms /   129 tokens
0.00.761.976 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.087s
sys	0m0.097s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.825 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.164 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.029.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.171 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.172 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.176 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.176 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.176 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.221 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.341 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.270 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.272 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.272 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.272 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.273 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.273 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.038.273 I llama_model_loader: - type  f32:  194 tensors
0.00.038.274 I llama_model_loader: - type q4_1:   97 tensors
0.00.038.274 I llama_model_loader: - type q6_K:    1 tensors
0.00.063.189 I llm_load_vocab: special tokens cache size = 25
0.00.070.828 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.831 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.832 I llm_load_print_meta: arch             = gptneox
0.00.070.833 I llm_load_print_meta: vocab type       = BPE
0.00.070.833 I llm_load_print_meta: n_vocab          = 50304
0.00.070.836 I llm_load_print_meta: n_merges         = 50009
0.00.070.836 I llm_load_print_meta: vocab_only       = 0
0.00.070.836 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.836 I llm_load_print_meta: n_embd           = 2048
0.00.070.836 I llm_load_print_meta: n_layer          = 24
0.00.070.839 I llm_load_print_meta: n_head           = 16
0.00.070.840 I llm_load_print_meta: n_head_kv        = 16
0.00.070.841 I llm_load_print_meta: n_rot            = 32
0.00.070.841 I llm_load_print_meta: n_swa            = 0
0.00.070.841 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.841 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.842 I llm_load_print_meta: n_gqa            = 1
0.00.070.843 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.843 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.845 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.845 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.845 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.845 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.846 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.846 I llm_load_print_meta: n_ff             = 8192
0.00.070.846 I llm_load_print_meta: n_expert         = 0
0.00.070.846 I llm_load_print_meta: n_expert_used    = 0
0.00.070.849 I llm_load_print_meta: causal attn      = 1
0.00.070.849 I llm_load_print_meta: pooling type     = 0
0.00.070.849 I llm_load_print_meta: rope type        = 2
0.00.070.850 I llm_load_print_meta: rope scaling     = linear
0.00.070.850 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.850 I llm_load_print_meta: freq_scale_train = 1
0.00.070.851 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.855 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.855 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.856 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.856 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.856 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.856 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.856 I llm_load_print_meta: model type       = 1.4B
0.00.070.857 I llm_load_print_meta: model ftype      = Q4_1
0.00.070.857 I llm_load_print_meta: model params     = 1.41 B
0.00.070.858 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.070.858 I llm_load_print_meta: general.name     = 1.4B
0.00.070.858 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.859 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.859 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.859 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.859 I llm_load_print_meta: LF token         = 128 ''
0.00.070.860 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.860 I llm_load_print_meta: max token length = 1024
0.00.073.097 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.097 I llm_load_tensors: offloading output layer to GPU
0.00.073.098 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.108 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.073.109 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.074.147 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.148 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.148 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.148 I llama_new_context_with_model: n_batch       = 2048
0.00.074.148 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.149 I llama_new_context_with_model: flash_attn    = 0
0.00.074.149 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.149 I llama_new_context_with_model: freq_scale    = 1
0.00.074.150 I ggml_metal_init: allocating
0.00.074.153 I ggml_metal_init: found device: Apple M4
0.00.074.155 I ggml_metal_init: picking default device: Apple M4
0.00.074.855 I ggml_metal_init: using embedded metal library
0.00.077.690 I ggml_metal_init: GPU name:   Apple M4
0.00.077.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.692 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.693 I ggml_metal_init: simdgroup reduction   = true
0.00.077.693 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.693 I ggml_metal_init: has bfloat            = true
0.00.077.695 I ggml_metal_init: use bfloat            = true
0.00.077.695 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.696 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.430 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.109.742 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.109.747 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.109.765 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.760 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.762 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.762 I llama_new_context_with_model: graph nodes  = 967
0.00.110.762 I llama_new_context_with_model: graph splits = 2
0.00.110.775 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.919 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.920 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.359 I main: llama threadpool init, n_threads = 4
0.00.797.394 I 
0.00.797.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.431 I 
0.00.797.660 I sampler seed: 1234
0.00.797.665 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.678 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.679 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.679 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.533.200 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.533.200 I llama_perf_context_print:        load time =     788.53 ms
0.01.533.201 I llama_perf_context_print: prompt eval time =      46.01 ms /     7 tokens (    6.57 ms per token,   152.13 tokens per second)
0.01.533.202 I llama_perf_context_print:        eval time =     686.48 ms /    63 runs   (   10.90 ms per token,    91.77 tokens per second)
0.01.533.202 I llama_perf_context_print:       total time =     735.84 ms /    70 tokens
0.01.533.409 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.119s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.177 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.181 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.185 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.185 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.186 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.186 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.187 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.187 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.187 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.188 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.188 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.188 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.189 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.192 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.150 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.158 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.158 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.158 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.159 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.160 I llama_model_loader: - type  f32:  194 tensors
0.00.027.160 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.160 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.963 I llm_load_vocab: special tokens cache size = 25
0.00.053.776 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.779 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.781 I llm_load_print_meta: arch             = gptneox
0.00.053.781 I llm_load_print_meta: vocab type       = BPE
0.00.053.781 I llm_load_print_meta: n_vocab          = 50304
0.00.053.781 I llm_load_print_meta: n_merges         = 50009
0.00.053.786 I llm_load_print_meta: vocab_only       = 0
0.00.053.787 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.787 I llm_load_print_meta: n_embd           = 2048
0.00.053.787 I llm_load_print_meta: n_layer          = 24
0.00.053.790 I llm_load_print_meta: n_head           = 16
0.00.053.791 I llm_load_print_meta: n_head_kv        = 16
0.00.053.791 I llm_load_print_meta: n_rot            = 32
0.00.053.791 I llm_load_print_meta: n_swa            = 0
0.00.053.791 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.791 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.792 I llm_load_print_meta: n_gqa            = 1
0.00.053.793 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.794 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.794 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.795 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.795 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.796 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.797 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.798 I llm_load_print_meta: n_ff             = 8192
0.00.053.798 I llm_load_print_meta: n_expert         = 0
0.00.053.798 I llm_load_print_meta: n_expert_used    = 0
0.00.053.798 I llm_load_print_meta: causal attn      = 1
0.00.053.798 I llm_load_print_meta: pooling type     = 0
0.00.053.798 I llm_load_print_meta: rope type        = 2
0.00.053.799 I llm_load_print_meta: rope scaling     = linear
0.00.053.799 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.799 I llm_load_print_meta: freq_scale_train = 1
0.00.053.800 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.800 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.801 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.801 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.801 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.802 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.802 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.802 I llm_load_print_meta: model type       = 1.4B
0.00.053.802 I llm_load_print_meta: model ftype      = Q4_1
0.00.053.803 I llm_load_print_meta: model params     = 1.41 B
0.00.053.803 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.053.803 I llm_load_print_meta: general.name     = 1.4B
0.00.053.804 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.804 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.804 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.804 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.804 I llm_load_print_meta: LF token         = 128 ''
0.00.053.804 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.805 I llm_load_print_meta: max token length = 1024
0.00.055.817 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.817 I llm_load_tensors: offloading output layer to GPU
0.00.055.817 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.828 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.829 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.056.722 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.723 I llama_new_context_with_model: n_ctx         = 128
0.00.056.723 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.723 I llama_new_context_with_model: n_batch       = 128
0.00.056.723 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.723 I llama_new_context_with_model: flash_attn    = 0
0.00.056.724 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.724 I llama_new_context_with_model: freq_scale    = 1
0.00.056.724 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.725 I ggml_metal_init: allocating
0.00.056.728 I ggml_metal_init: found device: Apple M4
0.00.056.730 I ggml_metal_init: picking default device: Apple M4
0.00.057.299 I ggml_metal_init: using embedded metal library
0.00.059.698 I ggml_metal_init: GPU name:   Apple M4
0.00.059.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.700 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.700 I ggml_metal_init: simdgroup reduction   = true
0.00.059.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.701 I ggml_metal_init: has bfloat            = true
0.00.059.701 I ggml_metal_init: use bfloat            = true
0.00.059.701 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.513 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.070.787 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.790 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.805 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.700 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.701 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.701 I llama_new_context_with_model: graph nodes  = 967
0.00.071.702 I llama_new_context_with_model: graph splits = 2
0.00.071.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.251 I 
0.00.740.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.299 I perplexity: tokenizing the input ..
0.00.748.318 I perplexity: tokenization took 8.017 ms
0.00.748.321 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.871.109 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.872.288 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.872.306 I llama_perf_context_print:        load time =     731.43 ms
0.00.872.307 I llama_perf_context_print: prompt eval time =     122.56 ms /   128 tokens (    0.96 ms per token,  1044.36 tokens per second)
0.00.872.307 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.872.308 I llama_perf_context_print:       total time =     132.06 ms /   129 tokens
0.00.872.745 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.080s
sys	0m0.120s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.011.219 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.301 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.024.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.308 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.310 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.314 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.357 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.430 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.428 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.429 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.430 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.430 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.430 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.431 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.033.431 I llama_model_loader: - type  f32:  194 tensors
0.00.033.431 I llama_model_loader: - type q5_0:   97 tensors
0.00.033.432 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.268 I llm_load_vocab: special tokens cache size = 25
0.00.062.209 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.212 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.213 I llm_load_print_meta: arch             = gptneox
0.00.062.214 I llm_load_print_meta: vocab type       = BPE
0.00.062.214 I llm_load_print_meta: n_vocab          = 50304
0.00.062.215 I llm_load_print_meta: n_merges         = 50009
0.00.062.215 I llm_load_print_meta: vocab_only       = 0
0.00.062.217 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.217 I llm_load_print_meta: n_embd           = 2048
0.00.062.217 I llm_load_print_meta: n_layer          = 24
0.00.062.220 I llm_load_print_meta: n_head           = 16
0.00.062.221 I llm_load_print_meta: n_head_kv        = 16
0.00.062.222 I llm_load_print_meta: n_rot            = 32
0.00.062.222 I llm_load_print_meta: n_swa            = 0
0.00.062.222 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.222 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.223 I llm_load_print_meta: n_gqa            = 1
0.00.062.224 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.224 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.225 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.225 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.225 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.225 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.225 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.226 I llm_load_print_meta: n_ff             = 8192
0.00.062.226 I llm_load_print_meta: n_expert         = 0
0.00.062.226 I llm_load_print_meta: n_expert_used    = 0
0.00.062.228 I llm_load_print_meta: causal attn      = 1
0.00.062.229 I llm_load_print_meta: pooling type     = 0
0.00.062.229 I llm_load_print_meta: rope type        = 2
0.00.062.230 I llm_load_print_meta: rope scaling     = linear
0.00.062.230 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.230 I llm_load_print_meta: freq_scale_train = 1
0.00.062.231 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.231 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.231 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.231 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.231 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.232 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.232 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.232 I llm_load_print_meta: model type       = 1.4B
0.00.062.234 I llm_load_print_meta: model ftype      = Q5_0
0.00.062.235 I llm_load_print_meta: model params     = 1.41 B
0.00.062.235 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.062.235 I llm_load_print_meta: general.name     = 1.4B
0.00.062.235 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.236 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.236 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.236 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.236 I llm_load_print_meta: LF token         = 128 ''
0.00.062.236 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.236 I llm_load_print_meta: max token length = 1024
0.00.064.245 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.245 I llm_load_tensors: offloading output layer to GPU
0.00.064.246 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.256 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.064.257 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.065.203 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.204 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.204 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.204 I llama_new_context_with_model: n_batch       = 2048
0.00.065.204 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.205 I llama_new_context_with_model: flash_attn    = 0
0.00.065.205 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.205 I llama_new_context_with_model: freq_scale    = 1
0.00.065.206 I ggml_metal_init: allocating
0.00.065.212 I ggml_metal_init: found device: Apple M4
0.00.065.215 I ggml_metal_init: picking default device: Apple M4
0.00.065.817 I ggml_metal_init: using embedded metal library
0.00.068.152 I ggml_metal_init: GPU name:   Apple M4
0.00.068.153 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.154 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.154 I ggml_metal_init: simdgroup reduction   = true
0.00.068.155 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.155 I ggml_metal_init: has bfloat            = true
0.00.068.155 I ggml_metal_init: use bfloat            = true
0.00.068.155 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.157 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.164 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.101.278 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.294 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.319 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.439 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.440 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.441 I llama_new_context_with_model: graph nodes  = 967
0.00.102.441 I llama_new_context_with_model: graph splits = 2
0.00.102.465 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.630 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.631 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.666 I main: llama threadpool init, n_threads = 4
0.00.809.704 I 
0.00.809.733 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.733 I 
0.00.809.968 I sampler seed: 1234
0.00.809.973 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.997 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.999 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.999 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.600.748 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.600.749 I llama_perf_context_print:        load time =     798.44 ms
0.01.600.750 I llama_perf_context_print: prompt eval time =      43.17 ms /     7 tokens (    6.17 ms per token,   162.15 tokens per second)
0.01.600.750 I llama_perf_context_print:        eval time =     744.55 ms /    63 runs   (   11.82 ms per token,    84.61 tokens per second)
0.01.600.751 I llama_perf_context_print:       total time =     791.08 ms /    70 tokens
0.01.600.919 I ggml_metal_free: deallocating

real	0m1.624s
user	0m0.115s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.829 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.657 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.662 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.664 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.665 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.665 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.665 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.666 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.670 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.671 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.552 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.568 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.568 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.568 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.569 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.569 I llama_model_loader: - type  f32:  194 tensors
0.00.024.569 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.570 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.438 I llm_load_vocab: special tokens cache size = 25
0.00.051.452 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.454 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.455 I llm_load_print_meta: arch             = gptneox
0.00.051.455 I llm_load_print_meta: vocab type       = BPE
0.00.051.455 I llm_load_print_meta: n_vocab          = 50304
0.00.051.456 I llm_load_print_meta: n_merges         = 50009
0.00.051.456 I llm_load_print_meta: vocab_only       = 0
0.00.051.456 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.456 I llm_load_print_meta: n_embd           = 2048
0.00.051.456 I llm_load_print_meta: n_layer          = 24
0.00.051.459 I llm_load_print_meta: n_head           = 16
0.00.051.460 I llm_load_print_meta: n_head_kv        = 16
0.00.051.460 I llm_load_print_meta: n_rot            = 32
0.00.051.460 I llm_load_print_meta: n_swa            = 0
0.00.051.460 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.462 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.463 I llm_load_print_meta: n_gqa            = 1
0.00.051.464 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.464 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.465 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.466 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.470 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.470 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.470 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.471 I llm_load_print_meta: n_ff             = 8192
0.00.051.471 I llm_load_print_meta: n_expert         = 0
0.00.051.471 I llm_load_print_meta: n_expert_used    = 0
0.00.051.471 I llm_load_print_meta: causal attn      = 1
0.00.051.471 I llm_load_print_meta: pooling type     = 0
0.00.051.471 I llm_load_print_meta: rope type        = 2
0.00.051.472 I llm_load_print_meta: rope scaling     = linear
0.00.051.472 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.473 I llm_load_print_meta: freq_scale_train = 1
0.00.051.473 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.473 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.473 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.473 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.473 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.473 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.474 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.474 I llm_load_print_meta: model type       = 1.4B
0.00.051.474 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.475 I llm_load_print_meta: model params     = 1.41 B
0.00.051.476 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.476 I llm_load_print_meta: general.name     = 1.4B
0.00.051.477 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: LF token         = 128 ''
0.00.051.478 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.478 I llm_load_print_meta: max token length = 1024
0.00.053.480 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.480 I llm_load_tensors: offloading output layer to GPU
0.00.053.480 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.490 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.491 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.445 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.446 I llama_new_context_with_model: n_ctx         = 128
0.00.054.446 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.446 I llama_new_context_with_model: n_batch       = 128
0.00.054.447 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.447 I llama_new_context_with_model: flash_attn    = 0
0.00.054.447 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.447 I llama_new_context_with_model: freq_scale    = 1
0.00.054.448 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.448 I ggml_metal_init: allocating
0.00.054.451 I ggml_metal_init: found device: Apple M4
0.00.054.453 I ggml_metal_init: picking default device: Apple M4
0.00.055.020 I ggml_metal_init: using embedded metal library
0.00.057.364 I ggml_metal_init: GPU name:   Apple M4
0.00.057.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.366 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.366 I ggml_metal_init: simdgroup reduction   = true
0.00.057.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.366 I ggml_metal_init: has bfloat            = true
0.00.057.366 I ggml_metal_init: use bfloat            = true
0.00.057.367 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.367 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.204 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.559 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.561 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.576 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.473 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.474 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.474 I llama_new_context_with_model: graph nodes  = 967
0.00.069.474 I llama_new_context_with_model: graph splits = 2
0.00.069.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.470 I 
0.00.721.502 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.515 I perplexity: tokenizing the input ..
0.00.729.543 I perplexity: tokenization took 8.027 ms
0.00.729.547 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.864.928 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.866.329 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.866.354 I llama_perf_context_print:        load time =     711.64 ms
0.00.866.355 I llama_perf_context_print: prompt eval time =     135.15 ms /   128 tokens (    1.06 ms per token,   947.08 tokens per second)
0.00.866.356 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.356 I llama_perf_context_print:       total time =     144.89 ms /   129 tokens
0.00.866.831 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.079s
sys	0m0.117s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.895 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.936 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.034.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.942 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.943 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.943 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.944 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.948 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.949 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.949 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.846 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.450 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.451 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.452 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.452 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.453 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.044.453 I llama_model_loader: - type  f32:  194 tensors
0.00.044.453 I llama_model_loader: - type q5_1:   97 tensors
0.00.044.454 I llama_model_loader: - type q6_K:    1 tensors
0.00.069.993 I llm_load_vocab: special tokens cache size = 25
0.00.078.737 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.741 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.743 I llm_load_print_meta: arch             = gptneox
0.00.078.743 I llm_load_print_meta: vocab type       = BPE
0.00.078.744 I llm_load_print_meta: n_vocab          = 50304
0.00.078.744 I llm_load_print_meta: n_merges         = 50009
0.00.078.744 I llm_load_print_meta: vocab_only       = 0
0.00.078.744 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.745 I llm_load_print_meta: n_embd           = 2048
0.00.078.745 I llm_load_print_meta: n_layer          = 24
0.00.078.748 I llm_load_print_meta: n_head           = 16
0.00.078.749 I llm_load_print_meta: n_head_kv        = 16
0.00.078.749 I llm_load_print_meta: n_rot            = 32
0.00.078.749 I llm_load_print_meta: n_swa            = 0
0.00.078.749 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.750 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.750 I llm_load_print_meta: n_gqa            = 1
0.00.078.751 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.752 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.753 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.753 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.755 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.755 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.757 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.758 I llm_load_print_meta: n_ff             = 8192
0.00.078.758 I llm_load_print_meta: n_expert         = 0
0.00.078.758 I llm_load_print_meta: n_expert_used    = 0
0.00.078.760 I llm_load_print_meta: causal attn      = 1
0.00.078.760 I llm_load_print_meta: pooling type     = 0
0.00.078.761 I llm_load_print_meta: rope type        = 2
0.00.078.761 I llm_load_print_meta: rope scaling     = linear
0.00.078.761 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.762 I llm_load_print_meta: freq_scale_train = 1
0.00.078.762 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.762 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.762 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.762 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.762 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.763 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.763 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.763 I llm_load_print_meta: model type       = 1.4B
0.00.078.767 I llm_load_print_meta: model ftype      = Q5_1
0.00.078.768 I llm_load_print_meta: model params     = 1.41 B
0.00.078.768 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.078.769 I llm_load_print_meta: general.name     = 1.4B
0.00.078.775 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.775 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.776 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.776 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.778 I llm_load_print_meta: LF token         = 128 ''
0.00.078.778 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.779 I llm_load_print_meta: max token length = 1024
0.00.081.261 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.261 I llm_load_tensors: offloading output layer to GPU
0.00.081.262 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.268 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.081.269 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.082.575 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.577 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.577 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.577 I llama_new_context_with_model: n_batch       = 2048
0.00.082.577 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.578 I llama_new_context_with_model: flash_attn    = 0
0.00.082.578 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.579 I llama_new_context_with_model: freq_scale    = 1
0.00.082.579 I ggml_metal_init: allocating
0.00.082.584 I ggml_metal_init: found device: Apple M4
0.00.082.586 I ggml_metal_init: picking default device: Apple M4
0.00.083.410 I ggml_metal_init: using embedded metal library
0.00.087.029 I ggml_metal_init: GPU name:   Apple M4
0.00.087.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.033 I ggml_metal_init: simdgroup reduction   = true
0.00.087.034 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.034 I ggml_metal_init: has bfloat            = true
0.00.087.035 I ggml_metal_init: use bfloat            = true
0.00.087.035 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.605 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.119.375 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.388 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.418 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.309 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.311 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.311 I llama_new_context_with_model: graph nodes  = 967
0.00.120.311 I llama_new_context_with_model: graph splits = 2
0.00.120.325 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.897.107 I main: llama threadpool init, n_threads = 4
0.00.897.153 I 
0.00.897.190 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.897.193 I 
0.00.897.452 I sampler seed: 1234
0.00.897.457 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.897.480 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.897.480 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.897.480 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.747.831 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.747.832 I llama_perf_context_print:        load time =     888.21 ms
0.01.747.833 I llama_perf_context_print: prompt eval time =      46.68 ms /     7 tokens (    6.67 ms per token,   149.96 tokens per second)
0.01.747.833 I llama_perf_context_print:        eval time =     800.97 ms /    63 runs   (   12.71 ms per token,    78.65 tokens per second)
0.01.747.834 I llama_perf_context_print:       total time =     850.73 ms /    70 tokens
0.01.748.010 I ggml_metal_free: deallocating

real	0m1.769s
user	0m0.125s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.199 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.321 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.326 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.331 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.332 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.333 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.335 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.335 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.336 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.338 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.339 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.362 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.257 I llama_model_loader: - type  f32:  194 tensors
0.00.024.257 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.258 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.769 I llm_load_vocab: special tokens cache size = 25
0.00.051.592 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.595 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.596 I llm_load_print_meta: arch             = gptneox
0.00.051.596 I llm_load_print_meta: vocab type       = BPE
0.00.051.596 I llm_load_print_meta: n_vocab          = 50304
0.00.051.597 I llm_load_print_meta: n_merges         = 50009
0.00.051.597 I llm_load_print_meta: vocab_only       = 0
0.00.051.597 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.597 I llm_load_print_meta: n_embd           = 2048
0.00.051.597 I llm_load_print_meta: n_layer          = 24
0.00.051.600 I llm_load_print_meta: n_head           = 16
0.00.051.601 I llm_load_print_meta: n_head_kv        = 16
0.00.051.604 I llm_load_print_meta: n_rot            = 32
0.00.051.604 I llm_load_print_meta: n_swa            = 0
0.00.051.604 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.604 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.605 I llm_load_print_meta: n_gqa            = 1
0.00.051.607 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.607 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.608 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.608 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.608 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.609 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.609 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.609 I llm_load_print_meta: n_ff             = 8192
0.00.051.610 I llm_load_print_meta: n_expert         = 0
0.00.051.610 I llm_load_print_meta: n_expert_used    = 0
0.00.051.610 I llm_load_print_meta: causal attn      = 1
0.00.051.610 I llm_load_print_meta: pooling type     = 0
0.00.051.610 I llm_load_print_meta: rope type        = 2
0.00.051.614 I llm_load_print_meta: rope scaling     = linear
0.00.051.615 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.617 I llm_load_print_meta: freq_scale_train = 1
0.00.051.617 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.617 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.617 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.617 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.617 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.617 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.617 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.618 I llm_load_print_meta: model type       = 1.4B
0.00.051.618 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.618 I llm_load_print_meta: model params     = 1.41 B
0.00.051.619 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.619 I llm_load_print_meta: general.name     = 1.4B
0.00.051.620 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.620 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.620 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.621 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.622 I llm_load_print_meta: LF token         = 128 ''
0.00.051.622 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.622 I llm_load_print_meta: max token length = 1024
0.00.053.732 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.732 I llm_load_tensors: offloading output layer to GPU
0.00.053.732 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.743 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.744 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.768 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.769 I llama_new_context_with_model: n_ctx         = 128
0.00.054.769 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.769 I llama_new_context_with_model: n_batch       = 128
0.00.054.769 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.769 I llama_new_context_with_model: flash_attn    = 0
0.00.054.770 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.770 I llama_new_context_with_model: freq_scale    = 1
0.00.054.770 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.771 I ggml_metal_init: allocating
0.00.054.776 I ggml_metal_init: found device: Apple M4
0.00.054.778 I ggml_metal_init: picking default device: Apple M4
0.00.055.333 I ggml_metal_init: using embedded metal library
0.00.057.671 I ggml_metal_init: GPU name:   Apple M4
0.00.057.673 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.673 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.673 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.673 I ggml_metal_init: simdgroup reduction   = true
0.00.057.674 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.674 I ggml_metal_init: has bfloat            = true
0.00.057.674 I ggml_metal_init: use bfloat            = true
0.00.057.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.390 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.870 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.873 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.887 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.824 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.825 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.825 I llama_new_context_with_model: graph nodes  = 967
0.00.069.826 I llama_new_context_with_model: graph splits = 2
0.00.069.839 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.840 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.514 I 
0.00.739.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.597 I perplexity: tokenizing the input ..
0.00.747.580 I perplexity: tokenization took 7.982 ms
0.00.747.589 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.882.749 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.883.914 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.883.943 I llama_perf_context_print:        load time =     730.31 ms
0.00.883.944 I llama_perf_context_print: prompt eval time =     134.93 ms /   128 tokens (    1.05 ms per token,   948.61 tokens per second)
0.00.883.945 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.883.945 I llama_perf_context_print:       total time =     144.43 ms /   129 tokens
0.00.884.396 I ggml_metal_free: deallocating

real	0m0.899s
user	0m0.080s
sys	0m0.113s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.834 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.547 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.554 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.556 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.557 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.557 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.559 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.559 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.634 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.631 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.633 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.633 I llama_model_loader: - type  f32:  194 tensors
0.00.024.634 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.634 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.634 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.491 I llm_load_vocab: special tokens cache size = 25
0.00.051.374 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.377 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.377 I llm_load_print_meta: arch             = gptneox
0.00.051.378 I llm_load_print_meta: vocab type       = BPE
0.00.051.378 I llm_load_print_meta: n_vocab          = 50304
0.00.051.378 I llm_load_print_meta: n_merges         = 50009
0.00.051.378 I llm_load_print_meta: vocab_only       = 0
0.00.051.379 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.379 I llm_load_print_meta: n_embd           = 2048
0.00.051.379 I llm_load_print_meta: n_layer          = 24
0.00.051.382 I llm_load_print_meta: n_head           = 16
0.00.051.382 I llm_load_print_meta: n_head_kv        = 16
0.00.051.383 I llm_load_print_meta: n_rot            = 32
0.00.051.383 I llm_load_print_meta: n_swa            = 0
0.00.051.383 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.383 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.384 I llm_load_print_meta: n_gqa            = 1
0.00.051.385 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.386 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.386 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.388 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.389 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.389 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.389 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.390 I llm_load_print_meta: n_ff             = 8192
0.00.051.390 I llm_load_print_meta: n_expert         = 0
0.00.051.390 I llm_load_print_meta: n_expert_used    = 0
0.00.051.391 I llm_load_print_meta: causal attn      = 1
0.00.051.391 I llm_load_print_meta: pooling type     = 0
0.00.051.391 I llm_load_print_meta: rope type        = 2
0.00.051.391 I llm_load_print_meta: rope scaling     = linear
0.00.051.392 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.392 I llm_load_print_meta: freq_scale_train = 1
0.00.051.392 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.394 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.394 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.394 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.394 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.394 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.394 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.395 I llm_load_print_meta: model type       = 1.4B
0.00.051.395 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.396 I llm_load_print_meta: model params     = 1.41 B
0.00.051.396 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.396 I llm_load_print_meta: general.name     = 1.4B
0.00.051.397 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.397 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.397 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.397 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.398 I llm_load_print_meta: LF token         = 128 ''
0.00.051.398 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.398 I llm_load_print_meta: max token length = 1024
0.00.053.012 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.012 I llm_load_tensors: offloading output layer to GPU
0.00.053.013 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.023 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.024 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.878 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.879 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.880 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.880 I llama_new_context_with_model: n_batch       = 2048
0.00.053.880 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.880 I llama_new_context_with_model: flash_attn    = 0
0.00.053.881 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.881 I llama_new_context_with_model: freq_scale    = 1
0.00.053.882 I ggml_metal_init: allocating
0.00.053.885 I ggml_metal_init: found device: Apple M4
0.00.053.887 I ggml_metal_init: picking default device: Apple M4
0.00.054.482 I ggml_metal_init: using embedded metal library
0.00.056.832 I ggml_metal_init: GPU name:   Apple M4
0.00.056.834 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.835 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.835 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.835 I ggml_metal_init: simdgroup reduction   = true
0.00.056.835 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.836 I ggml_metal_init: has bfloat            = true
0.00.056.836 I ggml_metal_init: use bfloat            = true
0.00.056.836 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.837 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.796 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.612 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.618 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.635 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.593 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.594 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.595 I llama_new_context_with_model: graph nodes  = 967
0.00.087.595 I llama_new_context_with_model: graph splits = 2
0.00.087.618 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.753 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.753 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.452.765 I main: llama threadpool init, n_threads = 4
0.00.452.803 I 
0.00.452.835 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.452.836 I 
0.00.453.058 I sampler seed: 1234
0.00.453.064 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.453.077 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.453.078 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.453.078 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.130.872 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65197.43 tokens per second)
0.01.130.873 I llama_perf_context_print:        load time =     442.93 ms
0.01.130.873 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.62 tokens per second)
0.01.130.874 I llama_perf_context_print:        eval time =     639.23 ms /    63 runs   (   10.15 ms per token,    98.56 tokens per second)
0.01.130.878 I llama_perf_context_print:       total time =     678.11 ms /    70 tokens
0.01.131.019 I ggml_metal_free: deallocating

real	0m1.150s
user	0m0.111s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.231 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.956 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.961 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.963 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.964 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.965 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.966 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.968 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.969 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.969 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.969 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.970 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.971 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.973 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.973 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.870 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.929 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.927 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.929 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.930 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.930 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.930 I llama_model_loader: - type  f32:  194 tensors
0.00.024.931 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.931 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.931 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.328 I llm_load_vocab: special tokens cache size = 25
0.00.052.249 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.252 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.252 I llm_load_print_meta: arch             = gptneox
0.00.052.253 I llm_load_print_meta: vocab type       = BPE
0.00.052.253 I llm_load_print_meta: n_vocab          = 50304
0.00.052.253 I llm_load_print_meta: n_merges         = 50009
0.00.052.253 I llm_load_print_meta: vocab_only       = 0
0.00.052.254 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.254 I llm_load_print_meta: n_embd           = 2048
0.00.052.254 I llm_load_print_meta: n_layer          = 24
0.00.052.257 I llm_load_print_meta: n_head           = 16
0.00.052.258 I llm_load_print_meta: n_head_kv        = 16
0.00.052.258 I llm_load_print_meta: n_rot            = 32
0.00.052.259 I llm_load_print_meta: n_swa            = 0
0.00.052.259 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.259 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.260 I llm_load_print_meta: n_gqa            = 1
0.00.052.261 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.261 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.262 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.262 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.262 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.263 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.263 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.263 I llm_load_print_meta: n_ff             = 8192
0.00.052.264 I llm_load_print_meta: n_expert         = 0
0.00.052.264 I llm_load_print_meta: n_expert_used    = 0
0.00.052.264 I llm_load_print_meta: causal attn      = 1
0.00.052.264 I llm_load_print_meta: pooling type     = 0
0.00.052.264 I llm_load_print_meta: rope type        = 2
0.00.052.265 I llm_load_print_meta: rope scaling     = linear
0.00.052.265 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.267 I llm_load_print_meta: freq_scale_train = 1
0.00.052.267 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.267 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.268 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.268 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.268 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.268 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.268 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.268 I llm_load_print_meta: model type       = 1.4B
0.00.052.269 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.269 I llm_load_print_meta: model params     = 1.41 B
0.00.052.270 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.270 I llm_load_print_meta: general.name     = 1.4B
0.00.052.270 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.271 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.271 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.271 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.271 I llm_load_print_meta: LF token         = 128 ''
0.00.052.272 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.272 I llm_load_print_meta: max token length = 1024
0.00.054.199 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.200 I llm_load_tensors: offloading output layer to GPU
0.00.054.200 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.210 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.212 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.138 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.139 I llama_new_context_with_model: n_ctx         = 128
0.00.055.139 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.140 I llama_new_context_with_model: n_batch       = 128
0.00.055.140 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.140 I llama_new_context_with_model: flash_attn    = 0
0.00.055.140 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.141 I llama_new_context_with_model: freq_scale    = 1
0.00.055.141 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.141 I ggml_metal_init: allocating
0.00.055.147 I ggml_metal_init: found device: Apple M4
0.00.055.149 I ggml_metal_init: picking default device: Apple M4
0.00.055.732 I ggml_metal_init: using embedded metal library
0.00.058.097 I ggml_metal_init: GPU name:   Apple M4
0.00.058.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.099 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.100 I ggml_metal_init: simdgroup reduction   = true
0.00.058.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.100 I ggml_metal_init: has bfloat            = true
0.00.058.100 I ggml_metal_init: use bfloat            = true
0.00.058.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.122 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.397 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.400 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.414 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.278 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.279 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.279 I llama_new_context_with_model: graph nodes  = 967
0.00.070.280 I llama_new_context_with_model: graph splits = 2
0.00.070.292 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.293 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.403.946 I 
0.00.403.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.000 I perplexity: tokenizing the input ..
0.00.412.040 I perplexity: tokenization took 8.04 ms
0.00.412.043 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.544.826 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.545.974 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.545.999 I llama_perf_context_print:        load time =     393.71 ms
0.00.546.000 I llama_perf_context_print: prompt eval time =     132.55 ms /   128 tokens (    1.04 ms per token,   965.70 tokens per second)
0.00.546.001 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.546.001 I llama_perf_context_print:       total time =     142.06 ms /   129 tokens
0.00.546.581 I ggml_metal_free: deallocating

real	0m0.562s
user	0m0.080s
sys	0m0.076s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.860 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.618 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.629 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.630 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.630 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.631 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.631 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.632 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.632 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.633 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.633 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.633 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.772 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.733 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.734 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.735 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.735 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.735 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.736 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.736 I llama_model_loader: - type  f32:  194 tensors
0.00.025.736 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.737 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.737 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.737 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.265 I llm_load_vocab: special tokens cache size = 25
0.00.053.145 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.148 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.148 I llm_load_print_meta: arch             = gptneox
0.00.053.149 I llm_load_print_meta: vocab type       = BPE
0.00.053.149 I llm_load_print_meta: n_vocab          = 50304
0.00.053.149 I llm_load_print_meta: n_merges         = 50009
0.00.053.149 I llm_load_print_meta: vocab_only       = 0
0.00.053.150 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.150 I llm_load_print_meta: n_embd           = 2048
0.00.053.150 I llm_load_print_meta: n_layer          = 24
0.00.053.152 I llm_load_print_meta: n_head           = 16
0.00.053.153 I llm_load_print_meta: n_head_kv        = 16
0.00.053.153 I llm_load_print_meta: n_rot            = 32
0.00.053.154 I llm_load_print_meta: n_swa            = 0
0.00.053.154 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.154 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.155 I llm_load_print_meta: n_gqa            = 1
0.00.053.156 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.156 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.157 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.157 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.158 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.158 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.158 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.159 I llm_load_print_meta: n_ff             = 8192
0.00.053.159 I llm_load_print_meta: n_expert         = 0
0.00.053.159 I llm_load_print_meta: n_expert_used    = 0
0.00.053.159 I llm_load_print_meta: causal attn      = 1
0.00.053.159 I llm_load_print_meta: pooling type     = 0
0.00.053.159 I llm_load_print_meta: rope type        = 2
0.00.053.160 I llm_load_print_meta: rope scaling     = linear
0.00.053.160 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.160 I llm_load_print_meta: freq_scale_train = 1
0.00.053.161 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.161 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.161 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.161 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.161 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.162 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.162 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.162 I llm_load_print_meta: model type       = 1.4B
0.00.053.165 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.165 I llm_load_print_meta: model params     = 1.41 B
0.00.053.166 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.166 I llm_load_print_meta: general.name     = 1.4B
0.00.053.166 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.166 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.166 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.166 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.167 I llm_load_print_meta: LF token         = 128 ''
0.00.053.171 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.171 I llm_load_print_meta: max token length = 1024
0.00.055.197 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.197 I llm_load_tensors: offloading output layer to GPU
0.00.055.197 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.208 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.209 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.166 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.167 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.167 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.167 I llama_new_context_with_model: n_batch       = 2048
0.00.056.167 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.167 I llama_new_context_with_model: flash_attn    = 0
0.00.056.168 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.168 I llama_new_context_with_model: freq_scale    = 1
0.00.056.169 I ggml_metal_init: allocating
0.00.056.176 I ggml_metal_init: found device: Apple M4
0.00.056.180 I ggml_metal_init: picking default device: Apple M4
0.00.056.778 I ggml_metal_init: using embedded metal library
0.00.059.114 I ggml_metal_init: GPU name:   Apple M4
0.00.059.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.116 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.116 I ggml_metal_init: simdgroup reduction   = true
0.00.059.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.117 I ggml_metal_init: has bfloat            = true
0.00.059.117 I ggml_metal_init: use bfloat            = true
0.00.059.117 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.960 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.517 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.524 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.544 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.555 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.556 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.557 I llama_new_context_with_model: graph nodes  = 967
0.00.090.557 I llama_new_context_with_model: graph splits = 2
0.00.090.582 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.097 I main: llama threadpool init, n_threads = 4
0.00.540.140 I 
0.00.540.191 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.192 I 
0.00.540.403 I sampler seed: 1234
0.00.540.407 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.450 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.454 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.455 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.286.734 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.286.735 I llama_perf_context_print:        load time =     530.23 ms
0.01.286.736 I llama_perf_context_print: prompt eval time =      40.41 ms /     7 tokens (    5.77 ms per token,   173.20 tokens per second)
0.01.286.737 I llama_perf_context_print:        eval time =     702.96 ms /    63 runs   (   11.16 ms per token,    89.62 tokens per second)
0.01.286.737 I llama_perf_context_print:       total time =     746.64 ms /    70 tokens
0.01.286.930 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.113s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.271 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.510 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.511 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.511 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.511 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.512 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.513 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.513 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.514 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.514 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.514 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.515 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.518 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.542 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.624 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.686 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.687 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.687 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.688 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.688 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.689 I llama_model_loader: - type  f32:  194 tensors
0.00.024.689 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.689 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.690 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.690 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.023 I llm_load_vocab: special tokens cache size = 25
0.00.051.904 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.908 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.909 I llm_load_print_meta: arch             = gptneox
0.00.051.909 I llm_load_print_meta: vocab type       = BPE
0.00.051.909 I llm_load_print_meta: n_vocab          = 50304
0.00.051.909 I llm_load_print_meta: n_merges         = 50009
0.00.051.909 I llm_load_print_meta: vocab_only       = 0
0.00.051.910 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.910 I llm_load_print_meta: n_embd           = 2048
0.00.051.912 I llm_load_print_meta: n_layer          = 24
0.00.051.917 I llm_load_print_meta: n_head           = 16
0.00.051.918 I llm_load_print_meta: n_head_kv        = 16
0.00.051.919 I llm_load_print_meta: n_rot            = 32
0.00.051.920 I llm_load_print_meta: n_swa            = 0
0.00.051.920 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.920 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.920 I llm_load_print_meta: n_gqa            = 1
0.00.051.922 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.922 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.923 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.924 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.924 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.924 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.924 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.925 I llm_load_print_meta: n_ff             = 8192
0.00.051.925 I llm_load_print_meta: n_expert         = 0
0.00.051.925 I llm_load_print_meta: n_expert_used    = 0
0.00.051.925 I llm_load_print_meta: causal attn      = 1
0.00.051.925 I llm_load_print_meta: pooling type     = 0
0.00.051.925 I llm_load_print_meta: rope type        = 2
0.00.051.927 I llm_load_print_meta: rope scaling     = linear
0.00.051.927 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.928 I llm_load_print_meta: freq_scale_train = 1
0.00.051.928 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.928 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.928 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.928 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.928 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.928 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.929 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.929 I llm_load_print_meta: model type       = 1.4B
0.00.051.930 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.930 I llm_load_print_meta: model params     = 1.41 B
0.00.051.930 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.930 I llm_load_print_meta: general.name     = 1.4B
0.00.051.931 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.931 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.931 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.931 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.931 I llm_load_print_meta: LF token         = 128 ''
0.00.051.932 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.932 I llm_load_print_meta: max token length = 1024
0.00.053.876 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.876 I llm_load_tensors: offloading output layer to GPU
0.00.053.876 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.887 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.888 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.788 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.789 I llama_new_context_with_model: n_ctx         = 128
0.00.054.789 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.789 I llama_new_context_with_model: n_batch       = 128
0.00.054.790 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.790 I llama_new_context_with_model: flash_attn    = 0
0.00.054.790 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.791 I llama_new_context_with_model: freq_scale    = 1
0.00.054.791 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.792 I ggml_metal_init: allocating
0.00.054.798 I ggml_metal_init: found device: Apple M4
0.00.054.801 I ggml_metal_init: picking default device: Apple M4
0.00.055.429 I ggml_metal_init: using embedded metal library
0.00.057.821 I ggml_metal_init: GPU name:   Apple M4
0.00.057.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.823 I ggml_metal_init: simdgroup reduction   = true
0.00.057.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.824 I ggml_metal_init: has bfloat            = true
0.00.057.824 I ggml_metal_init: use bfloat            = true
0.00.057.824 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.038 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.317 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.320 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.337 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.252 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.253 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.253 I llama_new_context_with_model: graph nodes  = 967
0.00.070.254 I llama_new_context_with_model: graph splits = 2
0.00.070.266 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.458.186 I 
0.00.458.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.458.234 I perplexity: tokenizing the input ..
0.00.465.781 I perplexity: tokenization took 7.545 ms
0.00.465.785 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.597.134 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.598.397 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.598.414 I llama_perf_context_print:        load time =     448.91 ms
0.00.598.415 I llama_perf_context_print: prompt eval time =     131.12 ms /   128 tokens (    1.02 ms per token,   976.24 tokens per second)
0.00.598.416 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.598.417 I llama_perf_context_print:       total time =     140.23 ms /   129 tokens
0.00.598.769 I ggml_metal_free: deallocating

real	0m0.613s
user	0m0.081s
sys	0m0.067s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.155 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.759 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.660 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.720 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.677 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.679 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.679 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.679 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.679 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.680 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.680 I llama_model_loader: - type  f32:  194 tensors
0.00.025.681 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.681 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.681 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.544 I llm_load_vocab: special tokens cache size = 25
0.00.052.461 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.463 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.465 I llm_load_print_meta: arch             = gptneox
0.00.052.466 I llm_load_print_meta: vocab type       = BPE
0.00.052.466 I llm_load_print_meta: n_vocab          = 50304
0.00.052.466 I llm_load_print_meta: n_merges         = 50009
0.00.052.467 I llm_load_print_meta: vocab_only       = 0
0.00.052.467 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.467 I llm_load_print_meta: n_embd           = 2048
0.00.052.467 I llm_load_print_meta: n_layer          = 24
0.00.052.470 I llm_load_print_meta: n_head           = 16
0.00.052.471 I llm_load_print_meta: n_head_kv        = 16
0.00.052.471 I llm_load_print_meta: n_rot            = 32
0.00.052.471 I llm_load_print_meta: n_swa            = 0
0.00.052.471 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.472 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.473 I llm_load_print_meta: n_gqa            = 1
0.00.052.474 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.475 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.475 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.476 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.476 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.476 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.476 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.477 I llm_load_print_meta: n_ff             = 8192
0.00.052.477 I llm_load_print_meta: n_expert         = 0
0.00.052.479 I llm_load_print_meta: n_expert_used    = 0
0.00.052.480 I llm_load_print_meta: causal attn      = 1
0.00.052.481 I llm_load_print_meta: pooling type     = 0
0.00.052.481 I llm_load_print_meta: rope type        = 2
0.00.052.481 I llm_load_print_meta: rope scaling     = linear
0.00.052.481 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.482 I llm_load_print_meta: freq_scale_train = 1
0.00.052.482 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.482 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.482 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.483 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.483 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.483 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.487 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.488 I llm_load_print_meta: model type       = 1.4B
0.00.052.489 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.489 I llm_load_print_meta: model params     = 1.41 B
0.00.052.489 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.490 I llm_load_print_meta: general.name     = 1.4B
0.00.052.490 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.490 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.490 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.490 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.491 I llm_load_print_meta: LF token         = 128 ''
0.00.052.491 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.491 I llm_load_print_meta: max token length = 1024
0.00.054.507 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.507 I llm_load_tensors: offloading output layer to GPU
0.00.054.508 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.518 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.520 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.418 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.418 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.419 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.419 I llama_new_context_with_model: n_batch       = 2048
0.00.055.419 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.419 I llama_new_context_with_model: flash_attn    = 0
0.00.055.420 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.420 I llama_new_context_with_model: freq_scale    = 1
0.00.055.420 I ggml_metal_init: allocating
0.00.055.423 I ggml_metal_init: found device: Apple M4
0.00.055.425 I ggml_metal_init: picking default device: Apple M4
0.00.056.021 I ggml_metal_init: using embedded metal library
0.00.058.401 I ggml_metal_init: GPU name:   Apple M4
0.00.058.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.402 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.405 I ggml_metal_init: simdgroup reduction   = true
0.00.058.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.405 I ggml_metal_init: has bfloat            = true
0.00.058.405 I ggml_metal_init: use bfloat            = true
0.00.058.406 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.320 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.606 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.614 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.631 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.670 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.671 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.672 I llama_new_context_with_model: graph nodes  = 967
0.00.089.672 I llama_new_context_with_model: graph splits = 2
0.00.089.695 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.635 I main: llama threadpool init, n_threads = 4
0.00.612.680 I 
0.00.612.713 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.714 I 
0.00.612.947 I sampler seed: 1234
0.00.612.952 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.612.985 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.612.986 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.612.986 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.370.740 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55642.63 tokens per second)
0.01.370.740 I llama_perf_context_print:        load time =     602.48 ms
0.01.370.741 I llama_perf_context_print: prompt eval time =      47.19 ms /     7 tokens (    6.74 ms per token,   148.33 tokens per second)
0.01.370.742 I llama_perf_context_print:        eval time =     707.46 ms /    63 runs   (   11.23 ms per token,    89.05 tokens per second)
0.01.370.742 I llama_perf_context_print:       total time =     758.11 ms /    70 tokens
0.01.370.905 I ggml_metal_free: deallocating

real	0m1.388s
user	0m0.111s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.216 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.190 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.194 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.196 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.196 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.197 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.197 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.197 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.198 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.198 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.199 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.201 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.201 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.202 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.202 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.203 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.204 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.208 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.142 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.213 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.280 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.280 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.281 I llama_model_loader: - type  f32:  194 tensors
0.00.024.281 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.282 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.282 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.672 I llm_load_vocab: special tokens cache size = 25
0.00.051.726 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.731 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.731 I llm_load_print_meta: arch             = gptneox
0.00.051.732 I llm_load_print_meta: vocab type       = BPE
0.00.051.732 I llm_load_print_meta: n_vocab          = 50304
0.00.051.732 I llm_load_print_meta: n_merges         = 50009
0.00.051.733 I llm_load_print_meta: vocab_only       = 0
0.00.051.733 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.733 I llm_load_print_meta: n_embd           = 2048
0.00.051.733 I llm_load_print_meta: n_layer          = 24
0.00.051.736 I llm_load_print_meta: n_head           = 16
0.00.051.737 I llm_load_print_meta: n_head_kv        = 16
0.00.051.739 I llm_load_print_meta: n_rot            = 32
0.00.051.739 I llm_load_print_meta: n_swa            = 0
0.00.051.740 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.740 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.741 I llm_load_print_meta: n_gqa            = 1
0.00.051.742 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.742 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.743 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.743 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.743 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.745 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.745 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.746 I llm_load_print_meta: n_ff             = 8192
0.00.051.746 I llm_load_print_meta: n_expert         = 0
0.00.051.746 I llm_load_print_meta: n_expert_used    = 0
0.00.051.746 I llm_load_print_meta: causal attn      = 1
0.00.051.746 I llm_load_print_meta: pooling type     = 0
0.00.051.747 I llm_load_print_meta: rope type        = 2
0.00.051.747 I llm_load_print_meta: rope scaling     = linear
0.00.051.747 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.747 I llm_load_print_meta: freq_scale_train = 1
0.00.051.748 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.748 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.748 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.749 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.749 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.749 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.749 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.751 I llm_load_print_meta: model type       = 1.4B
0.00.051.751 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.752 I llm_load_print_meta: model params     = 1.41 B
0.00.051.755 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.755 I llm_load_print_meta: general.name     = 1.4B
0.00.051.756 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.756 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.757 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.758 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.758 I llm_load_print_meta: LF token         = 128 ''
0.00.051.758 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.758 I llm_load_print_meta: max token length = 1024
0.00.053.869 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.869 I llm_load_tensors: offloading output layer to GPU
0.00.053.869 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.880 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.881 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.803 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.804 I llama_new_context_with_model: n_ctx         = 128
0.00.054.804 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.804 I llama_new_context_with_model: n_batch       = 128
0.00.054.804 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.805 I llama_new_context_with_model: flash_attn    = 0
0.00.054.805 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.805 I llama_new_context_with_model: freq_scale    = 1
0.00.054.806 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.806 I ggml_metal_init: allocating
0.00.054.810 I ggml_metal_init: found device: Apple M4
0.00.054.816 I ggml_metal_init: picking default device: Apple M4
0.00.055.407 I ggml_metal_init: using embedded metal library
0.00.057.762 I ggml_metal_init: GPU name:   Apple M4
0.00.057.763 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.764 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.764 I ggml_metal_init: simdgroup reduction   = true
0.00.057.764 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.765 I ggml_metal_init: has bfloat            = true
0.00.057.765 I ggml_metal_init: use bfloat            = true
0.00.057.765 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.766 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.905 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.191 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.193 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.210 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.087 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.088 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.089 I llama_new_context_with_model: graph nodes  = 967
0.00.070.089 I llama_new_context_with_model: graph splits = 2
0.00.070.103 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.104 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.407 I 
0.00.558.442 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.558.456 I perplexity: tokenizing the input ..
0.00.566.832 I perplexity: tokenization took 8.375 ms
0.00.566.837 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.701.060 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.702.228 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.702.244 I llama_perf_context_print:        load time =     549.19 ms
0.00.702.245 I llama_perf_context_print: prompt eval time =     134.00 ms /   128 tokens (    1.05 ms per token,   955.25 tokens per second)
0.00.702.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.702.246 I llama_perf_context_print:       total time =     143.84 ms /   129 tokens
0.00.702.717 I ggml_metal_free: deallocating

real	0m0.718s
user	0m0.081s
sys	0m0.097s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.816 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.147 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.152 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.162 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.162 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.162 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.163 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.314 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.415 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.354 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.354 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.355 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.355 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.355 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.356 I llama_model_loader: - type  f32:  194 tensors
0.00.025.356 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.357 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.168 I llm_load_vocab: special tokens cache size = 25
0.00.051.978 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.981 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.981 I llm_load_print_meta: arch             = gptneox
0.00.051.982 I llm_load_print_meta: vocab type       = BPE
0.00.051.982 I llm_load_print_meta: n_vocab          = 50304
0.00.051.982 I llm_load_print_meta: n_merges         = 50009
0.00.051.982 I llm_load_print_meta: vocab_only       = 0
0.00.051.983 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.983 I llm_load_print_meta: n_embd           = 2048
0.00.051.983 I llm_load_print_meta: n_layer          = 24
0.00.051.985 I llm_load_print_meta: n_head           = 16
0.00.051.986 I llm_load_print_meta: n_head_kv        = 16
0.00.051.986 I llm_load_print_meta: n_rot            = 32
0.00.051.986 I llm_load_print_meta: n_swa            = 0
0.00.051.987 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.987 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.990 I llm_load_print_meta: n_gqa            = 1
0.00.051.991 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.991 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.992 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.992 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.992 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.992 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.993 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.993 I llm_load_print_meta: n_ff             = 8192
0.00.051.994 I llm_load_print_meta: n_expert         = 0
0.00.051.994 I llm_load_print_meta: n_expert_used    = 0
0.00.051.994 I llm_load_print_meta: causal attn      = 1
0.00.051.994 I llm_load_print_meta: pooling type     = 0
0.00.051.994 I llm_load_print_meta: rope type        = 2
0.00.051.994 I llm_load_print_meta: rope scaling     = linear
0.00.051.995 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.995 I llm_load_print_meta: freq_scale_train = 1
0.00.051.995 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.997 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.997 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.997 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.998 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.998 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.998 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.998 I llm_load_print_meta: model type       = 1.4B
0.00.051.999 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.999 I llm_load_print_meta: model params     = 1.41 B
0.00.052.000 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.000 I llm_load_print_meta: general.name     = 1.4B
0.00.052.000 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.000 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.001 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.001 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.005 I llm_load_print_meta: LF token         = 128 ''
0.00.052.005 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.006 I llm_load_print_meta: max token length = 1024
0.00.054.084 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.084 I llm_load_tensors: offloading output layer to GPU
0.00.054.085 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.095 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.096 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.063 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.064 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.064 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.064 I llama_new_context_with_model: n_batch       = 2048
0.00.055.064 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.065 I llama_new_context_with_model: flash_attn    = 0
0.00.055.065 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.065 I llama_new_context_with_model: freq_scale    = 1
0.00.055.066 I ggml_metal_init: allocating
0.00.055.071 I ggml_metal_init: found device: Apple M4
0.00.055.074 I ggml_metal_init: picking default device: Apple M4
0.00.055.640 I ggml_metal_init: using embedded metal library
0.00.057.958 I ggml_metal_init: GPU name:   Apple M4
0.00.057.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.961 I ggml_metal_init: simdgroup reduction   = true
0.00.057.961 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.961 I ggml_metal_init: has bfloat            = true
0.00.057.961 I ggml_metal_init: use bfloat            = true
0.00.057.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.962 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.747 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.202 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.207 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.226 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.180 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.181 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.182 I llama_new_context_with_model: graph nodes  = 967
0.00.088.182 I llama_new_context_with_model: graph splits = 2
0.00.088.194 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.665 I main: llama threadpool init, n_threads = 4
0.00.701.713 I 
0.00.701.757 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.757 I 
0.00.701.991 I sampler seed: 1234
0.00.701.995 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.027 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.029 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.029 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.551.735 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62720.85 tokens per second)
0.01.551.736 I llama_perf_context_print:        load time =     692.84 ms
0.01.551.737 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.77 tokens per second)
0.01.551.737 I llama_perf_context_print:        eval time =     795.29 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.551.738 I llama_perf_context_print:       total time =     850.08 ms /    70 tokens
0.01.551.939 I ggml_metal_free: deallocating

real	0m1.569s
user	0m0.111s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.153 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.058 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.063 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.065 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.065 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.066 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.067 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.067 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.068 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.070 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.095 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.052 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.053 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.053 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.054 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.054 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.054 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.055 I llama_model_loader: - type  f32:  194 tensors
0.00.025.055 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.056 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.798 I llm_load_vocab: special tokens cache size = 25
0.00.051.659 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.661 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.662 I llm_load_print_meta: arch             = gptneox
0.00.051.662 I llm_load_print_meta: vocab type       = BPE
0.00.051.662 I llm_load_print_meta: n_vocab          = 50304
0.00.051.663 I llm_load_print_meta: n_merges         = 50009
0.00.051.663 I llm_load_print_meta: vocab_only       = 0
0.00.051.663 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.663 I llm_load_print_meta: n_embd           = 2048
0.00.051.663 I llm_load_print_meta: n_layer          = 24
0.00.051.666 I llm_load_print_meta: n_head           = 16
0.00.051.667 I llm_load_print_meta: n_head_kv        = 16
0.00.051.667 I llm_load_print_meta: n_rot            = 32
0.00.051.667 I llm_load_print_meta: n_swa            = 0
0.00.051.668 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.670 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.671 I llm_load_print_meta: n_gqa            = 1
0.00.051.672 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.674 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.674 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.674 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.675 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.675 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.675 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.676 I llm_load_print_meta: n_ff             = 8192
0.00.051.676 I llm_load_print_meta: n_expert         = 0
0.00.051.676 I llm_load_print_meta: n_expert_used    = 0
0.00.051.676 I llm_load_print_meta: causal attn      = 1
0.00.051.678 I llm_load_print_meta: pooling type     = 0
0.00.051.678 I llm_load_print_meta: rope type        = 2
0.00.051.678 I llm_load_print_meta: rope scaling     = linear
0.00.051.678 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.678 I llm_load_print_meta: freq_scale_train = 1
0.00.051.679 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.679 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.679 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.679 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.679 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.679 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.679 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.680 I llm_load_print_meta: model type       = 1.4B
0.00.051.680 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.685 I llm_load_print_meta: model params     = 1.41 B
0.00.051.685 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.686 I llm_load_print_meta: general.name     = 1.4B
0.00.051.687 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.688 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.688 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.688 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.688 I llm_load_print_meta: LF token         = 128 ''
0.00.051.688 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.689 I llm_load_print_meta: max token length = 1024
0.00.053.679 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.679 I llm_load_tensors: offloading output layer to GPU
0.00.053.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.689 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.690 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.656 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.657 I llama_new_context_with_model: n_ctx         = 128
0.00.054.657 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.657 I llama_new_context_with_model: n_batch       = 128
0.00.054.658 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.658 I llama_new_context_with_model: flash_attn    = 0
0.00.054.658 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.659 I llama_new_context_with_model: freq_scale    = 1
0.00.054.659 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.659 I ggml_metal_init: allocating
0.00.054.665 I ggml_metal_init: found device: Apple M4
0.00.054.667 I ggml_metal_init: picking default device: Apple M4
0.00.055.252 I ggml_metal_init: using embedded metal library
0.00.057.571 I ggml_metal_init: GPU name:   Apple M4
0.00.057.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.573 I ggml_metal_init: simdgroup reduction   = true
0.00.057.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.573 I ggml_metal_init: has bfloat            = true
0.00.057.573 I ggml_metal_init: use bfloat            = true
0.00.057.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.432 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.730 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.733 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.747 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.672 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.673 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.674 I llama_new_context_with_model: graph nodes  = 967
0.00.069.674 I llama_new_context_with_model: graph splits = 2
0.00.069.686 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.687 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.245 I 
0.00.663.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.294 I perplexity: tokenizing the input ..
0.00.671.867 I perplexity: tokenization took 8.571 ms
0.00.671.876 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.476 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.813.639 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.813.656 I llama_perf_context_print:        load time =     653.09 ms
0.00.813.657 I llama_perf_context_print: prompt eval time =     140.37 ms /   128 tokens (    1.10 ms per token,   911.86 tokens per second)
0.00.813.657 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.658 I llama_perf_context_print:       total time =     150.41 ms /   129 tokens
0.00.813.971 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.080s
sys	0m0.136s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.057 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.531 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.536 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.537 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.544 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.544 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.545 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.545 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.546 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.547 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.548 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.548 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.548 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.549 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.549 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.551 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.551 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.551 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.546 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.473 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.474 I llama_model_loader: - type  f32:  194 tensors
0.00.025.474 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.899 I llm_load_vocab: special tokens cache size = 25
0.00.052.786 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.788 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.789 I llm_load_print_meta: arch             = gptneox
0.00.052.789 I llm_load_print_meta: vocab type       = BPE
0.00.052.789 I llm_load_print_meta: n_vocab          = 50304
0.00.052.789 I llm_load_print_meta: n_merges         = 50009
0.00.052.790 I llm_load_print_meta: vocab_only       = 0
0.00.052.790 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.790 I llm_load_print_meta: n_embd           = 2048
0.00.052.790 I llm_load_print_meta: n_layer          = 24
0.00.052.793 I llm_load_print_meta: n_head           = 16
0.00.052.795 I llm_load_print_meta: n_head_kv        = 16
0.00.052.796 I llm_load_print_meta: n_rot            = 32
0.00.052.796 I llm_load_print_meta: n_swa            = 0
0.00.052.796 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.796 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.797 I llm_load_print_meta: n_gqa            = 1
0.00.052.798 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.798 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.799 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.799 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.799 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.800 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.800 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.800 I llm_load_print_meta: n_ff             = 8192
0.00.052.801 I llm_load_print_meta: n_expert         = 0
0.00.052.801 I llm_load_print_meta: n_expert_used    = 0
0.00.052.801 I llm_load_print_meta: causal attn      = 1
0.00.052.801 I llm_load_print_meta: pooling type     = 0
0.00.052.801 I llm_load_print_meta: rope type        = 2
0.00.052.802 I llm_load_print_meta: rope scaling     = linear
0.00.052.802 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.802 I llm_load_print_meta: freq_scale_train = 1
0.00.052.803 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.803 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.803 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.803 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.803 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.803 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.804 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.804 I llm_load_print_meta: model type       = 1.4B
0.00.052.805 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.805 I llm_load_print_meta: model params     = 1.41 B
0.00.052.806 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.806 I llm_load_print_meta: general.name     = 1.4B
0.00.052.806 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.806 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.806 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.806 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.807 I llm_load_print_meta: LF token         = 128 ''
0.00.052.807 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.807 I llm_load_print_meta: max token length = 1024
0.00.054.910 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.911 I llm_load_tensors: offloading output layer to GPU
0.00.054.911 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.921 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.923 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.816 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.817 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.817 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.817 I llama_new_context_with_model: n_batch       = 2048
0.00.055.818 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.818 I llama_new_context_with_model: flash_attn    = 0
0.00.055.818 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.819 I llama_new_context_with_model: freq_scale    = 1
0.00.055.819 I ggml_metal_init: allocating
0.00.055.822 I ggml_metal_init: found device: Apple M4
0.00.055.824 I ggml_metal_init: picking default device: Apple M4
0.00.056.444 I ggml_metal_init: using embedded metal library
0.00.058.932 I ggml_metal_init: GPU name:   Apple M4
0.00.058.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.934 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.934 I ggml_metal_init: simdgroup reduction   = true
0.00.058.934 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.935 I ggml_metal_init: has bfloat            = true
0.00.058.935 I ggml_metal_init: use bfloat            = true
0.00.058.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.095 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.843 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.855 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.876 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.905 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.907 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.907 I llama_new_context_with_model: graph nodes  = 967
0.00.088.908 I llama_new_context_with_model: graph splits = 2
0.00.088.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.648 I main: llama threadpool init, n_threads = 4
0.00.760.684 I 
0.00.760.712 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.713 I 
0.00.760.936 I sampler seed: 1234
0.00.760.943 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.989 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.993 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.993 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.642.639 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48168.25 tokens per second)
0.01.642.640 I llama_perf_context_print:        load time =     750.59 ms
0.01.642.640 I llama_perf_context_print: prompt eval time =      54.18 ms /     7 tokens (    7.74 ms per token,   129.20 tokens per second)
0.01.642.641 I llama_perf_context_print:        eval time =     824.97 ms /    63 runs   (   13.09 ms per token,    76.37 tokens per second)
0.01.642.641 I llama_perf_context_print:       total time =     881.99 ms /    70 tokens
0.01.642.880 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.112s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4367 (93aefa31) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.802 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.628 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.634 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.635 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.635 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.636 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.637 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.637 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.640 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.641 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.641 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.381 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.382 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.382 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.383 I llama_model_loader: - type  f32:  194 tensors
0.00.023.383 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.112 I llm_load_vocab: special tokens cache size = 25
0.00.049.963 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.966 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.966 I llm_load_print_meta: arch             = gptneox
0.00.049.967 I llm_load_print_meta: vocab type       = BPE
0.00.049.967 I llm_load_print_meta: n_vocab          = 50304
0.00.049.967 I llm_load_print_meta: n_merges         = 50009
0.00.049.967 I llm_load_print_meta: vocab_only       = 0
0.00.049.967 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.967 I llm_load_print_meta: n_embd           = 2048
0.00.049.968 I llm_load_print_meta: n_layer          = 24
0.00.049.971 I llm_load_print_meta: n_head           = 16
0.00.049.971 I llm_load_print_meta: n_head_kv        = 16
0.00.049.971 I llm_load_print_meta: n_rot            = 32
0.00.049.972 I llm_load_print_meta: n_swa            = 0
0.00.049.972 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.972 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.973 I llm_load_print_meta: n_gqa            = 1
0.00.049.974 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.975 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.975 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.975 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.977 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.977 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.984 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.992 I llm_load_print_meta: n_ff             = 8192
0.00.049.992 I llm_load_print_meta: n_expert         = 0
0.00.049.993 I llm_load_print_meta: n_expert_used    = 0
0.00.049.994 I llm_load_print_meta: causal attn      = 1
0.00.049.994 I llm_load_print_meta: pooling type     = 0
0.00.049.994 I llm_load_print_meta: rope type        = 2
0.00.049.994 I llm_load_print_meta: rope scaling     = linear
0.00.049.995 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.996 I llm_load_print_meta: freq_scale_train = 1
0.00.049.996 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.996 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.996 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.996 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.996 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.996 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.997 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.997 I llm_load_print_meta: model type       = 1.4B
0.00.050.000 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.000 I llm_load_print_meta: model params     = 1.41 B
0.00.050.002 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.002 I llm_load_print_meta: general.name     = 1.4B
0.00.050.002 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.002 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.003 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.003 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.003 I llm_load_print_meta: LF token         = 128 ''
0.00.050.004 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.004 I llm_load_print_meta: max token length = 1024
0.00.052.047 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.047 I llm_load_tensors: offloading output layer to GPU
0.00.052.047 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.058 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.059 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.981 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.982 I llama_new_context_with_model: n_ctx         = 128
0.00.052.982 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.982 I llama_new_context_with_model: n_batch       = 128
0.00.052.982 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.983 I llama_new_context_with_model: flash_attn    = 0
0.00.052.983 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.983 I llama_new_context_with_model: freq_scale    = 1
0.00.052.984 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.984 I ggml_metal_init: allocating
0.00.052.989 I ggml_metal_init: found device: Apple M4
0.00.052.991 I ggml_metal_init: picking default device: Apple M4
0.00.053.548 I ggml_metal_init: using embedded metal library
0.00.055.876 I ggml_metal_init: GPU name:   Apple M4
0.00.055.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.877 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.878 I ggml_metal_init: simdgroup reduction   = true
0.00.055.878 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.878 I ggml_metal_init: has bfloat            = true
0.00.055.878 I ggml_metal_init: use bfloat            = true
0.00.055.879 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.879 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.652 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.964 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.966 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.988 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.953 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.955 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.955 I llama_new_context_with_model: graph nodes  = 967
0.00.067.955 I llama_new_context_with_model: graph splits = 2
0.00.067.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.876 I 
0.00.434.918 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.943 I perplexity: tokenizing the input ..
0.00.443.193 I perplexity: tokenization took 8.248 ms
0.00.443.201 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.582.752 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.584.017 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.584.032 I llama_perf_context_print:        load time =     426.07 ms
0.00.584.033 I llama_perf_context_print: prompt eval time =     139.32 ms /   128 tokens (    1.09 ms per token,   918.73 tokens per second)
0.00.584.034 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.584.034 I llama_perf_context_print:       total time =     149.16 ms /   129 tokens
0.00.584.416 I ggml_metal_free: deallocating

real	0m0.599s
user	0m0.079s
sys	0m0.093s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4367 (93aefa31)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e40aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e40b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e40bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e40c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e40c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e40ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e40d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e40d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e40ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e40e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e40e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e40ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e40f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e40ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e4107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e410ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e4115e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e411d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e412420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e412bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e413310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e413a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e414150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e4149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e415110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e4153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e4159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e416650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e416b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e416e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e4172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e4175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e417e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e418380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e418640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e418ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e418f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e419420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e4198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e419d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e41a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e41a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e41ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e41afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e41b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e41b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e41bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e41c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e41cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e41d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e41da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e41e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e41e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e41ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e41f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e41f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e41fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e420030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e420640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e420e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e4210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e421590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e421a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e421ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e422370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e422810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e422cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e423150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e4235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e423a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e423f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e4243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e424870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e424dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e425310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e425860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e425db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e426300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e426850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e426da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e4272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e427840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e427d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e4282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e428830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e428d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e4292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e429820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e429d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e42a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e42a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e42ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e42b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e42b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e42bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e42c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e42c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e41c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e42cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e42d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e42d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e42deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e42e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e42e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e42eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e42f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e42f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e42fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e4303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e430930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e430e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e4313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e431920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e431dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e432260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e432700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e432ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e433040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e4334e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e433980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e433e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e4342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e434760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e434c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e4350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e435540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e4359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e435e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e436320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e4367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e436c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e437100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e4375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e437a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e437ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e438380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e438820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e438cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e439160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e439600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e439aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e439f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e43a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e43a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e43ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e43b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e43b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e43bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e43bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e43c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e43c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e43cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e43d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e43d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e43db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e43e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e43e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e43e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e43ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e43f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e43f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e43fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e440060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e440500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e4409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e440e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e4412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e441780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e441c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e4420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e442560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e442a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e442ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e443340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e4437e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e443c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e444120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e4445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e444a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e444f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e4453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e445840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e445ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e446180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e446620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e446ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e446f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e447400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e4478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e447d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e4481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e448680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e448b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e449070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e4495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e449b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e44a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e44a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e44a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e44af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e44b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e44bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e44c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e44c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e44cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e44d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e44d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e44dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e44e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e44e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e44ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e44f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e44f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e44fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e450380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e4508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e450e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e451370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e4518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e451e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e452360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e4528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e452e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e453350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e4538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e453df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e454340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e454890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e454de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e455330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e455880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e455dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e456320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e456870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e456dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e457310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e457860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e457db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e458300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e458850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e458da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e4592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e459840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e459d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e45a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e45a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e45ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e45b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e45b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e45bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e45c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e45c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e45cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e45d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e45d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e45dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e45e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e45e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e45ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e45f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e45f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e45fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e460280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e4607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e460d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e461270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e4617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e461c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e462100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e4625a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e462a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e462ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e463380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e463820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e463cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e464160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e464600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e464aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e464f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e4653e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e465880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e465d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e466270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e466990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e4670b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e4677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e467ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e4681b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e4689a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e468c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e469270 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.137.437 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.137.441 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e305c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e306100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e306570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e3069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e306e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e3072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e307730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e307ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e308010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e308480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e3088f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e308f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e309a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e30a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e30aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e30b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e30b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e30bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e30c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e30ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e30d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e30dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e30e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e30eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e30f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e30f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e30f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e30fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e310080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e3104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e310960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e310e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e311300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e3115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e311a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e311ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e312310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e312780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e312bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e313060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e3134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e313940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e313db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e314220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e314690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e314b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e314f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e3153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e315850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e315cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e316130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e3165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e316a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e316e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e3172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e317760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e317cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e3181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e318640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e318ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e318f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e319390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e319800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e319c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e31a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e31a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e31a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e31ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e31b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e31b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e31bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e31bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e31c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e31c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e31cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e31d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e31d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e31da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e31df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e31e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e31e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e31ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e31f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e31f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e31f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e31fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e320280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e3206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e320b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e320fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e321440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e3218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e321d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e322190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e322600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e322a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e322ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e323350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e3237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e323c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e3240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e324510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e324980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e324df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e325260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e3256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e325b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e325fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e326420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e326890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e326d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e327170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e3275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e327a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e327ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e328330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e3287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e328c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e329080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e3294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e329960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e329dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e32a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e32a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e32ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e32af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e32b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e32b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e32bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e32c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e32c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e32ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e32cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e32d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e32d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e32dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e32e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e32e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e32e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e32edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e32f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e32f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e32fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e32ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e3303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e330850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e330cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e331130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e3315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e331a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e331e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e3322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e332760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e332bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e333040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e3334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e333920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e333d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e334200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e334670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e334ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e334f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e3353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e335830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e335ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e336110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e336580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e3369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e336e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e3372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e337740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e337bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e338020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e338490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e338900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e338d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e3391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e339650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e339ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e339f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e33a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e33a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e33ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e33b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e33b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e33b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e33be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e33c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e33c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e33cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e33d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e33d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e33d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e33dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e33e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e33e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e33eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e33ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e33f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e33f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e33fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e3400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e340540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e3409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e340e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e341290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e341700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e341c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e342100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e342570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e3430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e343380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e343640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e343ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e343f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e344390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e344800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e344c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e3450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e345550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e3459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e345e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e3462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e346710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e346b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e346ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e347460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e3478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e347d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e3481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e348620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e348a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e348f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e349370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e3497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e349c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e34a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e34a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e34a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e34ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e34b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e34b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e34bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e34bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e34c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e34c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e34cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e34d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e34d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e34da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e34dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e34e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e34e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e34ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e34f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e34f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e34f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e34fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e350260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e3506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e350b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e350fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e351420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e351890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e351d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e352170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e3525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e352a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e352ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e353330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e3537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e353c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e354080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e3544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e354960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e354dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e355240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e3556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e355b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e355f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e356400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e356870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e356ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e357750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e357e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e358590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e358cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e358f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e3593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e4257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e425c40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e425980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e425df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e426260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e4266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e426b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e426fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e427420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e427890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e427d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e428170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e4285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e428bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e4294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e429c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e42a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e42ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e42b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e42b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e42bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e42c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e42d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e42d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e42de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e42e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e42ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e42f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e42f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e42f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e42fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e430230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e4306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e430b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e430f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e431240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e4316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e431b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e431f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e432400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e432870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e432ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e433150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e4335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e433a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e433ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e434310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e434780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e434bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e435060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e4354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e435940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e435db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e436220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e436690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e436b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e436f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e4373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e437850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e437cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e438130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e4385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e438a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e438e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e4392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e439760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e439bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e43a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e43a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e43a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e43ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e43b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e43b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e43bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e43bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e43c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e43c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e43cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e43d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e43d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e43d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e43de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e43e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e43e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e43ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e43f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e43f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e43f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e43fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e4401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e440650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e440ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e440f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e4413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e441810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e441c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e4420f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e442560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e4429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e442e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e4432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e443720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e443b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e444000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e444470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e4448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e444d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e4451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e445630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e445aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e445f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e446380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e4467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e446c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e4470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e447540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e4479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e447e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e448290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e448700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e448b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e448fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e449450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e4498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e449d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e44a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e44a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e44aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e44aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e44b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e44b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e44bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e44c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e44c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e44c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e44ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e44d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e44d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e44db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e44dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e44e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e44e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e44ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e44f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e44f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e44fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e44fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e450340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e4507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e450c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e451090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e451500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e451970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e451de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e452250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e4526c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e452b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e452fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e453410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e453880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e453cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e454160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e4545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e454a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e454eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e455320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e455790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e455c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e456070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e4564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e456950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e456dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e457230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e4576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e457b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e457f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e4583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e458860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e458cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e459140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e4595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e459a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e459e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e45a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e45a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e45abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e45b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e45b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e45b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e45bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e45c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e45c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e45caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e45cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e45d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e45d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e45dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e45e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e45e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e45ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e45ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e45f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e45f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e45fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e460030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e4604a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e460910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e460d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e4611f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e461660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e461ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e461f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e4626c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e462b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e462fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e463410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e463880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e463cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e464160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e4645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e464a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e464eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e465320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e465790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e465c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e466070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e4664e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e466950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e466dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e467230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e4676a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e467b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e467f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e4683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e468860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e468cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e469140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e40bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e40b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e40a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e40b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e418520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e418990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e418e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e419270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e4196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e419b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e419fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e41a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e41a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e41ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e41b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e41b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e41ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e41bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e41c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e41c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e41cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e41d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e41d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e41d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e41dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e41e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e41e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e41eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e41efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e41f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e41f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e41fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e420160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e4205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e420a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e420eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e421320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e421790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e421c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e422070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e4224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e422950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e422dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e423230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e4236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e423b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e423f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e4243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e424ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e4251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e416fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e4176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e417b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e40e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e40e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e40eb60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.775s
user	0m0.307s
sys	0m0.268s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4367 (93aefa31)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15170d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15170d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15170df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15170e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15170ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15170f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15170f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15170fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151710120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151710620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151710b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151711020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151711b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1517122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151712b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151713220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151713940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151714060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151714780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151714f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151715670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151715d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1517164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151716d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151717470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151717730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151717d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1517189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151718ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1517191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151719650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151719910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15171a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15171a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15171a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15171ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15171b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15171b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15171bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15171c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15171c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15171ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15171cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15171d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15171d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15171dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15171e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15171eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15171f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15171f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15171fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151720380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151720990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151720fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151721790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151721c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1517220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151722390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1517229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151723190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151723450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1517238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151723d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151724230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1517246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151725010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1517254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151725950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151725df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151726290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151726730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151726bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151727120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151727670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151727bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151728110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151728660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151728bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151729100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151729650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151729ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15172a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15172a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15172ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15172b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15172b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15172bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15172c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15172c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15172cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15172d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15172d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15172db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15172e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15172e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15172eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15171e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15172efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15172f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15172fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151730210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151730760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151730cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151731200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151731750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151731ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1517321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151732740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151732c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1517331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151733730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151733c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151734120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1517345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151734a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151734f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1517353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151735840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151735ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151736180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151736620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151736ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151736f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151737400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1517378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151737d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1517381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151738b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151738fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151739460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151739900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151739da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15173a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15173a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15173ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15173b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15173b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15173b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15173be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15173c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15173c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15173cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15173d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15173d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15173d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15173de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15173e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15173e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15173ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15173f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15173f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15173fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15173fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151740360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151740800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151740ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151741140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1517415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151741a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151741f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1517423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151742860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151742d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1517431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151743640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151743ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151743f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151744420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1517448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151744d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151745200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1517456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151745b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151745fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151746480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151746920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151746dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151747260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151747700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151747ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151748040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1517484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151748980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151748e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1517492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151749760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151749c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15174a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15174a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15174a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15174ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15174b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15174b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15174be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15174c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15174c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15174cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15174d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15174d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15174e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15174e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15174e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15174ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15174f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15174fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1517500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151750550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1517509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1517511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1517516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151751c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151752190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1517526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151752c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151753180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1517536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151753c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151754170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1517546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151754c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151755160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1517556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151755c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151756150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1517566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151756bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151757140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151757690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151757be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151758130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151758680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151758bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151759120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151759670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151759bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15175a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15175a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15175abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15175b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15175b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15175bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15175c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15175c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15175cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15175d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15175d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15175db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15175e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15175e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15175eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15175f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15175f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15175fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1517600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151760600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151760b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1517610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1517615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151761b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151762090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1517625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151762b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151763080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1517635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151763b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151763fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151764460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151764900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151764da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151765240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1517656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151765b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151766020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1517664c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151766960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151766e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1517672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151767740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151767be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151768080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1517685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151768cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151769410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151769b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15176a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15176a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15176ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15176afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15176b5d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.305 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151608f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1516093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151609850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151609cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15160a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15160a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15160aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15160ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15160b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15160b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15160bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15160c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15160ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15160d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15160ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15160e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15160ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15160f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15160fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151610210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151610930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151611050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151611770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151611e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1516125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151612870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151612b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151612fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151613410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151613880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151613d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151614290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1516149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1516152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151615800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151615d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151616700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151617100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151617600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151618000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151618470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1516188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151618d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1516191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151619630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151619aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151619f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15161a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15161a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15161ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15161b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15161b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15161bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15161c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15161c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15161ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15161d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15161d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15161dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15161e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15161e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15161e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15161ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15161f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15161f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15161fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151620110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1516205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151620b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151621050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1516215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151621af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151622040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151622590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151622ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151623030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151623580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151623ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151624020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151624570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151624ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151625010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151625560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151625ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151626000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151626550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151626aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151627540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151627a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151627fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151628530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151628fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151629520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151629a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151629fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15162a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15162aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15162afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15162b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15162ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15162bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15162c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15162ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15162cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15162d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15162da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15162ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15162e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15162e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15162ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15162f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15162f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15162fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15162ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1516303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151630870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151630d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1516311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151631650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151631af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151631f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151632430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1516328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151632d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151633210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1516336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151633b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151633ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151634490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151634930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151635270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151635710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151635bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1516364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151636990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151636e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1516372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151637770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151637c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1516380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151638550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1516389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151638e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151639330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1516397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151639c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15163a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15163a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15163aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15163aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15163b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15163b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15163bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15163c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15163c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15163cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15163cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15163d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15163d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15163dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15163e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15163e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15163eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15163efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15163f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15163f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15163fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151640230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1516406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151640b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151641010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1516414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151641950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151641df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151642290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151642730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151642bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151643070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151643510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1516439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151643e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1516442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151644790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151644c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151645180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1516456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151645c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151646170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151646430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151646a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151647050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151647660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151647e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1516482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1516485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151648bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1516491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1516499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151649e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15164a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15164a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15164af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15164b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15164b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15164bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15164c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15164c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15164cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15164d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15164d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15164df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15164e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15164e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15164ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15164f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15164f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15164ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151650450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1516509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151650ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151651440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151651990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151651ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151652430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151652980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151652ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151653420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151653970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151653ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151654410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151654960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151654eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151655400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151655950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151655ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1516563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151656940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151656e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1516573e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151657930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151657e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1516583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151658920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151658e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1516593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151659910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151659e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15165a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15165a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15165ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15165b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15165b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15165be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15165c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15165c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15165ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15165d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15165d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15165dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15165e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15165e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15165eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15165eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15165f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15165f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15165fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151660270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151660710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151660bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151661050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1516614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151661990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151661e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151662380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151662aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1516631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1516638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151664000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1516642c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151664ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151664d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151665380 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155e044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155e04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155e04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155e05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155e056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155e05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155e05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155e063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155e06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155e06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155e07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155e07870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155e08390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155e08b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155e09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155e09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155e0a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155e0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155e0afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155e0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155e0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155e0c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155e0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155e0d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155e0daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155e0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155e0e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155e0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155e0e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155e0ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155e0f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155e0f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155e0fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155e0fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155e102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155e10720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155e10b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155e11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155e11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155e118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155e11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155e121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155e12630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155e12aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155e12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155e13380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155e137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155e13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155e140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155e14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155e149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155e14e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155e15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155e15700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155e15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155e15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155e16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155e16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155e16ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155e17330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155e177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155e17c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155e18080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155e184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155e18960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155e18dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155e19240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155e196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155e19b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155e19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155e1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155e1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155e1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155e1b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155e1b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155e1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155e1bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155e1c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155e1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155e1cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155e1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155e1d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155e1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155e1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155e1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155e1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155e1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155e1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155e1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155e1f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155e1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155e20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155e205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155e20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155e20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155e212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155e21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x155e21bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155e22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155e224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155e22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155e22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155e23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155e23670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x155e23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x155e23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x155e243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155e24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x155e24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155e25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155e25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155e259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155e25e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155e262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155e26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155e26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155e27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155e27490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155e27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155e27d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155e281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155e28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155e28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155e28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155e293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155e29810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155e29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155e2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155e2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155e2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155e2ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155e2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155e2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155e2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155e2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155e2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155e2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155e2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155e2d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155e2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155e2daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155e2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155e2e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155e2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155e2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155e2f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155e2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155e2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155e2fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155e30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155e30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155e30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155e30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155e31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155e318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155e31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155e321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155e32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155e32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155e32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155e33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155e337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155e33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155e340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155e34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155e34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155e34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155e35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155e356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155e35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155e35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155e36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155e36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155e37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155e375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155e37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155e37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155e38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155e387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155e38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155e39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155e39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155e39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155e39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155e3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155e3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155e3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155e3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155e3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155e3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155e3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155e3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155e3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155e3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155e3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155e3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155e3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155e3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155e3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x155e3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155e3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155e3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155e3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155e3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155e3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155e40510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155e40980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155e40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155e41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155e41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155e41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155e42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155e427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155e42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155e43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155e434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155e43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155e43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155e44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155e446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155e44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155e44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155e45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155e45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155e45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155e46150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155e465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155e46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155e46ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155e47310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155e47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155e47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155e48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155e484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155e48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155e48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155e49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155e49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155e49b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155e49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155e4a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155e4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155e4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155e4b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155e4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155e4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155e4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155e4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155e4cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155e4cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155e4d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155e4d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155e4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155e4e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155e4e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155e4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155e4ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155e4f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155e4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155e4fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155e4fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155e50460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155e508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155e50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155e511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155e51620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155e51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155e51f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155e52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155e527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155e52c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155e530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155e53530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155e539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155e53e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155e54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155e546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155e54b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155e54fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155e55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155e558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155e56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155e56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155e57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155e57880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155e57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155e57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155e585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155e58bc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.936s
user	0m0.246s
sys	0m0.150s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.75 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
