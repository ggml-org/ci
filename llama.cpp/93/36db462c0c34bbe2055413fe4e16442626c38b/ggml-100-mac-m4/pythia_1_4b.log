Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:43 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:106 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.1s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.311s
user	0m0.540s
sys	0m0.794s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  8%] Built target sha1
[  8%] Built target build_info
[  8%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  8%] Built target sha256
[  8%] Built target xxhash
[  9%] Linking CXX shared library libggml-base.dylib
[  9%] Built target ggml-base
[ 10%] Generate assembly for embedded Metal library
Embedding Metal library
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 13%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 13%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-cpu
[ 15%] Built target ggml-blas
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Linking CXX shared library libllama.dylib
[ 23%] Built target llama-gguf
[ 23%] Built target llama-gguf-hash
[ 23%] Built target llama
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 24%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Linking C executable ../bin/test-c
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Built target llava
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX static library libllava_static.a
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 33%] Linking CXX static library libcommon.a
[ 33%] Built target llama-quantize-stats
[ 33%] Built target test-c
[ 33%] Built target llama-simple-chat
[ 33%] Built target llama-simple
[ 33%] Built target llava_static
[ 33%] Built target llava_shared
[ 33%] Built target common
[ 33%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 37%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-arg-parser
[ 44%] Linking CXX executable ../bin/test-quantize-fns
[ 44%] Linking CXX executable ../bin/test-quantize-perf
[ 44%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-chat-template
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-quantize-fns
[ 46%] Built target test-chat-template
[ 46%] Built target test-log
[ 46%] Built target test-quantize-perf
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 47%] Built target test-grammar-parser
[ 47%] Built target test-arg-parser
[ 47%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 47%] Built target test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-grammar-integration
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-llama-grammar
[ 57%] Linking CXX executable ../../bin/llama-cvector-generator
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-rope
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-grammar-integration
[ 63%] Built target test-llama-grammar
[ 63%] Built target test-autorelease
[ 63%] Built target llama-cvector-generator
[ 63%] Built target test-barrier
[ 63%] Built target test-backend-ops
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 63%] Built target test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-json-schema-to-grammar
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Built target llama-batched-bench
[ 66%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-export-lora
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-batched
[ 73%] Built target llama-convert-llama2c-to-ggml
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-export-lora
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 75%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 76%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 76%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 76%] Built target llama-infill
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 78%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 78%] Built target llama-imatrix
[ 79%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 79%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-bench
[ 80%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-llava-cli
[ 80%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-lookup-create
[ 81%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-lookup-stats
[ 84%] Linking CXX executable ../../bin/llama-cli
[ 85%] Linking CXX executable ../../bin/llama-parallel
[ 85%] Built target llama-minicpmv-cli
[ 85%] Built target llama-bench
[ 85%] Built target llama-llava-cli
[ 85%] Built target llama-lookahead
[ 85%] Built target llama-lookup-create
[ 85%] Built target llama-lookup
[ 85%] Built target llama-lookup-merge
[ 85%] Generating loading.html.hpp
[ 85%] Built target llama-lookup-stats
[ 85%] Built target llama-cli
[ 85%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 86%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 86%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 87%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 88%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 88%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 89%] Built target llama-parallel
[ 89%] Generating completion.js.hpp
[ 90%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-perplexity
[ 91%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-passkey
[ 93%] Linking CXX executable ../../bin/llama-save-load-state
[ 94%] Linking CXX executable ../../bin/llama-retrieval
[ 94%] Linking CXX executable ../../bin/llama-quantize
[ 95%] Linking CXX executable ../../bin/llama-speculative
[ 96%] Generating deps_daisyui.min.css.hpp
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-tokenize
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 97%] Linking CXX executable ../../bin/llama-q8dot
[ 97%] Built target llama-passkey
[ 97%] Built target llama-quantize
[ 97%] Built target llama-retrieval
[ 97%] Built target llama-save-load-state
[ 97%] Built target llama-perplexity
[ 97%] Built target llama-speculative
[ 97%] Built target llama-tokenize
[ 97%] Generating deps_markdown-it.js.hpp
[ 97%] Generating index.html.hpp
[ 98%] Generating deps_vue.esm-browser.js.hpp
[ 99%] Generating deps_tailwindcss.js.hpp
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.641s
user	0m5.552s
sys	0m8.699s

main: quantize time =  4155.97 ms
main:    total time =  4155.97 ms

main: quantize time =  2046.13 ms
main:    total time =  2046.13 ms

main: quantize time =  2471.89 ms
main:    total time =  2471.89 ms

main: quantize time =  2198.18 ms
main:    total time =  2198.18 ms

main: quantize time =  2286.82 ms
main:    total time =  2286.82 ms

main: quantize time =  5097.61 ms
main:    total time =  5097.61 ms

main: quantize time =  5771.60 ms
main:    total time =  5771.60 ms

main: quantize time =  6847.81 ms
main:    total time =  6847.81 ms

main: quantize time =  5843.53 ms
main:    total time =  5843.53 ms

main: quantize time =  4467.34 ms
main:    total time =  4467.34 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.104 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.235 I main: llama backend init
0.00.000.240 I main: load the model and apply lora adapter, if any
0.00.031.601 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.827 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.855 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.859 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.865 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.865 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.866 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.868 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.872 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.873 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.874 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.878 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.879 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.879 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.075 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.078 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.081 I llama_model_loader: - type  f32:  194 tensors
0.00.062.081 I llama_model_loader: - type  f16:   98 tensors
0.00.093.517 I llm_load_vocab: special tokens cache size = 25
0.00.100.724 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.100.727 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.100.727 I llm_load_print_meta: arch             = gptneox
0.00.100.728 I llm_load_print_meta: vocab type       = BPE
0.00.100.728 I llm_load_print_meta: n_vocab          = 50304
0.00.100.728 I llm_load_print_meta: n_merges         = 50009
0.00.100.728 I llm_load_print_meta: vocab_only       = 0
0.00.100.728 I llm_load_print_meta: n_ctx_train      = 2048
0.00.100.729 I llm_load_print_meta: n_embd           = 2048
0.00.100.729 I llm_load_print_meta: n_layer          = 24
0.00.100.731 I llm_load_print_meta: n_head           = 16
0.00.100.732 I llm_load_print_meta: n_head_kv        = 16
0.00.100.732 I llm_load_print_meta: n_rot            = 32
0.00.100.732 I llm_load_print_meta: n_swa            = 0
0.00.100.732 I llm_load_print_meta: n_embd_head_k    = 128
0.00.100.732 I llm_load_print_meta: n_embd_head_v    = 128
0.00.100.733 I llm_load_print_meta: n_gqa            = 1
0.00.100.734 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.100.734 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.100.735 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.100.735 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.100.737 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.100.737 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.100.737 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.100.738 I llm_load_print_meta: n_ff             = 8192
0.00.100.738 I llm_load_print_meta: n_expert         = 0
0.00.100.738 I llm_load_print_meta: n_expert_used    = 0
0.00.100.740 I llm_load_print_meta: causal attn      = 1
0.00.100.740 I llm_load_print_meta: pooling type     = 0
0.00.100.740 I llm_load_print_meta: rope type        = 2
0.00.100.740 I llm_load_print_meta: rope scaling     = linear
0.00.100.741 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.100.741 I llm_load_print_meta: freq_scale_train = 1
0.00.100.741 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.100.741 I llm_load_print_meta: rope_finetuned   = unknown
0.00.100.741 I llm_load_print_meta: ssm_d_conv       = 0
0.00.100.742 I llm_load_print_meta: ssm_d_inner      = 0
0.00.100.742 I llm_load_print_meta: ssm_d_state      = 0
0.00.100.742 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.100.742 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.100.749 I llm_load_print_meta: model type       = 1.4B
0.00.100.749 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.100.750 I llm_load_print_meta: model params     = 1.41 B
0.00.100.750 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.100.750 I llm_load_print_meta: general.name     = 1.4B
0.00.100.751 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.100.751 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.100.751 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.100.751 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.100.751 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.100.752 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.100.752 I llm_load_print_meta: max token length = 1024
0.00.102.855 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.102.855 I llm_load_tensors: offloading output layer to GPU
0.00.102.855 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.102.867 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.102.868 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.103.838 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.839 I llama_new_context_with_model: n_ctx         = 2048
0.00.103.839 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.103.839 I llama_new_context_with_model: n_batch       = 2048
0.00.103.839 I llama_new_context_with_model: n_ubatch      = 512
0.00.103.839 I llama_new_context_with_model: flash_attn    = 0
0.00.103.840 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.840 I llama_new_context_with_model: freq_scale    = 1
0.00.103.841 I ggml_metal_init: allocating
0.00.103.850 I ggml_metal_init: found device: Apple M4
0.00.103.852 I ggml_metal_init: picking default device: Apple M4
0.00.104.512 I ggml_metal_init: using embedded metal library
0.00.113.858 I ggml_metal_init: GPU name:   Apple M4
0.00.113.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.861 I ggml_metal_init: simdgroup reduction   = true
0.00.113.861 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.861 I ggml_metal_init: has bfloat            = true
0.00.113.861 I ggml_metal_init: use bfloat            = true
0.00.113.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.148.004 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.148.009 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.148.027 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.148.953 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.148.954 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.148.954 I llama_new_context_with_model: graph nodes  = 967
0.00.148.954 I llama_new_context_with_model: graph splits = 2
0.00.148.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.240.589 I main: llama threadpool init, n_threads = 4
0.00.240.622 I 
0.00.240.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.240.640 I 
0.00.240.720 I sampler seed: 1234
0.00.240.724 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.240.749 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.240.752 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.240.752 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.091.460 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.02.091.460 I llama_perf_context_print:        load time =     208.97 ms
0.02.091.461 I llama_perf_context_print: prompt eval time =      38.16 ms /     7 tokens (    5.45 ms per token,   183.42 tokens per second)
0.02.091.462 I llama_perf_context_print:        eval time =    1809.56 ms /    63 runs   (   28.72 ms per token,    34.82 tokens per second)
0.02.091.462 I llama_perf_context_print:       total time =    1850.87 ms /    70 tokens
0.02.091.634 I ggml_metal_free: deallocating

real	0m2.440s
user	0m0.145s
sys	0m0.092s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.450 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.388 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.393 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.398 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.402 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.402 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.403 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.403 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.403 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.406 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.406 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.408 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.409 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.409 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.321 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.338 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.338 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.339 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.339 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.339 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.339 I llama_model_loader: - type  f32:  194 tensors
0.00.039.340 I llama_model_loader: - type q8_0:   98 tensors
0.00.065.215 I llm_load_vocab: special tokens cache size = 25
0.00.072.524 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.528 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.528 I llm_load_print_meta: arch             = gptneox
0.00.072.528 I llm_load_print_meta: vocab type       = BPE
0.00.072.529 I llm_load_print_meta: n_vocab          = 50304
0.00.072.529 I llm_load_print_meta: n_merges         = 50009
0.00.072.529 I llm_load_print_meta: vocab_only       = 0
0.00.072.529 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.529 I llm_load_print_meta: n_embd           = 2048
0.00.072.531 I llm_load_print_meta: n_layer          = 24
0.00.072.536 I llm_load_print_meta: n_head           = 16
0.00.072.537 I llm_load_print_meta: n_head_kv        = 16
0.00.072.537 I llm_load_print_meta: n_rot            = 32
0.00.072.537 I llm_load_print_meta: n_swa            = 0
0.00.072.538 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.538 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.538 I llm_load_print_meta: n_gqa            = 1
0.00.072.539 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.540 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.540 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.541 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.541 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.541 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.541 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.542 I llm_load_print_meta: n_ff             = 8192
0.00.072.542 I llm_load_print_meta: n_expert         = 0
0.00.072.542 I llm_load_print_meta: n_expert_used    = 0
0.00.072.543 I llm_load_print_meta: causal attn      = 1
0.00.072.544 I llm_load_print_meta: pooling type     = 0
0.00.072.546 I llm_load_print_meta: rope type        = 2
0.00.072.546 I llm_load_print_meta: rope scaling     = linear
0.00.072.546 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.547 I llm_load_print_meta: freq_scale_train = 1
0.00.072.547 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.547 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.547 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.548 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.548 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.548 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.548 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.562 I llm_load_print_meta: model type       = 1.4B
0.00.072.562 I llm_load_print_meta: model ftype      = Q8_0
0.00.072.563 I llm_load_print_meta: model params     = 1.41 B
0.00.072.563 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.072.564 I llm_load_print_meta: general.name     = 1.4B
0.00.072.564 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.564 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.564 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.564 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.565 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.565 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.566 I llm_load_print_meta: max token length = 1024
0.00.074.684 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.685 I llm_load_tensors: offloading output layer to GPU
0.00.074.685 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.695 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.074.696 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.075.680 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.681 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.681 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.681 I llama_new_context_with_model: n_batch       = 2048
0.00.075.681 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.682 I llama_new_context_with_model: flash_attn    = 0
0.00.075.682 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.682 I llama_new_context_with_model: freq_scale    = 1
0.00.075.683 I ggml_metal_init: allocating
0.00.075.688 I ggml_metal_init: found device: Apple M4
0.00.075.691 I ggml_metal_init: picking default device: Apple M4
0.00.076.460 I ggml_metal_init: using embedded metal library
0.00.078.942 I ggml_metal_init: GPU name:   Apple M4
0.00.078.944 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.945 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.945 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.945 I ggml_metal_init: simdgroup reduction   = true
0.00.078.946 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.946 I ggml_metal_init: has bfloat            = true
0.00.078.946 I ggml_metal_init: use bfloat            = true
0.00.078.948 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.510 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.520 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.545 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.608 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.114.609 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.114.610 I llama_new_context_with_model: graph nodes  = 967
0.00.114.610 I llama_new_context_with_model: graph splits = 2
0.00.114.623 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.237.570 I main: llama threadpool init, n_threads = 4
0.01.237.599 I 
0.01.237.617 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.237.617 I 
0.01.237.756 I sampler seed: 1234
0.01.237.762 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.237.771 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.237.772 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.237.773 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.319.661 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.02.319.662 I llama_perf_context_print:        load time =    1228.12 ms
0.02.319.663 I llama_perf_context_print: prompt eval time =      33.38 ms /     7 tokens (    4.77 ms per token,   209.68 tokens per second)
0.02.319.664 I llama_perf_context_print:        eval time =    1045.50 ms /    63 runs   (   16.60 ms per token,    60.26 tokens per second)
0.02.319.664 I llama_perf_context_print:       total time =    1082.09 ms /    70 tokens
0.02.319.844 I ggml_metal_free: deallocating

real	0m2.336s
user	0m0.117s
sys	0m0.240s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.843 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.447 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.451 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.453 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.453 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.453 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.454 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.455 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.455 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.456 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.456 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.457 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.457 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.458 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.459 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.459 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.099 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.100 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.101 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.101 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.101 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.102 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.102 I llama_model_loader: - type  f32:  194 tensors
0.00.026.103 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.103 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.993 I llm_load_vocab: special tokens cache size = 25
0.00.053.055 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.058 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.059 I llm_load_print_meta: arch             = gptneox
0.00.053.059 I llm_load_print_meta: vocab type       = BPE
0.00.053.060 I llm_load_print_meta: n_vocab          = 50304
0.00.053.060 I llm_load_print_meta: n_merges         = 50009
0.00.053.060 I llm_load_print_meta: vocab_only       = 0
0.00.053.060 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.061 I llm_load_print_meta: n_embd           = 2048
0.00.053.063 I llm_load_print_meta: n_layer          = 24
0.00.053.068 I llm_load_print_meta: n_head           = 16
0.00.053.069 I llm_load_print_meta: n_head_kv        = 16
0.00.053.069 I llm_load_print_meta: n_rot            = 32
0.00.053.069 I llm_load_print_meta: n_swa            = 0
0.00.053.070 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.070 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.070 I llm_load_print_meta: n_gqa            = 1
0.00.053.071 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.072 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.073 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.073 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.073 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.073 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.073 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.074 I llm_load_print_meta: n_ff             = 8192
0.00.053.074 I llm_load_print_meta: n_expert         = 0
0.00.053.075 I llm_load_print_meta: n_expert_used    = 0
0.00.053.075 I llm_load_print_meta: causal attn      = 1
0.00.053.075 I llm_load_print_meta: pooling type     = 0
0.00.053.075 I llm_load_print_meta: rope type        = 2
0.00.053.075 I llm_load_print_meta: rope scaling     = linear
0.00.053.076 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.076 I llm_load_print_meta: freq_scale_train = 1
0.00.053.076 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.076 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.077 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.077 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.077 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.077 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.077 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.090 I llm_load_print_meta: model type       = 1.4B
0.00.053.090 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.091 I llm_load_print_meta: model params     = 1.41 B
0.00.053.091 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.092 I llm_load_print_meta: general.name     = 1.4B
0.00.053.092 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.092 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.092 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.092 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.092 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.093 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.093 I llm_load_print_meta: max token length = 1024
0.00.055.364 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.364 I llm_load_tensors: offloading output layer to GPU
0.00.055.364 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.375 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.376 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.399 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.399 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.400 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.400 I llama_new_context_with_model: n_batch       = 2048
0.00.056.400 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.400 I llama_new_context_with_model: flash_attn    = 0
0.00.056.401 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.401 I llama_new_context_with_model: freq_scale    = 1
0.00.056.401 I ggml_metal_init: allocating
0.00.056.405 I ggml_metal_init: found device: Apple M4
0.00.056.406 I ggml_metal_init: picking default device: Apple M4
0.00.057.072 I ggml_metal_init: using embedded metal library
0.00.059.185 I ggml_metal_init: GPU name:   Apple M4
0.00.059.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.187 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.188 I ggml_metal_init: simdgroup reduction   = true
0.00.059.188 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.188 I ggml_metal_init: has bfloat            = true
0.00.059.188 I ggml_metal_init: use bfloat            = true
0.00.059.189 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.189 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.503 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.516 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.539 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.645 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.647 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.647 I llama_new_context_with_model: graph nodes  = 967
0.00.092.648 I llama_new_context_with_model: graph splits = 2
0.00.092.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.510 I main: llama threadpool init, n_threads = 4
0.00.703.538 I 
0.00.703.556 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.703.556 I 
0.00.703.697 I sampler seed: 1234
0.00.703.700 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.710 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.710 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.710 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.377.635 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.377.636 I llama_perf_context_print:        load time =     692.66 ms
0.01.377.637 I llama_perf_context_print: prompt eval time =      32.66 ms /     7 tokens (    4.67 ms per token,   214.30 tokens per second)
0.01.377.637 I llama_perf_context_print:        eval time =     638.27 ms /    63 runs   (   10.13 ms per token,    98.70 tokens per second)
0.01.377.638 I llama_perf_context_print:       total time =     674.13 ms /    70 tokens
0.01.377.791 I ggml_metal_free: deallocating

real	0m1.393s
user	0m0.108s
sys	0m0.181s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.806 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.476 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.480 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.482 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.487 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.488 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.488 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.490 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.490 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.491 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.491 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.491 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.492 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.495 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.341 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.038 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.039 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.040 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.040 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.041 I llama_model_loader: - type  f32:  194 tensors
0.00.024.041 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.041 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.427 I llm_load_vocab: special tokens cache size = 25
0.00.050.231 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.234 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.234 I llm_load_print_meta: arch             = gptneox
0.00.050.235 I llm_load_print_meta: vocab type       = BPE
0.00.050.235 I llm_load_print_meta: n_vocab          = 50304
0.00.050.235 I llm_load_print_meta: n_merges         = 50009
0.00.050.235 I llm_load_print_meta: vocab_only       = 0
0.00.050.236 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.236 I llm_load_print_meta: n_embd           = 2048
0.00.050.236 I llm_load_print_meta: n_layer          = 24
0.00.050.239 I llm_load_print_meta: n_head           = 16
0.00.050.240 I llm_load_print_meta: n_head_kv        = 16
0.00.050.240 I llm_load_print_meta: n_rot            = 32
0.00.050.240 I llm_load_print_meta: n_swa            = 0
0.00.050.240 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.242 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.243 I llm_load_print_meta: n_gqa            = 1
0.00.050.244 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.245 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.245 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.246 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.246 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.246 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.246 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.247 I llm_load_print_meta: n_ff             = 8192
0.00.050.247 I llm_load_print_meta: n_expert         = 0
0.00.050.247 I llm_load_print_meta: n_expert_used    = 0
0.00.050.247 I llm_load_print_meta: causal attn      = 1
0.00.050.248 I llm_load_print_meta: pooling type     = 0
0.00.050.248 I llm_load_print_meta: rope type        = 2
0.00.050.248 I llm_load_print_meta: rope scaling     = linear
0.00.050.248 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.249 I llm_load_print_meta: freq_scale_train = 1
0.00.050.249 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.249 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.249 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.249 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.249 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.249 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.251 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.263 I llm_load_print_meta: model type       = 1.4B
0.00.050.263 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.264 I llm_load_print_meta: model params     = 1.41 B
0.00.050.264 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.264 I llm_load_print_meta: general.name     = 1.4B
0.00.050.264 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.265 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.265 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.265 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.265 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.266 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.266 I llm_load_print_meta: max token length = 1024
0.00.052.224 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.224 I llm_load_tensors: offloading output layer to GPU
0.00.052.224 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.234 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.235 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.219 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.220 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.220 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.220 I llama_new_context_with_model: n_batch       = 2048
0.00.053.221 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.221 I llama_new_context_with_model: flash_attn    = 0
0.00.053.221 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.222 I llama_new_context_with_model: freq_scale    = 1
0.00.053.222 I ggml_metal_init: allocating
0.00.053.228 I ggml_metal_init: found device: Apple M4
0.00.053.231 I ggml_metal_init: picking default device: Apple M4
0.00.053.795 I ggml_metal_init: using embedded metal library
0.00.055.730 I ggml_metal_init: GPU name:   Apple M4
0.00.055.731 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.732 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.732 I ggml_metal_init: simdgroup reduction   = true
0.00.055.732 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.732 I ggml_metal_init: has bfloat            = true
0.00.055.732 I ggml_metal_init: use bfloat            = true
0.00.055.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.027 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.033 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.051 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.969 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.970 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.971 I llama_new_context_with_model: graph nodes  = 967
0.00.083.971 I llama_new_context_with_model: graph splits = 2
0.00.083.983 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.821 I main: llama threadpool init, n_threads = 4
0.00.712.849 I 
0.00.712.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.712.868 I 
0.00.713.028 I sampler seed: 1234
0.00.713.033 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.069 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.070 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.070 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.433.723 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.433.724 I llama_perf_context_print:        load time =     704.01 ms
0.01.433.724 I llama_perf_context_print: prompt eval time =      32.76 ms /     7 tokens (    4.68 ms per token,   213.65 tokens per second)
0.01.433.725 I llama_perf_context_print:        eval time =     684.93 ms /    63 runs   (   10.87 ms per token,    91.98 tokens per second)
0.01.433.726 I llama_perf_context_print:       total time =     720.90 ms /    70 tokens
0.01.433.901 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.107s
sys	0m0.176s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.960 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.822 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.825 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.827 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.829 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.830 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.830 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.831 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.831 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.832 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.834 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.834 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.624 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.625 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.626 I llama_model_loader: - type  f32:  194 tensors
0.00.025.626 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.626 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.021 I llm_load_vocab: special tokens cache size = 25
0.00.052.094 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.097 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.097 I llm_load_print_meta: arch             = gptneox
0.00.052.098 I llm_load_print_meta: vocab type       = BPE
0.00.052.098 I llm_load_print_meta: n_vocab          = 50304
0.00.052.098 I llm_load_print_meta: n_merges         = 50009
0.00.052.098 I llm_load_print_meta: vocab_only       = 0
0.00.052.098 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.099 I llm_load_print_meta: n_embd           = 2048
0.00.052.099 I llm_load_print_meta: n_layer          = 24
0.00.052.102 I llm_load_print_meta: n_head           = 16
0.00.052.102 I llm_load_print_meta: n_head_kv        = 16
0.00.052.103 I llm_load_print_meta: n_rot            = 32
0.00.052.103 I llm_load_print_meta: n_swa            = 0
0.00.052.103 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.103 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.104 I llm_load_print_meta: n_gqa            = 1
0.00.052.105 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.108 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.108 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.108 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.109 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.109 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.109 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.110 I llm_load_print_meta: n_ff             = 8192
0.00.052.110 I llm_load_print_meta: n_expert         = 0
0.00.052.110 I llm_load_print_meta: n_expert_used    = 0
0.00.052.111 I llm_load_print_meta: causal attn      = 1
0.00.052.112 I llm_load_print_meta: pooling type     = 0
0.00.052.112 I llm_load_print_meta: rope type        = 2
0.00.052.113 I llm_load_print_meta: rope scaling     = linear
0.00.052.113 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.113 I llm_load_print_meta: freq_scale_train = 1
0.00.052.114 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.114 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.114 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.114 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.114 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.114 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.114 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.125 I llm_load_print_meta: model type       = 1.4B
0.00.052.126 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.126 I llm_load_print_meta: model params     = 1.41 B
0.00.052.127 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.127 I llm_load_print_meta: general.name     = 1.4B
0.00.052.127 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.127 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.127 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.127 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.128 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.128 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.128 I llm_load_print_meta: max token length = 1024
0.00.053.703 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.703 I llm_load_tensors: offloading output layer to GPU
0.00.053.703 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.713 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.714 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.596 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.597 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.597 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.597 I llama_new_context_with_model: n_batch       = 2048
0.00.054.597 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.597 I llama_new_context_with_model: flash_attn    = 0
0.00.054.598 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.598 I llama_new_context_with_model: freq_scale    = 1
0.00.054.598 I ggml_metal_init: allocating
0.00.054.604 I ggml_metal_init: found device: Apple M4
0.00.054.606 I ggml_metal_init: picking default device: Apple M4
0.00.055.167 I ggml_metal_init: using embedded metal library
0.00.057.103 I ggml_metal_init: GPU name:   Apple M4
0.00.057.104 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.105 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.105 I ggml_metal_init: simdgroup reduction   = true
0.00.057.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.106 I ggml_metal_init: has bfloat            = true
0.00.057.106 I ggml_metal_init: use bfloat            = true
0.00.057.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.572 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.582 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.603 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.480 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.481 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.481 I llama_new_context_with_model: graph nodes  = 967
0.00.084.481 I llama_new_context_with_model: graph splits = 2
0.00.084.494 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.374 I main: llama threadpool init, n_threads = 4
0.00.739.404 I 
0.00.739.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.739.424 I 
0.00.739.570 I sampler seed: 1234
0.00.739.575 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.585 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.587 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.587 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.527.130 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.527.131 I llama_perf_context_print:        load time =     729.41 ms
0.01.527.132 I llama_perf_context_print: prompt eval time =      36.51 ms /     7 tokens (    5.22 ms per token,   191.73 tokens per second)
0.01.527.132 I llama_perf_context_print:        eval time =     748.00 ms /    63 runs   (   11.87 ms per token,    84.22 tokens per second)
0.01.527.133 I llama_perf_context_print:       total time =     787.76 ms /    70 tokens
0.01.527.297 I ggml_metal_free: deallocating

real	0m1.542s
user	0m0.108s
sys	0m0.181s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.499 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.185 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.190 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.192 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.193 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.193 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.193 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.195 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.198 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.199 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.009 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.036 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.790 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.790 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.791 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.791 I llama_model_loader: - type  f32:  194 tensors
0.00.023.792 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.792 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.963 I llm_load_vocab: special tokens cache size = 25
0.00.050.938 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.941 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.941 I llm_load_print_meta: arch             = gptneox
0.00.050.941 I llm_load_print_meta: vocab type       = BPE
0.00.050.941 I llm_load_print_meta: n_vocab          = 50304
0.00.050.942 I llm_load_print_meta: n_merges         = 50009
0.00.050.942 I llm_load_print_meta: vocab_only       = 0
0.00.050.942 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.942 I llm_load_print_meta: n_embd           = 2048
0.00.050.942 I llm_load_print_meta: n_layer          = 24
0.00.050.945 I llm_load_print_meta: n_head           = 16
0.00.050.946 I llm_load_print_meta: n_head_kv        = 16
0.00.050.946 I llm_load_print_meta: n_rot            = 32
0.00.050.946 I llm_load_print_meta: n_swa            = 0
0.00.050.946 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.946 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.947 I llm_load_print_meta: n_gqa            = 1
0.00.050.948 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.951 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.951 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.952 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.952 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.952 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.952 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.953 I llm_load_print_meta: n_ff             = 8192
0.00.050.953 I llm_load_print_meta: n_expert         = 0
0.00.050.953 I llm_load_print_meta: n_expert_used    = 0
0.00.050.954 I llm_load_print_meta: causal attn      = 1
0.00.050.956 I llm_load_print_meta: pooling type     = 0
0.00.050.956 I llm_load_print_meta: rope type        = 2
0.00.050.956 I llm_load_print_meta: rope scaling     = linear
0.00.050.956 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.956 I llm_load_print_meta: freq_scale_train = 1
0.00.050.957 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.957 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.957 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.957 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.957 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.957 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.957 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.969 I llm_load_print_meta: model type       = 1.4B
0.00.050.969 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.969 I llm_load_print_meta: model params     = 1.41 B
0.00.050.970 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.970 I llm_load_print_meta: general.name     = 1.4B
0.00.050.970 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.970 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.970 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.971 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: max token length = 1024
0.00.052.604 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.604 I llm_load_tensors: offloading output layer to GPU
0.00.052.604 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.614 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.615 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.514 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.515 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.515 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.515 I llama_new_context_with_model: n_batch       = 2048
0.00.053.515 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.516 I llama_new_context_with_model: flash_attn    = 0
0.00.053.516 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.516 I llama_new_context_with_model: freq_scale    = 1
0.00.053.517 I ggml_metal_init: allocating
0.00.053.520 I ggml_metal_init: found device: Apple M4
0.00.053.522 I ggml_metal_init: picking default device: Apple M4
0.00.054.080 I ggml_metal_init: using embedded metal library
0.00.056.017 I ggml_metal_init: GPU name:   Apple M4
0.00.056.018 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.019 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.019 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.019 I ggml_metal_init: simdgroup reduction   = true
0.00.056.020 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.020 I ggml_metal_init: has bfloat            = true
0.00.056.020 I ggml_metal_init: use bfloat            = true
0.00.056.023 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.019 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.027 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.048 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.962 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.964 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.964 I llama_new_context_with_model: graph nodes  = 967
0.00.084.964 I llama_new_context_with_model: graph splits = 2
0.00.084.977 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.492 I main: llama threadpool init, n_threads = 4
0.00.773.530 I 
0.00.773.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.773.549 I 
0.00.773.788 I sampler seed: 1234
0.00.773.792 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.828 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.829 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.829 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.610.929 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.610.929 I llama_perf_context_print:        load time =     764.99 ms
0.01.610.930 I llama_perf_context_print: prompt eval time =      36.69 ms /     7 tokens (    5.24 ms per token,   190.77 tokens per second)
0.01.610.931 I llama_perf_context_print:        eval time =     797.27 ms /    63 runs   (   12.66 ms per token,    79.02 tokens per second)
0.01.610.931 I llama_perf_context_print:       total time =     837.44 ms /    70 tokens
0.01.611.095 I ggml_metal_free: deallocating

real	0m1.627s
user	0m0.108s
sys	0m0.197s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.839 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.297 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.301 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.303 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.304 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.304 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.304 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.305 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.306 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.306 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.307 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.307 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.308 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.311 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.311 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.312 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.097 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.164 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.938 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.939 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.940 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.940 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.940 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.941 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.941 I llama_model_loader: - type  f32:  194 tensors
0.00.023.941 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.942 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.942 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.286 I llm_load_vocab: special tokens cache size = 25
0.00.050.380 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.384 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.384 I llm_load_print_meta: arch             = gptneox
0.00.050.384 I llm_load_print_meta: vocab type       = BPE
0.00.050.384 I llm_load_print_meta: n_vocab          = 50304
0.00.050.385 I llm_load_print_meta: n_merges         = 50009
0.00.050.385 I llm_load_print_meta: vocab_only       = 0
0.00.050.385 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.390 I llm_load_print_meta: n_embd           = 2048
0.00.050.390 I llm_load_print_meta: n_layer          = 24
0.00.050.393 I llm_load_print_meta: n_head           = 16
0.00.050.394 I llm_load_print_meta: n_head_kv        = 16
0.00.050.394 I llm_load_print_meta: n_rot            = 32
0.00.050.394 I llm_load_print_meta: n_swa            = 0
0.00.050.394 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.395 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.395 I llm_load_print_meta: n_gqa            = 1
0.00.050.396 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.397 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.397 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.398 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.398 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.398 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.400 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.401 I llm_load_print_meta: n_ff             = 8192
0.00.050.401 I llm_load_print_meta: n_expert         = 0
0.00.050.403 I llm_load_print_meta: n_expert_used    = 0
0.00.050.403 I llm_load_print_meta: causal attn      = 1
0.00.050.403 I llm_load_print_meta: pooling type     = 0
0.00.050.403 I llm_load_print_meta: rope type        = 2
0.00.050.403 I llm_load_print_meta: rope scaling     = linear
0.00.050.404 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.407 I llm_load_print_meta: freq_scale_train = 1
0.00.050.407 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.407 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.407 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.407 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.407 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.408 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.408 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.419 I llm_load_print_meta: model type       = 1.4B
0.00.050.420 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.420 I llm_load_print_meta: model params     = 1.41 B
0.00.050.421 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.421 I llm_load_print_meta: general.name     = 1.4B
0.00.050.421 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.421 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.422 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.422 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.422 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.422 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.422 I llm_load_print_meta: max token length = 1024
0.00.052.342 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.343 I llm_load_tensors: offloading output layer to GPU
0.00.052.343 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.353 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.354 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.390 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.390 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.391 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.391 I llama_new_context_with_model: n_batch       = 2048
0.00.053.391 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.391 I llama_new_context_with_model: flash_attn    = 0
0.00.053.391 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.392 I llama_new_context_with_model: freq_scale    = 1
0.00.053.392 I ggml_metal_init: allocating
0.00.053.395 I ggml_metal_init: found device: Apple M4
0.00.053.397 I ggml_metal_init: picking default device: Apple M4
0.00.053.982 I ggml_metal_init: using embedded metal library
0.00.055.903 I ggml_metal_init: GPU name:   Apple M4
0.00.055.904 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.905 I ggml_metal_init: simdgroup reduction   = true
0.00.055.905 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.907 I ggml_metal_init: has bfloat            = true
0.00.055.907 I ggml_metal_init: use bfloat            = true
0.00.055.908 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.555 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.565 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.586 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.527 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.529 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.529 I llama_new_context_with_model: graph nodes  = 967
0.00.084.529 I llama_new_context_with_model: graph splits = 2
0.00.084.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.914 I main: llama threadpool init, n_threads = 4
0.00.519.943 I 
0.00.519.962 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.519.962 I 
0.00.520.197 I sampler seed: 1234
0.00.520.202 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.520.213 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.520.213 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.520.213 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.203.227 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.203.227 I llama_perf_context_print:        load time =     510.07 ms
0.01.203.228 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.30 tokens per second)
0.01.203.229 I llama_perf_context_print:        eval time =     644.13 ms /    63 runs   (   10.22 ms per token,    97.81 tokens per second)
0.01.203.229 I llama_perf_context_print:       total time =     683.31 ms /    70 tokens
0.01.203.421 I ggml_metal_free: deallocating

real	0m1.226s
user	0m0.107s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.372 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.046 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.028.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.053 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.054 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.054 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.058 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.021 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.572 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.573 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.039.573 I llama_model_loader: - type  f32:  194 tensors
0.00.039.574 I llama_model_loader: - type q3_K:   25 tensors
0.00.039.574 I llama_model_loader: - type q4_K:   71 tensors
0.00.039.574 I llama_model_loader: - type q5_K:    1 tensors
0.00.039.574 I llama_model_loader: - type q6_K:    1 tensors
0.00.075.599 I llm_load_vocab: special tokens cache size = 25
0.00.085.832 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.836 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.836 I llm_load_print_meta: arch             = gptneox
0.00.085.837 I llm_load_print_meta: vocab type       = BPE
0.00.085.837 I llm_load_print_meta: n_vocab          = 50304
0.00.085.837 I llm_load_print_meta: n_merges         = 50009
0.00.085.837 I llm_load_print_meta: vocab_only       = 0
0.00.085.838 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.838 I llm_load_print_meta: n_embd           = 2048
0.00.085.840 I llm_load_print_meta: n_layer          = 24
0.00.085.843 I llm_load_print_meta: n_head           = 16
0.00.085.844 I llm_load_print_meta: n_head_kv        = 16
0.00.085.844 I llm_load_print_meta: n_rot            = 32
0.00.085.845 I llm_load_print_meta: n_swa            = 0
0.00.085.845 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.845 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.846 I llm_load_print_meta: n_gqa            = 1
0.00.085.847 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.850 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.851 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.851 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.851 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.852 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.852 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.853 I llm_load_print_meta: n_ff             = 8192
0.00.085.853 I llm_load_print_meta: n_expert         = 0
0.00.085.855 I llm_load_print_meta: n_expert_used    = 0
0.00.085.855 I llm_load_print_meta: causal attn      = 1
0.00.085.855 I llm_load_print_meta: pooling type     = 0
0.00.085.855 I llm_load_print_meta: rope type        = 2
0.00.085.856 I llm_load_print_meta: rope scaling     = linear
0.00.085.856 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.856 I llm_load_print_meta: freq_scale_train = 1
0.00.085.857 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.857 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.857 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.857 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.857 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.857 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.858 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.865 I llm_load_print_meta: model type       = 1.4B
0.00.085.866 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.085.866 I llm_load_print_meta: model params     = 1.41 B
0.00.085.867 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.085.867 I llm_load_print_meta: general.name     = 1.4B
0.00.085.867 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.868 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.868 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.868 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.868 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.869 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.869 I llm_load_print_meta: max token length = 1024
0.00.088.329 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.330 I llm_load_tensors: offloading output layer to GPU
0.00.088.330 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.336 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.088.336 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.089.753 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.754 I llama_new_context_with_model: n_ctx         = 2048
0.00.089.754 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.089.754 I llama_new_context_with_model: n_batch       = 2048
0.00.089.755 I llama_new_context_with_model: n_ubatch      = 512
0.00.089.755 I llama_new_context_with_model: flash_attn    = 0
0.00.089.756 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.756 I llama_new_context_with_model: freq_scale    = 1
0.00.089.757 I ggml_metal_init: allocating
0.00.089.765 I ggml_metal_init: found device: Apple M4
0.00.089.767 I ggml_metal_init: picking default device: Apple M4
0.00.090.564 I ggml_metal_init: using embedded metal library
0.00.093.528 I ggml_metal_init: GPU name:   Apple M4
0.00.093.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.531 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.531 I ggml_metal_init: simdgroup reduction   = true
0.00.093.532 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.532 I ggml_metal_init: has bfloat            = true
0.00.093.532 I ggml_metal_init: use bfloat            = true
0.00.093.533 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.126.055 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.126.062 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.126.081 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.156 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.158 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.158 I llama_new_context_with_model: graph nodes  = 967
0.00.127.158 I llama_new_context_with_model: graph splits = 2
0.00.127.171 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.091 I main: llama threadpool init, n_threads = 4
0.00.763.139 I 
0.00.763.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.763.158 I 
0.00.763.390 I sampler seed: 1234
0.00.763.393 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.414 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.416 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.416 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.507.068 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.507.069 I llama_perf_context_print:        load time =     754.72 ms
0.01.507.069 I llama_perf_context_print: prompt eval time =      35.71 ms /     7 tokens (    5.10 ms per token,   196.03 tokens per second)
0.01.507.070 I llama_perf_context_print:        eval time =     704.91 ms /    63 runs   (   11.19 ms per token,    89.37 tokens per second)
0.01.507.071 I llama_perf_context_print:       total time =     743.98 ms /    70 tokens
0.01.507.236 I ggml_metal_free: deallocating

real	0m1.529s
user	0m0.137s
sys	0m0.176s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.011.880 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.983 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.990 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.990 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.991 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.992 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.992 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.993 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.994 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.996 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.996 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.739 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.837 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.616 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.616 I llama_model_loader: - type  f32:  194 tensors
0.00.028.616 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.617 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.617 I llama_model_loader: - type q6_K:   13 tensors
0.00.049.273 I llm_load_vocab: special tokens cache size = 25
0.00.055.242 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.245 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.246 I llm_load_print_meta: arch             = gptneox
0.00.055.246 I llm_load_print_meta: vocab type       = BPE
0.00.055.246 I llm_load_print_meta: n_vocab          = 50304
0.00.055.246 I llm_load_print_meta: n_merges         = 50009
0.00.055.247 I llm_load_print_meta: vocab_only       = 0
0.00.055.247 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.247 I llm_load_print_meta: n_embd           = 2048
0.00.055.247 I llm_load_print_meta: n_layer          = 24
0.00.055.250 I llm_load_print_meta: n_head           = 16
0.00.055.251 I llm_load_print_meta: n_head_kv        = 16
0.00.055.251 I llm_load_print_meta: n_rot            = 32
0.00.055.251 I llm_load_print_meta: n_swa            = 0
0.00.055.252 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.252 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.252 I llm_load_print_meta: n_gqa            = 1
0.00.055.253 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.254 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.257 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.257 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.257 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.257 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.258 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.258 I llm_load_print_meta: n_ff             = 8192
0.00.055.258 I llm_load_print_meta: n_expert         = 0
0.00.055.259 I llm_load_print_meta: n_expert_used    = 0
0.00.055.259 I llm_load_print_meta: causal attn      = 1
0.00.055.259 I llm_load_print_meta: pooling type     = 0
0.00.055.259 I llm_load_print_meta: rope type        = 2
0.00.055.259 I llm_load_print_meta: rope scaling     = linear
0.00.055.260 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.260 I llm_load_print_meta: freq_scale_train = 1
0.00.055.260 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.260 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.260 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.261 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.261 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.261 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.261 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.273 I llm_load_print_meta: model type       = 1.4B
0.00.055.273 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.055.274 I llm_load_print_meta: model params     = 1.41 B
0.00.055.275 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.055.275 I llm_load_print_meta: general.name     = 1.4B
0.00.055.275 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.277 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.277 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.277 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.277 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.278 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.278 I llm_load_print_meta: max token length = 1024
0.00.057.257 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.258 I llm_load_tensors: offloading output layer to GPU
0.00.057.258 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.268 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.057.269 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.058.301 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.302 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.302 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.302 I llama_new_context_with_model: n_batch       = 2048
0.00.058.302 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.303 I llama_new_context_with_model: flash_attn    = 0
0.00.058.303 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.303 I llama_new_context_with_model: freq_scale    = 1
0.00.058.304 I ggml_metal_init: allocating
0.00.058.307 I ggml_metal_init: found device: Apple M4
0.00.058.309 I ggml_metal_init: picking default device: Apple M4
0.00.058.857 I ggml_metal_init: using embedded metal library
0.00.060.788 I ggml_metal_init: GPU name:   Apple M4
0.00.060.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.791 I ggml_metal_init: simdgroup reduction   = true
0.00.060.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.791 I ggml_metal_init: has bfloat            = true
0.00.060.791 I ggml_metal_init: use bfloat            = true
0.00.060.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.934 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.939 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.960 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.148 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.149 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.150 I llama_new_context_with_model: graph nodes  = 967
0.00.091.150 I llama_new_context_with_model: graph splits = 2
0.00.091.164 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.915 I main: llama threadpool init, n_threads = 4
0.00.743.946 I 
0.00.743.975 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.743.975 I 
0.00.744.210 I sampler seed: 1234
0.00.744.217 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.228 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.228 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.228 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.499.063 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.499.064 I llama_perf_context_print:        load time =     732.03 ms
0.01.499.064 I llama_perf_context_print: prompt eval time =      36.44 ms /     7 tokens (    5.21 ms per token,   192.11 tokens per second)
0.01.499.065 I llama_perf_context_print:        eval time =     715.49 ms /    63 runs   (   11.36 ms per token,    88.05 tokens per second)
0.01.499.065 I llama_perf_context_print:       total time =     755.15 ms /    70 tokens
0.01.499.253 I ggml_metal_free: deallocating

real	0m1.516s
user	0m0.108s
sys	0m0.189s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.118 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.629 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.029.633 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.635 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.637 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.638 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.638 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.639 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.640 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.640 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.640 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.641 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.643 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.643 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.646 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.646 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.628 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.630 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.039.631 I llama_model_loader: - type  f32:  194 tensors
0.00.039.632 I llama_model_loader: - type q5_K:   61 tensors
0.00.039.632 I llama_model_loader: - type q6_K:   37 tensors
0.00.068.129 I llm_load_vocab: special tokens cache size = 25
0.00.078.441 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.445 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.446 I llm_load_print_meta: arch             = gptneox
0.00.078.446 I llm_load_print_meta: vocab type       = BPE
0.00.078.447 I llm_load_print_meta: n_vocab          = 50304
0.00.078.447 I llm_load_print_meta: n_merges         = 50009
0.00.078.447 I llm_load_print_meta: vocab_only       = 0
0.00.078.447 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.448 I llm_load_print_meta: n_embd           = 2048
0.00.078.448 I llm_load_print_meta: n_layer          = 24
0.00.078.451 I llm_load_print_meta: n_head           = 16
0.00.078.452 I llm_load_print_meta: n_head_kv        = 16
0.00.078.453 I llm_load_print_meta: n_rot            = 32
0.00.078.453 I llm_load_print_meta: n_swa            = 0
0.00.078.453 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.453 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.455 I llm_load_print_meta: n_gqa            = 1
0.00.078.456 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.457 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.457 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.458 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.458 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.458 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.459 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.460 I llm_load_print_meta: n_ff             = 8192
0.00.078.463 I llm_load_print_meta: n_expert         = 0
0.00.078.463 I llm_load_print_meta: n_expert_used    = 0
0.00.078.464 I llm_load_print_meta: causal attn      = 1
0.00.078.465 I llm_load_print_meta: pooling type     = 0
0.00.078.465 I llm_load_print_meta: rope type        = 2
0.00.078.465 I llm_load_print_meta: rope scaling     = linear
0.00.078.466 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.466 I llm_load_print_meta: freq_scale_train = 1
0.00.078.467 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.467 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.467 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.467 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.468 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.468 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.468 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.475 I llm_load_print_meta: model type       = 1.4B
0.00.078.475 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.078.476 I llm_load_print_meta: model params     = 1.41 B
0.00.078.477 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.078.477 I llm_load_print_meta: general.name     = 1.4B
0.00.078.478 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.478 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.478 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.478 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.479 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.078.479 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.479 I llm_load_print_meta: max token length = 1024
0.00.080.951 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.951 I llm_load_tensors: offloading output layer to GPU
0.00.080.952 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.958 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.080.958 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.082.417 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.418 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.418 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.418 I llama_new_context_with_model: n_batch       = 2048
0.00.082.419 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.419 I llama_new_context_with_model: flash_attn    = 0
0.00.082.419 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.420 I llama_new_context_with_model: freq_scale    = 1
0.00.082.420 I ggml_metal_init: allocating
0.00.082.428 I ggml_metal_init: found device: Apple M4
0.00.082.431 I ggml_metal_init: picking default device: Apple M4
0.00.083.259 I ggml_metal_init: using embedded metal library
0.00.086.369 I ggml_metal_init: GPU name:   Apple M4
0.00.086.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.373 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.373 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.374 I ggml_metal_init: simdgroup reduction   = true
0.00.086.374 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.374 I ggml_metal_init: has bfloat            = true
0.00.086.374 I ggml_metal_init: use bfloat            = true
0.00.086.375 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.539 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.545 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.563 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.526 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.527 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.528 I llama_new_context_with_model: graph nodes  = 967
0.00.120.528 I llama_new_context_with_model: graph splits = 2
0.00.120.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.871.547 I main: llama threadpool init, n_threads = 4
0.00.871.578 I 
0.00.871.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.871.597 I 
0.00.871.814 I sampler seed: 1234
0.00.871.819 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.871.839 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.871.841 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.871.841 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.714.642 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64486.83 tokens per second)
0.01.714.643 I llama_perf_context_print:        load time =     861.43 ms
0.01.714.643 I llama_perf_context_print: prompt eval time =      38.73 ms /     7 tokens (    5.53 ms per token,   180.74 tokens per second)
0.01.714.644 I llama_perf_context_print:        eval time =     801.19 ms /    63 runs   (   12.72 ms per token,    78.63 tokens per second)
0.01.714.645 I llama_perf_context_print:       total time =     843.10 ms /    70 tokens
0.01.714.837 I ggml_metal_free: deallocating

real	0m1.741s
user	0m0.129s
sys	0m0.204s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.017.236 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.544 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.030.548 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.549 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.557 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.558 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.558 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.558 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.559 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.559 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.559 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.560 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.561 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.824 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.343 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.344 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.345 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.040.345 I llama_model_loader: - type  f32:  194 tensors
0.00.040.346 I llama_model_loader: - type q6_K:   98 tensors
0.00.068.758 I llm_load_vocab: special tokens cache size = 25
0.00.079.037 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.041 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.041 I llm_load_print_meta: arch             = gptneox
0.00.079.042 I llm_load_print_meta: vocab type       = BPE
0.00.079.042 I llm_load_print_meta: n_vocab          = 50304
0.00.079.042 I llm_load_print_meta: n_merges         = 50009
0.00.079.043 I llm_load_print_meta: vocab_only       = 0
0.00.079.043 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.043 I llm_load_print_meta: n_embd           = 2048
0.00.079.043 I llm_load_print_meta: n_layer          = 24
0.00.079.047 I llm_load_print_meta: n_head           = 16
0.00.079.051 I llm_load_print_meta: n_head_kv        = 16
0.00.079.051 I llm_load_print_meta: n_rot            = 32
0.00.079.051 I llm_load_print_meta: n_swa            = 0
0.00.079.051 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.052 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.053 I llm_load_print_meta: n_gqa            = 1
0.00.079.059 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.063 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.063 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.064 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.064 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.064 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.064 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.066 I llm_load_print_meta: n_ff             = 8192
0.00.079.066 I llm_load_print_meta: n_expert         = 0
0.00.079.066 I llm_load_print_meta: n_expert_used    = 0
0.00.079.066 I llm_load_print_meta: causal attn      = 1
0.00.079.066 I llm_load_print_meta: pooling type     = 0
0.00.079.067 I llm_load_print_meta: rope type        = 2
0.00.079.067 I llm_load_print_meta: rope scaling     = linear
0.00.079.068 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.068 I llm_load_print_meta: freq_scale_train = 1
0.00.079.068 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.069 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.069 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.069 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.069 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.069 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.069 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.082 I llm_load_print_meta: model type       = 1.4B
0.00.079.083 I llm_load_print_meta: model ftype      = Q6_K
0.00.079.083 I llm_load_print_meta: model params     = 1.41 B
0.00.079.084 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.079.084 I llm_load_print_meta: general.name     = 1.4B
0.00.079.085 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.085 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.085 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.086 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.086 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.079.086 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.087 I llm_load_print_meta: max token length = 1024
0.00.081.843 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.843 I llm_load_tensors: offloading output layer to GPU
0.00.081.843 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.854 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.081.855 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.083.270 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.271 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.271 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.271 I llama_new_context_with_model: n_batch       = 2048
0.00.083.272 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.272 I llama_new_context_with_model: flash_attn    = 0
0.00.083.272 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.273 I llama_new_context_with_model: freq_scale    = 1
0.00.083.273 I ggml_metal_init: allocating
0.00.083.277 I ggml_metal_init: found device: Apple M4
0.00.083.280 I ggml_metal_init: picking default device: Apple M4
0.00.084.039 I ggml_metal_init: using embedded metal library
0.00.087.107 I ggml_metal_init: GPU name:   Apple M4
0.00.087.110 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.111 I ggml_metal_init: simdgroup reduction   = true
0.00.087.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.111 I ggml_metal_init: has bfloat            = true
0.00.087.112 I ggml_metal_init: use bfloat            = true
0.00.087.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.113 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.258 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.267 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.289 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.258 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.259 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.260 I llama_new_context_with_model: graph nodes  = 967
0.00.120.260 I llama_new_context_with_model: graph splits = 2
0.00.120.274 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.929.696 I main: llama threadpool init, n_threads = 4
0.00.929.726 I 
0.00.929.747 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.929.747 I 
0.00.929.889 I sampler seed: 1234
0.00.929.893 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.929.902 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.929.903 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.929.903 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.803.923 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62171.63 tokens per second)
0.01.803.924 I llama_perf_context_print:        load time =     912.46 ms
0.01.803.925 I llama_perf_context_print: prompt eval time =      38.47 ms /     7 tokens (    5.50 ms per token,   181.96 tokens per second)
0.01.803.926 I llama_perf_context_print:        eval time =     832.66 ms /    63 runs   (   13.22 ms per token,    75.66 tokens per second)
0.01.803.926 I llama_perf_context_print:       total time =     874.23 ms /    70 tokens
0.01.804.102 I ggml_metal_free: deallocating

real	0m1.825s
user	0m0.128s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.002.712 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.703 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.370 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.375 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.377 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.377 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.378 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.378 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.378 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.380 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.380 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.380 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.381 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.381 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.381 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.382 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.384 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.384 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.387 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.794 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.841 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.843 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.844 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.844 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.845 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.845 I llama_model_loader: - type  f32:  194 tensors
0.00.055.846 I llama_model_loader: - type  f16:   98 tensors
0.00.086.644 I llm_load_vocab: special tokens cache size = 25
0.00.093.476 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.478 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.478 I llm_load_print_meta: arch             = gptneox
0.00.093.479 I llm_load_print_meta: vocab type       = BPE
0.00.093.479 I llm_load_print_meta: n_vocab          = 50304
0.00.093.479 I llm_load_print_meta: n_merges         = 50009
0.00.093.479 I llm_load_print_meta: vocab_only       = 0
0.00.093.479 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.480 I llm_load_print_meta: n_embd           = 2048
0.00.093.480 I llm_load_print_meta: n_layer          = 24
0.00.093.483 I llm_load_print_meta: n_head           = 16
0.00.093.483 I llm_load_print_meta: n_head_kv        = 16
0.00.093.484 I llm_load_print_meta: n_rot            = 32
0.00.093.484 I llm_load_print_meta: n_swa            = 0
0.00.093.484 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.484 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.485 I llm_load_print_meta: n_gqa            = 1
0.00.093.485 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.486 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.487 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.488 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.488 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.489 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.489 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.489 I llm_load_print_meta: n_ff             = 8192
0.00.093.490 I llm_load_print_meta: n_expert         = 0
0.00.093.490 I llm_load_print_meta: n_expert_used    = 0
0.00.093.490 I llm_load_print_meta: causal attn      = 1
0.00.093.490 I llm_load_print_meta: pooling type     = 0
0.00.093.490 I llm_load_print_meta: rope type        = 2
0.00.093.490 I llm_load_print_meta: rope scaling     = linear
0.00.093.491 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.491 I llm_load_print_meta: freq_scale_train = 1
0.00.093.491 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.491 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.492 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.492 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.492 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.492 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.492 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.504 I llm_load_print_meta: model type       = 1.4B
0.00.093.505 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.093.506 I llm_load_print_meta: model params     = 1.41 B
0.00.093.506 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.093.506 I llm_load_print_meta: general.name     = 1.4B
0.00.093.507 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.507 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.507 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.507 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.508 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.093.508 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.508 I llm_load_print_meta: max token length = 1024
0.00.096.126 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.126 I llm_load_tensors: offloading output layer to GPU
0.00.096.126 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.136 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.137 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.149 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.151 I llama_new_context_with_model: n_ctx         = 128
0.00.097.151 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.097.151 I llama_new_context_with_model: n_batch       = 128
0.00.097.151 I llama_new_context_with_model: n_ubatch      = 128
0.00.097.151 I llama_new_context_with_model: flash_attn    = 0
0.00.097.152 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.152 I llama_new_context_with_model: freq_scale    = 1
0.00.097.152 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.097.153 I ggml_metal_init: allocating
0.00.097.156 I ggml_metal_init: found device: Apple M4
0.00.097.158 I ggml_metal_init: picking default device: Apple M4
0.00.097.749 I ggml_metal_init: using embedded metal library
0.00.099.928 I ggml_metal_init: GPU name:   Apple M4
0.00.099.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.930 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.931 I ggml_metal_init: simdgroup reduction   = true
0.00.099.931 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.931 I ggml_metal_init: has bfloat            = true
0.00.099.932 I ggml_metal_init: use bfloat            = true
0.00.099.932 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.933 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.578 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.581 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.595 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.474 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.475 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.475 I llama_new_context_with_model: graph nodes  = 967
0.00.110.476 I llama_new_context_with_model: graph splits = 2
0.00.110.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.401.221 I 
0.01.401.279 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.401.345 I perplexity: tokenizing the input ..
0.01.415.473 I perplexity: tokenization took 14.122 ms
0.01.415.479 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.536.189 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.537.787 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.537.804 I llama_perf_context_print:        load time =    1375.50 ms
0.01.537.805 I llama_perf_context_print: prompt eval time =     119.78 ms /   128 tokens (    0.94 ms per token,  1068.61 tokens per second)
0.01.537.806 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.537.807 I llama_perf_context_print:       total time =     136.59 ms /   129 tokens
0.01.538.144 I ggml_metal_free: deallocating

real	0m1.725s
user	0m0.122s
sys	0m0.239s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.002.200 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.700 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.946 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.953 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.955 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.956 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.956 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.957 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.957 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.959 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.959 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.960 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.960 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.961 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.961 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.962 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.964 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.965 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.194 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.194 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.195 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.049.197 I llama_model_loader: - type  f32:  194 tensors
0.00.049.198 I llama_model_loader: - type q8_0:   98 tensors
0.00.088.959 I llm_load_vocab: special tokens cache size = 25
0.00.096.944 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.096.949 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.096.949 I llm_load_print_meta: arch             = gptneox
0.00.096.950 I llm_load_print_meta: vocab type       = BPE
0.00.096.950 I llm_load_print_meta: n_vocab          = 50304
0.00.096.950 I llm_load_print_meta: n_merges         = 50009
0.00.096.950 I llm_load_print_meta: vocab_only       = 0
0.00.096.951 I llm_load_print_meta: n_ctx_train      = 2048
0.00.096.951 I llm_load_print_meta: n_embd           = 2048
0.00.096.952 I llm_load_print_meta: n_layer          = 24
0.00.096.956 I llm_load_print_meta: n_head           = 16
0.00.096.957 I llm_load_print_meta: n_head_kv        = 16
0.00.096.957 I llm_load_print_meta: n_rot            = 32
0.00.096.957 I llm_load_print_meta: n_swa            = 0
0.00.096.957 I llm_load_print_meta: n_embd_head_k    = 128
0.00.096.960 I llm_load_print_meta: n_embd_head_v    = 128
0.00.096.960 I llm_load_print_meta: n_gqa            = 1
0.00.096.961 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.096.962 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.096.969 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.096.970 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.096.971 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.096.971 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.096.972 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.096.989 I llm_load_print_meta: n_ff             = 8192
0.00.096.989 I llm_load_print_meta: n_expert         = 0
0.00.096.990 I llm_load_print_meta: n_expert_used    = 0
0.00.096.990 I llm_load_print_meta: causal attn      = 1
0.00.096.991 I llm_load_print_meta: pooling type     = 0
0.00.096.991 I llm_load_print_meta: rope type        = 2
0.00.096.992 I llm_load_print_meta: rope scaling     = linear
0.00.096.992 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.096.992 I llm_load_print_meta: freq_scale_train = 1
0.00.096.993 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.096.993 I llm_load_print_meta: rope_finetuned   = unknown
0.00.096.993 I llm_load_print_meta: ssm_d_conv       = 0
0.00.096.993 I llm_load_print_meta: ssm_d_inner      = 0
0.00.096.994 I llm_load_print_meta: ssm_d_state      = 0
0.00.096.994 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.096.994 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.097.006 I llm_load_print_meta: model type       = 1.4B
0.00.097.007 I llm_load_print_meta: model ftype      = Q8_0
0.00.097.007 I llm_load_print_meta: model params     = 1.41 B
0.00.097.008 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.097.008 I llm_load_print_meta: general.name     = 1.4B
0.00.097.009 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.097.009 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.097.009 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.097.009 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.097.010 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.097.010 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.097.010 I llm_load_print_meta: max token length = 1024
0.00.099.675 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.099.675 I llm_load_tensors: offloading output layer to GPU
0.00.099.676 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.099.686 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.099.687 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.100.825 I llama_new_context_with_model: n_seq_max     = 1
0.00.100.826 I llama_new_context_with_model: n_ctx         = 128
0.00.100.827 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.100.827 I llama_new_context_with_model: n_batch       = 128
0.00.100.827 I llama_new_context_with_model: n_ubatch      = 128
0.00.100.827 I llama_new_context_with_model: flash_attn    = 0
0.00.100.828 I llama_new_context_with_model: freq_base     = 10000.0
0.00.100.828 I llama_new_context_with_model: freq_scale    = 1
0.00.100.828 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.100.829 I ggml_metal_init: allocating
0.00.100.832 I ggml_metal_init: found device: Apple M4
0.00.100.834 I ggml_metal_init: picking default device: Apple M4
0.00.101.534 I ggml_metal_init: using embedded metal library
0.00.103.980 I ggml_metal_init: GPU name:   Apple M4
0.00.103.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.103.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.103.983 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.103.983 I ggml_metal_init: simdgroup reduction   = true
0.00.103.983 I ggml_metal_init: simdgroup matrix mul. = true
0.00.103.983 I ggml_metal_init: has bfloat            = true
0.00.103.984 I ggml_metal_init: use bfloat            = true
0.00.103.984 I ggml_metal_init: hasUnifiedMemory      = true
0.00.103.986 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.661 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.113.665 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.113.680 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.725 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.114.726 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.114.726 I llama_new_context_with_model: graph nodes  = 967
0.00.114.726 I llama_new_context_with_model: graph splits = 2
0.00.114.739 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.052.066 I 
0.01.052.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.052.095 I perplexity: tokenizing the input ..
0.01.062.006 I perplexity: tokenization took 9.909 ms
0.01.062.012 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.184.668 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.185.938 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.185.957 I llama_perf_context_print:        load time =    1033.36 ms
0.01.185.958 I llama_perf_context_print: prompt eval time =     122.27 ms /   128 tokens (    0.96 ms per token,  1046.83 tokens per second)
0.01.185.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.185.960 I llama_perf_context_print:       total time =     133.89 ms /   129 tokens
0.01.186.374 I ggml_metal_free: deallocating

real	0m1.206s
user	0m0.114s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.002.376 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.628 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.708 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.713 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.714 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.715 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.715 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.715 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.715 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.716 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.717 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.717 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.719 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.723 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.723 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.723 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.979 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.980 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.980 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.981 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.981 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.981 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.982 I llama_model_loader: - type  f32:  194 tensors
0.00.038.982 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.982 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.832 I llm_load_vocab: special tokens cache size = 25
0.00.074.020 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.023 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.024 I llm_load_print_meta: arch             = gptneox
0.00.074.024 I llm_load_print_meta: vocab type       = BPE
0.00.074.024 I llm_load_print_meta: n_vocab          = 50304
0.00.074.024 I llm_load_print_meta: n_merges         = 50009
0.00.074.024 I llm_load_print_meta: vocab_only       = 0
0.00.074.025 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.025 I llm_load_print_meta: n_embd           = 2048
0.00.074.025 I llm_load_print_meta: n_layer          = 24
0.00.074.028 I llm_load_print_meta: n_head           = 16
0.00.074.029 I llm_load_print_meta: n_head_kv        = 16
0.00.074.032 I llm_load_print_meta: n_rot            = 32
0.00.074.032 I llm_load_print_meta: n_swa            = 0
0.00.074.032 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.032 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.033 I llm_load_print_meta: n_gqa            = 1
0.00.074.034 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.035 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.035 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.036 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.036 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.036 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.036 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.037 I llm_load_print_meta: n_ff             = 8192
0.00.074.037 I llm_load_print_meta: n_expert         = 0
0.00.074.037 I llm_load_print_meta: n_expert_used    = 0
0.00.074.037 I llm_load_print_meta: causal attn      = 1
0.00.074.037 I llm_load_print_meta: pooling type     = 0
0.00.074.038 I llm_load_print_meta: rope type        = 2
0.00.074.038 I llm_load_print_meta: rope scaling     = linear
0.00.074.039 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.040 I llm_load_print_meta: freq_scale_train = 1
0.00.074.040 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.040 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.040 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.040 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.041 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.041 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.041 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.052 I llm_load_print_meta: model type       = 1.4B
0.00.074.054 I llm_load_print_meta: model ftype      = Q4_0
0.00.074.054 I llm_load_print_meta: model params     = 1.41 B
0.00.074.054 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.074.055 I llm_load_print_meta: general.name     = 1.4B
0.00.074.055 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.055 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.055 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.055 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.056 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.074.056 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.056 I llm_load_print_meta: max token length = 1024
0.00.076.224 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.225 I llm_load_tensors: offloading output layer to GPU
0.00.076.225 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.235 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.076.236 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.077.322 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.323 I llama_new_context_with_model: n_ctx         = 128
0.00.077.323 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.077.324 I llama_new_context_with_model: n_batch       = 128
0.00.077.324 I llama_new_context_with_model: n_ubatch      = 128
0.00.077.324 I llama_new_context_with_model: flash_attn    = 0
0.00.077.324 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.325 I llama_new_context_with_model: freq_scale    = 1
0.00.077.325 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.077.325 I ggml_metal_init: allocating
0.00.077.328 I ggml_metal_init: found device: Apple M4
0.00.077.331 I ggml_metal_init: picking default device: Apple M4
0.00.077.964 I ggml_metal_init: using embedded metal library
0.00.080.432 I ggml_metal_init: GPU name:   Apple M4
0.00.080.434 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.435 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.436 I ggml_metal_init: simdgroup reduction   = true
0.00.080.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.436 I ggml_metal_init: has bfloat            = true
0.00.080.436 I ggml_metal_init: use bfloat            = true
0.00.080.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.450 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.092.453 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.092.469 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.563 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.093.565 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.093.565 I llama_new_context_with_model: graph nodes  = 967
0.00.093.566 I llama_new_context_with_model: graph splits = 2
0.00.093.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.785 I 
0.00.657.821 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.657.846 I perplexity: tokenizing the input ..
0.00.665.237 I perplexity: tokenization took 7.39 ms
0.00.665.242 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.143 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.788.290 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.788.305 I llama_perf_context_print:        load time =     641.15 ms
0.00.788.306 I llama_perf_context_print: prompt eval time =     121.68 ms /   128 tokens (    0.95 ms per token,  1051.95 tokens per second)
0.00.788.307 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.307 I llama_perf_context_print:       total time =     130.52 ms /   129 tokens
0.00.788.645 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.089s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.908 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.135 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.136 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.962 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.986 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.987 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.987 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.988 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.988 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.988 I llama_model_loader: - type  f32:  194 tensors
0.00.031.989 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.989 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.306 I llm_load_vocab: special tokens cache size = 25
0.00.059.230 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.233 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.234 I llm_load_print_meta: arch             = gptneox
0.00.059.234 I llm_load_print_meta: vocab type       = BPE
0.00.059.234 I llm_load_print_meta: n_vocab          = 50304
0.00.059.234 I llm_load_print_meta: n_merges         = 50009
0.00.059.234 I llm_load_print_meta: vocab_only       = 0
0.00.059.235 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.235 I llm_load_print_meta: n_embd           = 2048
0.00.059.235 I llm_load_print_meta: n_layer          = 24
0.00.059.238 I llm_load_print_meta: n_head           = 16
0.00.059.240 I llm_load_print_meta: n_head_kv        = 16
0.00.059.241 I llm_load_print_meta: n_rot            = 32
0.00.059.241 I llm_load_print_meta: n_swa            = 0
0.00.059.241 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.241 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.242 I llm_load_print_meta: n_gqa            = 1
0.00.059.243 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.244 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.244 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.245 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.246 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.246 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.246 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.247 I llm_load_print_meta: n_ff             = 8192
0.00.059.247 I llm_load_print_meta: n_expert         = 0
0.00.059.247 I llm_load_print_meta: n_expert_used    = 0
0.00.059.247 I llm_load_print_meta: causal attn      = 1
0.00.059.247 I llm_load_print_meta: pooling type     = 0
0.00.059.248 I llm_load_print_meta: rope type        = 2
0.00.059.248 I llm_load_print_meta: rope scaling     = linear
0.00.059.248 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.249 I llm_load_print_meta: freq_scale_train = 1
0.00.059.253 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.253 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.253 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.255 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.255 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.255 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.255 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.262 I llm_load_print_meta: model type       = 1.4B
0.00.059.262 I llm_load_print_meta: model ftype      = Q4_1
0.00.059.263 I llm_load_print_meta: model params     = 1.41 B
0.00.059.263 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.059.263 I llm_load_print_meta: general.name     = 1.4B
0.00.059.264 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.264 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.264 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.264 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.264 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.265 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.266 I llm_load_print_meta: max token length = 1024
0.00.061.035 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.036 I llm_load_tensors: offloading output layer to GPU
0.00.061.036 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.041 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.061.042 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.062.404 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.405 I llama_new_context_with_model: n_ctx         = 128
0.00.062.405 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.405 I llama_new_context_with_model: n_batch       = 128
0.00.062.405 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.405 I llama_new_context_with_model: flash_attn    = 0
0.00.062.406 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.406 I llama_new_context_with_model: freq_scale    = 1
0.00.062.406 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.407 I ggml_metal_init: allocating
0.00.062.414 I ggml_metal_init: found device: Apple M4
0.00.062.416 I ggml_metal_init: picking default device: Apple M4
0.00.062.954 I ggml_metal_init: using embedded metal library
0.00.064.864 I ggml_metal_init: GPU name:   Apple M4
0.00.064.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.867 I ggml_metal_init: simdgroup reduction   = true
0.00.064.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.867 I ggml_metal_init: has bfloat            = true
0.00.064.867 I ggml_metal_init: use bfloat            = true
0.00.064.867 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.703 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.073.707 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.073.722 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.074.588 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.074.589 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.074.589 I llama_new_context_with_model: graph nodes  = 967
0.00.074.590 I llama_new_context_with_model: graph splits = 2
0.00.074.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.774 I 
0.00.803.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.803.831 I perplexity: tokenizing the input ..
0.00.811.624 I perplexity: tokenization took 7.792 ms
0.00.811.628 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.934.062 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.935.201 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.935.218 I llama_perf_context_print:        load time =     794.86 ms
0.00.935.219 I llama_perf_context_print: prompt eval time =     122.20 ms /   128 tokens (    0.95 ms per token,  1047.43 tokens per second)
0.00.935.220 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.935.221 I llama_perf_context_print:       total time =     131.44 ms /   129 tokens
0.00.935.617 I ggml_metal_free: deallocating

real	0m0.947s
user	0m0.077s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.352 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.093 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.022.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.100 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.100 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.100 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.102 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.103 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.745 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.736 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.469 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.470 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.471 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.471 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.471 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.472 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.030.472 I llama_model_loader: - type  f32:  194 tensors
0.00.030.472 I llama_model_loader: - type q5_0:   97 tensors
0.00.030.473 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.357 I llm_load_vocab: special tokens cache size = 25
0.00.057.235 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.238 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.238 I llm_load_print_meta: arch             = gptneox
0.00.057.239 I llm_load_print_meta: vocab type       = BPE
0.00.057.239 I llm_load_print_meta: n_vocab          = 50304
0.00.057.239 I llm_load_print_meta: n_merges         = 50009
0.00.057.239 I llm_load_print_meta: vocab_only       = 0
0.00.057.239 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.239 I llm_load_print_meta: n_embd           = 2048
0.00.057.240 I llm_load_print_meta: n_layer          = 24
0.00.057.242 I llm_load_print_meta: n_head           = 16
0.00.057.243 I llm_load_print_meta: n_head_kv        = 16
0.00.057.243 I llm_load_print_meta: n_rot            = 32
0.00.057.243 I llm_load_print_meta: n_swa            = 0
0.00.057.243 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.244 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.244 I llm_load_print_meta: n_gqa            = 1
0.00.057.245 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.246 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.246 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.247 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.247 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.247 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.247 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.248 I llm_load_print_meta: n_ff             = 8192
0.00.057.248 I llm_load_print_meta: n_expert         = 0
0.00.057.248 I llm_load_print_meta: n_expert_used    = 0
0.00.057.248 I llm_load_print_meta: causal attn      = 1
0.00.057.248 I llm_load_print_meta: pooling type     = 0
0.00.057.249 I llm_load_print_meta: rope type        = 2
0.00.057.249 I llm_load_print_meta: rope scaling     = linear
0.00.057.257 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.260 I llm_load_print_meta: freq_scale_train = 1
0.00.057.260 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.260 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.260 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.260 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.261 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.261 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.261 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.273 I llm_load_print_meta: model type       = 1.4B
0.00.057.273 I llm_load_print_meta: model ftype      = Q5_0
0.00.057.273 I llm_load_print_meta: model params     = 1.41 B
0.00.057.274 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.057.274 I llm_load_print_meta: general.name     = 1.4B
0.00.057.274 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.274 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.275 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.275 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.275 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.057.275 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.276 I llm_load_print_meta: max token length = 1024
0.00.058.818 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.819 I llm_load_tensors: offloading output layer to GPU
0.00.058.819 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.828 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.058.830 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.059.730 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.731 I llama_new_context_with_model: n_ctx         = 128
0.00.059.731 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.059.731 I llama_new_context_with_model: n_batch       = 128
0.00.059.732 I llama_new_context_with_model: n_ubatch      = 128
0.00.059.732 I llama_new_context_with_model: flash_attn    = 0
0.00.059.732 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.732 I llama_new_context_with_model: freq_scale    = 1
0.00.059.733 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.059.733 I ggml_metal_init: allocating
0.00.059.743 I ggml_metal_init: found device: Apple M4
0.00.059.746 I ggml_metal_init: picking default device: Apple M4
0.00.060.266 I ggml_metal_init: using embedded metal library
0.00.062.219 I ggml_metal_init: GPU name:   Apple M4
0.00.062.220 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.221 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.221 I ggml_metal_init: simdgroup reduction   = true
0.00.062.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.222 I ggml_metal_init: has bfloat            = true
0.00.062.222 I ggml_metal_init: use bfloat            = true
0.00.062.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.388 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.393 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.407 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.303 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.304 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.305 I llama_new_context_with_model: graph nodes  = 967
0.00.072.305 I llama_new_context_with_model: graph splits = 2
0.00.072.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.697 I 
0.00.742.735 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.742.761 I perplexity: tokenizing the input ..
0.00.750.649 I perplexity: tokenization took 7.887 ms
0.00.750.652 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.885.167 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.886.390 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.886.403 I llama_perf_context_print:        load time =     733.34 ms
0.00.886.404 I llama_perf_context_print: prompt eval time =     134.26 ms /   128 tokens (    1.05 ms per token,   953.35 tokens per second)
0.00.886.405 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.886.405 I llama_perf_context_print:       total time =     143.71 ms /   129 tokens
0.00.886.739 I ggml_metal_free: deallocating

real	0m0.899s
user	0m0.076s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.989 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.465 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.471 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.472 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.473 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.473 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.473 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.474 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.474 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.475 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.475 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.475 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.476 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.478 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.478 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.478 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.231 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.958 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.959 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.961 I llama_model_loader: - type  f32:  194 tensors
0.00.025.961 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.962 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.922 I llm_load_vocab: special tokens cache size = 25
0.00.051.968 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.971 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.971 I llm_load_print_meta: arch             = gptneox
0.00.051.972 I llm_load_print_meta: vocab type       = BPE
0.00.051.972 I llm_load_print_meta: n_vocab          = 50304
0.00.051.972 I llm_load_print_meta: n_merges         = 50009
0.00.051.972 I llm_load_print_meta: vocab_only       = 0
0.00.051.972 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.972 I llm_load_print_meta: n_embd           = 2048
0.00.051.973 I llm_load_print_meta: n_layer          = 24
0.00.051.975 I llm_load_print_meta: n_head           = 16
0.00.051.976 I llm_load_print_meta: n_head_kv        = 16
0.00.051.976 I llm_load_print_meta: n_rot            = 32
0.00.051.977 I llm_load_print_meta: n_swa            = 0
0.00.051.977 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.977 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.978 I llm_load_print_meta: n_gqa            = 1
0.00.051.978 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.980 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.981 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.982 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.982 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.983 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.983 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.984 I llm_load_print_meta: n_ff             = 8192
0.00.051.984 I llm_load_print_meta: n_expert         = 0
0.00.051.984 I llm_load_print_meta: n_expert_used    = 0
0.00.051.984 I llm_load_print_meta: causal attn      = 1
0.00.051.984 I llm_load_print_meta: pooling type     = 0
0.00.051.984 I llm_load_print_meta: rope type        = 2
0.00.051.984 I llm_load_print_meta: rope scaling     = linear
0.00.051.985 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.987 I llm_load_print_meta: freq_scale_train = 1
0.00.051.987 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.987 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.987 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.988 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.988 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.988 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.988 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.994 I llm_load_print_meta: model type       = 1.4B
0.00.051.994 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.995 I llm_load_print_meta: model params     = 1.41 B
0.00.051.995 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.995 I llm_load_print_meta: general.name     = 1.4B
0.00.051.996 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.996 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.996 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.996 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.996 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.997 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.997 I llm_load_print_meta: max token length = 1024
0.00.053.507 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.507 I llm_load_tensors: offloading output layer to GPU
0.00.053.507 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.511 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.512 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.373 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.374 I llama_new_context_with_model: n_ctx         = 128
0.00.054.374 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.375 I llama_new_context_with_model: n_batch       = 128
0.00.054.375 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.375 I llama_new_context_with_model: flash_attn    = 0
0.00.054.375 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.376 I llama_new_context_with_model: freq_scale    = 1
0.00.054.376 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.377 I ggml_metal_init: allocating
0.00.054.382 I ggml_metal_init: found device: Apple M4
0.00.054.385 I ggml_metal_init: picking default device: Apple M4
0.00.054.917 I ggml_metal_init: using embedded metal library
0.00.056.840 I ggml_metal_init: GPU name:   Apple M4
0.00.056.842 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.843 I ggml_metal_init: simdgroup reduction   = true
0.00.056.843 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.843 I ggml_metal_init: has bfloat            = true
0.00.056.843 I ggml_metal_init: use bfloat            = true
0.00.056.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.844 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.012 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.015 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.030 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.923 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.924 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.925 I llama_new_context_with_model: graph nodes  = 967
0.00.066.925 I llama_new_context_with_model: graph splits = 2
0.00.066.931 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.812.659 I 
0.00.812.692 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.812.723 I perplexity: tokenizing the input ..
0.00.820.937 I perplexity: tokenization took 8.212 ms
0.00.820.942 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.956.225 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.957.493 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.957.514 I llama_perf_context_print:        load time =     803.67 ms
0.00.957.514 I llama_perf_context_print: prompt eval time =     135.05 ms /   128 tokens (    1.06 ms per token,   947.79 tokens per second)
0.00.957.515 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.957.516 I llama_perf_context_print:       total time =     144.85 ms /   129 tokens
0.00.957.937 I ggml_metal_free: deallocating

real	0m0.971s
user	0m0.076s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.772 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.130 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.027.134 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.136 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.138 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.138 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.139 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.139 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.140 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.141 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.141 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.141 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.141 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.142 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.142 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.143 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.144 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.531 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.790 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.409 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.410 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.410 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.411 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.411 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.411 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.037.412 I llama_model_loader: - type  f32:  194 tensors
0.00.037.412 I llama_model_loader: - type q2_K:   49 tensors
0.00.037.412 I llama_model_loader: - type q3_K:   48 tensors
0.00.037.413 I llama_model_loader: - type q6_K:    1 tensors
0.00.067.200 I llm_load_vocab: special tokens cache size = 25
0.00.077.639 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.643 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.643 I llm_load_print_meta: arch             = gptneox
0.00.077.643 I llm_load_print_meta: vocab type       = BPE
0.00.077.644 I llm_load_print_meta: n_vocab          = 50304
0.00.077.644 I llm_load_print_meta: n_merges         = 50009
0.00.077.644 I llm_load_print_meta: vocab_only       = 0
0.00.077.644 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.645 I llm_load_print_meta: n_embd           = 2048
0.00.077.645 I llm_load_print_meta: n_layer          = 24
0.00.077.648 I llm_load_print_meta: n_head           = 16
0.00.077.649 I llm_load_print_meta: n_head_kv        = 16
0.00.077.649 I llm_load_print_meta: n_rot            = 32
0.00.077.650 I llm_load_print_meta: n_swa            = 0
0.00.077.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.650 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.651 I llm_load_print_meta: n_gqa            = 1
0.00.077.652 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.653 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.654 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.654 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.654 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.656 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.656 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.657 I llm_load_print_meta: n_ff             = 8192
0.00.077.657 I llm_load_print_meta: n_expert         = 0
0.00.077.658 I llm_load_print_meta: n_expert_used    = 0
0.00.077.658 I llm_load_print_meta: causal attn      = 1
0.00.077.660 I llm_load_print_meta: pooling type     = 0
0.00.077.660 I llm_load_print_meta: rope type        = 2
0.00.077.661 I llm_load_print_meta: rope scaling     = linear
0.00.077.661 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.661 I llm_load_print_meta: freq_scale_train = 1
0.00.077.662 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.662 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.662 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.662 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.662 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.663 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.663 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.667 I llm_load_print_meta: model type       = 1.4B
0.00.077.667 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.077.668 I llm_load_print_meta: model params     = 1.41 B
0.00.077.669 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.077.669 I llm_load_print_meta: general.name     = 1.4B
0.00.077.671 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.671 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.672 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.672 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.672 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.077.673 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.673 I llm_load_print_meta: max token length = 1024
0.00.080.000 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.000 I llm_load_tensors: offloading output layer to GPU
0.00.080.001 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.006 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.080.007 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.081.407 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.408 I llama_new_context_with_model: n_ctx         = 128
0.00.081.408 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.081.409 I llama_new_context_with_model: n_batch       = 128
0.00.081.409 I llama_new_context_with_model: n_ubatch      = 128
0.00.081.409 I llama_new_context_with_model: flash_attn    = 0
0.00.081.410 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.410 I llama_new_context_with_model: freq_scale    = 1
0.00.081.410 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.081.411 I ggml_metal_init: allocating
0.00.081.419 I ggml_metal_init: found device: Apple M4
0.00.081.422 I ggml_metal_init: picking default device: Apple M4
0.00.082.160 I ggml_metal_init: using embedded metal library
0.00.085.192 I ggml_metal_init: GPU name:   Apple M4
0.00.085.195 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.195 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.196 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.196 I ggml_metal_init: simdgroup reduction   = true
0.00.085.196 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.197 I ggml_metal_init: has bfloat            = true
0.00.085.197 I ggml_metal_init: use bfloat            = true
0.00.085.197 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.215 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.097.220 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.097.236 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.362 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.098.364 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.098.364 I llama_new_context_with_model: graph nodes  = 967
0.00.098.365 I llama_new_context_with_model: graph splits = 2
0.00.098.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.081 I 
0.00.608.118 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.608.151 I perplexity: tokenizing the input ..
0.00.620.135 I perplexity: tokenization took 11.982 ms
0.00.620.140 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.754.060 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.755.378 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.755.400 I llama_perf_context_print:        load time =     591.30 ms
0.00.755.401 I llama_perf_context_print: prompt eval time =     132.92 ms /   128 tokens (    1.04 ms per token,   963.00 tokens per second)
0.00.755.402 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.755.403 I llama_perf_context_print:       total time =     147.32 ms /   129 tokens
0.00.755.981 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.103s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.595 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.895 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.020.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.904 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.904 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.906 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.907 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.907 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.908 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.908 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.913 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.914 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.914 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.914 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.916 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.917 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.517 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.190 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.191 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.191 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.192 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.029.192 I llama_model_loader: - type  f32:  194 tensors
0.00.029.192 I llama_model_loader: - type q3_K:   25 tensors
0.00.029.193 I llama_model_loader: - type q4_K:   71 tensors
0.00.029.193 I llama_model_loader: - type q5_K:    1 tensors
0.00.029.193 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.968 I llm_load_vocab: special tokens cache size = 25
0.00.055.858 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.860 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.861 I llm_load_print_meta: arch             = gptneox
0.00.055.861 I llm_load_print_meta: vocab type       = BPE
0.00.055.861 I llm_load_print_meta: n_vocab          = 50304
0.00.055.861 I llm_load_print_meta: n_merges         = 50009
0.00.055.861 I llm_load_print_meta: vocab_only       = 0
0.00.055.862 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.862 I llm_load_print_meta: n_embd           = 2048
0.00.055.862 I llm_load_print_meta: n_layer          = 24
0.00.055.864 I llm_load_print_meta: n_head           = 16
0.00.055.865 I llm_load_print_meta: n_head_kv        = 16
0.00.055.866 I llm_load_print_meta: n_rot            = 32
0.00.055.866 I llm_load_print_meta: n_swa            = 0
0.00.055.866 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.866 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.867 I llm_load_print_meta: n_gqa            = 1
0.00.055.868 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.868 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.869 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.869 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.869 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.869 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.871 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.872 I llm_load_print_meta: n_ff             = 8192
0.00.055.872 I llm_load_print_meta: n_expert         = 0
0.00.055.872 I llm_load_print_meta: n_expert_used    = 0
0.00.055.872 I llm_load_print_meta: causal attn      = 1
0.00.055.873 I llm_load_print_meta: pooling type     = 0
0.00.055.874 I llm_load_print_meta: rope type        = 2
0.00.055.876 I llm_load_print_meta: rope scaling     = linear
0.00.055.876 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.877 I llm_load_print_meta: freq_scale_train = 1
0.00.055.877 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.878 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.879 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.879 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.879 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.879 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.879 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.886 I llm_load_print_meta: model type       = 1.4B
0.00.055.886 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.055.887 I llm_load_print_meta: model params     = 1.41 B
0.00.055.888 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.055.888 I llm_load_print_meta: general.name     = 1.4B
0.00.055.888 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.888 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.888 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.888 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.889 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.889 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.889 I llm_load_print_meta: max token length = 1024
0.00.057.651 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.651 I llm_load_tensors: offloading output layer to GPU
0.00.057.652 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.657 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.057.657 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.058.733 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.734 I llama_new_context_with_model: n_ctx         = 128
0.00.058.734 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.058.734 I llama_new_context_with_model: n_batch       = 128
0.00.058.734 I llama_new_context_with_model: n_ubatch      = 128
0.00.058.734 I llama_new_context_with_model: flash_attn    = 0
0.00.058.735 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.735 I llama_new_context_with_model: freq_scale    = 1
0.00.058.735 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.736 I ggml_metal_init: allocating
0.00.058.741 I ggml_metal_init: found device: Apple M4
0.00.058.744 I ggml_metal_init: picking default device: Apple M4
0.00.059.271 I ggml_metal_init: using embedded metal library
0.00.061.198 I ggml_metal_init: GPU name:   Apple M4
0.00.061.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.201 I ggml_metal_init: simdgroup reduction   = true
0.00.061.201 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.202 I ggml_metal_init: has bfloat            = true
0.00.061.202 I ggml_metal_init: use bfloat            = true
0.00.061.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.227 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.231 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.246 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.173 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.175 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.175 I llama_new_context_with_model: graph nodes  = 967
0.00.071.175 I llama_new_context_with_model: graph splits = 2
0.00.071.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.954 I 
0.00.549.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.550.018 I perplexity: tokenizing the input ..
0.00.557.856 I perplexity: tokenization took 7.838 ms
0.00.557.859 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.689.757 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.691.015 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.691.028 I llama_perf_context_print:        load time =     541.36 ms
0.00.691.029 I llama_perf_context_print: prompt eval time =     131.66 ms /   128 tokens (    1.03 ms per token,   972.24 tokens per second)
0.00.691.030 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.691.030 I llama_perf_context_print:       total time =     141.07 ms /   129 tokens
0.00.691.368 I ggml_metal_free: deallocating

real	0m0.702s
user	0m0.075s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.587 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.847 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.853 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.856 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.857 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.857 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.859 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.197 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.198 I llama_model_loader: - type  f32:  194 tensors
0.00.028.198 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.198 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.198 I llama_model_loader: - type q6_K:   13 tensors
0.00.048.367 I llm_load_vocab: special tokens cache size = 25
0.00.054.404 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.407 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.407 I llm_load_print_meta: arch             = gptneox
0.00.054.407 I llm_load_print_meta: vocab type       = BPE
0.00.054.408 I llm_load_print_meta: n_vocab          = 50304
0.00.054.408 I llm_load_print_meta: n_merges         = 50009
0.00.054.408 I llm_load_print_meta: vocab_only       = 0
0.00.054.408 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.408 I llm_load_print_meta: n_embd           = 2048
0.00.054.409 I llm_load_print_meta: n_layer          = 24
0.00.054.412 I llm_load_print_meta: n_head           = 16
0.00.054.413 I llm_load_print_meta: n_head_kv        = 16
0.00.054.413 I llm_load_print_meta: n_rot            = 32
0.00.054.413 I llm_load_print_meta: n_swa            = 0
0.00.054.414 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.414 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.414 I llm_load_print_meta: n_gqa            = 1
0.00.054.415 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.416 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.417 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.417 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.417 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.418 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.418 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.418 I llm_load_print_meta: n_ff             = 8192
0.00.054.419 I llm_load_print_meta: n_expert         = 0
0.00.054.419 I llm_load_print_meta: n_expert_used    = 0
0.00.054.419 I llm_load_print_meta: causal attn      = 1
0.00.054.419 I llm_load_print_meta: pooling type     = 0
0.00.054.421 I llm_load_print_meta: rope type        = 2
0.00.054.421 I llm_load_print_meta: rope scaling     = linear
0.00.054.422 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.422 I llm_load_print_meta: freq_scale_train = 1
0.00.054.422 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.422 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.423 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.423 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.423 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.423 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.423 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.429 I llm_load_print_meta: model type       = 1.4B
0.00.054.430 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.054.430 I llm_load_print_meta: model params     = 1.41 B
0.00.054.431 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.054.431 I llm_load_print_meta: general.name     = 1.4B
0.00.054.431 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.431 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.431 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.432 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.432 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.432 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.432 I llm_load_print_meta: max token length = 1024
0.00.055.947 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.947 I llm_load_tensors: offloading output layer to GPU
0.00.055.947 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.951 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.952 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.805 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.806 I llama_new_context_with_model: n_ctx         = 128
0.00.056.806 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.807 I llama_new_context_with_model: n_batch       = 128
0.00.056.807 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.807 I llama_new_context_with_model: flash_attn    = 0
0.00.056.807 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.808 I llama_new_context_with_model: freq_scale    = 1
0.00.056.808 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.808 I ggml_metal_init: allocating
0.00.056.811 I ggml_metal_init: found device: Apple M4
0.00.056.813 I ggml_metal_init: picking default device: Apple M4
0.00.057.331 I ggml_metal_init: using embedded metal library
0.00.059.233 I ggml_metal_init: GPU name:   Apple M4
0.00.059.235 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.236 I ggml_metal_init: simdgroup reduction   = true
0.00.059.236 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.236 I ggml_metal_init: has bfloat            = true
0.00.059.236 I ggml_metal_init: use bfloat            = true
0.00.059.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.371 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.373 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.389 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.293 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.294 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.294 I llama_new_context_with_model: graph nodes  = 967
0.00.069.294 I llama_new_context_with_model: graph splits = 2
0.00.069.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.038 I 
0.00.686.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.686.066 I perplexity: tokenizing the input ..
0.00.693.782 I perplexity: tokenization took 7.715 ms
0.00.693.786 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.155 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.829.402 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.829.415 I llama_perf_context_print:        load time =     677.45 ms
0.00.829.416 I llama_perf_context_print: prompt eval time =     134.12 ms /   128 tokens (    1.05 ms per token,   954.38 tokens per second)
0.00.829.420 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.420 I llama_perf_context_print:       total time =     143.38 ms /   129 tokens
0.00.829.811 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.076s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.029 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.862 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.869 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.870 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.873 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.873 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.874 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.875 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.655 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.445 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.446 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.446 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.447 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.447 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.448 I llama_model_loader: - type  f32:  194 tensors
0.00.024.448 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.449 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.669 I llm_load_vocab: special tokens cache size = 25
0.00.050.769 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.771 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.772 I llm_load_print_meta: arch             = gptneox
0.00.050.772 I llm_load_print_meta: vocab type       = BPE
0.00.050.772 I llm_load_print_meta: n_vocab          = 50304
0.00.050.772 I llm_load_print_meta: n_merges         = 50009
0.00.050.773 I llm_load_print_meta: vocab_only       = 0
0.00.050.773 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.773 I llm_load_print_meta: n_embd           = 2048
0.00.050.773 I llm_load_print_meta: n_layer          = 24
0.00.050.776 I llm_load_print_meta: n_head           = 16
0.00.050.777 I llm_load_print_meta: n_head_kv        = 16
0.00.050.777 I llm_load_print_meta: n_rot            = 32
0.00.050.777 I llm_load_print_meta: n_swa            = 0
0.00.050.777 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.777 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.778 I llm_load_print_meta: n_gqa            = 1
0.00.050.779 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.780 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.780 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.781 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.782 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.782 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.782 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.783 I llm_load_print_meta: n_ff             = 8192
0.00.050.783 I llm_load_print_meta: n_expert         = 0
0.00.050.783 I llm_load_print_meta: n_expert_used    = 0
0.00.050.783 I llm_load_print_meta: causal attn      = 1
0.00.050.783 I llm_load_print_meta: pooling type     = 0
0.00.050.783 I llm_load_print_meta: rope type        = 2
0.00.050.784 I llm_load_print_meta: rope scaling     = linear
0.00.050.786 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.786 I llm_load_print_meta: freq_scale_train = 1
0.00.050.786 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.786 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.787 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.787 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.787 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.787 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.787 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.799 I llm_load_print_meta: model type       = 1.4B
0.00.050.800 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.800 I llm_load_print_meta: model params     = 1.41 B
0.00.050.801 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.801 I llm_load_print_meta: general.name     = 1.4B
0.00.050.801 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.802 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.802 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.802 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.802 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.803 I llm_load_print_meta: max token length = 1024
0.00.052.726 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.726 I llm_load_tensors: offloading output layer to GPU
0.00.052.727 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.737 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.738 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.716 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.717 I llama_new_context_with_model: n_ctx         = 128
0.00.053.717 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.717 I llama_new_context_with_model: n_batch       = 128
0.00.053.717 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.717 I llama_new_context_with_model: flash_attn    = 0
0.00.053.718 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.718 I llama_new_context_with_model: freq_scale    = 1
0.00.053.718 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.719 I ggml_metal_init: allocating
0.00.053.724 I ggml_metal_init: found device: Apple M4
0.00.053.726 I ggml_metal_init: picking default device: Apple M4
0.00.054.301 I ggml_metal_init: using embedded metal library
0.00.056.229 I ggml_metal_init: GPU name:   Apple M4
0.00.056.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.231 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.231 I ggml_metal_init: simdgroup reduction   = true
0.00.056.231 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.232 I ggml_metal_init: has bfloat            = true
0.00.056.232 I ggml_metal_init: use bfloat            = true
0.00.056.232 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.233 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.295 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.297 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.312 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.265 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.266 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.266 I llama_new_context_with_model: graph nodes  = 967
0.00.066.267 I llama_new_context_with_model: graph splits = 2
0.00.066.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.485 I 
0.00.664.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.664.559 I perplexity: tokenizing the input ..
0.00.672.343 I perplexity: tokenization took 7.783 ms
0.00.672.347 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.775 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.813.907 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.813.921 I llama_perf_context_print:        load time =     654.45 ms
0.00.813.922 I llama_perf_context_print: prompt eval time =     140.20 ms /   128 tokens (    1.10 ms per token,   912.97 tokens per second)
0.00.813.923 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.924 I llama_perf_context_print:       total time =     149.44 ms /   129 tokens
0.00.814.315 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.075s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.513 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.259 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.263 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.265 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.266 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.266 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.266 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.267 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.268 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.268 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.269 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.269 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.269 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.270 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.271 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.271 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.097 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.872 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.873 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.873 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.874 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.874 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.874 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.875 I llama_model_loader: - type  f32:  194 tensors
0.00.022.875 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.799 I llm_load_vocab: special tokens cache size = 25
0.00.049.651 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.654 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.654 I llm_load_print_meta: arch             = gptneox
0.00.049.655 I llm_load_print_meta: vocab type       = BPE
0.00.049.655 I llm_load_print_meta: n_vocab          = 50304
0.00.049.655 I llm_load_print_meta: n_merges         = 50009
0.00.049.655 I llm_load_print_meta: vocab_only       = 0
0.00.049.655 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.655 I llm_load_print_meta: n_embd           = 2048
0.00.049.656 I llm_load_print_meta: n_layer          = 24
0.00.049.659 I llm_load_print_meta: n_head           = 16
0.00.049.659 I llm_load_print_meta: n_head_kv        = 16
0.00.049.660 I llm_load_print_meta: n_rot            = 32
0.00.049.660 I llm_load_print_meta: n_swa            = 0
0.00.049.660 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.660 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.661 I llm_load_print_meta: n_gqa            = 1
0.00.049.662 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.663 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.663 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.664 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.664 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.664 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.664 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.665 I llm_load_print_meta: n_ff             = 8192
0.00.049.665 I llm_load_print_meta: n_expert         = 0
0.00.049.667 I llm_load_print_meta: n_expert_used    = 0
0.00.049.668 I llm_load_print_meta: causal attn      = 1
0.00.049.668 I llm_load_print_meta: pooling type     = 0
0.00.049.668 I llm_load_print_meta: rope type        = 2
0.00.049.668 I llm_load_print_meta: rope scaling     = linear
0.00.049.669 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.669 I llm_load_print_meta: freq_scale_train = 1
0.00.049.669 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.669 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.669 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.670 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.670 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.670 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.670 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.681 I llm_load_print_meta: model type       = 1.4B
0.00.049.681 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.682 I llm_load_print_meta: model params     = 1.41 B
0.00.049.682 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.684 I llm_load_print_meta: general.name     = 1.4B
0.00.049.684 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.684 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.684 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.684 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.685 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: max token length = 1024
0.00.051.232 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.232 I llm_load_tensors: offloading output layer to GPU
0.00.051.233 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.242 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.243 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.065 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.065 I llama_new_context_with_model: n_ctx         = 128
0.00.052.065 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.066 I llama_new_context_with_model: n_batch       = 128
0.00.052.066 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.066 I llama_new_context_with_model: flash_attn    = 0
0.00.052.066 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.066 I llama_new_context_with_model: freq_scale    = 1
0.00.052.067 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.067 I ggml_metal_init: allocating
0.00.052.070 I ggml_metal_init: found device: Apple M4
0.00.052.072 I ggml_metal_init: picking default device: Apple M4
0.00.052.596 I ggml_metal_init: using embedded metal library
0.00.054.566 I ggml_metal_init: GPU name:   Apple M4
0.00.054.568 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.568 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.568 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.569 I ggml_metal_init: simdgroup reduction   = true
0.00.054.569 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.569 I ggml_metal_init: has bfloat            = true
0.00.054.569 I ggml_metal_init: use bfloat            = true
0.00.054.569 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.560 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.563 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.578 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.456 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.457 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.458 I llama_new_context_with_model: graph nodes  = 967
0.00.064.458 I llama_new_context_with_model: graph splits = 2
0.00.064.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.760 I 
0.00.521.785 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.521.799 I perplexity: tokenizing the input ..
0.00.529.373 I perplexity: tokenization took 7.572 ms
0.00.529.377 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.669.801 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.671.228 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.671.250 I llama_perf_context_print:        load time =     513.24 ms
0.00.671.251 I llama_perf_context_print: prompt eval time =     140.20 ms /   128 tokens (    1.10 ms per token,   912.98 tokens per second)
0.00.671.252 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.671.252 I llama_perf_context_print:       total time =     149.49 ms /   129 tokens
0.00.671.699 I ggml_metal_free: deallocating

real	0m0.684s
user	0m0.075s
sys	0m0.105s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.297 I build: 4156 (9336db46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.271 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.359 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.367 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.372 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.372 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.373 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.374 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.374 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.375 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.377 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.378 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.388 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.388 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.392 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.956 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.007 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.352 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.354 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.354 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.355 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.355 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.356 I llama_model_loader: - type  f32:  194 tensors
0.00.051.357 I llama_model_loader: - type  f16:   98 tensors
0.00.081.239 I llm_load_vocab: special tokens cache size = 25
0.00.087.734 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.737 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.737 I llm_load_print_meta: arch             = gptneox
0.00.087.738 I llm_load_print_meta: vocab type       = BPE
0.00.087.738 I llm_load_print_meta: n_vocab          = 50304
0.00.087.738 I llm_load_print_meta: n_merges         = 50009
0.00.087.738 I llm_load_print_meta: vocab_only       = 0
0.00.087.738 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.738 I llm_load_print_meta: n_embd           = 2048
0.00.087.738 I llm_load_print_meta: n_layer          = 24
0.00.087.741 I llm_load_print_meta: n_head           = 16
0.00.087.742 I llm_load_print_meta: n_head_kv        = 16
0.00.087.742 I llm_load_print_meta: n_rot            = 32
0.00.087.744 I llm_load_print_meta: n_swa            = 0
0.00.087.744 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.744 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.745 I llm_load_print_meta: n_gqa            = 1
0.00.087.746 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.746 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.747 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.747 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.747 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.747 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.747 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.748 I llm_load_print_meta: n_ff             = 8192
0.00.087.748 I llm_load_print_meta: n_expert         = 0
0.00.087.748 I llm_load_print_meta: n_expert_used    = 0
0.00.087.748 I llm_load_print_meta: causal attn      = 1
0.00.087.749 I llm_load_print_meta: pooling type     = 0
0.00.087.749 I llm_load_print_meta: rope type        = 2
0.00.087.749 I llm_load_print_meta: rope scaling     = linear
0.00.087.750 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.751 I llm_load_print_meta: freq_scale_train = 1
0.00.087.751 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.751 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.751 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.751 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.751 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.751 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.752 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.763 I llm_load_print_meta: model type       = 1.4B
0.00.087.763 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.764 I llm_load_print_meta: model params     = 1.41 B
0.00.087.764 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.764 I llm_load_print_meta: general.name     = 1.4B
0.00.087.765 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.765 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.765 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.765 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.765 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.766 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.766 I llm_load_print_meta: max token length = 1024
0.00.089.526 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.526 I llm_load_tensors: offloading output layer to GPU
0.00.089.526 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.535 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.537 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.510 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.511 I llama_new_context_with_model: n_ctx         = 128
0.00.090.511 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.511 I llama_new_context_with_model: n_batch       = 128
0.00.090.511 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.512 I llama_new_context_with_model: flash_attn    = 0
0.00.090.512 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.512 I llama_new_context_with_model: freq_scale    = 1
0.00.090.513 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.513 I ggml_metal_init: allocating
0.00.090.519 I ggml_metal_init: found device: Apple M4
0.00.090.522 I ggml_metal_init: picking default device: Apple M4
0.00.091.127 I ggml_metal_init: using embedded metal library
0.00.093.238 I ggml_metal_init: GPU name:   Apple M4
0.00.093.240 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.241 I ggml_metal_init: simdgroup reduction   = true
0.00.093.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.242 I ggml_metal_init: has bfloat            = true
0.00.093.242 I ggml_metal_init: use bfloat            = true
0.00.093.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.395 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.397 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.412 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.256 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.257 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.258 I llama_new_context_with_model: graph nodes  = 967
0.00.103.258 I llama_new_context_with_model: graph splits = 2
0.00.103.269 I 
0.00.103.291 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.103.292 I compute_imatrix: tokenizing the input ..
0.00.109.972 I compute_imatrix: tokenization took 6.68 ms
0.00.109.974 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.661.424 I compute_imatrix: 1.55 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.664.109 I llama_perf_context_print:        load time =    1640.15 ms
0.01.664.109 I llama_perf_context_print: prompt eval time =    1550.95 ms /   128 tokens (   12.12 ms per token,    82.53 tokens per second)
0.01.664.110 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.664.111 I llama_perf_context_print:       total time =    1642.82 ms /   129 tokens
0.01.664.619 I ggml_metal_free: deallocating

real	0m1.850s
user	0m0.166s
sys	0m0.255s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4156 (9336db46)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142f0a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142f0a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142f0ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142f0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142f0b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142f0be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142f0c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142f0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142f0cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142f0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142f0d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142f0de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142f0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142f0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142f0f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142f10070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142f10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142f10eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142f115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142f11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142f124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142f12be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142f13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142f13ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142f142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142f14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142f14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142f15800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142f15d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142f16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142f164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142f16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142f16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142f17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142f177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142f17c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142f18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142f185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142f18a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142f18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142f193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142f19850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142f19cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142f1a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142f1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142f1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142f1b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142f1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142f1bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142f1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142f1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142f1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142f1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142f1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142f1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142f1ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142f1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142f1f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142f1f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142f1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142f202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142f20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142f20be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142f21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142f21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142f219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142f21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142f22300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142f227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142f22c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142f230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142f23580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142f23a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142f23ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142f24360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142f24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142f24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142f25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142f255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142f25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142f25f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142f263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142f26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142f26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142f271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142f27640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142f27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142f27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142f28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142f288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142f28d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142f29200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142f296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142f29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142f29fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142f2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142f2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142f1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142f2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142f2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142f2bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142f2c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142f2c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142f2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142f2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142f2d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142f2d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142f2ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142f2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142f2e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142f2eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142f2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142f2f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142f2f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142f2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142f302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142f30750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142f30bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142f31090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142f31530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142f319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142f31e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142f32310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142f327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142f32c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142f330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142f33590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142f33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142f33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142f34370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142f34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142f34cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142f35150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142f355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142f35a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142f35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142f363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142f36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142f36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142f371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142f37650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142f37af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142f37f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142f38430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142f388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142f38d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142f396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142f39b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142f39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142f3a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142f3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142f3ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142f3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142f3b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142f3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142f3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142f3cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142f3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142f3d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142f3df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142f3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142f3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142f3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142f3f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142f3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142f40250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142f407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142f40cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142f41240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142f41790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142f41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142f42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142f42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142f42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142f43220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142f43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142f43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142f44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142f44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142f44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142f45200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142f45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142f45ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142f461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142f46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142f46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142f471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142f47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142f47c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142f481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142f48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142f48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142f491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142f49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142f49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142f4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142f4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142f4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142f4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142f4b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142f4bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142f4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142f4c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142f4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142f4d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142f4d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142f4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142f4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142f4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142f4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142f4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142f4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142f4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142f50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142f506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142f50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142f51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142f51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142f51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142f52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142f52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142f52b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142f52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142f53460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142f53900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142f53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142f54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142f546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142f54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142f55020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142f554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142f55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142f55e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142f562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142f567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142f56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142f57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142f57d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142f58470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142f58730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142f59350 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.139.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136304ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136304f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1363053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136305830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136305ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136306110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136306580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1363069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136306e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136307360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1363077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136307e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136308970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136309120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136309930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13630a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13630a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13630ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13630b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13630bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13630c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13630cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13630d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13630da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13630e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13630e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13630e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13630eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13630ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13630f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13630f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13630fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136310200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1363104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136310930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136310da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136311210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136311680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136311af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136311f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1363123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136312840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136312cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136313120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136313590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136313a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136313e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1363142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136314750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136314bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136315030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1363154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136315910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136315d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1363161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136316660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136316bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1363170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136317540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1363179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136317e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136318290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136318700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136318b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136318fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136319450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1363198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136319d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13631a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13631a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13631aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13631aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13631b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13631b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13631bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13631c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13631c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13631c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13631ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13631d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13631d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13631db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13631dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13631e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13631e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13631ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13631f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13631f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13631fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13631fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136320340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1363207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136320c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136321090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136321500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136321970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136321de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136322250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1363226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136322b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136322fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136323410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136323880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136323cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136324160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1363245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136324a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136324eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136325320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136325790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136325c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136326070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1363264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136326950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136326dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136327230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1363276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136327b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136327f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1363283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136328860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136328cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136329140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1363295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136329a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136329e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13632a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13632a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13632abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13632b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13632b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13632b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13632bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13632c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13632c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13632caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13632cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13632d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13632d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13632dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13632e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13632e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13632ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13632ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13632f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13632f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13632fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136330030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1363304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136330910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136330d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1363311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136331660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136331ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136331f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1363323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136332820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136332c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136333100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136333570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1363339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136333e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1363342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136334730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136334ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136335010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136335480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136336010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1363362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136336590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136336a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136336e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1363372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136337750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136337bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136338030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1363384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136338910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136338d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1363391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136339660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136339ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136339f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13633a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13633a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13633ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13633b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13633b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13633b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13633be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13633c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13633c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13633cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13633d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13633d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13633d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13633dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13633e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13633e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13633eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13633ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13633f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13633f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13633fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1363400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136340550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1363409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136340e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1363412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136341710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136341b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136341ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136342460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1363428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136342d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1363431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136343620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136343a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136343f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136344370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1363447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136344c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1363450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136345530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1363459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136345e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136346280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1363466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136346b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136346fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136347440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1363478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136347d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136348190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136348600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136348a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136348ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136349350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136349e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13634a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13634acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13634b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13634b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13634b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13634bde0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136304ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136304f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1363053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136305830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136305ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136306110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136306580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1363069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136306e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1363072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136307740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136307d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136308610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136308d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136309570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136309c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13630a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13630aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13630b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13630bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13630c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13630c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13630cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13630d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13630dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13630e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13630e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13630eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13630ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13630f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13630f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13630fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1363100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1363103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136310810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136310c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1363110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136311560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1363119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136311e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1363122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136312720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136312b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136313000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136313470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1363138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136313d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1363141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136314630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136314aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136314f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136315380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1363157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136315c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1363160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136316540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1363169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136316e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136317290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136317700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136317b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136317fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136318450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1363188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136318d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1363191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136319610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136319a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136319ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13631a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13631a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13631ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13631b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13631b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13631b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13631be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13631c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13631c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13631cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13631cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13631d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13631d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13631dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13631e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13631e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13631ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13631eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13631f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13631f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13631fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136320090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136320500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136320970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136320de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136321250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1363216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136321b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136321fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136322410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136322880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136322cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136323160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1363235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136323a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136323eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136324320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136324790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136324c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136325070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1363254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136325950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136325dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136326230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1363266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136326b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136326f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1363273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136327860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136327cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136328140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1363285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136328a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136328e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136329300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136329770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136329be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13632a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13632a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13632a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13632ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13632b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13632b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13632baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13632bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13632c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13632c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13632ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13632d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13632d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13632da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13632de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13632e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13632e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13632ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13632f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13632f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13632f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13632fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1363301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136330660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136330ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136330f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1363313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136331820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136331c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136332100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136332570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1363329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136332e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1363332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136333730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136333ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136334010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136334480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1363348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136334d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1363351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136335950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136335dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136336230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1363366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136336b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136336f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1363373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136337860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136337cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136338140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1363385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136338a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136338e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136339300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136339770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136339be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13633a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13633a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13633a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13633ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13633b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13633b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13633baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13633bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13633c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13633c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13633ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13633d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13633d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13633da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13633de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13633e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13633e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13633ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13633f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13633f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13633f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13633fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1363401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136340660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136340ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136340f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1363413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136341820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136341c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136342100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136342570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1363429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136342e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1363432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136343730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136343ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136344010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136344480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1363448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136344d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1363451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136345640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136345ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136345f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136346390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136346800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136346c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1363470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136347550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1363479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136347e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1363482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136348710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136348b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136348ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1363496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136349dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13634a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13634abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13634b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13634b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13634b900 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.773s
user	0m0.291s
sys	0m0.269s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4156 (9336db46)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13360ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13360f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13360fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133610180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133610730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133610ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133611290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133611840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133611df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1336122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1336127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133612cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133613810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1336147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133614ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133615610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133615d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133616450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133617340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133617a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133618180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133618a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133619140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133619400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133619a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13361a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13361abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13361ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13361b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13361b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13361be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13361c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13361c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13361cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13361cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13361d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13361d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13361dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13361e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13361e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13361eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13361f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13361f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13361f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13361fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133620810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133620e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133621430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133621a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133622050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133622660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133622c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133623460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133623900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133623da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133624060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133624670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133624e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133625120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1336255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133625a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133625f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1336263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133626840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133626ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133627620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133627ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133627f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1336288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133628d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1336291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133629680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133629b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133629fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13362a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13362a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13362ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13362b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13362b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13362bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13362c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13362c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13362c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13362ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13362d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13362d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13362dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13362e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13362e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13362e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13362ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13362f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13362f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133620500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13362fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133630290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133630730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133630bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133631070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133631510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1336319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133631e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1336322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133632790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133632c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1336330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133633570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133633a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133633eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133634350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1336347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133634c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133635130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1336355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133635a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133635f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1336363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133636850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133636cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133637190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133637630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133637ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133637f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133638410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1336388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133638d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1336391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133639690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133639b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133639fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13363a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13363a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13363adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13363b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13363b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13363bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13363c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13363c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13363c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13363ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13363d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13363d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13363dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13363e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13363e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13363e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13363ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13363f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13363f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13363fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133640250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1336407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133640cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133640fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1336415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133641bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1336421e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1336427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133642e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1336435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133643a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133643f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1336443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133644b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1336450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133645620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133645b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1336460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133646610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133646b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1336470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133647600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133647b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1336480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1336485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133648b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133649090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1336495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133649b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13364a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13364a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13364ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13364b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13364b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13364bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13364c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13364c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13364cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13364d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13364d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13364daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13364e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13364e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13364eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13364f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13364f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13364fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133650020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133650570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133650ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133651010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133651560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133651ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133652000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133652550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133652aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133652ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133653540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133653a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133653fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133654530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133654a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133654fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133655520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133655a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133655fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133656510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133656a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133656fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133657500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1336579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133657e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1336582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133658780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133658c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1336590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133659560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133659a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133659ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13365a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13365a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13365ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13365b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13365b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13365bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13365c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13365cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13365d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13365d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13365dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13365e1d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133705760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133705bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133706040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1337064b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133706920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133706d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133707200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133707670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133707ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133707f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1337083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133708ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1337095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133709d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13370a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13370acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13370b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13370baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13370c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13370c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13370d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13370d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13370dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13370e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13370ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13370efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13370f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13370f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13370fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13370ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133710420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133710950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133710dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133711080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1337114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133711960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133711dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133712240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1337126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133712b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133712f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133713400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133713870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133713ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133714150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1337145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133714a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133714ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133715310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133715780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133715bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133716060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1337164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133716940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133716db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133717220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133717790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133717c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133718100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133718570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1337189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133718e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1337192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133719730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133719ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13371a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13371a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13371a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13371ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13371b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13371b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13371bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13371bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13371c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13371c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13371cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13371d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13371d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13371d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13371de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13371e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13371e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13371eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13371eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13371f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13371f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13371fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1337201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133720620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133720a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133720f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133721370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1337217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133721c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1337220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133722530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1337229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133722e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133723280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1337236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133723fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133724440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1337248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133724d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133725190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133725600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133725a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133725ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133726350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1337267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133726c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1337270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133727510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133727980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133727df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133728260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1337286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133728b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133728fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133729420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133729890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133729d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13372a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13372a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13372aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13372aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13372b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13372b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13372bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13372c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13372c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13372c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13372cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13372d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13372d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13372db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13372df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13372e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13372e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13372ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13372f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13372f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13372fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13372fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133730310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133730780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133730bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133731060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1337314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133731940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133731db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133732220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133732690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133732b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133732f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1337333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133733850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133733cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133734130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1337345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133734a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133734e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1337352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133735760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133735bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133736040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133736bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133736e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133737150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1337375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133737a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133737ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133738310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133738780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133738bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133739060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1337394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133739940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133739db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13373a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13373a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13373ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13373af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13373b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13373b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13373bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13373c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13373c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13373ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13373ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13373d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13373d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13373dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13373e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13373e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13373e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13373ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13373f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13373f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13373fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13373ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1337403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133740830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133740ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133741110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133741580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1337419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133741e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1337422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133742740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133742bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133743020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133743490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133743900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133743d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1337441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133744650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133744ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133744f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1337453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133745810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133745c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1337460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133746560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1337469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133746e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1337472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133747720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133747b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133748000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133748470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1337488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133748d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1337491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133749630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133749aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133749f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13374aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13374b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13374b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13374bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13374c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13374c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13374c9a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1352044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135204950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135204dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135205230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1352056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135205b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135205f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1352063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135206860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135206cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135207140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135207870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135208390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135208b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135209350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135209a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13520a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13520a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13520afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13520b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13520be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13520c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13520cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13520d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13520daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13520dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13520e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13520e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13520e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13520ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13520f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13520f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13520fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13520fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1352102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135210720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135210b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135211000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135211470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1352118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135211d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1352121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135212630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135212aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135212f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135213380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1352137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135213c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1352140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135214540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1352149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135214e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135215290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135215700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135215b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135215fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135216550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135216a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135216ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135217330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1352177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135217c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135218080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1352184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135218960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135218dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135219240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1352196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135219b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x135219f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13521a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13521a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13521ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13521b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13521b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13521ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13521bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13521c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13521c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13521cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13521d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13521d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13521d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13521ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13521e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13521e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13521eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13521ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13521f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13521f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13521fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x135220130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1352205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x135220a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x135220e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1352212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x135221760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x135221bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x135222040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1352224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x135222920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x135222d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x135223200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x135223670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x135223ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x135223f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1352243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x135224830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x135224ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135225110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135225580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1352259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135225e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1352262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135226740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135226bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135227020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135227490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135227900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x135227d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1352281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135228650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135228ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135228f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1352293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135229810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135229c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13522a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13522a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13522a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13522ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13522b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13522b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13522bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13522c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13522c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13522c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13522cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13522d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13522d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13522daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13522df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13522e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13522e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13522ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13522f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13522f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13522f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13522fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x135230290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x135230700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x135230b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x135230fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x135231450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1352318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x135231d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1352321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x135232610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x135232a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x135232ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x135233360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1352337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x135233c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1352340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x135234520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135234990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x135234e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135235990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135235c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135235f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135236380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1352367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135236c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1352370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135237540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1352379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135237e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135238290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135238700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135238b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135238fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135239450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1352398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135239d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13523a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13523a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13523aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13523aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13523b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13523b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13523bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13523c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13523c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13523c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13523ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13523d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13523d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13523db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13523dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13523e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13523e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13523ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13523f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13523f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13523fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13523fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x135240340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1352407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x135240c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x135241090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x135241500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x135241970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x135241de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x135242250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1352426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x135242b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x135242fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x135243410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135243880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x135243cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135244160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1352445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135244a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135244eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135245320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135245790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135245c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135246070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1352464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135246950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135246dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135247230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1352476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135247b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135247f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1352483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135248860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135248cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135249810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135249f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13524a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13524ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13524b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13524b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13524b760 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.927s
user	0m0.239s
sys	0m0.125s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
