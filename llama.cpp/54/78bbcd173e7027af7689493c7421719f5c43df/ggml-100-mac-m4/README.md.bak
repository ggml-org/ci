### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.43 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  177.24 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.91 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   25.78 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.33 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 219.51 sec*proc (27 tests)

Total Test time (real) = 219.52 sec

real	3m39.577s
user	7m32.202s
sys	0m6.111s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.23 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.39 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.03 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.21 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.81 sec*proc (27 tests)

Total Test time (real) =  50.83 sec

real	0m50.838s
user	1m10.990s
sys	0m5.647s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.115 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.971 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.134 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.141 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.144 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.145 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.146 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.146 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.147 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.149 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.149 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.150 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.150 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.151 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.155 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.155 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.156 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.156 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.157 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.158 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.158 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.500 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.502 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.503 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.503 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.504 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.029.504 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.505 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.029.505 I llama_model_loader: - type  f32:  124 tensors
0.00.029.506 I llama_model_loader: - type  f16:   73 tensors
0.00.033.933 I llm_load_vocab: special tokens cache size = 5
0.00.036.310 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.036.315 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.036.315 I llm_load_print_meta: arch             = bert
0.00.036.316 I llm_load_print_meta: vocab type       = WPM
0.00.036.316 I llm_load_print_meta: n_vocab          = 30522
0.00.036.316 I llm_load_print_meta: n_merges         = 0
0.00.036.316 I llm_load_print_meta: vocab_only       = 0
0.00.036.317 I llm_load_print_meta: n_ctx_train      = 512
0.00.036.317 I llm_load_print_meta: n_embd           = 384
0.00.036.317 I llm_load_print_meta: n_layer          = 12
0.00.036.321 I llm_load_print_meta: n_head           = 12
0.00.036.322 I llm_load_print_meta: n_head_kv        = 12
0.00.036.323 I llm_load_print_meta: n_rot            = 32
0.00.036.323 I llm_load_print_meta: n_swa            = 0
0.00.036.323 I llm_load_print_meta: n_embd_head_k    = 32
0.00.036.324 I llm_load_print_meta: n_embd_head_v    = 32
0.00.036.324 I llm_load_print_meta: n_gqa            = 1
0.00.036.325 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.036.326 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.036.327 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.036.328 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.036.328 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.036.328 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.036.329 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.036.330 I llm_load_print_meta: n_ff             = 1536
0.00.036.330 I llm_load_print_meta: n_expert         = 0
0.00.036.330 I llm_load_print_meta: n_expert_used    = 0
0.00.036.330 I llm_load_print_meta: causal attn      = 0
0.00.036.331 I llm_load_print_meta: pooling type     = 2
0.00.036.331 I llm_load_print_meta: rope type        = 2
0.00.036.331 I llm_load_print_meta: rope scaling     = linear
0.00.036.332 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.036.333 I llm_load_print_meta: freq_scale_train = 1
0.00.036.336 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.036.337 I llm_load_print_meta: rope_finetuned   = unknown
0.00.036.337 I llm_load_print_meta: ssm_d_conv       = 0
0.00.036.337 I llm_load_print_meta: ssm_d_inner      = 0
0.00.036.337 I llm_load_print_meta: ssm_d_state      = 0
0.00.036.337 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.036.338 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.036.338 I llm_load_print_meta: model type       = 33M
0.00.036.339 I llm_load_print_meta: model ftype      = F16
0.00.036.339 I llm_load_print_meta: model params     = 33.21 M
0.00.036.340 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.036.341 I llm_load_print_meta: general.name     = Bge Small
0.00.036.341 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.036.342 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.036.343 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.036.349 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.036.349 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.036.349 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.036.354 I llm_load_print_meta: max token length = 21
0.00.038.441 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.038.441 I llm_load_tensors: offloading output layer to GPU
0.00.038.447 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.038.473 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.038.475 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.039.050 I llama_new_context_with_model: n_seq_max     = 1
0.00.039.052 I llama_new_context_with_model: n_ctx         = 512
0.00.039.052 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.039.052 I llama_new_context_with_model: n_batch       = 2048
0.00.039.053 I llama_new_context_with_model: n_ubatch      = 2048
0.00.039.053 I llama_new_context_with_model: flash_attn    = 0
0.00.039.053 I llama_new_context_with_model: freq_base     = 10000.0
0.00.039.054 I llama_new_context_with_model: freq_scale    = 1
0.00.039.055 I ggml_metal_init: allocating
0.00.039.059 I ggml_metal_init: found device: Apple M4
0.00.039.062 I ggml_metal_init: picking default device: Apple M4
0.00.039.896 I ggml_metal_init: using embedded metal library
0.00.044.047 I ggml_metal_init: GPU name:   Apple M4
0.00.044.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.044.050 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.044.051 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.044.051 I ggml_metal_init: simdgroup reduction   = true
0.00.044.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.044.052 I ggml_metal_init: has bfloat            = true
0.00.044.052 I ggml_metal_init: use bfloat            = true
0.00.044.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.044.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.035 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.057.037 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.057.038 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.057.820 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.057.822 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.057.822 I llama_new_context_with_model: graph nodes  = 429
0.00.057.822 I llama_new_context_with_model: graph splits = 2
0.00.057.843 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.064.329 I 
0.00.064.354 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.065.005 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.069.778 I llama_perf_context_print:        load time =      45.35 ms
0.00.069.779 I llama_perf_context_print: prompt eval time =       4.61 ms /     9 tokens (    0.51 ms per token,  1952.70 tokens per second)
0.00.069.780 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.069.780 I llama_perf_context_print:       total time =       5.45 ms /    10 tokens
0.00.069.906 I ggml_metal_free: deallocating

real	0m0.249s
user	0m0.049s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.032 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.984 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.041 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.046 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.047 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.048 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.049 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.050 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.050 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.051 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.051 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.051 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.054 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.054 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.055 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.056 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.056 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.056 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.056 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.516 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.270 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.271 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.271 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.271 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.272 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.272 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.272 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.273 I llama_model_loader: - type  f32:  124 tensors
0.00.014.273 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.838 I llm_load_vocab: special tokens cache size = 5
0.00.018.147 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.150 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.150 I llm_load_print_meta: arch             = bert
0.00.018.150 I llm_load_print_meta: vocab type       = WPM
0.00.018.151 I llm_load_print_meta: n_vocab          = 30522
0.00.018.151 I llm_load_print_meta: n_merges         = 0
0.00.018.151 I llm_load_print_meta: vocab_only       = 0
0.00.018.151 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.151 I llm_load_print_meta: n_embd           = 384
0.00.018.151 I llm_load_print_meta: n_layer          = 12
0.00.018.154 I llm_load_print_meta: n_head           = 12
0.00.018.154 I llm_load_print_meta: n_head_kv        = 12
0.00.018.155 I llm_load_print_meta: n_rot            = 32
0.00.018.155 I llm_load_print_meta: n_swa            = 0
0.00.018.156 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.156 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.157 I llm_load_print_meta: n_gqa            = 1
0.00.018.159 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.160 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.160 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.161 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.161 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.161 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.161 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.162 I llm_load_print_meta: n_ff             = 1536
0.00.018.162 I llm_load_print_meta: n_expert         = 0
0.00.018.162 I llm_load_print_meta: n_expert_used    = 0
0.00.018.162 I llm_load_print_meta: causal attn      = 0
0.00.018.162 I llm_load_print_meta: pooling type     = 2
0.00.018.163 I llm_load_print_meta: rope type        = 2
0.00.018.163 I llm_load_print_meta: rope scaling     = linear
0.00.018.164 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.164 I llm_load_print_meta: freq_scale_train = 1
0.00.018.164 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.165 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.165 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.165 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.165 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.165 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.165 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.165 I llm_load_print_meta: model type       = 33M
0.00.018.166 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.166 I llm_load_print_meta: model params     = 33.21 M
0.00.018.167 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.167 I llm_load_print_meta: general.name     = Bge Small
0.00.018.167 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.167 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.167 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.167 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.168 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.168 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.168 I llm_load_print_meta: max token length = 21
0.00.019.377 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.377 I llm_load_tensors: offloading output layer to GPU
0.00.019.380 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.387 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.388 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.744 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.744 I llama_new_context_with_model: n_ctx         = 512
0.00.019.745 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.745 I llama_new_context_with_model: n_batch       = 2048
0.00.019.745 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.745 I llama_new_context_with_model: flash_attn    = 0
0.00.019.746 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.746 I llama_new_context_with_model: freq_scale    = 1
0.00.019.746 I ggml_metal_init: allocating
0.00.019.752 I ggml_metal_init: found device: Apple M4
0.00.019.754 I ggml_metal_init: picking default device: Apple M4
0.00.020.370 I ggml_metal_init: using embedded metal library
0.00.022.893 I ggml_metal_init: GPU name:   Apple M4
0.00.022.895 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.895 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.896 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.896 I ggml_metal_init: simdgroup reduction   = true
0.00.022.896 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.896 I ggml_metal_init: has bfloat            = true
0.00.022.897 I ggml_metal_init: use bfloat            = true
0.00.022.897 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.898 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.607 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.609 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.610 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.198 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.199 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.199 I llama_new_context_with_model: graph nodes  = 429
0.00.034.200 I llama_new_context_with_model: graph splits = 2
0.00.034.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.620 I 
0.00.038.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.159 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.552 I llama_perf_context_print:        load time =      29.63 ms
0.00.043.553 I llama_perf_context_print: prompt eval time =       4.27 ms /     9 tokens (    0.47 ms per token,  2109.70 tokens per second)
0.00.043.554 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.554 I llama_perf_context_print:       total time =       4.93 ms /    10 tokens
0.00.043.748 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.140 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.247 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.252 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.255 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.258 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.259 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.259 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.260 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.261 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.262 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.262 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.263 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.264 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.267 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.267 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.268 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.269 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.562 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.563 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.563 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.563 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.051.564 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.564 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.564 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.565 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.565 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.051.566 I llama_model_loader: - type  f32:   41 tensors
0.00.051.566 I llama_model_loader: - type  f16:   29 tensors
0.00.069.922 W llm_load_vocab: empty token at index 5
0.00.074.496 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.075.817 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.862 I llm_load_vocab: special tokens cache size = 5
0.00.332.647 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.332.652 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.332.653 I llm_load_print_meta: arch             = jina-bert-v2
0.00.332.654 I llm_load_print_meta: vocab type       = BPE
0.00.332.654 I llm_load_print_meta: n_vocab          = 61056
0.00.332.654 I llm_load_print_meta: n_merges         = 39382
0.00.332.657 I llm_load_print_meta: vocab_only       = 0
0.00.332.657 I llm_load_print_meta: n_ctx_train      = 8192
0.00.332.657 I llm_load_print_meta: n_embd           = 384
0.00.332.657 I llm_load_print_meta: n_layer          = 4
0.00.332.666 I llm_load_print_meta: n_head           = 12
0.00.332.666 I llm_load_print_meta: n_head_kv        = 12
0.00.332.666 I llm_load_print_meta: n_rot            = 32
0.00.332.667 I llm_load_print_meta: n_swa            = 0
0.00.332.668 I llm_load_print_meta: n_embd_head_k    = 32
0.00.332.668 I llm_load_print_meta: n_embd_head_v    = 32
0.00.332.668 I llm_load_print_meta: n_gqa            = 1
0.00.332.669 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.332.672 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.332.673 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.332.674 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.332.674 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.332.674 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.332.674 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.332.675 I llm_load_print_meta: n_ff             = 1536
0.00.332.675 I llm_load_print_meta: n_expert         = 0
0.00.332.675 I llm_load_print_meta: n_expert_used    = 0
0.00.332.675 I llm_load_print_meta: causal attn      = 0
0.00.332.676 I llm_load_print_meta: pooling type     = -1
0.00.332.676 I llm_load_print_meta: rope type        = -1
0.00.332.676 I llm_load_print_meta: rope scaling     = linear
0.00.332.677 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.332.682 I llm_load_print_meta: freq_scale_train = 1
0.00.332.683 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.332.688 I llm_load_print_meta: rope_finetuned   = unknown
0.00.332.691 I llm_load_print_meta: ssm_d_conv       = 0
0.00.332.691 I llm_load_print_meta: ssm_d_inner      = 0
0.00.332.691 I llm_load_print_meta: ssm_d_state      = 0
0.00.332.691 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.332.691 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.332.693 I llm_load_print_meta: model type       = 33M
0.00.332.694 I llm_load_print_meta: model ftype      = F16
0.00.332.694 I llm_load_print_meta: model params     = 32.90 M
0.00.332.695 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.332.695 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.332.695 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.332.695 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.332.696 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.332.700 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.332.700 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.332.700 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.332.701 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.332.701 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.332.701 I llm_load_print_meta: max token length = 45
0.00.333.994 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.333.994 I llm_load_tensors: offloading output layer to GPU
0.00.333.995 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.334.022 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.334.023 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.334.982 I llama_new_context_with_model: n_seq_max     = 1
0.00.334.983 I llama_new_context_with_model: n_ctx         = 8192
0.00.334.983 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.334.983 I llama_new_context_with_model: n_batch       = 2048
0.00.334.984 I llama_new_context_with_model: n_ubatch      = 2048
0.00.334.984 I llama_new_context_with_model: flash_attn    = 0
0.00.334.984 I llama_new_context_with_model: freq_base     = 10000.0
0.00.334.984 I llama_new_context_with_model: freq_scale    = 1
0.00.334.985 I ggml_metal_init: allocating
0.00.334.989 I ggml_metal_init: found device: Apple M4
0.00.334.991 I ggml_metal_init: picking default device: Apple M4
0.00.335.770 I ggml_metal_init: using embedded metal library
0.00.338.646 I ggml_metal_init: GPU name:   Apple M4
0.00.338.648 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.338.648 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.338.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.338.649 I ggml_metal_init: simdgroup reduction   = true
0.00.338.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.338.649 I ggml_metal_init: has bfloat            = true
0.00.338.649 I ggml_metal_init: use bfloat            = true
0.00.338.650 I ggml_metal_init: hasUnifiedMemory      = true
0.00.338.651 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.350.749 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.350.751 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.350.752 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.351.322 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.351.323 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.351.323 I llama_new_context_with_model: graph nodes  = 154
0.00.351.323 I llama_new_context_with_model: graph splits = 2
0.00.351.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.996 I 
0.00.362.031 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.362.187 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.362.188 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.362.191 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.362.191 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.362.197 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.362.197 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.362.740 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.366.579 I llama_perf_context_print:        load time =     337.35 ms
0.00.366.580 I llama_perf_context_print: prompt eval time =       3.83 ms /    62 tokens (    0.06 ms per token, 16200.68 tokens per second)
0.00.366.581 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.366.582 I llama_perf_context_print:       total time =       4.58 ms /    63 tokens
0.00.366.843 I ggml_metal_free: deallocating

real	0m1.053s
user	0m0.338s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.103 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.221 I main: llama backend init
0.00.000.229 I main: load the model and apply lora adapter, if any
0.00.029.166 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.904 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.918 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.934 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.935 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.937 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.946 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.948 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.949 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.950 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.955 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.956 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.710 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.076 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.076 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.077 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.077 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.078 I llama_model_loader: - type  f32:  194 tensors
0.00.059.079 I llama_model_loader: - type  f16:   98 tensors
0.00.089.294 I llm_load_vocab: special tokens cache size = 25
0.00.096.102 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.096.104 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.096.105 I llm_load_print_meta: arch             = gptneox
0.00.096.105 I llm_load_print_meta: vocab type       = BPE
0.00.096.105 I llm_load_print_meta: n_vocab          = 50304
0.00.096.105 I llm_load_print_meta: n_merges         = 50009
0.00.096.106 I llm_load_print_meta: vocab_only       = 0
0.00.096.106 I llm_load_print_meta: n_ctx_train      = 2048
0.00.096.106 I llm_load_print_meta: n_embd           = 2048
0.00.096.106 I llm_load_print_meta: n_layer          = 24
0.00.096.109 I llm_load_print_meta: n_head           = 16
0.00.096.110 I llm_load_print_meta: n_head_kv        = 16
0.00.096.112 I llm_load_print_meta: n_rot            = 32
0.00.096.112 I llm_load_print_meta: n_swa            = 0
0.00.096.113 I llm_load_print_meta: n_embd_head_k    = 128
0.00.096.113 I llm_load_print_meta: n_embd_head_v    = 128
0.00.096.113 I llm_load_print_meta: n_gqa            = 1
0.00.096.114 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.096.115 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.096.116 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.096.117 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.096.117 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.096.117 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.096.117 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.096.118 I llm_load_print_meta: n_ff             = 8192
0.00.096.118 I llm_load_print_meta: n_expert         = 0
0.00.096.119 I llm_load_print_meta: n_expert_used    = 0
0.00.096.120 I llm_load_print_meta: causal attn      = 1
0.00.096.120 I llm_load_print_meta: pooling type     = 0
0.00.096.120 I llm_load_print_meta: rope type        = 2
0.00.096.120 I llm_load_print_meta: rope scaling     = linear
0.00.096.120 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.096.121 I llm_load_print_meta: freq_scale_train = 1
0.00.096.121 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.096.121 I llm_load_print_meta: rope_finetuned   = unknown
0.00.096.121 I llm_load_print_meta: ssm_d_conv       = 0
0.00.096.121 I llm_load_print_meta: ssm_d_inner      = 0
0.00.096.121 I llm_load_print_meta: ssm_d_state      = 0
0.00.096.122 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.096.122 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.096.122 I llm_load_print_meta: model type       = 1.4B
0.00.096.126 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.096.126 I llm_load_print_meta: model params     = 1.41 B
0.00.096.127 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.096.129 I llm_load_print_meta: general.name     = 1.4B
0.00.096.129 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.096.129 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.096.129 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.096.129 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.096.130 I llm_load_print_meta: LF token         = 128 ''
0.00.096.130 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.096.130 I llm_load_print_meta: max token length = 1024
0.00.098.650 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.098.651 I llm_load_tensors: offloading output layer to GPU
0.00.098.651 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.098.669 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.098.670 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.099.599 I llama_new_context_with_model: n_seq_max     = 1
0.00.099.601 I llama_new_context_with_model: n_ctx         = 2048
0.00.099.601 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.099.601 I llama_new_context_with_model: n_batch       = 2048
0.00.099.601 I llama_new_context_with_model: n_ubatch      = 512
0.00.099.601 I llama_new_context_with_model: flash_attn    = 0
0.00.099.602 I llama_new_context_with_model: freq_base     = 10000.0
0.00.099.602 I llama_new_context_with_model: freq_scale    = 1
0.00.099.602 I ggml_metal_init: allocating
0.00.099.609 I ggml_metal_init: found device: Apple M4
0.00.099.611 I ggml_metal_init: picking default device: Apple M4
0.00.100.298 I ggml_metal_init: using embedded metal library
0.00.109.788 I ggml_metal_init: GPU name:   Apple M4
0.00.109.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.109.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.109.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.109.791 I ggml_metal_init: simdgroup reduction   = true
0.00.109.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.109.791 I ggml_metal_init: has bfloat            = true
0.00.109.791 I ggml_metal_init: use bfloat            = true
0.00.109.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.109.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.151.052 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.151.058 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.151.081 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.152.039 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.152.041 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.152.041 I llama_new_context_with_model: graph nodes  = 967
0.00.152.041 I llama_new_context_with_model: graph splits = 2
0.00.152.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.231.564 I main: llama threadpool init, n_threads = 4
0.00.231.598 I 
0.00.231.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.231.638 I 
0.00.231.718 I sampler seed: 1234
0.00.231.723 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.231.747 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.231.749 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.231.749 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.085.422 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.02.085.423 I llama_perf_context_print:        load time =     202.38 ms
0.02.085.423 I llama_perf_context_print: prompt eval time =      54.53 ms /     7 tokens (    7.79 ms per token,   128.37 tokens per second)
0.02.085.424 I llama_perf_context_print:        eval time =    1796.17 ms /    63 runs   (   28.51 ms per token,    35.07 tokens per second)
0.02.085.424 I llama_perf_context_print:       total time =    1853.86 ms /    70 tokens
0.02.085.611 I ggml_metal_free: deallocating

real	0m2.384s
user	0m0.142s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.585 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.381 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.650 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.659 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.662 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.663 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.664 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.664 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.665 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.666 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.666 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.667 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.673 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.676 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.677 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.310 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.642 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.646 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.646 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.647 I llama_model_loader: - type  f32:  194 tensors
0.00.056.647 I llama_model_loader: - type  f16:   98 tensors
0.00.085.810 I llm_load_vocab: special tokens cache size = 25
0.00.092.498 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.501 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.501 I llm_load_print_meta: arch             = gptneox
0.00.092.502 I llm_load_print_meta: vocab type       = BPE
0.00.092.502 I llm_load_print_meta: n_vocab          = 50304
0.00.092.502 I llm_load_print_meta: n_merges         = 50009
0.00.092.502 I llm_load_print_meta: vocab_only       = 0
0.00.092.502 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.502 I llm_load_print_meta: n_embd           = 2048
0.00.092.503 I llm_load_print_meta: n_layer          = 24
0.00.092.505 I llm_load_print_meta: n_head           = 16
0.00.092.506 I llm_load_print_meta: n_head_kv        = 16
0.00.092.506 I llm_load_print_meta: n_rot            = 32
0.00.092.506 I llm_load_print_meta: n_swa            = 0
0.00.092.507 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.507 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.507 I llm_load_print_meta: n_gqa            = 1
0.00.092.508 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.509 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.509 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.509 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.510 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.510 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.510 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.510 I llm_load_print_meta: n_ff             = 8192
0.00.092.511 I llm_load_print_meta: n_expert         = 0
0.00.092.511 I llm_load_print_meta: n_expert_used    = 0
0.00.092.511 I llm_load_print_meta: causal attn      = 1
0.00.092.511 I llm_load_print_meta: pooling type     = 0
0.00.092.511 I llm_load_print_meta: rope type        = 2
0.00.092.511 I llm_load_print_meta: rope scaling     = linear
0.00.092.514 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.514 I llm_load_print_meta: freq_scale_train = 1
0.00.092.514 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.514 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.514 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.514 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.515 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.515 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.515 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.515 I llm_load_print_meta: model type       = 1.4B
0.00.092.516 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.516 I llm_load_print_meta: model params     = 1.41 B
0.00.092.516 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.516 I llm_load_print_meta: general.name     = 1.4B
0.00.092.517 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.517 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.520 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.521 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.521 I llm_load_print_meta: LF token         = 128 ''
0.00.092.521 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.521 I llm_load_print_meta: max token length = 1024
0.00.094.521 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.522 I llm_load_tensors: offloading output layer to GPU
0.00.094.522 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.527 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.528 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.461 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.462 I llama_new_context_with_model: n_ctx         = 128
0.00.095.462 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.462 I llama_new_context_with_model: n_batch       = 128
0.00.095.462 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.463 I llama_new_context_with_model: flash_attn    = 0
0.00.095.463 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.464 I llama_new_context_with_model: freq_scale    = 1
0.00.095.464 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.464 I ggml_metal_init: allocating
0.00.095.472 I ggml_metal_init: found device: Apple M4
0.00.095.474 I ggml_metal_init: picking default device: Apple M4
0.00.096.108 I ggml_metal_init: using embedded metal library
0.00.098.615 I ggml_metal_init: GPU name:   Apple M4
0.00.098.617 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.618 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.618 I ggml_metal_init: simdgroup reduction   = true
0.00.098.618 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.618 I ggml_metal_init: has bfloat            = true
0.00.098.619 I ggml_metal_init: use bfloat            = true
0.00.098.619 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.620 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.324 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.327 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.342 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.189 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.190 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.190 I llama_new_context_with_model: graph nodes  = 967
0.00.110.190 I llama_new_context_with_model: graph splits = 2
0.00.110.198 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.063.442 I 
0.01.063.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.063.502 I perplexity: tokenizing the input ..
0.01.076.161 I perplexity: tokenization took 12.655 ms
0.01.076.193 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.199.387 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.201.276 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.201.301 I llama_perf_context_print:        load time =    1038.05 ms
0.01.201.303 I llama_perf_context_print: prompt eval time =     122.25 ms /   128 tokens (    0.96 ms per token,  1046.99 tokens per second)
0.01.201.305 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.201.305 I llama_perf_context_print:       total time =     137.86 ms /   129 tokens
0.01.201.832 I ggml_metal_free: deallocating

real	0m1.392s
user	0m0.126s
sys	0m0.209s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.733 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.891 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.895 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.897 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.898 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.898 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.900 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.901 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.902 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.904 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.904 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.820 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.955 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.955 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.956 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.956 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.956 I llama_model_loader: - type  f32:  194 tensors
0.00.034.957 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.276 I llm_load_vocab: special tokens cache size = 25
0.00.064.331 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.336 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.337 I llm_load_print_meta: arch             = gptneox
0.00.064.337 I llm_load_print_meta: vocab type       = BPE
0.00.064.337 I llm_load_print_meta: n_vocab          = 50304
0.00.064.338 I llm_load_print_meta: n_merges         = 50009
0.00.064.338 I llm_load_print_meta: vocab_only       = 0
0.00.064.338 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.338 I llm_load_print_meta: n_embd           = 2048
0.00.064.338 I llm_load_print_meta: n_layer          = 24
0.00.064.342 I llm_load_print_meta: n_head           = 16
0.00.064.345 I llm_load_print_meta: n_head_kv        = 16
0.00.064.345 I llm_load_print_meta: n_rot            = 32
0.00.064.345 I llm_load_print_meta: n_swa            = 0
0.00.064.346 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.350 I llm_load_print_meta: n_gqa            = 1
0.00.064.351 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.352 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.353 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.353 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.354 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.354 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.354 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.355 I llm_load_print_meta: n_ff             = 8192
0.00.064.355 I llm_load_print_meta: n_expert         = 0
0.00.064.355 I llm_load_print_meta: n_expert_used    = 0
0.00.064.355 I llm_load_print_meta: causal attn      = 1
0.00.064.355 I llm_load_print_meta: pooling type     = 0
0.00.064.356 I llm_load_print_meta: rope type        = 2
0.00.064.356 I llm_load_print_meta: rope scaling     = linear
0.00.064.357 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.357 I llm_load_print_meta: freq_scale_train = 1
0.00.064.358 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.358 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.358 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.359 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.359 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.359 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.359 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.359 I llm_load_print_meta: model type       = 1.4B
0.00.064.361 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.361 I llm_load_print_meta: model params     = 1.41 B
0.00.064.362 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.362 I llm_load_print_meta: general.name     = 1.4B
0.00.064.362 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.362 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.362 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.362 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.363 I llm_load_print_meta: LF token         = 128 ''
0.00.064.363 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.363 I llm_load_print_meta: max token length = 1024
0.00.066.795 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.796 I llm_load_tensors: offloading output layer to GPU
0.00.066.796 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.807 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.808 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.764 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.765 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.766 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.766 I llama_new_context_with_model: n_batch       = 2048
0.00.067.766 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.766 I llama_new_context_with_model: flash_attn    = 0
0.00.067.767 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.767 I llama_new_context_with_model: freq_scale    = 1
0.00.067.767 I ggml_metal_init: allocating
0.00.067.772 I ggml_metal_init: found device: Apple M4
0.00.067.774 I ggml_metal_init: picking default device: Apple M4
0.00.068.542 I ggml_metal_init: using embedded metal library
0.00.071.077 I ggml_metal_init: GPU name:   Apple M4
0.00.071.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.079 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.080 I ggml_metal_init: simdgroup reduction   = true
0.00.071.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.080 I ggml_metal_init: has bfloat            = true
0.00.071.080 I ggml_metal_init: use bfloat            = true
0.00.071.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.082 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.972 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.983 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.003 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.206 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.108.207 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.108.208 I llama_new_context_with_model: graph nodes  = 967
0.00.108.208 I llama_new_context_with_model: graph splits = 2
0.00.108.219 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.451.367 I main: llama threadpool init, n_threads = 4
0.01.451.401 I 
0.01.451.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.451.430 I 
0.01.451.657 I sampler seed: 1234
0.01.451.661 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.451.701 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.451.704 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.451.704 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.541.743 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.02.541.744 I llama_perf_context_print:        load time =    1441.63 ms
0.02.541.744 I llama_perf_context_print: prompt eval time =      39.81 ms /     7 tokens (    5.69 ms per token,   175.84 tokens per second)
0.02.541.746 I llama_perf_context_print:        eval time =    1047.29 ms /    63 runs   (   16.62 ms per token,    60.16 tokens per second)
0.02.541.746 I llama_perf_context_print:       total time =    1090.38 ms /    70 tokens
0.02.541.950 I ggml_metal_free: deallocating

real	0m2.560s
user	0m0.114s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.645 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.376 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.381 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.388 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.390 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.390 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.393 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.393 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.394 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.394 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.394 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.396 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.396 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.397 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.859 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.974 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.976 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.976 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.977 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.977 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.977 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.978 I llama_model_loader: - type  f32:  194 tensors
0.00.029.978 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.131 I llm_load_vocab: special tokens cache size = 25
0.00.060.286 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.288 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.289 I llm_load_print_meta: arch             = gptneox
0.00.060.289 I llm_load_print_meta: vocab type       = BPE
0.00.060.289 I llm_load_print_meta: n_vocab          = 50304
0.00.060.289 I llm_load_print_meta: n_merges         = 50009
0.00.060.290 I llm_load_print_meta: vocab_only       = 0
0.00.060.290 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.290 I llm_load_print_meta: n_embd           = 2048
0.00.060.290 I llm_load_print_meta: n_layer          = 24
0.00.060.293 I llm_load_print_meta: n_head           = 16
0.00.060.294 I llm_load_print_meta: n_head_kv        = 16
0.00.060.294 I llm_load_print_meta: n_rot            = 32
0.00.060.295 I llm_load_print_meta: n_swa            = 0
0.00.060.295 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.295 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.297 I llm_load_print_meta: n_gqa            = 1
0.00.060.298 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.298 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.299 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.300 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.300 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.300 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.301 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.301 I llm_load_print_meta: n_ff             = 8192
0.00.060.301 I llm_load_print_meta: n_expert         = 0
0.00.060.301 I llm_load_print_meta: n_expert_used    = 0
0.00.060.301 I llm_load_print_meta: causal attn      = 1
0.00.060.301 I llm_load_print_meta: pooling type     = 0
0.00.060.302 I llm_load_print_meta: rope type        = 2
0.00.060.302 I llm_load_print_meta: rope scaling     = linear
0.00.060.302 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.302 I llm_load_print_meta: freq_scale_train = 1
0.00.060.302 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.303 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.303 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.303 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.303 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.303 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.303 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.303 I llm_load_print_meta: model type       = 1.4B
0.00.060.307 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.308 I llm_load_print_meta: model params     = 1.41 B
0.00.060.308 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.308 I llm_load_print_meta: general.name     = 1.4B
0.00.060.308 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.308 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.309 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.309 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.312 I llm_load_print_meta: LF token         = 128 ''
0.00.060.312 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.312 I llm_load_print_meta: max token length = 1024
0.00.062.441 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.441 I llm_load_tensors: offloading output layer to GPU
0.00.062.441 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.452 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.453 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.418 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.418 I llama_new_context_with_model: n_ctx         = 128
0.00.063.419 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.419 I llama_new_context_with_model: n_batch       = 128
0.00.063.419 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.419 I llama_new_context_with_model: flash_attn    = 0
0.00.063.419 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.420 I llama_new_context_with_model: freq_scale    = 1
0.00.063.420 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.420 I ggml_metal_init: allocating
0.00.063.423 I ggml_metal_init: found device: Apple M4
0.00.063.425 I ggml_metal_init: picking default device: Apple M4
0.00.063.988 I ggml_metal_init: using embedded metal library
0.00.066.321 I ggml_metal_init: GPU name:   Apple M4
0.00.066.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.323 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.323 I ggml_metal_init: simdgroup reduction   = true
0.00.066.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.323 I ggml_metal_init: has bfloat            = true
0.00.066.323 I ggml_metal_init: use bfloat            = true
0.00.066.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.340 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.342 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.356 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.077.292 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.077.293 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.077.293 I llama_new_context_with_model: graph nodes  = 967
0.00.077.293 I llama_new_context_with_model: graph splits = 2
0.00.077.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.856.385 I 
0.00.856.448 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.856.466 I perplexity: tokenizing the input ..
0.00.864.442 I perplexity: tokenization took 7.975 ms
0.00.864.451 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.988.372 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.989.514 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.989.529 I llama_perf_context_print:        load time =     845.73 ms
0.00.989.530 I llama_perf_context_print: prompt eval time =     123.70 ms /   128 tokens (    0.97 ms per token,  1034.79 tokens per second)
0.00.989.531 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.989.532 I llama_perf_context_print:       total time =     133.15 ms /   129 tokens
0.00.990.005 I ggml_metal_free: deallocating

real	0m1.006s
user	0m0.088s
sys	0m0.139s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.014.029 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.007 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.012 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.014 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.015 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.015 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.015 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.015 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.017 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.017 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.017 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.018 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.018 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.018 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.019 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.021 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.021 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.652 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.998 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.602 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.031.606 I llama_model_loader: - type  f32:  194 tensors
0.00.031.606 I llama_model_loader: - type q4_0:   97 tensors
0.00.031.606 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.909 I llm_load_vocab: special tokens cache size = 25
0.00.068.969 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.973 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.974 I llm_load_print_meta: arch             = gptneox
0.00.068.974 I llm_load_print_meta: vocab type       = BPE
0.00.068.975 I llm_load_print_meta: n_vocab          = 50304
0.00.068.975 I llm_load_print_meta: n_merges         = 50009
0.00.068.975 I llm_load_print_meta: vocab_only       = 0
0.00.068.975 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.975 I llm_load_print_meta: n_embd           = 2048
0.00.068.979 I llm_load_print_meta: n_layer          = 24
0.00.068.983 I llm_load_print_meta: n_head           = 16
0.00.068.984 I llm_load_print_meta: n_head_kv        = 16
0.00.068.984 I llm_load_print_meta: n_rot            = 32
0.00.068.984 I llm_load_print_meta: n_swa            = 0
0.00.068.985 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.985 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.986 I llm_load_print_meta: n_gqa            = 1
0.00.068.987 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.988 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.989 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.991 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.993 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.994 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.994 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.995 I llm_load_print_meta: n_ff             = 8192
0.00.068.995 I llm_load_print_meta: n_expert         = 0
0.00.068.995 I llm_load_print_meta: n_expert_used    = 0
0.00.068.997 I llm_load_print_meta: causal attn      = 1
0.00.068.997 I llm_load_print_meta: pooling type     = 0
0.00.068.998 I llm_load_print_meta: rope type        = 2
0.00.068.998 I llm_load_print_meta: rope scaling     = linear
0.00.068.998 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.999 I llm_load_print_meta: freq_scale_train = 1
0.00.068.999 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.999 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.999 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.000 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.000 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.000 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.000 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.001 I llm_load_print_meta: model type       = 1.4B
0.00.069.001 I llm_load_print_meta: model ftype      = Q4_0
0.00.069.002 I llm_load_print_meta: model params     = 1.41 B
0.00.069.002 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.069.002 I llm_load_print_meta: general.name     = 1.4B
0.00.069.003 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.003 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.003 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.003 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.004 I llm_load_print_meta: LF token         = 128 ''
0.00.069.010 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.010 I llm_load_print_meta: max token length = 1024
0.00.072.110 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.111 I llm_load_tensors: offloading output layer to GPU
0.00.072.111 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.124 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.072.125 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.073.628 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.630 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.630 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.630 I llama_new_context_with_model: n_batch       = 2048
0.00.073.630 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.631 I llama_new_context_with_model: flash_attn    = 0
0.00.073.631 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.632 I llama_new_context_with_model: freq_scale    = 1
0.00.073.632 I ggml_metal_init: allocating
0.00.073.636 I ggml_metal_init: found device: Apple M4
0.00.073.639 I ggml_metal_init: picking default device: Apple M4
0.00.074.597 I ggml_metal_init: using embedded metal library
0.00.078.451 I ggml_metal_init: GPU name:   Apple M4
0.00.078.453 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.454 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.454 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.454 I ggml_metal_init: simdgroup reduction   = true
0.00.078.454 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.455 I ggml_metal_init: has bfloat            = true
0.00.078.455 I ggml_metal_init: use bfloat            = true
0.00.078.455 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.868 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.876 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.898 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.116.025 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.116.027 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.116.027 I llama_new_context_with_model: graph nodes  = 967
0.00.116.027 I llama_new_context_with_model: graph splits = 2
0.00.116.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.787.918 I main: llama threadpool init, n_threads = 4
0.00.787.959 I 
0.00.787.989 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.787.990 I 
0.00.788.219 I sampler seed: 1234
0.00.788.224 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.244 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.244 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.244 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.471.983 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57258.06 tokens per second)
0.01.471.984 I llama_perf_context_print:        load time =     773.88 ms
0.01.471.985 I llama_perf_context_print: prompt eval time =      42.11 ms /     7 tokens (    6.02 ms per token,   166.23 tokens per second)
0.01.471.986 I llama_perf_context_print:        eval time =     638.60 ms /    63 runs   (   10.14 ms per token,    98.65 tokens per second)
0.01.471.986 I llama_perf_context_print:       total time =     684.07 ms /    70 tokens
0.01.472.201 I ggml_metal_free: deallocating

real	0m1.505s
user	0m0.128s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.933 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.529 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.533 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.535 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.536 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.536 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.536 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.536 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.537 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.538 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.538 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.538 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.539 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.540 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.541 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.542 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.542 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.543 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.164 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.165 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.165 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.165 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.165 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.166 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.166 I llama_model_loader: - type  f32:  194 tensors
0.00.024.166 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.167 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.060 I llm_load_vocab: special tokens cache size = 25
0.00.050.059 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.063 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.063 I llm_load_print_meta: arch             = gptneox
0.00.050.063 I llm_load_print_meta: vocab type       = BPE
0.00.050.064 I llm_load_print_meta: n_vocab          = 50304
0.00.050.064 I llm_load_print_meta: n_merges         = 50009
0.00.050.069 I llm_load_print_meta: vocab_only       = 0
0.00.050.070 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.070 I llm_load_print_meta: n_embd           = 2048
0.00.050.072 I llm_load_print_meta: n_layer          = 24
0.00.050.074 I llm_load_print_meta: n_head           = 16
0.00.050.075 I llm_load_print_meta: n_head_kv        = 16
0.00.050.075 I llm_load_print_meta: n_rot            = 32
0.00.050.076 I llm_load_print_meta: n_swa            = 0
0.00.050.076 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.076 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.077 I llm_load_print_meta: n_gqa            = 1
0.00.050.077 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.078 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.079 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.079 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.079 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.079 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.079 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.080 I llm_load_print_meta: n_ff             = 8192
0.00.050.080 I llm_load_print_meta: n_expert         = 0
0.00.050.080 I llm_load_print_meta: n_expert_used    = 0
0.00.050.080 I llm_load_print_meta: causal attn      = 1
0.00.050.081 I llm_load_print_meta: pooling type     = 0
0.00.050.081 I llm_load_print_meta: rope type        = 2
0.00.050.082 I llm_load_print_meta: rope scaling     = linear
0.00.050.082 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.082 I llm_load_print_meta: freq_scale_train = 1
0.00.050.083 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.085 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.086 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.086 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.086 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.086 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.086 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.086 I llm_load_print_meta: model type       = 1.4B
0.00.050.087 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.087 I llm_load_print_meta: model params     = 1.41 B
0.00.050.088 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.088 I llm_load_print_meta: general.name     = 1.4B
0.00.050.088 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.088 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.088 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.088 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.089 I llm_load_print_meta: LF token         = 128 ''
0.00.050.089 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.089 I llm_load_print_meta: max token length = 1024
0.00.051.883 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.883 I llm_load_tensors: offloading output layer to GPU
0.00.051.883 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.889 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.889 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.815 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.816 I llama_new_context_with_model: n_ctx         = 128
0.00.052.816 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.816 I llama_new_context_with_model: n_batch       = 128
0.00.052.816 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.816 I llama_new_context_with_model: flash_attn    = 0
0.00.052.817 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.817 I llama_new_context_with_model: freq_scale    = 1
0.00.052.817 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.818 I ggml_metal_init: allocating
0.00.052.824 I ggml_metal_init: found device: Apple M4
0.00.052.826 I ggml_metal_init: picking default device: Apple M4
0.00.053.387 I ggml_metal_init: using embedded metal library
0.00.055.693 I ggml_metal_init: GPU name:   Apple M4
0.00.055.694 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.695 I ggml_metal_init: simdgroup reduction   = true
0.00.055.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.696 I ggml_metal_init: has bfloat            = true
0.00.055.696 I ggml_metal_init: use bfloat            = true
0.00.055.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.438 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.441 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.462 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.312 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.313 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.313 I llama_new_context_with_model: graph nodes  = 967
0.00.067.314 I llama_new_context_with_model: graph splits = 2
0.00.067.321 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.413 I 
0.00.618.463 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.475 I perplexity: tokenizing the input ..
0.00.626.306 I perplexity: tokenization took 7.829 ms
0.00.626.320 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.749.086 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.750.214 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.750.230 I llama_perf_context_print:        load time =     608.47 ms
0.00.750.231 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.57 tokens per second)
0.00.750.232 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.232 I llama_perf_context_print:       total time =     131.82 ms /   129 tokens
0.00.750.639 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.077s
sys	0m0.112s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.665 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.568 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.577 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.580 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.580 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.388 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.391 I llama_model_loader: - type  f32:  194 tensors
0.00.024.391 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.391 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.364 I llm_load_vocab: special tokens cache size = 25
0.00.050.294 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.297 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.297 I llm_load_print_meta: arch             = gptneox
0.00.050.298 I llm_load_print_meta: vocab type       = BPE
0.00.050.298 I llm_load_print_meta: n_vocab          = 50304
0.00.050.298 I llm_load_print_meta: n_merges         = 50009
0.00.050.298 I llm_load_print_meta: vocab_only       = 0
0.00.050.299 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.299 I llm_load_print_meta: n_embd           = 2048
0.00.050.299 I llm_load_print_meta: n_layer          = 24
0.00.050.301 I llm_load_print_meta: n_head           = 16
0.00.050.302 I llm_load_print_meta: n_head_kv        = 16
0.00.050.302 I llm_load_print_meta: n_rot            = 32
0.00.050.302 I llm_load_print_meta: n_swa            = 0
0.00.050.304 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.304 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.305 I llm_load_print_meta: n_gqa            = 1
0.00.050.306 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.307 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.307 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.308 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.308 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.308 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.309 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.309 I llm_load_print_meta: n_ff             = 8192
0.00.050.309 I llm_load_print_meta: n_expert         = 0
0.00.050.310 I llm_load_print_meta: n_expert_used    = 0
0.00.050.310 I llm_load_print_meta: causal attn      = 1
0.00.050.310 I llm_load_print_meta: pooling type     = 0
0.00.050.310 I llm_load_print_meta: rope type        = 2
0.00.050.310 I llm_load_print_meta: rope scaling     = linear
0.00.050.311 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.311 I llm_load_print_meta: freq_scale_train = 1
0.00.050.311 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.312 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.312 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.312 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.312 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.312 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.312 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.313 I llm_load_print_meta: model type       = 1.4B
0.00.050.313 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.313 I llm_load_print_meta: model params     = 1.41 B
0.00.050.314 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.314 I llm_load_print_meta: general.name     = 1.4B
0.00.050.315 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.315 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.315 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.315 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.316 I llm_load_print_meta: LF token         = 128 ''
0.00.050.316 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.316 I llm_load_print_meta: max token length = 1024
0.00.052.176 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.176 I llm_load_tensors: offloading output layer to GPU
0.00.052.177 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.187 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.188 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.071 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.072 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.072 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.072 I llama_new_context_with_model: n_batch       = 2048
0.00.053.072 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.073 I llama_new_context_with_model: flash_attn    = 0
0.00.053.073 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.073 I llama_new_context_with_model: freq_scale    = 1
0.00.053.074 I ggml_metal_init: allocating
0.00.053.077 I ggml_metal_init: found device: Apple M4
0.00.053.079 I ggml_metal_init: picking default device: Apple M4
0.00.053.661 I ggml_metal_init: using embedded metal library
0.00.055.968 I ggml_metal_init: GPU name:   Apple M4
0.00.055.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.970 I ggml_metal_init: simdgroup reduction   = true
0.00.055.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.970 I ggml_metal_init: has bfloat            = true
0.00.055.970 I ggml_metal_init: use bfloat            = true
0.00.055.971 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.971 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.089 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.098 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.117 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.072 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.073 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.074 I llama_new_context_with_model: graph nodes  = 967
0.00.086.074 I llama_new_context_with_model: graph splits = 2
0.00.086.088 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.524 I main: llama threadpool init, n_threads = 4
0.00.747.570 I 
0.00.747.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.603 I 
0.00.747.833 I sampler seed: 1234
0.00.747.837 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.869 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.870 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.871 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.473.981 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.473.981 I llama_perf_context_print:        load time =     738.85 ms
0.01.473.982 I llama_perf_context_print: prompt eval time =      42.50 ms /     7 tokens (    6.07 ms per token,   164.69 tokens per second)
0.01.473.983 I llama_perf_context_print:        eval time =     680.65 ms /    63 runs   (   10.80 ms per token,    92.56 tokens per second)
0.01.473.983 I llama_perf_context_print:       total time =     726.46 ms /    70 tokens
0.01.474.174 I ggml_metal_free: deallocating

real	0m1.489s
user	0m0.108s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.521 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.071 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.078 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.079 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.081 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.082 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.082 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.083 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.084 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.084 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.845 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.718 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.719 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.719 I llama_model_loader: - type  f32:  194 tensors
0.00.022.720 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.720 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.502 I llm_load_vocab: special tokens cache size = 25
0.00.048.568 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.570 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.571 I llm_load_print_meta: arch             = gptneox
0.00.048.571 I llm_load_print_meta: vocab type       = BPE
0.00.048.571 I llm_load_print_meta: n_vocab          = 50304
0.00.048.571 I llm_load_print_meta: n_merges         = 50009
0.00.048.572 I llm_load_print_meta: vocab_only       = 0
0.00.048.572 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.572 I llm_load_print_meta: n_embd           = 2048
0.00.048.572 I llm_load_print_meta: n_layer          = 24
0.00.048.575 I llm_load_print_meta: n_head           = 16
0.00.048.576 I llm_load_print_meta: n_head_kv        = 16
0.00.048.576 I llm_load_print_meta: n_rot            = 32
0.00.048.576 I llm_load_print_meta: n_swa            = 0
0.00.048.576 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.576 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.577 I llm_load_print_meta: n_gqa            = 1
0.00.048.578 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.578 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.579 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.579 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.580 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.582 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.583 I llm_load_print_meta: n_ff             = 8192
0.00.048.583 I llm_load_print_meta: n_expert         = 0
0.00.048.583 I llm_load_print_meta: n_expert_used    = 0
0.00.048.583 I llm_load_print_meta: causal attn      = 1
0.00.048.584 I llm_load_print_meta: pooling type     = 0
0.00.048.584 I llm_load_print_meta: rope type        = 2
0.00.048.584 I llm_load_print_meta: rope scaling     = linear
0.00.048.584 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.585 I llm_load_print_meta: freq_scale_train = 1
0.00.048.585 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.585 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.585 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.585 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.585 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.585 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.586 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.586 I llm_load_print_meta: model type       = 1.4B
0.00.048.586 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.586 I llm_load_print_meta: model params     = 1.41 B
0.00.048.587 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.591 I llm_load_print_meta: general.name     = 1.4B
0.00.048.591 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.592 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.593 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.594 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.594 I llm_load_print_meta: LF token         = 128 ''
0.00.048.594 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.594 I llm_load_print_meta: max token length = 1024
0.00.050.492 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.492 I llm_load_tensors: offloading output layer to GPU
0.00.050.493 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.503 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.504 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.390 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.390 I llama_new_context_with_model: n_ctx         = 128
0.00.051.390 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.391 I llama_new_context_with_model: n_batch       = 128
0.00.051.391 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.391 I llama_new_context_with_model: flash_attn    = 0
0.00.051.391 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.392 I llama_new_context_with_model: freq_scale    = 1
0.00.051.392 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.392 I ggml_metal_init: allocating
0.00.051.395 I ggml_metal_init: found device: Apple M4
0.00.051.397 I ggml_metal_init: picking default device: Apple M4
0.00.051.942 I ggml_metal_init: using embedded metal library
0.00.054.227 I ggml_metal_init: GPU name:   Apple M4
0.00.054.229 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.229 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.229 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.230 I ggml_metal_init: simdgroup reduction   = true
0.00.054.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.230 I ggml_metal_init: has bfloat            = true
0.00.054.230 I ggml_metal_init: use bfloat            = true
0.00.054.230 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.231 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.912 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.916 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.929 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.829 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.830 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.830 I llama_new_context_with_model: graph nodes  = 967
0.00.065.830 I llama_new_context_with_model: graph splits = 2
0.00.065.842 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.982 I 
0.00.642.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.035 I perplexity: tokenizing the input ..
0.00.650.122 I perplexity: tokenization took 8.086 ms
0.00.650.133 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.772.727 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.773.859 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.773.878 I llama_perf_context_print:        load time =     633.45 ms
0.00.773.881 I llama_perf_context_print: prompt eval time =     122.36 ms /   128 tokens (    0.96 ms per token,  1046.06 tokens per second)
0.00.773.882 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.773.883 I llama_perf_context_print:       total time =     131.90 ms /   129 tokens
0.00.774.291 I ggml_metal_free: deallocating

real	0m0.787s
user	0m0.077s
sys	0m0.117s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.010.981 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.025.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.797 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.799 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.168 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.526 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.035.527 I llama_model_loader: - type  f32:  194 tensors
0.00.035.528 I llama_model_loader: - type q5_0:   97 tensors
0.00.035.528 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.712 I llm_load_vocab: special tokens cache size = 25
0.00.071.416 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.420 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.420 I llm_load_print_meta: arch             = gptneox
0.00.071.421 I llm_load_print_meta: vocab type       = BPE
0.00.071.421 I llm_load_print_meta: n_vocab          = 50304
0.00.071.421 I llm_load_print_meta: n_merges         = 50009
0.00.071.421 I llm_load_print_meta: vocab_only       = 0
0.00.071.422 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.422 I llm_load_print_meta: n_embd           = 2048
0.00.071.422 I llm_load_print_meta: n_layer          = 24
0.00.071.425 I llm_load_print_meta: n_head           = 16
0.00.071.426 I llm_load_print_meta: n_head_kv        = 16
0.00.071.427 I llm_load_print_meta: n_rot            = 32
0.00.071.427 I llm_load_print_meta: n_swa            = 0
0.00.071.427 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.427 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.430 I llm_load_print_meta: n_gqa            = 1
0.00.071.432 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.432 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.433 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.435 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.437 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.437 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.437 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.438 I llm_load_print_meta: n_ff             = 8192
0.00.071.438 I llm_load_print_meta: n_expert         = 0
0.00.071.438 I llm_load_print_meta: n_expert_used    = 0
0.00.071.438 I llm_load_print_meta: causal attn      = 1
0.00.071.438 I llm_load_print_meta: pooling type     = 0
0.00.071.439 I llm_load_print_meta: rope type        = 2
0.00.071.439 I llm_load_print_meta: rope scaling     = linear
0.00.071.439 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.440 I llm_load_print_meta: freq_scale_train = 1
0.00.071.440 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.440 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.440 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.440 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.441 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.441 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.441 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.445 I llm_load_print_meta: model type       = 1.4B
0.00.071.446 I llm_load_print_meta: model ftype      = Q5_0
0.00.071.447 I llm_load_print_meta: model params     = 1.41 B
0.00.071.447 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.071.448 I llm_load_print_meta: general.name     = 1.4B
0.00.071.448 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.448 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.448 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.449 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.449 I llm_load_print_meta: LF token         = 128 ''
0.00.071.451 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.451 I llm_load_print_meta: max token length = 1024
0.00.073.903 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.903 I llm_load_tensors: offloading output layer to GPU
0.00.073.903 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.910 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.073.910 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.075.383 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.385 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.385 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.386 I llama_new_context_with_model: n_batch       = 2048
0.00.075.386 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.386 I llama_new_context_with_model: flash_attn    = 0
0.00.075.387 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.387 I llama_new_context_with_model: freq_scale    = 1
0.00.075.388 I ggml_metal_init: allocating
0.00.075.398 I ggml_metal_init: found device: Apple M4
0.00.075.401 I ggml_metal_init: picking default device: Apple M4
0.00.076.343 I ggml_metal_init: using embedded metal library
0.00.080.157 I ggml_metal_init: GPU name:   Apple M4
0.00.080.160 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.162 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.163 I ggml_metal_init: simdgroup reduction   = true
0.00.080.163 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.163 I ggml_metal_init: has bfloat            = true
0.00.080.163 I ggml_metal_init: use bfloat            = true
0.00.080.164 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.940 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.955 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.977 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.962 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.964 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.964 I llama_new_context_with_model: graph nodes  = 967
0.00.115.964 I llama_new_context_with_model: graph splits = 2
0.00.115.979 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.860.738 I main: llama threadpool init, n_threads = 4
0.00.860.788 I 
0.00.860.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.860.836 I 
0.00.861.043 I sampler seed: 1234
0.00.861.050 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.861.064 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.861.066 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.861.066 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.653.342 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.653.342 I llama_perf_context_print:        load time =     849.75 ms
0.01.653.343 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.83 tokens per second)
0.01.653.344 I llama_perf_context_print:        eval time =     745.72 ms /    63 runs   (   11.84 ms per token,    84.48 tokens per second)
0.01.653.344 I llama_perf_context_print:       total time =     792.61 ms /    70 tokens
0.01.653.510 I ggml_metal_free: deallocating

real	0m1.679s
user	0m0.128s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.755 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.510 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.514 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.516 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.516 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.516 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.517 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.517 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.518 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.518 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.519 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.520 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.523 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.523 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.246 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.114 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.115 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.115 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.116 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.116 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.116 I llama_model_loader: - type  f32:  194 tensors
0.00.024.117 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.117 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.900 I llm_load_vocab: special tokens cache size = 25
0.00.049.781 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.783 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.784 I llm_load_print_meta: arch             = gptneox
0.00.049.784 I llm_load_print_meta: vocab type       = BPE
0.00.049.784 I llm_load_print_meta: n_vocab          = 50304
0.00.049.784 I llm_load_print_meta: n_merges         = 50009
0.00.049.784 I llm_load_print_meta: vocab_only       = 0
0.00.049.785 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.785 I llm_load_print_meta: n_embd           = 2048
0.00.049.785 I llm_load_print_meta: n_layer          = 24
0.00.049.788 I llm_load_print_meta: n_head           = 16
0.00.049.788 I llm_load_print_meta: n_head_kv        = 16
0.00.049.789 I llm_load_print_meta: n_rot            = 32
0.00.049.789 I llm_load_print_meta: n_swa            = 0
0.00.049.789 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.789 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.790 I llm_load_print_meta: n_gqa            = 1
0.00.049.791 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.791 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.792 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.792 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.793 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.793 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.793 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.796 I llm_load_print_meta: n_ff             = 8192
0.00.049.796 I llm_load_print_meta: n_expert         = 0
0.00.049.796 I llm_load_print_meta: n_expert_used    = 0
0.00.049.796 I llm_load_print_meta: causal attn      = 1
0.00.049.796 I llm_load_print_meta: pooling type     = 0
0.00.049.797 I llm_load_print_meta: rope type        = 2
0.00.049.797 I llm_load_print_meta: rope scaling     = linear
0.00.049.797 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.798 I llm_load_print_meta: freq_scale_train = 1
0.00.049.798 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.800 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.800 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.800 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.800 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.800 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.800 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.801 I llm_load_print_meta: model type       = 1.4B
0.00.049.801 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.801 I llm_load_print_meta: model params     = 1.41 B
0.00.049.802 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.802 I llm_load_print_meta: general.name     = 1.4B
0.00.049.806 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.806 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.807 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.807 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.807 I llm_load_print_meta: LF token         = 128 ''
0.00.049.808 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.808 I llm_load_print_meta: max token length = 1024
0.00.051.762 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.762 I llm_load_tensors: offloading output layer to GPU
0.00.051.763 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.773 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.774 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.662 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.662 I llama_new_context_with_model: n_ctx         = 128
0.00.052.662 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.663 I llama_new_context_with_model: n_batch       = 128
0.00.052.663 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.663 I llama_new_context_with_model: flash_attn    = 0
0.00.052.663 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.664 I llama_new_context_with_model: freq_scale    = 1
0.00.052.664 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.664 I ggml_metal_init: allocating
0.00.052.670 I ggml_metal_init: found device: Apple M4
0.00.052.672 I ggml_metal_init: picking default device: Apple M4
0.00.053.211 I ggml_metal_init: using embedded metal library
0.00.055.507 I ggml_metal_init: GPU name:   Apple M4
0.00.055.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.509 I ggml_metal_init: simdgroup reduction   = true
0.00.055.509 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.509 I ggml_metal_init: has bfloat            = true
0.00.055.510 I ggml_metal_init: use bfloat            = true
0.00.055.510 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.510 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.982 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.985 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.998 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.874 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.875 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.875 I llama_new_context_with_model: graph nodes  = 967
0.00.066.875 I llama_new_context_with_model: graph splits = 2
0.00.066.887 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.390 I 
0.00.712.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.460 I perplexity: tokenizing the input ..
0.00.720.072 I perplexity: tokenization took 7.611 ms
0.00.720.086 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.527 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.856.784 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.856.803 I llama_perf_context_print:        load time =     702.63 ms
0.00.856.803 I llama_perf_context_print: prompt eval time =     135.21 ms /   128 tokens (    1.06 ms per token,   946.66 tokens per second)
0.00.856.804 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.856.805 I llama_perf_context_print:       total time =     144.42 ms /   129 tokens
0.00.857.296 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.076s
sys	0m0.119s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.894 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.833 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.026.837 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.842 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.843 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.845 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.845 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.846 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.929 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.148 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.148 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.149 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.149 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.149 I llama_model_loader: - type  f32:  194 tensors
0.00.036.150 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.150 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.510 I llm_load_vocab: special tokens cache size = 25
0.00.065.820 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.823 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.823 I llm_load_print_meta: arch             = gptneox
0.00.065.823 I llm_load_print_meta: vocab type       = BPE
0.00.065.823 I llm_load_print_meta: n_vocab          = 50304
0.00.065.824 I llm_load_print_meta: n_merges         = 50009
0.00.065.824 I llm_load_print_meta: vocab_only       = 0
0.00.065.824 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.824 I llm_load_print_meta: n_embd           = 2048
0.00.065.824 I llm_load_print_meta: n_layer          = 24
0.00.065.827 I llm_load_print_meta: n_head           = 16
0.00.065.828 I llm_load_print_meta: n_head_kv        = 16
0.00.065.828 I llm_load_print_meta: n_rot            = 32
0.00.065.830 I llm_load_print_meta: n_swa            = 0
0.00.065.830 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.831 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.831 I llm_load_print_meta: n_gqa            = 1
0.00.065.833 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.835 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.835 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.835 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.835 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.836 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.836 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.836 I llm_load_print_meta: n_ff             = 8192
0.00.065.836 I llm_load_print_meta: n_expert         = 0
0.00.065.837 I llm_load_print_meta: n_expert_used    = 0
0.00.065.837 I llm_load_print_meta: causal attn      = 1
0.00.065.837 I llm_load_print_meta: pooling type     = 0
0.00.065.837 I llm_load_print_meta: rope type        = 2
0.00.065.837 I llm_load_print_meta: rope scaling     = linear
0.00.065.839 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.839 I llm_load_print_meta: freq_scale_train = 1
0.00.065.839 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.839 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.839 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.839 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.839 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.839 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.840 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.840 I llm_load_print_meta: model type       = 1.4B
0.00.065.841 I llm_load_print_meta: model ftype      = Q5_1
0.00.065.841 I llm_load_print_meta: model params     = 1.41 B
0.00.065.841 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.065.842 I llm_load_print_meta: general.name     = 1.4B
0.00.065.842 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.843 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.843 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.843 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.843 I llm_load_print_meta: LF token         = 128 ''
0.00.065.844 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.844 I llm_load_print_meta: max token length = 1024
0.00.067.699 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.700 I llm_load_tensors: offloading output layer to GPU
0.00.067.700 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.705 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.067.705 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.068.622 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.622 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.623 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.623 I llama_new_context_with_model: n_batch       = 2048
0.00.068.623 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.623 I llama_new_context_with_model: flash_attn    = 0
0.00.068.624 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.624 I llama_new_context_with_model: freq_scale    = 1
0.00.068.624 I ggml_metal_init: allocating
0.00.068.630 I ggml_metal_init: found device: Apple M4
0.00.068.632 I ggml_metal_init: picking default device: Apple M4
0.00.069.225 I ggml_metal_init: using embedded metal library
0.00.071.787 I ggml_metal_init: GPU name:   Apple M4
0.00.071.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.789 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.789 I ggml_metal_init: simdgroup reduction   = true
0.00.071.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.789 I ggml_metal_init: has bfloat            = true
0.00.071.789 I ggml_metal_init: use bfloat            = true
0.00.071.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.901 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.906 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.924 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.118 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.119 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.120 I llama_new_context_with_model: graph nodes  = 967
0.00.104.120 I llama_new_context_with_model: graph splits = 2
0.00.104.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.046.370 I main: llama threadpool init, n_threads = 4
0.01.046.482 I 
0.01.046.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.046.564 I 
0.01.047.236 I sampler seed: 1234
0.01.047.242 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.047.312 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.047.314 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.047.314 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.901.185 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51005.75 tokens per second)
0.01.901.186 I llama_perf_context_print:        load time =    1037.45 ms
0.01.901.186 I llama_perf_context_print: prompt eval time =      53.11 ms /     7 tokens (    7.59 ms per token,   131.80 tokens per second)
0.01.901.188 I llama_perf_context_print:        eval time =     797.62 ms /    63 runs   (   12.66 ms per token,    78.98 tokens per second)
0.01.901.188 I llama_perf_context_print:       total time =     854.84 ms /    70 tokens
0.01.901.386 I ggml_metal_free: deallocating

real	0m1.918s
user	0m0.127s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.869 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.369 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.370 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.370 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.370 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.371 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.371 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.372 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.372 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.373 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.373 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.373 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.375 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.378 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.379 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.379 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.166 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.284 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.095 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.096 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.097 I llama_model_loader: - type  f32:  194 tensors
0.00.023.098 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.017 I llm_load_vocab: special tokens cache size = 25
0.00.049.040 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.043 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.043 I llm_load_print_meta: arch             = gptneox
0.00.049.043 I llm_load_print_meta: vocab type       = BPE
0.00.049.044 I llm_load_print_meta: n_vocab          = 50304
0.00.049.044 I llm_load_print_meta: n_merges         = 50009
0.00.049.044 I llm_load_print_meta: vocab_only       = 0
0.00.049.044 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.044 I llm_load_print_meta: n_embd           = 2048
0.00.049.044 I llm_load_print_meta: n_layer          = 24
0.00.049.047 I llm_load_print_meta: n_head           = 16
0.00.049.048 I llm_load_print_meta: n_head_kv        = 16
0.00.049.048 I llm_load_print_meta: n_rot            = 32
0.00.049.048 I llm_load_print_meta: n_swa            = 0
0.00.049.048 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.048 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.049 I llm_load_print_meta: n_gqa            = 1
0.00.049.050 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.050 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.051 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.052 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.052 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.052 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.052 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.055 I llm_load_print_meta: n_ff             = 8192
0.00.049.055 I llm_load_print_meta: n_expert         = 0
0.00.049.055 I llm_load_print_meta: n_expert_used    = 0
0.00.049.055 I llm_load_print_meta: causal attn      = 1
0.00.049.055 I llm_load_print_meta: pooling type     = 0
0.00.049.055 I llm_load_print_meta: rope type        = 2
0.00.049.055 I llm_load_print_meta: rope scaling     = linear
0.00.049.056 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.056 I llm_load_print_meta: freq_scale_train = 1
0.00.049.056 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.057 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.057 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.059 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.059 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.059 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.059 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.059 I llm_load_print_meta: model type       = 1.4B
0.00.049.060 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.060 I llm_load_print_meta: model params     = 1.41 B
0.00.049.061 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.061 I llm_load_print_meta: general.name     = 1.4B
0.00.049.061 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.061 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.061 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.061 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.062 I llm_load_print_meta: LF token         = 128 ''
0.00.049.067 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.067 I llm_load_print_meta: max token length = 1024
0.00.051.035 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.035 I llm_load_tensors: offloading output layer to GPU
0.00.051.035 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.045 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.047 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.951 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.952 I llama_new_context_with_model: n_ctx         = 128
0.00.051.952 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.952 I llama_new_context_with_model: n_batch       = 128
0.00.051.952 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.952 I llama_new_context_with_model: flash_attn    = 0
0.00.051.953 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.953 I llama_new_context_with_model: freq_scale    = 1
0.00.051.953 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.954 I ggml_metal_init: allocating
0.00.051.957 I ggml_metal_init: found device: Apple M4
0.00.051.959 I ggml_metal_init: picking default device: Apple M4
0.00.052.510 I ggml_metal_init: using embedded metal library
0.00.054.787 I ggml_metal_init: GPU name:   Apple M4
0.00.054.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.789 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.789 I ggml_metal_init: simdgroup reduction   = true
0.00.054.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.790 I ggml_metal_init: has bfloat            = true
0.00.054.790 I ggml_metal_init: use bfloat            = true
0.00.054.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.791 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.427 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.434 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.449 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.287 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.288 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.288 I llama_new_context_with_model: graph nodes  = 967
0.00.066.288 I llama_new_context_with_model: graph splits = 2
0.00.066.296 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.874 I 
0.00.743.910 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.922 I perplexity: tokenizing the input ..
0.00.752.066 I perplexity: tokenization took 8.143 ms
0.00.752.077 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.887.246 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.888.487 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.888.501 I llama_perf_context_print:        load time =     735.00 ms
0.00.888.502 I llama_perf_context_print: prompt eval time =     134.94 ms /   128 tokens (    1.05 ms per token,   948.54 tokens per second)
0.00.888.503 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.888.504 I llama_perf_context_print:       total time =     144.63 ms /   129 tokens
0.00.888.948 I ggml_metal_free: deallocating

real	0m0.902s
user	0m0.077s
sys	0m0.115s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.012.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.199 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.020.203 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.205 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.205 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.206 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.207 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.211 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.114 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.307 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.208 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.209 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.029.209 I llama_model_loader: - type  f32:  194 tensors
0.00.029.210 I llama_model_loader: - type q2_K:   49 tensors
0.00.029.210 I llama_model_loader: - type q3_K:   48 tensors
0.00.029.210 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.660 I llm_load_vocab: special tokens cache size = 25
0.00.057.403 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.406 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.406 I llm_load_print_meta: arch             = gptneox
0.00.057.407 I llm_load_print_meta: vocab type       = BPE
0.00.057.407 I llm_load_print_meta: n_vocab          = 50304
0.00.057.407 I llm_load_print_meta: n_merges         = 50009
0.00.057.407 I llm_load_print_meta: vocab_only       = 0
0.00.057.408 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.408 I llm_load_print_meta: n_embd           = 2048
0.00.057.408 I llm_load_print_meta: n_layer          = 24
0.00.057.411 I llm_load_print_meta: n_head           = 16
0.00.057.412 I llm_load_print_meta: n_head_kv        = 16
0.00.057.412 I llm_load_print_meta: n_rot            = 32
0.00.057.412 I llm_load_print_meta: n_swa            = 0
0.00.057.413 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.413 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.414 I llm_load_print_meta: n_gqa            = 1
0.00.057.414 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.415 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.415 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.418 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.419 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.419 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.419 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.420 I llm_load_print_meta: n_ff             = 8192
0.00.057.420 I llm_load_print_meta: n_expert         = 0
0.00.057.420 I llm_load_print_meta: n_expert_used    = 0
0.00.057.420 I llm_load_print_meta: causal attn      = 1
0.00.057.421 I llm_load_print_meta: pooling type     = 0
0.00.057.421 I llm_load_print_meta: rope type        = 2
0.00.057.421 I llm_load_print_meta: rope scaling     = linear
0.00.057.421 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.422 I llm_load_print_meta: freq_scale_train = 1
0.00.057.422 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.422 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.422 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.422 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.422 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.422 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.423 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.423 I llm_load_print_meta: model type       = 1.4B
0.00.057.423 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.057.424 I llm_load_print_meta: model params     = 1.41 B
0.00.057.424 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.057.425 I llm_load_print_meta: general.name     = 1.4B
0.00.057.425 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.425 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.425 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.425 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.426 I llm_load_print_meta: LF token         = 128 ''
0.00.057.427 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.427 I llm_load_print_meta: max token length = 1024
0.00.059.388 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.389 I llm_load_tensors: offloading output layer to GPU
0.00.059.389 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.400 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.059.401 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.060.288 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.289 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.289 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.289 I llama_new_context_with_model: n_batch       = 2048
0.00.060.289 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.289 I llama_new_context_with_model: flash_attn    = 0
0.00.060.290 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.290 I llama_new_context_with_model: freq_scale    = 1
0.00.060.290 I ggml_metal_init: allocating
0.00.060.297 I ggml_metal_init: found device: Apple M4
0.00.060.299 I ggml_metal_init: picking default device: Apple M4
0.00.060.938 I ggml_metal_init: using embedded metal library
0.00.063.305 I ggml_metal_init: GPU name:   Apple M4
0.00.063.306 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.306 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.307 I ggml_metal_init: simdgroup reduction   = true
0.00.063.307 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.307 I ggml_metal_init: has bfloat            = true
0.00.063.307 I ggml_metal_init: use bfloat            = true
0.00.063.308 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.309 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.588 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.595 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.612 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.666 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.667 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.668 I llama_new_context_with_model: graph nodes  = 967
0.00.095.668 I llama_new_context_with_model: graph splits = 2
0.00.095.681 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.953 I main: llama threadpool init, n_threads = 4
0.00.582.994 I 
0.00.583.025 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.025 I 
0.00.583.258 I sampler seed: 1234
0.00.583.262 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.583.285 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.583.285 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.583.285 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.262.322 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64021.64 tokens per second)
0.01.262.322 I llama_perf_context_print:        load time =     570.28 ms
0.01.262.324 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.67 tokens per second)
0.01.262.325 I llama_perf_context_print:        eval time =     640.43 ms /    63 runs   (   10.17 ms per token,    98.37 tokens per second)
0.01.262.326 I llama_perf_context_print:       total time =     679.38 ms /    70 tokens
0.01.262.512 I ggml_metal_free: deallocating

real	0m1.285s
user	0m0.111s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.676 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.010 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.015 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.017 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.017 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.018 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.018 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.018 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.020 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.022 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.022 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.023 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.023 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.029 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.989 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.781 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.782 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.783 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.783 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.783 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.784 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.784 I llama_model_loader: - type  f32:  194 tensors
0.00.023.784 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.785 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.785 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.551 I llm_load_vocab: special tokens cache size = 25
0.00.049.490 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.494 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.494 I llm_load_print_meta: arch             = gptneox
0.00.049.494 I llm_load_print_meta: vocab type       = BPE
0.00.049.495 I llm_load_print_meta: n_vocab          = 50304
0.00.049.495 I llm_load_print_meta: n_merges         = 50009
0.00.049.495 I llm_load_print_meta: vocab_only       = 0
0.00.049.495 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.496 I llm_load_print_meta: n_embd           = 2048
0.00.049.497 I llm_load_print_meta: n_layer          = 24
0.00.049.499 I llm_load_print_meta: n_head           = 16
0.00.049.500 I llm_load_print_meta: n_head_kv        = 16
0.00.049.500 I llm_load_print_meta: n_rot            = 32
0.00.049.500 I llm_load_print_meta: n_swa            = 0
0.00.049.501 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.501 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.502 I llm_load_print_meta: n_gqa            = 1
0.00.049.506 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.507 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.507 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.508 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.508 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.508 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.508 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.509 I llm_load_print_meta: n_ff             = 8192
0.00.049.509 I llm_load_print_meta: n_expert         = 0
0.00.049.509 I llm_load_print_meta: n_expert_used    = 0
0.00.049.510 I llm_load_print_meta: causal attn      = 1
0.00.049.510 I llm_load_print_meta: pooling type     = 0
0.00.049.510 I llm_load_print_meta: rope type        = 2
0.00.049.512 I llm_load_print_meta: rope scaling     = linear
0.00.049.513 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.514 I llm_load_print_meta: freq_scale_train = 1
0.00.049.514 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.514 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.514 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.517 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.518 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.518 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.518 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.518 I llm_load_print_meta: model type       = 1.4B
0.00.049.519 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.520 I llm_load_print_meta: model params     = 1.41 B
0.00.049.520 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.520 I llm_load_print_meta: general.name     = 1.4B
0.00.049.521 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.521 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.521 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.521 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.521 I llm_load_print_meta: LF token         = 128 ''
0.00.049.521 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.522 I llm_load_print_meta: max token length = 1024
0.00.051.433 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.433 I llm_load_tensors: offloading output layer to GPU
0.00.051.433 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.443 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.444 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.467 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.468 I llama_new_context_with_model: n_ctx         = 128
0.00.052.468 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.469 I llama_new_context_with_model: n_batch       = 128
0.00.052.469 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.469 I llama_new_context_with_model: flash_attn    = 0
0.00.052.469 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.470 I llama_new_context_with_model: freq_scale    = 1
0.00.052.470 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.470 I ggml_metal_init: allocating
0.00.052.474 I ggml_metal_init: found device: Apple M4
0.00.052.476 I ggml_metal_init: picking default device: Apple M4
0.00.053.062 I ggml_metal_init: using embedded metal library
0.00.055.360 I ggml_metal_init: GPU name:   Apple M4
0.00.055.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.362 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.363 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.363 I ggml_metal_init: simdgroup reduction   = true
0.00.055.363 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.363 I ggml_metal_init: has bfloat            = true
0.00.055.363 I ggml_metal_init: use bfloat            = true
0.00.055.364 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.366 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.076 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.078 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.093 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.012 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.013 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.014 I llama_new_context_with_model: graph nodes  = 967
0.00.067.014 I llama_new_context_with_model: graph splits = 2
0.00.067.026 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.462.815 I 
0.00.462.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.462.868 I perplexity: tokenizing the input ..
0.00.470.799 I perplexity: tokenization took 7.93 ms
0.00.470.809 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.603.367 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.604.539 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.604.559 I llama_perf_context_print:        load time =     453.13 ms
0.00.604.560 I llama_perf_context_print: prompt eval time =     132.32 ms /   128 tokens (    1.03 ms per token,   967.33 tokens per second)
0.00.604.561 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.604.561 I llama_perf_context_print:       total time =     141.74 ms /   129 tokens
0.00.605.091 I ggml_metal_free: deallocating

real	0m0.620s
user	0m0.077s
sys	0m0.077s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.010.826 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.826 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.830 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.831 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.832 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.832 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.832 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.834 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.834 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.835 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.836 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.836 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.838 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.838 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.838 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.785 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.678 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.679 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.679 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.679 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.680 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.680 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.680 I llama_model_loader: - type  f32:  194 tensors
0.00.025.681 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.681 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.681 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.681 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.705 I llm_load_vocab: special tokens cache size = 25
0.00.051.630 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.632 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.633 I llm_load_print_meta: arch             = gptneox
0.00.051.633 I llm_load_print_meta: vocab type       = BPE
0.00.051.633 I llm_load_print_meta: n_vocab          = 50304
0.00.051.633 I llm_load_print_meta: n_merges         = 50009
0.00.051.634 I llm_load_print_meta: vocab_only       = 0
0.00.051.634 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.634 I llm_load_print_meta: n_embd           = 2048
0.00.051.634 I llm_load_print_meta: n_layer          = 24
0.00.051.636 I llm_load_print_meta: n_head           = 16
0.00.051.637 I llm_load_print_meta: n_head_kv        = 16
0.00.051.637 I llm_load_print_meta: n_rot            = 32
0.00.051.637 I llm_load_print_meta: n_swa            = 0
0.00.051.638 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.638 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.639 I llm_load_print_meta: n_gqa            = 1
0.00.051.639 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.642 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.643 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.643 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.643 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.644 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.644 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.644 I llm_load_print_meta: n_ff             = 8192
0.00.051.646 I llm_load_print_meta: n_expert         = 0
0.00.051.646 I llm_load_print_meta: n_expert_used    = 0
0.00.051.646 I llm_load_print_meta: causal attn      = 1
0.00.051.646 I llm_load_print_meta: pooling type     = 0
0.00.051.646 I llm_load_print_meta: rope type        = 2
0.00.051.647 I llm_load_print_meta: rope scaling     = linear
0.00.051.647 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.647 I llm_load_print_meta: freq_scale_train = 1
0.00.051.648 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.648 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.648 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.648 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.648 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.648 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.649 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.649 I llm_load_print_meta: model type       = 1.4B
0.00.051.649 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.650 I llm_load_print_meta: model params     = 1.41 B
0.00.051.650 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.650 I llm_load_print_meta: general.name     = 1.4B
0.00.051.651 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.651 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.651 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.651 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.651 I llm_load_print_meta: LF token         = 128 ''
0.00.051.652 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.652 I llm_load_print_meta: max token length = 1024
0.00.053.552 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.553 I llm_load_tensors: offloading output layer to GPU
0.00.053.553 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.564 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.565 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.497 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.497 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.498 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.498 I llama_new_context_with_model: n_batch       = 2048
0.00.054.498 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.498 I llama_new_context_with_model: flash_attn    = 0
0.00.054.499 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.499 I llama_new_context_with_model: freq_scale    = 1
0.00.054.499 I ggml_metal_init: allocating
0.00.054.502 I ggml_metal_init: found device: Apple M4
0.00.054.504 I ggml_metal_init: picking default device: Apple M4
0.00.055.103 I ggml_metal_init: using embedded metal library
0.00.057.406 I ggml_metal_init: GPU name:   Apple M4
0.00.057.408 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.408 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.409 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.409 I ggml_metal_init: simdgroup reduction   = true
0.00.057.409 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.409 I ggml_metal_init: has bfloat            = true
0.00.057.409 I ggml_metal_init: use bfloat            = true
0.00.057.410 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.259 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.269 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.288 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.298 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.299 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.299 I llama_new_context_with_model: graph nodes  = 967
0.00.087.300 I llama_new_context_with_model: graph splits = 2
0.00.087.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.815 I main: llama threadpool init, n_threads = 4
0.00.591.855 I 
0.00.591.889 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.891 I 
0.00.592.126 I sampler seed: 1234
0.00.592.130 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.592.170 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.592.187 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.592.187 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.340.146 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.340.146 I llama_perf_context_print:        load time =     580.98 ms
0.01.340.147 I llama_perf_context_print: prompt eval time =      40.52 ms /     7 tokens (    5.79 ms per token,   172.75 tokens per second)
0.01.340.148 I llama_perf_context_print:        eval time =     704.41 ms /    63 runs   (   11.18 ms per token,    89.44 tokens per second)
0.01.340.148 I llama_perf_context_print:       total time =     748.34 ms /    70 tokens
0.01.340.370 I ggml_metal_free: deallocating

real	0m1.356s
user	0m0.109s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.730 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.385 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.392 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.392 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.393 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.394 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.394 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.395 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.399 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.399 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.400 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.246 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.403 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.326 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.326 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.327 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.327 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.327 I llama_model_loader: - type  f32:  194 tensors
0.00.023.328 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.328 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.328 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.328 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.879 I llm_load_vocab: special tokens cache size = 25
0.00.049.901 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.904 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.904 I llm_load_print_meta: arch             = gptneox
0.00.049.904 I llm_load_print_meta: vocab type       = BPE
0.00.049.905 I llm_load_print_meta: n_vocab          = 50304
0.00.049.905 I llm_load_print_meta: n_merges         = 50009
0.00.049.905 I llm_load_print_meta: vocab_only       = 0
0.00.049.905 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.905 I llm_load_print_meta: n_embd           = 2048
0.00.049.906 I llm_load_print_meta: n_layer          = 24
0.00.049.908 I llm_load_print_meta: n_head           = 16
0.00.049.909 I llm_load_print_meta: n_head_kv        = 16
0.00.049.909 I llm_load_print_meta: n_rot            = 32
0.00.049.909 I llm_load_print_meta: n_swa            = 0
0.00.049.909 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.910 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.910 I llm_load_print_meta: n_gqa            = 1
0.00.049.911 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.914 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.914 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.915 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.915 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.915 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.915 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.916 I llm_load_print_meta: n_ff             = 8192
0.00.049.917 I llm_load_print_meta: n_expert         = 0
0.00.049.917 I llm_load_print_meta: n_expert_used    = 0
0.00.049.918 I llm_load_print_meta: causal attn      = 1
0.00.049.918 I llm_load_print_meta: pooling type     = 0
0.00.049.918 I llm_load_print_meta: rope type        = 2
0.00.049.918 I llm_load_print_meta: rope scaling     = linear
0.00.049.918 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.919 I llm_load_print_meta: freq_scale_train = 1
0.00.049.919 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.919 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.919 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.920 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.920 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.920 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.920 I llm_load_print_meta: model type       = 1.4B
0.00.049.921 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.921 I llm_load_print_meta: model params     = 1.41 B
0.00.049.922 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.922 I llm_load_print_meta: general.name     = 1.4B
0.00.049.922 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.922 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.922 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.923 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.923 I llm_load_print_meta: LF token         = 128 ''
0.00.049.923 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.923 I llm_load_print_meta: max token length = 1024
0.00.051.876 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.876 I llm_load_tensors: offloading output layer to GPU
0.00.051.877 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.887 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.889 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.769 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.770 I llama_new_context_with_model: n_ctx         = 128
0.00.052.770 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.770 I llama_new_context_with_model: n_batch       = 128
0.00.052.770 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.770 I llama_new_context_with_model: flash_attn    = 0
0.00.052.771 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.771 I llama_new_context_with_model: freq_scale    = 1
0.00.052.771 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.772 I ggml_metal_init: allocating
0.00.052.775 I ggml_metal_init: found device: Apple M4
0.00.052.776 I ggml_metal_init: picking default device: Apple M4
0.00.053.364 I ggml_metal_init: using embedded metal library
0.00.055.665 I ggml_metal_init: GPU name:   Apple M4
0.00.055.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.667 I ggml_metal_init: simdgroup reduction   = true
0.00.055.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.668 I ggml_metal_init: has bfloat            = true
0.00.055.668 I ggml_metal_init: use bfloat            = true
0.00.055.668 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.555 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.558 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.571 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.504 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.505 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.505 I llama_new_context_with_model: graph nodes  = 967
0.00.067.505 I llama_new_context_with_model: graph splits = 2
0.00.067.518 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.265 I 
0.00.521.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.305 I perplexity: tokenizing the input ..
0.00.529.505 I perplexity: tokenization took 8.198 ms
0.00.529.517 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.661.867 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.663.027 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.663.047 I llama_perf_context_print:        load time =     512.53 ms
0.00.663.048 I llama_perf_context_print: prompt eval time =     132.10 ms /   128 tokens (    1.03 ms per token,   968.96 tokens per second)
0.00.663.050 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.663.050 I llama_perf_context_print:       total time =     141.78 ms /   129 tokens
0.00.663.503 I ggml_metal_free: deallocating

real	0m0.676s
user	0m0.079s
sys	0m0.097s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.811 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.563 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.571 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.573 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.575 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.576 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.576 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.577 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.578 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.580 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.539 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.722 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.683 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.684 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.685 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.686 I llama_model_loader: - type  f32:  194 tensors
0.00.023.686 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.686 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.687 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.652 I llm_load_vocab: special tokens cache size = 25
0.00.050.732 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.735 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.735 I llm_load_print_meta: arch             = gptneox
0.00.050.736 I llm_load_print_meta: vocab type       = BPE
0.00.050.736 I llm_load_print_meta: n_vocab          = 50304
0.00.050.736 I llm_load_print_meta: n_merges         = 50009
0.00.050.736 I llm_load_print_meta: vocab_only       = 0
0.00.050.737 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.737 I llm_load_print_meta: n_embd           = 2048
0.00.050.737 I llm_load_print_meta: n_layer          = 24
0.00.050.740 I llm_load_print_meta: n_head           = 16
0.00.050.740 I llm_load_print_meta: n_head_kv        = 16
0.00.050.741 I llm_load_print_meta: n_rot            = 32
0.00.050.741 I llm_load_print_meta: n_swa            = 0
0.00.050.741 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.741 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.742 I llm_load_print_meta: n_gqa            = 1
0.00.050.743 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.743 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.744 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.745 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.745 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.745 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.747 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.748 I llm_load_print_meta: n_ff             = 8192
0.00.050.748 I llm_load_print_meta: n_expert         = 0
0.00.050.748 I llm_load_print_meta: n_expert_used    = 0
0.00.050.748 I llm_load_print_meta: causal attn      = 1
0.00.050.748 I llm_load_print_meta: pooling type     = 0
0.00.050.748 I llm_load_print_meta: rope type        = 2
0.00.050.748 I llm_load_print_meta: rope scaling     = linear
0.00.050.750 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.750 I llm_load_print_meta: freq_scale_train = 1
0.00.050.750 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.750 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.750 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.751 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.752 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.752 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.753 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.753 I llm_load_print_meta: model type       = 1.4B
0.00.050.753 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.754 I llm_load_print_meta: model params     = 1.41 B
0.00.050.754 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.754 I llm_load_print_meta: general.name     = 1.4B
0.00.050.755 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.755 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.755 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.755 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.756 I llm_load_print_meta: LF token         = 128 ''
0.00.050.760 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.760 I llm_load_print_meta: max token length = 1024
0.00.052.348 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.349 I llm_load_tensors: offloading output layer to GPU
0.00.052.349 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.359 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.360 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.231 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.232 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.232 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.232 I llama_new_context_with_model: n_batch       = 2048
0.00.053.232 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.233 I llama_new_context_with_model: flash_attn    = 0
0.00.053.233 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.233 I llama_new_context_with_model: freq_scale    = 1
0.00.053.234 I ggml_metal_init: allocating
0.00.053.237 I ggml_metal_init: found device: Apple M4
0.00.053.239 I ggml_metal_init: picking default device: Apple M4
0.00.053.855 I ggml_metal_init: using embedded metal library
0.00.056.166 I ggml_metal_init: GPU name:   Apple M4
0.00.056.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.168 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.168 I ggml_metal_init: simdgroup reduction   = true
0.00.056.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.169 I ggml_metal_init: has bfloat            = true
0.00.056.169 I ggml_metal_init: use bfloat            = true
0.00.056.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.170 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.228 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.234 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.252 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.198 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.199 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.200 I llama_new_context_with_model: graph nodes  = 967
0.00.086.200 I llama_new_context_with_model: graph splits = 2
0.00.086.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.847 I main: llama threadpool init, n_threads = 4
0.00.654.890 I 
0.00.654.928 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.929 I 
0.00.655.165 I sampler seed: 1234
0.00.655.169 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.655.216 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.655.218 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.655.218 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.417.189 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.01.417.190 I llama_perf_context_print:        load time =     646.03 ms
0.01.417.191 I llama_perf_context_print: prompt eval time =      47.17 ms /     7 tokens (    6.74 ms per token,   148.41 tokens per second)
0.01.417.193 I llama_perf_context_print:        eval time =     711.75 ms /    63 runs   (   11.30 ms per token,    88.51 tokens per second)
0.01.417.193 I llama_perf_context_print:       total time =     762.35 ms /    70 tokens
0.01.417.394 I ggml_metal_free: deallocating

real	0m1.433s
user	0m0.110s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.077 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.681 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.159 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.159 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.160 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.161 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.162 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.162 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.162 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.163 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.969 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.963 I llama_model_loader: - type  f32:  194 tensors
0.00.022.964 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.964 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.964 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.814 I llm_load_vocab: special tokens cache size = 25
0.00.048.592 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.595 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.596 I llm_load_print_meta: arch             = gptneox
0.00.048.596 I llm_load_print_meta: vocab type       = BPE
0.00.048.596 I llm_load_print_meta: n_vocab          = 50304
0.00.048.596 I llm_load_print_meta: n_merges         = 50009
0.00.048.597 I llm_load_print_meta: vocab_only       = 0
0.00.048.597 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.597 I llm_load_print_meta: n_embd           = 2048
0.00.048.597 I llm_load_print_meta: n_layer          = 24
0.00.048.600 I llm_load_print_meta: n_head           = 16
0.00.048.601 I llm_load_print_meta: n_head_kv        = 16
0.00.048.601 I llm_load_print_meta: n_rot            = 32
0.00.048.601 I llm_load_print_meta: n_swa            = 0
0.00.048.602 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.602 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.603 I llm_load_print_meta: n_gqa            = 1
0.00.048.603 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.604 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.605 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.605 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.605 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.605 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.606 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.606 I llm_load_print_meta: n_ff             = 8192
0.00.048.606 I llm_load_print_meta: n_expert         = 0
0.00.048.607 I llm_load_print_meta: n_expert_used    = 0
0.00.048.607 I llm_load_print_meta: causal attn      = 1
0.00.048.607 I llm_load_print_meta: pooling type     = 0
0.00.048.607 I llm_load_print_meta: rope type        = 2
0.00.048.607 I llm_load_print_meta: rope scaling     = linear
0.00.048.608 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.608 I llm_load_print_meta: freq_scale_train = 1
0.00.048.608 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.608 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.610 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.611 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.611 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.611 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.611 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.611 I llm_load_print_meta: model type       = 1.4B
0.00.048.612 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.612 I llm_load_print_meta: model params     = 1.41 B
0.00.048.613 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.613 I llm_load_print_meta: general.name     = 1.4B
0.00.048.613 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.614 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.614 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.614 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.618 I llm_load_print_meta: LF token         = 128 ''
0.00.048.618 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.618 I llm_load_print_meta: max token length = 1024
0.00.050.566 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.567 I llm_load_tensors: offloading output layer to GPU
0.00.050.567 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.577 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.579 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.470 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.470 I llama_new_context_with_model: n_ctx         = 128
0.00.051.471 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.471 I llama_new_context_with_model: n_batch       = 128
0.00.051.471 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.471 I llama_new_context_with_model: flash_attn    = 0
0.00.051.471 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.472 I llama_new_context_with_model: freq_scale    = 1
0.00.051.472 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.473 I ggml_metal_init: allocating
0.00.051.479 I ggml_metal_init: found device: Apple M4
0.00.051.482 I ggml_metal_init: picking default device: Apple M4
0.00.052.062 I ggml_metal_init: using embedded metal library
0.00.054.407 I ggml_metal_init: GPU name:   Apple M4
0.00.054.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.409 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.409 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.410 I ggml_metal_init: simdgroup reduction   = true
0.00.054.410 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.410 I ggml_metal_init: has bfloat            = true
0.00.054.410 I ggml_metal_init: use bfloat            = true
0.00.054.410 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.411 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.980 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.985 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.001 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.827 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.828 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.828 I llama_new_context_with_model: graph nodes  = 967
0.00.065.829 I llama_new_context_with_model: graph splits = 2
0.00.065.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.420 I 
0.00.559.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.559.460 I perplexity: tokenizing the input ..
0.00.567.081 I perplexity: tokenization took 7.62 ms
0.00.567.092 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.701.182 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.702.467 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.702.490 I llama_perf_context_print:        load time =     550.74 ms
0.00.702.491 I llama_perf_context_print: prompt eval time =     133.86 ms /   128 tokens (    1.05 ms per token,   956.21 tokens per second)
0.00.702.492 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.702.492 I llama_perf_context_print:       total time =     143.07 ms /   129 tokens
0.00.702.992 I ggml_metal_free: deallocating

real	0m0.716s
user	0m0.076s
sys	0m0.102s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.015.755 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.026.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.792 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.799 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.803 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.557 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.028 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.089 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.090 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.091 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.091 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.091 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.092 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.038.092 I llama_model_loader: - type  f32:  194 tensors
0.00.038.092 I llama_model_loader: - type q5_K:   61 tensors
0.00.038.093 I llama_model_loader: - type q6_K:   37 tensors
0.00.070.649 I llm_load_vocab: special tokens cache size = 25
0.00.080.835 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.080.839 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.080.839 I llm_load_print_meta: arch             = gptneox
0.00.080.840 I llm_load_print_meta: vocab type       = BPE
0.00.080.840 I llm_load_print_meta: n_vocab          = 50304
0.00.080.840 I llm_load_print_meta: n_merges         = 50009
0.00.080.840 I llm_load_print_meta: vocab_only       = 0
0.00.080.841 I llm_load_print_meta: n_ctx_train      = 2048
0.00.080.841 I llm_load_print_meta: n_embd           = 2048
0.00.080.841 I llm_load_print_meta: n_layer          = 24
0.00.080.845 I llm_load_print_meta: n_head           = 16
0.00.080.845 I llm_load_print_meta: n_head_kv        = 16
0.00.080.846 I llm_load_print_meta: n_rot            = 32
0.00.080.846 I llm_load_print_meta: n_swa            = 0
0.00.080.846 I llm_load_print_meta: n_embd_head_k    = 128
0.00.080.846 I llm_load_print_meta: n_embd_head_v    = 128
0.00.080.847 I llm_load_print_meta: n_gqa            = 1
0.00.080.848 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.080.849 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.080.850 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.080.850 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.080.850 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.080.850 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.080.853 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.080.854 I llm_load_print_meta: n_ff             = 8192
0.00.080.854 I llm_load_print_meta: n_expert         = 0
0.00.080.854 I llm_load_print_meta: n_expert_used    = 0
0.00.080.856 I llm_load_print_meta: causal attn      = 1
0.00.080.857 I llm_load_print_meta: pooling type     = 0
0.00.080.857 I llm_load_print_meta: rope type        = 2
0.00.080.858 I llm_load_print_meta: rope scaling     = linear
0.00.080.858 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.080.858 I llm_load_print_meta: freq_scale_train = 1
0.00.080.859 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.080.859 I llm_load_print_meta: rope_finetuned   = unknown
0.00.080.859 I llm_load_print_meta: ssm_d_conv       = 0
0.00.080.859 I llm_load_print_meta: ssm_d_inner      = 0
0.00.080.859 I llm_load_print_meta: ssm_d_state      = 0
0.00.080.860 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.080.860 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.080.860 I llm_load_print_meta: model type       = 1.4B
0.00.080.861 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.080.865 I llm_load_print_meta: model params     = 1.41 B
0.00.080.866 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.080.866 I llm_load_print_meta: general.name     = 1.4B
0.00.080.867 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.080.867 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.080.867 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.080.867 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.080.869 I llm_load_print_meta: LF token         = 128 ''
0.00.080.870 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.080.870 I llm_load_print_meta: max token length = 1024
0.00.083.611 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.083.612 I llm_load_tensors: offloading output layer to GPU
0.00.083.612 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.083.623 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.083.625 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.084.959 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.960 I llama_new_context_with_model: n_ctx         = 2048
0.00.084.961 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.084.961 I llama_new_context_with_model: n_batch       = 2048
0.00.084.961 I llama_new_context_with_model: n_ubatch      = 512
0.00.084.961 I llama_new_context_with_model: flash_attn    = 0
0.00.084.962 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.962 I llama_new_context_with_model: freq_scale    = 1
0.00.084.963 I ggml_metal_init: allocating
0.00.084.971 I ggml_metal_init: found device: Apple M4
0.00.084.974 I ggml_metal_init: picking default device: Apple M4
0.00.085.740 I ggml_metal_init: using embedded metal library
0.00.089.204 I ggml_metal_init: GPU name:   Apple M4
0.00.089.206 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.209 I ggml_metal_init: simdgroup reduction   = true
0.00.089.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.209 I ggml_metal_init: has bfloat            = true
0.00.089.210 I ggml_metal_init: use bfloat            = true
0.00.089.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.216 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.121.714 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.723 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.121.744 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.122.728 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.122.729 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.122.729 I llama_new_context_with_model: graph nodes  = 967
0.00.122.730 I llama_new_context_with_model: graph splits = 2
0.00.122.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.447 I main: llama threadpool init, n_threads = 4
0.00.750.498 I 
0.00.750.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.542 I 
0.00.750.890 I sampler seed: 1234
0.00.750.896 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.924 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.926 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.926 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.600.823 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.01.600.823 I llama_perf_context_print:        load time =     734.69 ms
0.01.600.824 I llama_perf_context_print: prompt eval time =      51.79 ms /     7 tokens (    7.40 ms per token,   135.15 tokens per second)
0.01.600.825 I llama_perf_context_print:        eval time =     795.14 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.600.826 I llama_perf_context_print:       total time =     850.38 ms /    70 tokens
0.01.601.016 I ggml_metal_free: deallocating

real	0m1.634s
user	0m0.135s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.613 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.433 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.440 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.441 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.443 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.443 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.444 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.445 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.445 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.445 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.449 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.341 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.402 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.403 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.403 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.403 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.403 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.404 I llama_model_loader: - type  f32:  194 tensors
0.00.024.404 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.404 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.367 I llm_load_vocab: special tokens cache size = 25
0.00.051.416 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.419 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.419 I llm_load_print_meta: arch             = gptneox
0.00.051.420 I llm_load_print_meta: vocab type       = BPE
0.00.051.420 I llm_load_print_meta: n_vocab          = 50304
0.00.051.420 I llm_load_print_meta: n_merges         = 50009
0.00.051.420 I llm_load_print_meta: vocab_only       = 0
0.00.051.420 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.420 I llm_load_print_meta: n_embd           = 2048
0.00.051.421 I llm_load_print_meta: n_layer          = 24
0.00.051.424 I llm_load_print_meta: n_head           = 16
0.00.051.424 I llm_load_print_meta: n_head_kv        = 16
0.00.051.425 I llm_load_print_meta: n_rot            = 32
0.00.051.425 I llm_load_print_meta: n_swa            = 0
0.00.051.425 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.425 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.426 I llm_load_print_meta: n_gqa            = 1
0.00.051.427 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.428 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.428 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.428 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.429 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.429 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.429 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.429 I llm_load_print_meta: n_ff             = 8192
0.00.051.430 I llm_load_print_meta: n_expert         = 0
0.00.051.432 I llm_load_print_meta: n_expert_used    = 0
0.00.051.433 I llm_load_print_meta: causal attn      = 1
0.00.051.433 I llm_load_print_meta: pooling type     = 0
0.00.051.433 I llm_load_print_meta: rope type        = 2
0.00.051.433 I llm_load_print_meta: rope scaling     = linear
0.00.051.435 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.435 I llm_load_print_meta: freq_scale_train = 1
0.00.051.435 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.436 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.436 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.436 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.436 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.436 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.436 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.441 I llm_load_print_meta: model type       = 1.4B
0.00.051.441 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.442 I llm_load_print_meta: model params     = 1.41 B
0.00.051.443 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.446 I llm_load_print_meta: general.name     = 1.4B
0.00.051.446 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.446 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.446 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.446 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.447 I llm_load_print_meta: LF token         = 128 ''
0.00.051.447 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.447 I llm_load_print_meta: max token length = 1024
0.00.053.518 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.519 I llm_load_tensors: offloading output layer to GPU
0.00.053.519 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.530 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.531 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.498 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.499 I llama_new_context_with_model: n_ctx         = 128
0.00.054.500 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.500 I llama_new_context_with_model: n_batch       = 128
0.00.054.500 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.500 I llama_new_context_with_model: flash_attn    = 0
0.00.054.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.501 I llama_new_context_with_model: freq_scale    = 1
0.00.054.501 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.501 I ggml_metal_init: allocating
0.00.054.505 I ggml_metal_init: found device: Apple M4
0.00.054.507 I ggml_metal_init: picking default device: Apple M4
0.00.055.106 I ggml_metal_init: using embedded metal library
0.00.057.452 I ggml_metal_init: GPU name:   Apple M4
0.00.057.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.454 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.454 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.455 I ggml_metal_init: simdgroup reduction   = true
0.00.057.456 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.457 I ggml_metal_init: has bfloat            = true
0.00.057.457 I ggml_metal_init: use bfloat            = true
0.00.057.457 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.356 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.358 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.374 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.331 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.332 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.333 I llama_new_context_with_model: graph nodes  = 967
0.00.069.333 I llama_new_context_with_model: graph splits = 2
0.00.069.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.137 I 
0.00.643.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.178 I perplexity: tokenizing the input ..
0.00.651.136 I perplexity: tokenization took 7.956 ms
0.00.651.148 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.869 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.793.040 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.793.052 I llama_perf_context_print:        load time =     633.52 ms
0.00.793.054 I llama_perf_context_print: prompt eval time =     140.50 ms /   128 tokens (    1.10 ms per token,   911.06 tokens per second)
0.00.793.054 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.055 I llama_perf_context_print:       total time =     149.92 ms /   129 tokens
0.00.793.370 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.079s
sys	0m0.113s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.105 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.924 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.928 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.929 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.930 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.931 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.931 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.932 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.932 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.932 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.933 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.933 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.933 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.934 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.936 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.937 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.937 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.866 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.638 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.639 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.640 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.640 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.640 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.641 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.641 I llama_model_loader: - type  f32:  194 tensors
0.00.025.642 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.816 I llm_load_vocab: special tokens cache size = 25
0.00.051.816 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.819 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.819 I llm_load_print_meta: arch             = gptneox
0.00.051.819 I llm_load_print_meta: vocab type       = BPE
0.00.051.820 I llm_load_print_meta: n_vocab          = 50304
0.00.051.820 I llm_load_print_meta: n_merges         = 50009
0.00.051.820 I llm_load_print_meta: vocab_only       = 0
0.00.051.820 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.820 I llm_load_print_meta: n_embd           = 2048
0.00.051.821 I llm_load_print_meta: n_layer          = 24
0.00.051.823 I llm_load_print_meta: n_head           = 16
0.00.051.824 I llm_load_print_meta: n_head_kv        = 16
0.00.051.824 I llm_load_print_meta: n_rot            = 32
0.00.051.824 I llm_load_print_meta: n_swa            = 0
0.00.051.825 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.825 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.826 I llm_load_print_meta: n_gqa            = 1
0.00.051.827 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.827 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.828 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.828 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.828 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.828 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.829 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.829 I llm_load_print_meta: n_ff             = 8192
0.00.051.829 I llm_load_print_meta: n_expert         = 0
0.00.051.830 I llm_load_print_meta: n_expert_used    = 0
0.00.051.830 I llm_load_print_meta: causal attn      = 1
0.00.051.832 I llm_load_print_meta: pooling type     = 0
0.00.051.834 I llm_load_print_meta: rope type        = 2
0.00.051.834 I llm_load_print_meta: rope scaling     = linear
0.00.051.834 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.835 I llm_load_print_meta: freq_scale_train = 1
0.00.051.835 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.835 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.835 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.835 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.835 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.836 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.836 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.836 I llm_load_print_meta: model type       = 1.4B
0.00.051.836 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.837 I llm_load_print_meta: model params     = 1.41 B
0.00.051.837 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.841 I llm_load_print_meta: general.name     = 1.4B
0.00.051.842 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.842 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.842 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.842 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.843 I llm_load_print_meta: LF token         = 128 ''
0.00.051.843 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.843 I llm_load_print_meta: max token length = 1024
0.00.053.912 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.912 I llm_load_tensors: offloading output layer to GPU
0.00.053.913 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.923 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.925 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.878 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.878 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.878 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.879 I llama_new_context_with_model: n_batch       = 2048
0.00.054.879 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.879 I llama_new_context_with_model: flash_attn    = 0
0.00.054.879 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.880 I llama_new_context_with_model: freq_scale    = 1
0.00.054.880 I ggml_metal_init: allocating
0.00.054.883 I ggml_metal_init: found device: Apple M4
0.00.054.885 I ggml_metal_init: picking default device: Apple M4
0.00.055.481 I ggml_metal_init: using embedded metal library
0.00.057.774 I ggml_metal_init: GPU name:   Apple M4
0.00.057.775 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.776 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.776 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.776 I ggml_metal_init: simdgroup reduction   = true
0.00.057.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.778 I ggml_metal_init: has bfloat            = true
0.00.057.778 I ggml_metal_init: use bfloat            = true
0.00.057.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.779 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.538 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.543 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.560 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.686 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.688 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.688 I llama_new_context_with_model: graph nodes  = 967
0.00.087.688 I llama_new_context_with_model: graph splits = 2
0.00.087.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.274 I main: llama threadpool init, n_threads = 4
0.00.752.313 I 
0.00.752.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.359 I 
0.00.752.592 I sampler seed: 1234
0.00.752.597 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.626 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.627 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.627 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.632.897 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61900.61 tokens per second)
0.01.632.898 I llama_perf_context_print:        load time =     743.16 ms
0.01.632.899 I llama_perf_context_print: prompt eval time =      54.45 ms /     7 tokens (    7.78 ms per token,   128.55 tokens per second)
0.01.632.900 I llama_perf_context_print:        eval time =     822.90 ms /    63 runs   (   13.06 ms per token,    76.56 tokens per second)
0.01.632.901 I llama_perf_context_print:       total time =     880.63 ms /    70 tokens
0.01.633.093 I ggml_metal_free: deallocating

real	0m1.651s
user	0m0.110s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4331 (5478bbcd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.798 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.617 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.622 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.625 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.625 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.626 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.626 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.516 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.755 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.570 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.571 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.572 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.572 I llama_model_loader: - type  f32:  194 tensors
0.00.023.573 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.154 I llm_load_vocab: special tokens cache size = 25
0.00.050.008 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.010 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.010 I llm_load_print_meta: arch             = gptneox
0.00.050.011 I llm_load_print_meta: vocab type       = BPE
0.00.050.011 I llm_load_print_meta: n_vocab          = 50304
0.00.050.011 I llm_load_print_meta: n_merges         = 50009
0.00.050.011 I llm_load_print_meta: vocab_only       = 0
0.00.050.011 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.012 I llm_load_print_meta: n_embd           = 2048
0.00.050.012 I llm_load_print_meta: n_layer          = 24
0.00.050.015 I llm_load_print_meta: n_head           = 16
0.00.050.015 I llm_load_print_meta: n_head_kv        = 16
0.00.050.015 I llm_load_print_meta: n_rot            = 32
0.00.050.016 I llm_load_print_meta: n_swa            = 0
0.00.050.016 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.016 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.017 I llm_load_print_meta: n_gqa            = 1
0.00.050.018 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.018 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.019 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.019 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.019 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.020 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.020 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.020 I llm_load_print_meta: n_ff             = 8192
0.00.050.021 I llm_load_print_meta: n_expert         = 0
0.00.050.021 I llm_load_print_meta: n_expert_used    = 0
0.00.050.021 I llm_load_print_meta: causal attn      = 1
0.00.050.021 I llm_load_print_meta: pooling type     = 0
0.00.050.021 I llm_load_print_meta: rope type        = 2
0.00.050.021 I llm_load_print_meta: rope scaling     = linear
0.00.050.022 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.022 I llm_load_print_meta: freq_scale_train = 1
0.00.050.022 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.022 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.022 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.023 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.025 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.025 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.025 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.025 I llm_load_print_meta: model type       = 1.4B
0.00.050.026 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.026 I llm_load_print_meta: model params     = 1.41 B
0.00.050.027 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.027 I llm_load_print_meta: general.name     = 1.4B
0.00.050.027 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.027 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.027 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.028 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.028 I llm_load_print_meta: LF token         = 128 ''
0.00.050.028 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.028 I llm_load_print_meta: max token length = 1024
0.00.051.661 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.661 I llm_load_tensors: offloading output layer to GPU
0.00.051.662 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.671 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.672 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.528 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.529 I llama_new_context_with_model: n_ctx         = 128
0.00.052.529 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.529 I llama_new_context_with_model: n_batch       = 128
0.00.052.529 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.529 I llama_new_context_with_model: flash_attn    = 0
0.00.052.530 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.530 I llama_new_context_with_model: freq_scale    = 1
0.00.052.530 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.531 I ggml_metal_init: allocating
0.00.052.534 I ggml_metal_init: found device: Apple M4
0.00.052.536 I ggml_metal_init: picking default device: Apple M4
0.00.053.095 I ggml_metal_init: using embedded metal library
0.00.055.416 I ggml_metal_init: GPU name:   Apple M4
0.00.055.418 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.418 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.419 I ggml_metal_init: simdgroup reduction   = true
0.00.055.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.419 I ggml_metal_init: has bfloat            = true
0.00.055.419 I ggml_metal_init: use bfloat            = true
0.00.055.420 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.420 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.342 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.344 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.369 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.254 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.256 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.256 I llama_new_context_with_model: graph nodes  = 967
0.00.067.256 I llama_new_context_with_model: graph splits = 2
0.00.067.268 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.221 I 
0.00.445.262 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.445.271 I perplexity: tokenizing the input ..
0.00.453.127 I perplexity: tokenization took 7.855 ms
0.00.453.143 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.592.522 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.593.680 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.593.693 I llama_perf_context_print:        load time =     436.42 ms
0.00.593.693 I llama_perf_context_print: prompt eval time =     139.13 ms /   128 tokens (    1.09 ms per token,   919.99 tokens per second)
0.00.593.694 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.593.695 I llama_perf_context_print:       total time =     148.47 ms /   129 tokens
0.00.594.070 I ggml_metal_free: deallocating

real	0m0.607s
user	0m0.078s
sys	0m0.095s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4331 (5478bbcd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15530a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15530aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15530b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15530ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15530c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15530c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15530cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15530d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15530d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15530dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15530e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15530e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15530f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15530f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1553100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1553107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155310ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155311600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155311d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1553124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155312c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155313330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155313a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1553142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155314a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155314cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1553152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155315f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155316490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155316750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155316bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155316eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155317740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155317c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155317f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1553183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155318880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155318d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1553191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155319660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155319b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155319fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15531a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15531a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15531aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15531b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15531b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15531c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15531c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15531cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15531d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15531d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15531df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15531e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15531ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15531f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15531f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15531f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15531ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155320730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1553209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155320e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155321330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1553217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155321c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155322110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1553225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155322a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155322ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155323390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155323830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155323cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155324170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1553246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155324c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155325160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1553256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155325c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155326150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1553266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155326bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155327140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155327690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155327be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155328130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155328680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155328bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155329120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155329670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155329bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15532a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15532a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15532abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15532b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15532b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15532bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15532c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15531bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15532c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15532cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15532d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15532d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15532dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15532e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15532e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15532ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15532f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15532f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15532fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155330230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155330780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155330cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155331220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1553316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155331b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155332000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1553324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155332940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155332de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155333280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155333720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155333bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155334060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155334500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1553349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155334e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1553352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155335780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155335c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1553360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155336560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155336a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155336ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155337340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1553377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155337c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155338120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1553385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155338a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155338f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1553393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155339840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155339ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15533a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15533a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15533aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15533af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15533b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15533b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15533bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15533c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15533c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15533cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15533cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15533d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15533d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15533dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15533e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15533e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15533eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15533f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15533f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15533f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15533fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1553402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155340740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155340be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155341080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155341520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1553419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155341e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155342300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1553427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155342c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1553430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155343580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155343a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155343ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155344360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155344800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155344ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155345140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1553455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155345a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155345f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1553463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155346860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155346d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1553471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155347640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155347ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155347f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155348420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155348970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155348ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155349410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155349960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155349c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15534a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15534a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15534ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15534b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15534bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15534bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15534c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15534c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15534d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15534d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15534daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15534df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15534e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15534ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15534f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15534f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15534fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1553501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155350720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155350c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1553511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155351710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155351c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1553521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155352700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155352c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1553531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1553536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155353c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155354190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1553546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155354c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155355180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1553556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155355c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155356170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1553566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155356c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155357160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1553576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155357c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155358150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1553586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155358bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155359140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155359690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155359be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15535a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15535a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15535abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15535b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15535b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15535bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15535c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15535c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15535cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15535d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15535d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15535dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15535e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15535e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15535eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15535f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15535f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15535fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1553600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155360620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155360b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1553610c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155361560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155361a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155361ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155362340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1553627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155362c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155363120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1553635c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155363a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155363f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1553643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155364840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155364ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155365180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155365620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155365b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155366290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1553669b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1553670d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1553677f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155367ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1553682a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155368560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155368b70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.145.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155207530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1552079a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155207e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155208280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1552086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155208b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155208fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1552057b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155205c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155206090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155209720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155209d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15520a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15520b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15520b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15520bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15520c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15520cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15520d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15520dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15520e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15520eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15520f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15520f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155210020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1552102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1552105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155210a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155210e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1552112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1552117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155211d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155212170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155212430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1552128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155212d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155213270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155213770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155213c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155214170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155214670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155214b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155215070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155215570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155215a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155215ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155216350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1552167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155216c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1552170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155217510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155217980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155217df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155218260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1552186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155218ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155219340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155219600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155219c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15521a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15521a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15521ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15521b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15521b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15521bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15521bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15521c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15521c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15521cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15521d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15521d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15521db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15521e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15521e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15521eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15521f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15521f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15521fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155220000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155220550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155220aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155220ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155221540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155221a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155221fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155222530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155222a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155222fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155223520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155223a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155223fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155224510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155224a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155224fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155225500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155225a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155225fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1552264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155226a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155226f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1552274e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155227a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155227f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1552284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x155228a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x155228f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1552294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155229a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x155229f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15522a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15522aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15522af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15522b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15522b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15522bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15522c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15522c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15522cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15522d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15522d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15522d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15522de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15522e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15522e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15522ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15522f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15522f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15522fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15522fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155230340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1552307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155230c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155231120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1552315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155231a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155231f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1552323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155232840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155232ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155233180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155233620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155233ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155233f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155234400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1552348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155234d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1552351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155235680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155235b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155235fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155236460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155236900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155236da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155237240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1552376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155237b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155238020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1552384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155238960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155238e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1552392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155239740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155239be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15523a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15523a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15523a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15523ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15523b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15523b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15523bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15523c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15523c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15523ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15523cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15523d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15523d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15523dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15523e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15523e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15523ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15523ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15523f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15523f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15523fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1552401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155240640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155240ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155240f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155241420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1552418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155241d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155242200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1552426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155242bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155243140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155243690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155243be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155243ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1552444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155244ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1552450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1552458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155245d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155246020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155246630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155246c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155247430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1552478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155247d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155248210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1552489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155248f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155249460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1552499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155249f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15524a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15524a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15524aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15524b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15524b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15524bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15524c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15524c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15524ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15524d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15524d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15524dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15524e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15524e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15524eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15524f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15524f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15524fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1552503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155250940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155250e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1552513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155251930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155251e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1552523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155252920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155252e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1552533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155253910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155253e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1552543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155254900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155254e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1552553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1552558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155255e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155256390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1552568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155256e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155257380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1552578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155257e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155258370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1552588c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155258e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155259360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1552598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155259e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15525a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15525a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15525adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15525b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15525b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15525bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15525c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15525c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15525ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15525cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15525d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15525d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15525dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15525e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15525e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15525eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15525ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15525f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15525f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15525fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155260510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155260c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155261350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155261a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155261d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155262520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1552627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155262df0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15530dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15530e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15530e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15530ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15530eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15530f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15530f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15530fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1553100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155310520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155310990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155310f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155311860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155311fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1553127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155312eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1553135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155313c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155314380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155314d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1553153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155315ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1553161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1553168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155316fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155317420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155317890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155317d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155318170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1553185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155318a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155318ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155319330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1553195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155319a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155319ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15531a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15531a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15531ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15531b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15531b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15531b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15531bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15531c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15531c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15531cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15531cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15531d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15531d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15531dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15531e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15531e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15531ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15531eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15531f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15531f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15531fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155320070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1553204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155320950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155320dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155321230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1553216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155321b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155321f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1553223f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155322860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155322cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155323140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1553235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155323a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155323e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155324300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155324770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155324be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155325050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1553254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155325930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155325da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155326210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155326680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155326af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155326f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1553273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155327840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155327cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155328120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155328590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155328a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155328e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1553292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155329750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155329bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15532a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15532a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15532a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15532ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15532b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15532b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15532bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15532bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15532c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15532c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15532cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15532d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15532d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15532d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15532de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15532e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15532e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15532eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15532f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15532f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15532f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15532fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1553301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155330640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155330ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155330f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155331390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155331800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155331c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1553320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155332550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1553329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155332e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1553332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155333710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155333b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155333ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155334460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1553348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155334d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1553351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155335620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155335a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155335f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155336370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1553367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155336c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1553370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155337530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1553379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155337e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155338280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1553386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155338b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155338fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155339440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1553398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155339d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15533a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15533a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15533aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15533aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15533b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15533b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15533bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15533c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15533c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15533c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15533cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15533d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15533d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15533db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15533dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15533e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15533e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15533ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15533f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15533f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15533fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15533fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155340330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1553407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155340c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155341080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1553414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155341960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155341dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155342240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1553426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155342b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155342f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155343400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155343870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155343ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155344150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1553445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155344a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155344ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155345310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155345780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155345bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155346060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1553464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155346940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155346db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155347220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155347690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155347b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x155347f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1553483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155348850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155348cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155349130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1553495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155349a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155349e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15534a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15534aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15534aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15534b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15534b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15534bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15534c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15534c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15534c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15534cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15534d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15534d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15534db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15534dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15534e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15534e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15534ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15534f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15534f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15534fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15534fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155350330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1553507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155350c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155351080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1553514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155351960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155351dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155352240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1553526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155352b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155352f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155353400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155353870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155353ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155354150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1553545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155354a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155354ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155355310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155355780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155355bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155356060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1553564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155356940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155356db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155357220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155357690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155357b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155357f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1553583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155358850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155358cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155359130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1553595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155359a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155359e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15535a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15535a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15535abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15535b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15535b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15535b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15535bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15535c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15535c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15535cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15535cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15535d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15535d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15535dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15535e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15535e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15535e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15535f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15535f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155360030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155360720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155360b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155361000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155361470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1553618e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.823s
user	0m0.315s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4331 (5478bbcd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11c60b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11c60bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11c60c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11c60c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11c60cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11c60d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11c60d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11c60dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11c60e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11c60e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11c60eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11c60f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11c60fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11c610470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11c610c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11c6113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11c611ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11c6121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11c612900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11c6130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11c6137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11c613f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11c614630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11c614ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11c6155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11c6158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11c615ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11c616b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11c617070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11c617330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11c6177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11c617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11c618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11c618860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11c618b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11c618fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11c619460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11c619900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11c619da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11c61a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11c61a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11c61ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11c61b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11c61b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11c61b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11c61bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11c61c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11c61ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11c61d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11c61d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11c61def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11c61e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11c61eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11c61f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11c61f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11c61fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11c620250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11c620510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11c620b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11c621310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11c6215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11c621a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11c621f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11c6223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11c622850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11c622cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11c623190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11c623630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11c623ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11c623f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11c624410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11c6248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11c624d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11c6252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11c6257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11c625d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11c626290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11c6267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11c626d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11c627280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11c6277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11c627d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11c628270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11c6287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11c628d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11c629260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11c6297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11c629d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11c62a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11c62a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11c62acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11c62b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11c62b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11c62bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11c62c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11c62c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11c62ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11c61c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11c62d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11c62d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11c62de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11c62e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11c62e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11c62ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11c62f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11c62f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11c62fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11c630370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11c6308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11c630e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11c631360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11c6318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11c631e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11c6322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11c632740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11c632be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11c633080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11c633520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11c6339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11c633e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11c634300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11c6347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11c634c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11c6350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11c635580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11c635a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11c635ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11c636360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11c636800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11c636ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11c637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11c6375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11c637a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11c637f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11c6383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11c638860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11c638d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11c6391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11c639640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11c639ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11c639f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11c63a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11c63a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11c63ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11c63b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11c63b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11c63bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11c63bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11c63c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11c63c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11c63cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11c63d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11c63d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11c63dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11c63e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11c63e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11c63e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11c63ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11c63f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11c63f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11c63fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11c6400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11c640540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11c6409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11c640e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11c641320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11c6417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11c641c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11c642100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11c6425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11c642a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11c642ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11c643380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11c643820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11c643cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11c644160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11c644600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11c644aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11c644f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11c6453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11c645880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11c645d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c6461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c646660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c646b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11c646fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c647440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c6478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11c647d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c648220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c6486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c648b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c649000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c649550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c649aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c649ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c64a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c64a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c64ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c64b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c64ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c64c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c64c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c64c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c64cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c64d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c64dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c64e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c64e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c64eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c64f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c64f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c64fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c650310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c650860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c650db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c651300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c651850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c651da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c6522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c652840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c652d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c6532e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c653830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c653d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c6542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c654820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c654d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c6552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c655810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c655d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c6562b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c656800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c656d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c6572a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c6577f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c657d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c658290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c6587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c658d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c659280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c6597d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c659d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c65a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c65a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c65ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c65b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c65b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c65bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c65c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c65c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c65ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c65d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c65d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c65dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c65e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c65e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c65ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c65f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c65f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c65fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c660210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c660760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c660cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c661200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c661750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c661ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c662140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c6625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c662a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c662f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c6633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c663860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c663d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c6641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c664640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c664ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c664f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c665420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c6658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c665d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c666200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c666750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c666e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c667590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c667cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c6683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c668690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c668e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c669140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c669750 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.092.259 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11bb05e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11bb062f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11bb06760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11bb06bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11bb07040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11bb074b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11bb07920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11bb07d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11bb04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11bb046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11bb08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11bb08720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11bb09240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11bb099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11bb0a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11bb0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11bb0b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11bb0b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11bb0be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11bb0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11bb0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11bb0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11bb0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11bb0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11bb0e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11bb0ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11bb0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11bb0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11bb0f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11bb0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11bb101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11bb106d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11bb10b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11bb10e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11bb11270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11bb116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11bb11c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11bb12140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11bb12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11bb12b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11bb13040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11bb13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11bb13a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11bb13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11bb14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11bb148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11bb14d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11bb15190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11bb15600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11bb15a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11bb15ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11bb16350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11bb167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11bb16c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11bb170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11bb17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11bb17d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11bb17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11bb185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11bb18dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11bb19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11bb19710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11bb19bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11bb1a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11bb1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11bb1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11bb1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11bb1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11bb1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11bb1bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11bb1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11bb1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11bb1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11bb1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11bb1d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11bb1d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11bb1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11bb1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11bb1e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11bb1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11bb1f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11bb1f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11bb1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11bb20460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11bb209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11bb20f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11bb21450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11bb219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11bb21ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11bb22440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11bb22990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11bb22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11bb23430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11bb23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11bb23ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11bb24420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11bb24970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11bb24ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11bb25410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11bb25960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11bb25eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11bb26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11bb26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11bb26ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11bb273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11bb27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11bb27e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11bb283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11bb28930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11bb28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11bb293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11bb29920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11bb29e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11bb2a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11bb2a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11bb2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11bb2b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11bb2b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11bb2ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11bb2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11bb2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11bb2c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11bb2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11bb2d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11bb2d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11bb2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11bb2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11bb2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11bb2e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11bb2ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11bb2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11bb2f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11bb2faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11bb2ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11bb30430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11bb308d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11bb30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11bb31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11bb316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11bb31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11bb31ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11bb32490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11bb32930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11bb32dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11bb33270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11bb33710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11bb33bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11bb34050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11bb344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11bb34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11bb34e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11bb352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11bb35770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11bb35c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11bb360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11bb36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11bb369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11bb36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11bb37330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11bb377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11bb37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11bb38110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11bb385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11bb38a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11bb38ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11bb39390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11bb39830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11bb39cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11bb3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11bb3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11bb3aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11bb3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11bb3b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11bb3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11bb3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11bb3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11bb3c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11bb3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11bb3cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11bb3d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11bb3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11bb3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11bb3e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11bb3e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11bb3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11bb3f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11bb3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11bb3f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11bb3fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11bb40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11bb40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11bb40bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11bb41070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11bb415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11bb41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11bb42060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11bb425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11bb42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11bb42e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11bb43490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11bb43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11bb44290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11bb44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11bb449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11bb45000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11bb45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11bb45e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11bb462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11bb46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11bb46be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11bb47390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11bb478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11bb47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11bb48380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11bb488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11bb48e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11bb49370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11bb498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11bb49e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11bb4a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11bb4a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11bb4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11bb4b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11bb4b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11bb4bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11bb4c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11bb4c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11bb4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11bb4d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11bb4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11bb4ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11bb4e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11bb4e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11bb4edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11bb4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11bb4f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11bb4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11bb50300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11bb50850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11bb50da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11bb512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11bb51840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11bb51d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11bb522e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11bb52830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11bb52d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11bb532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11bb53820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11bb53d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11bb542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11bb54810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11bb54d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11bb552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11bb55800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11bb55d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11bb562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11bb567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11bb56d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11bb57290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11bb577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11bb57d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11bb58280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11bb587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11bb58d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11bb59270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11bb597c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11bb59d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11bb5a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11bb5a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11bb5aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11bb5af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11bb5b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11bb5b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11bb5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11bb5c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11bb5c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11bb5cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11bb5cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11bb5d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11bb5d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11bb5ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11bb5e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11bb5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11bb5eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11bb5f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11bb5fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11bb60440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11bb60700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11bb60ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11bb611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11bb617c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11bb062e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11bb06750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11bb06bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11bb07030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11bb074a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11bb07910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11bb07d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11bb081f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11bb08660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11bb08ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11bb08f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11bb09520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11bb09e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11bb0a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11bb0ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11bb0b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11bb0bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11bb0c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11bb0c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11bb0d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11bb0d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11bb0e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11bb0e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11bb0ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11bb0f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11bb0f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11bb0fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11bb102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11bb10720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11bb10b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11bb11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11bb11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11bb118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11bb11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11bb12010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11bb12480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11bb128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11bb12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11bb131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11bb13640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11bb13ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11bb13f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11bb14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11bb14800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11bb14c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11bb150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11bb15550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11bb159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11bb15e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11bb162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11bb16710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11bb16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11bb16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11bb17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11bb178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11bb17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11bb181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11bb18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11bb18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11bb18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11bb19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11bb197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11bb19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11bb1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11bb1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11bb1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11bb1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11bb1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11bb1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11bb1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11bb1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11bb1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11bb1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11bb1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11bb1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11bb1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11bb1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11bb1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11bb1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11bb1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11bb1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11bb1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11bb1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11bb1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11bb1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11bb20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11bb206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11bb20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11bb20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11bb21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11bb21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11bb21d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11bb22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11bb225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11bb22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11bb22ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11bb23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11bb237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11bb23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11bb24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11bb244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11bb24960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11bb24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11bb25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11bb256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11bb25b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11bb25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11bb26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11bb26870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11bb26ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11bb27150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11bb275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11bb27a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11bb27ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11bb28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11bb28780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11bb28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11bb29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11bb294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11bb29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11bb29db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11bb2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11bb2a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11bb2ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11bb2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11bb2b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11bb2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11bb2bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11bb2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11bb2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11bb2ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11bb2ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11bb2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11bb2d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11bb2dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11bb2e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11bb2e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11bb2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11bb2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11bb2f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11bb2f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11bb2fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11bb2ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11bb303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11bb30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11bb30ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11bb31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11bb31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11bb319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11bb31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11bb322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11bb32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11bb32bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11bb33020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11bb33490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11bb33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11bb33d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11bb341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11bb34650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11bb34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11bb34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11bb353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11bb35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11bb35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11bb360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11bb36560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11bb369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11bb36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11bb372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11bb37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11bb37b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11bb38000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11bb38470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11bb388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11bb38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11bb391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11bb39630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11bb39aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11bb39f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11bb3a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11bb3a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11bb3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11bb3b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11bb3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11bb3b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11bb3be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11bb3c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11bb3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11bb3cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11bb3cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11bb3d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11bb3d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11bb3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11bb3e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11bb3e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11bb3ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11bb3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11bb3f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11bb3f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11bb3fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11bb400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11bb40520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11bb40990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11bb40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11bb41270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11bb416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11bb41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11bb41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11bb42430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11bb428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11bb43020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11bb43490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11bb43900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11bb43d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11bb441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11bb44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11bb44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11bb44f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11bb453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11bb45810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11bb45c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11bb460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11bb46560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11bb469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11bb46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11bb472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11bb47720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11bb47b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11bb48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11bb48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11bb488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11bb48d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11bb491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11bb49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11bb49aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11bb49f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11bb4a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11bb4a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11bb4ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11bb4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11bb4b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11bb4b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11bb4be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11bb4c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11bb4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11bb4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11bb4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11bb4d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11bb4d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11bb4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11bb4e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11bb4e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11bb4ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11bb4eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11bb4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11bb4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11bb4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11bb500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11bb50520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11bb50990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11bb50e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11bb51270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11bb516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11bb51b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11bb51fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11bb52430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11bb528a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11bb52d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11bb53180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11bb535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11bb53a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11bb53ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11bb54340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11bb547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11bb54c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11bb55090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11bb55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11bb55970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11bb55de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11bb56250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11bb566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11bb56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11bb56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11bb57800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11bb57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11bb585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11bb58cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11bb59140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11bb595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11bb59a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11bb59e90 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.973s
user	0m0.247s
sys	0m0.143s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.57 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.18 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.24 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.51 real         0.14 user         0.04 sys
```
