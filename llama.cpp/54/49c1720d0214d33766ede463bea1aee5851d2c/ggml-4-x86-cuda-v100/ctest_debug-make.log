++ nproc
+ make -j6
[  0%] Generating build details from Git
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Built target sha256
[  3%] Built target sha1
[  3%] Built target xxhash
[  3%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Built target build_info
[  5%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o
[  5%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o
[  5%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o
[  6%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o
[  6%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o
[  7%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/conv-transpose-1d.cu.o
[  7%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o
[  7%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o
[  8%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/cross-entropy-loss.cu.o
[  8%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o
[  9%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o
[  9%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn-tile-f16.cu.o
[ 10%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn-tile-f32.cu.o
[ 10%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/fattn.cu.o
[ 10%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o
[ 11%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o
[ 11%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o
[ 12%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o
[ 12%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o
[ 12%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o
[ 13%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o
[ 13%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o
[ 14%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o
[ 14%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o
[ 15%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o
[ 15%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/sum.cu.o
[ 15%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o
[ 16%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o
[ 16%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o
[ 17%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o
[ 17%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda.cu.o
[ 17%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.cu.o
[ 18%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.cu.o
[ 18%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.cu.o
[ 19%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.cu.o
[ 19%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.cu.o
[ 19%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq1_s.cu.o
[ 20%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq2_s.cu.o
[ 20%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu.o
[ 21%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu.o
[ 21%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq3_s.cu.o
[ 22%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu.o
[ 22%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu.o
[ 22%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu.o
[ 23%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q2_k.cu.o
[ 23%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q3_k.cu.o
[ 24%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_0.cu.o
[ 24%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_1.cu.o
[ 24%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q4_k.cu.o
[ 25%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_0.cu.o
[ 25%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_1.cu.o
[ 26%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q5_k.cu.o
[ 26%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q6_k.cu.o
[ 27%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/mmq-instance-q8_0.cu.o
[ 27%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o
[ 27%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o
[ 28%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o
[ 28%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o
[ 29%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o
[ 29%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o
[ 29%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o
[ 30%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o
[ 30%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o
[ 31%] Building CUDA object ggml/src/CMakeFiles/ggml.dir/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o
[ 31%] Building CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o
[ 32%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o
[ 32%] Linking CXX shared library libggml.so
[ 32%] Built target ggml
[ 32%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 32%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 32%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 33%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 33%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 34%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-gguf
[ 35%] Built target llama-gguf
[ 35%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-gguf-hash
[ 35%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 35%] Built target llama-gguf-hash
[ 36%] Linking CXX shared library libllama.so
[ 36%] Built target llama
[ 36%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 37%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 38%] Building CXX object examples/benchmark/CMakeFiles/llama-bench-matmult.dir/benchmark-matmult.cpp.o
[ 38%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 38%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 38%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 39%] Linking C executable ../bin/test-c
[ 39%] Built target test-c
[ 40%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 40%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
/home/ggml/work/llama.cpp/examples/llava/llava.cpp: In function ‘std::pair<int, int> select_best_resolution(const std::pair<int, int>&, const std::vector<std::pair<int, int> >&)’:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:50:38: error: ‘numeric_limits’ is not a member of ‘std’
   50 |     int min_wasted_resolution = std::numeric_limits<int>::max();
      |                                      ^~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:50:53: error: expected primary-expression before ‘int’
   50 |     int min_wasted_resolution = std::numeric_limits<int>::max();
      |                                                     ^~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp: In function ‘bool clip_llava_handle_patches(clip_ctx*, std::vector<float*>&, clip_image_grid_shape, float*, int*)’:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:160:9: error: ‘memcpy’ was not declared in this scope
  160 |         memcpy((uint8_t *)(image_features->data) + offset, image_embd_v[i], clip_embd_nbytes(ctx_clip));
      |         ^~~~~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:14:1: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
   13 | #include <numeric>
  +++ |+#include <cstring>
   14 | 
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:192:5: error: ‘memcpy’ was not declared in this scope
  192 |     memcpy(image_embd_out, image_embd_v[0], clip_embd_nbytes(ctx_clip)); // main image as global context
      |     ^~~~~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:192:5: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
/home/ggml/work/llama.cpp/examples/llava/llava.cpp: In function ‘bool encode_image_with_clip(clip_ctx*, int, const clip_image_u8*, float*, int*)’:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:282:18: error: ‘memcpy’ is not a member of ‘std’; did you mean ‘wmemcpy’?
  282 |             std::memcpy(image_embd + n_img_pos_out * clip_n_mmproj_embd(ctx_clip), image_embd_v[i], clip_embd_nbytes(ctx_clip));
      |                  ^~~~~~
      |                  wmemcpy
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:295:14: error: ‘strcmp’ was not declared in this scope
  295 |     else if (strcmp(mm_patch_merge_type, "spatial_unpad") != 0) {
      |              ^~~~~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:295:14: note: ‘strcmp’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
In file included from /home/ggml/work/llama.cpp/examples/llava/llava.cpp:5:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp: In function ‘bool load_file_to_bytes(const char*, unsigned char**, long int*)’:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:460:35: error: ‘strerror’ was not declared in this scope; did you mean ‘stderr’?
  460 |         die_fmt("read error: %s", strerror(errno));
      |                                   ^~~~~~~~
/home/ggml/work/llama.cpp/examples/llava/../../common/common.h:18:68: note: in definition of macro ‘die_fmt’
   18 | #define die_fmt(fmt, ...) do { fprintf(stderr, "error: " fmt "\n", __VA_ARGS__); exit(1); } while (0)
      |                                                                    ^~~~~~~~~~~
make[2]: *** [examples/llava/CMakeFiles/llava.dir/build.make:76: examples/llava/CMakeFiles/llava.dir/llava.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:2830: examples/llava/CMakeFiles/llava.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 40%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 40%] Linking CXX executable ../../bin/llama-bench-matmult
/home/ggml/work/llama.cpp/common/log.cpp:43:10: error: ‘vector’ in namespace ‘std’ does not name a template type
   43 |     std::vector<char> msg;
      |          ^~~~~~
/home/ggml/work/llama.cpp/common/log.cpp:7:1: note: ‘std::vector’ is defined in header ‘<vector>’; did you forget to ‘#include <vector>’?
    6 | #include <thread>
  +++ |+#include <vector>
    7 | 
/home/ggml/work/llama.cpp/common/log.cpp: In member function ‘void gpt_log_entry::print(FILE*) const’:
/home/ggml/work/llama.cpp/common/log.cpp:84:29: error: ‘msg’ was not declared in this scope
   84 |         fprintf(fcur, "%s", msg.data());
      |                             ^~~
/home/ggml/work/llama.cpp/common/log.cpp: At global scope:
/home/ggml/work/llama.cpp/common/log.cpp:127:10: error: ‘vector’ in namespace ‘std’ does not name a template type
  127 |     std::vector<gpt_log_entry> entries;
      |          ^~~~~~
/home/ggml/work/llama.cpp/common/log.cpp:127:5: note: ‘std::vector’ is defined in header ‘<vector>’; did you forget to ‘#include <vector>’?
  127 |     std::vector<gpt_log_entry> entries;
      |     ^~~
/home/ggml/work/llama.cpp/common/log.cpp: In constructor ‘gpt_log::gpt_log(size_t)’:
/home/ggml/work/llama.cpp/common/log.cpp:100:9: error: ‘entries’ was not declared in this scope
  100 |         entries.resize(capacity);
      |         ^~~~~~~
/home/ggml/work/llama.cpp/common/log.cpp: In member function ‘void gpt_log::add(ggml_log_level, const char*, __va_list_tag*)’:
/home/ggml/work/llama.cpp/common/log.cpp:142:24: error: ‘entries’ was not declared in this scope; did you mean ‘entry’?
  142 |         auto & entry = entries[tail];
      |                        ^~~~~~~
      |                        entry
/home/ggml/work/llama.cpp/common/log.cpp:182:18: error: ‘vector’ is not a member of ‘std’
  182 |             std::vector<gpt_log_entry> new_entries(2*entries.size());
      |                  ^~~~~~
/home/ggml/work/llama.cpp/common/log.cpp:182:18: note: ‘std::vector’ is defined in header ‘<vector>’; did you forget to ‘#include <vector>’?
/home/ggml/work/llama.cpp/common/log.cpp:182:38: error: expected primary-expression before ‘>’ token
  182 |             std::vector<gpt_log_entry> new_entries(2*entries.size());
      |                                      ^
/home/ggml/work/llama.cpp/common/log.cpp:182:40: error: ‘new_entries’ was not declared in this scope
  182 |             std::vector<gpt_log_entry> new_entries(2*entries.size());
      |                                        ^~~~~~~~~~~
/home/ggml/work/llama.cpp/common/log.cpp: In lambda function:
/home/ggml/work/llama.cpp/common/log.cpp:217:27: error: ‘entries’ was not declared in this scope
  217 |                     cur = entries[head];
      |                           ^~~~~~~
/home/ggml/work/llama.cpp/common/log.cpp: In member function ‘void gpt_log::pause()’:
/home/ggml/work/llama.cpp/common/log.cpp:245:28: error: ‘entries’ was not declared in this scope; did you mean ‘entry’?
  245 |             auto & entry = entries[tail];
      |                            ^~~~~~~
      |                            entry
/home/ggml/work/llama.cpp/common/log.cpp: In function ‘void gpt_log_add(gpt_log*, ggml_log_level, const char*, ...)’:
/home/ggml/work/llama.cpp/common/log.cpp:303:5: error: ‘va_start’ was not declared in this scope
  303 |     va_start(args, fmt);
      |     ^~~~~~~~
/home/ggml/work/llama.cpp/common/log.cpp:305:5: error: ‘va_end’ was not declared in this scope
  305 |     va_end(args);
      |     ^~~~~~
make[2]: *** [common/CMakeFiles/common.dir/build.make:132: common/CMakeFiles/common.dir/log.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
[ 40%] Built target llama-bench-matmult
[ 41%] Linking CXX executable ../../bin/llama-quantize-stats
[ 41%] Built target llama-quantize-stats
make[1]: *** [CMakeFiles/Makefile2:1687: common/CMakeFiles/common.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	6m35.196s
user	34m38.651s
sys	1m6.384s
