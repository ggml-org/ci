+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/54/49c1720d0214d33766ede463bea1aee5851d2c/ggml-1-arm64-cpu-low-perf/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/54/49c1720d0214d33766ede463bea1aee5851d2c/ggml-1-arm64-cpu-low-perf/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- OpenMP found
-- Using llamafile
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.999s
user	0m0.667s
sys	0m0.335s
+ tee -a /home/ggml/results/llama.cpp/54/49c1720d0214d33766ede463bea1aee5851d2c/ggml-1-arm64-cpu-low-perf/ctest_debug-make.log
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Built target sha256
[  2%] Built target xxhash
[  3%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Built target sha1
[  4%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Built target build_info
[  5%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o
[  7%] Linking CXX shared library libggml.so
[  7%] Built target ggml
[ 10%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 10%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 10%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 10%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 10%] Linking CXX executable ../../bin/llama-gguf
[ 10%] Built target llama-gguf
[ 10%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 11%] Linking CXX executable ../../bin/llama-gguf-hash
[ 11%] Built target llama-gguf-hash
[ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 14%] Linking CXX shared library libllama.so
[ 14%] Built target llama
[ 16%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 17%] Building CXX object examples/benchmark/CMakeFiles/llama-bench-matmult.dir/benchmark-matmult.cpp.o
[ 17%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 17%] Linking C executable ../bin/test-c
[ 17%] Built target test-c
[ 17%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
/home/ggml/work/llama.cpp/examples/llava/llava.cpp: In function ‘std::pair<int, int> select_best_resolution(const std::pair<int, int>&, const std::vector<std::pair<int, int> >&)’:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:50:38: error: ‘numeric_limits’ is not a member of ‘std’
   50 |     int min_wasted_resolution = std::numeric_limits<int>::max();
      |                                      ^~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:50:53: error: expected primary-expression before ‘int’
   50 |     int min_wasted_resolution = std::numeric_limits<int>::max();
      |                                                     ^~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp: In function ‘bool clip_llava_handle_patches(clip_ctx*, std::vector<float*>&, clip_image_grid_shape, float*, int*)’:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:160:9: error: ‘memcpy’ was not declared in this scope
  160 |         memcpy((uint8_t *)(image_features->data) + offset, image_embd_v[i], clip_embd_nbytes(ctx_clip));
      |         ^~~~~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:14:1: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
   13 | #include <numeric>
  +++ |+#include <cstring>
   14 | 
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:192:5: error: ‘memcpy’ was not declared in this scope
  192 |     memcpy(image_embd_out, image_embd_v[0], clip_embd_nbytes(ctx_clip)); // main image as global context
      |     ^~~~~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:192:5: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
/home/ggml/work/llama.cpp/examples/llava/llava.cpp: In function ‘bool encode_image_with_clip(clip_ctx*, int, const clip_image_u8*, float*, int*)’:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:282:18: error: ‘memcpy’ is not a member of ‘std’; did you mean ‘wmemcpy’?
  282 |             std::memcpy(image_embd + n_img_pos_out * clip_n_mmproj_embd(ctx_clip), image_embd_v[i], clip_embd_nbytes(ctx_clip));
      |                  ^~~~~~
      |                  wmemcpy
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:295:14: error: ‘strcmp’ was not declared in this scope
  295 |     else if (strcmp(mm_patch_merge_type, "spatial_unpad") != 0) {
      |              ^~~~~~
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:295:14: note: ‘strcmp’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
In file included from /home/ggml/work/llama.cpp/examples/llava/llava.cpp:5:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp: In function ‘bool load_file_to_bytes(const char*, unsigned char**, long int*)’:
/home/ggml/work/llama.cpp/examples/llava/llava.cpp:460:35: error: ‘strerror’ was not declared in this scope; did you mean ‘stderr’?
  460 |         die_fmt("read error: %s", strerror(errno));
      |                                   ^~~~~~~~
/home/ggml/work/llama.cpp/examples/llava/../../common/common.h:18:68: note: in definition of macro ‘die_fmt’
   18 | #define die_fmt(fmt, ...) do { fprintf(stderr, "error: " fmt "\n", __VA_ARGS__); exit(1); } while (0)
      |                                                                    ^~~~~~~~~~~
make[2]: *** [examples/llava/CMakeFiles/llava.dir/build.make:76: examples/llava/CMakeFiles/llava.dir/llava.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:2830: examples/llava/CMakeFiles/llava.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 18%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 18%] Linking CXX executable ../../bin/llama-bench-matmult
[ 18%] Built target llama-bench-matmult
[ 18%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 19%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-quantize-stats
[ 20%] Built target llama-quantize-stats
[ 20%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
/home/ggml/work/llama.cpp/common/log.cpp:43:10: error: ‘vector’ in namespace ‘std’ does not name a template type
   43 |     std::vector<char> msg;
      |          ^~~~~~
/home/ggml/work/llama.cpp/common/log.cpp:7:1: note: ‘std::vector’ is defined in header ‘<vector>’; did you forget to ‘#include <vector>’?
    6 | #include <thread>
  +++ |+#include <vector>
    7 | 
/home/ggml/work/llama.cpp/common/log.cpp: In member function ‘void gpt_log_entry::print(FILE*) const’:
/home/ggml/work/llama.cpp/common/log.cpp:84:29: error: ‘msg’ was not declared in this scope
   84 |         fprintf(fcur, "%s", msg.data());
      |                             ^~~
/home/ggml/work/llama.cpp/common/log.cpp: At global scope:
/home/ggml/work/llama.cpp/common/log.cpp:127:10: error: ‘vector’ in namespace ‘std’ does not name a template type
  127 |     std::vector<gpt_log_entry> entries;
      |          ^~~~~~
/home/ggml/work/llama.cpp/common/log.cpp:127:5: note: ‘std::vector’ is defined in header ‘<vector>’; did you forget to ‘#include <vector>’?
  127 |     std::vector<gpt_log_entry> entries;
      |     ^~~
/home/ggml/work/llama.cpp/common/log.cpp: In constructor ‘gpt_log::gpt_log(size_t)’:
/home/ggml/work/llama.cpp/common/log.cpp:100:9: error: ‘entries’ was not declared in this scope
  100 |         entries.resize(capacity);
      |         ^~~~~~~
/home/ggml/work/llama.cpp/common/log.cpp: In member function ‘void gpt_log::add(ggml_log_level, const char*, va_list)’:
/home/ggml/work/llama.cpp/common/log.cpp:142:24: error: ‘entries’ was not declared in this scope; did you mean ‘entry’?
  142 |         auto & entry = entries[tail];
      |                        ^~~~~~~
      |                        entry
/home/ggml/work/llama.cpp/common/log.cpp:182:18: error: ‘vector’ is not a member of ‘std’
  182 |             std::vector<gpt_log_entry> new_entries(2*entries.size());
      |                  ^~~~~~
/home/ggml/work/llama.cpp/common/log.cpp:182:18: note: ‘std::vector’ is defined in header ‘<vector>’; did you forget to ‘#include <vector>’?
/home/ggml/work/llama.cpp/common/log.cpp:182:38: error: expected primary-expression before ‘>’ token
  182 |             std::vector<gpt_log_entry> new_entries(2*entries.size());
      |                                      ^
/home/ggml/work/llama.cpp/common/log.cpp:182:40: error: ‘new_entries’ was not declared in this scope
  182 |             std::vector<gpt_log_entry> new_entries(2*entries.size());
      |                                        ^~~~~~~~~~~
/home/ggml/work/llama.cpp/common/log.cpp: In lambda function:
/home/ggml/work/llama.cpp/common/log.cpp:217:27: error: ‘entries’ was not declared in this scope
  217 |                     cur = entries[head];
      |                           ^~~~~~~
/home/ggml/work/llama.cpp/common/log.cpp: In member function ‘void gpt_log::pause()’:
/home/ggml/work/llama.cpp/common/log.cpp:245:28: error: ‘entries’ was not declared in this scope; did you mean ‘entry’?
  245 |             auto & entry = entries[tail];
      |                            ^~~~~~~
      |                            entry
/home/ggml/work/llama.cpp/common/log.cpp: In function ‘void gpt_log_add(gpt_log*, ggml_log_level, const char*, ...)’:
/home/ggml/work/llama.cpp/common/log.cpp:303:5: error: ‘va_start’ was not declared in this scope
  303 |     va_start(args, fmt);
      |     ^~~~~~~~
/home/ggml/work/llama.cpp/common/log.cpp:305:5: error: ‘va_end’ was not declared in this scope
  305 |     va_end(args);
      |     ^~~~~~
make[2]: *** [common/CMakeFiles/common.dir/build.make:132: common/CMakeFiles/common.dir/log.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/Makefile2:1687: common/CMakeFiles/common.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m22.666s
user	0m45.340s
sys	0m4.189s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/54/49c1720d0214d33766ede463bea1aee5851d2c/ggml-1-arm64-cpu-low-perf/ctest_debug-ctest.log: No such file or directory
