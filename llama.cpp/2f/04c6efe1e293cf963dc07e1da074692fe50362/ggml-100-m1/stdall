Requirement already satisfied: numpy~=1.24.4 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.36.2)
Requirement already satisfied: gguf>=0.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.6.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.2)
Requirement already satisfied: torch~=2.1.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.1)
Requirement already satisfied: requests in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: filelock in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: packaging>=20.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)
Requirement already satisfied: regex!=2019.12.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)
Requirement already satisfied: safetensors>=0.3.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.1)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.2)
Requirement already satisfied: networkx in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)
Requirement already satisfied: jinja2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: typing-extensions in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: fsspec in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.12.2)
Requirement already satisfied: sympy in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from jinja2->torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.4)
Requirement already satisfied: certifi>=2017.4.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.11.17)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.1.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)
Requirement already satisfied: mpmath>=0.19 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.9/site-packages (from sympy->torch~=2.1.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
ERROR: File "setup.py" or "setup.cfg" not found. Directory cannot be installed in editable mode: /Users/ggml/work/llama.cpp/gguf-py
(A "pyproject.toml" file was found, but editable mode currently requires a setuptools-based build.)
+ gg_run_ctest_debug
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /Users/ggml/results/llama.cpp/2f/04c6efe1e293cf963dc07e1da074692fe50362/ggml-100-m1/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/2f/04c6efe1e293cf963dc07e1da074692fe50362/ggml-100-m1/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_METAL_SHADER_DEBUG=ON ..
-- The C compiler identification is AppleClang 15.0.0.15000100
-- The CXX compiler identification is AppleClang 15.0.0.15000100
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.3 (Apple Git-145)") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Accelerate framework found
-- Metal framework found
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done (1.0s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-debug

real	0m1.221s
user	0m0.408s
sys	0m0.747s
+ tee -a /Users/ggml/results/llama.cpp/2f/04c6efe1e293cf963dc07e1da074692fe50362/ggml-100-m1/ctest_debug-make.log
+ make -j
[  1%] Generating build details from Git
[  2%] Compiling Metal kernels
[  5%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  6%] Building C object CMakeFiles/ggml.dir/ggml-metal.m.o
-- Found Git: /usr/bin/git (found version "2.39.3 (Apple Git-145)") 
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
/Users/ggml/work/llama.cpp/ggml-backend.c:1414:1: warning: non-void function does not return a value in all control paths [-Wreturn-type]
}
^
[  6%] Built target build_info
1 warning generated.
[  6%] Built target ggml
[  8%] Linking C static library libggml_static.a
[  8%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  8%] Built target ggml_static
[  8%] Built target ggml-metal
/Users/ggml/work/llama.cpp/llama.cpp:7003:29: error: use of undeclared identifier 'ggml_tallocr_is_measure'
    const bool worst_case = ggml_tallocr_is_measure(lctx.alloc);
                            ^
/Users/ggml/work/llama.cpp/llama.cpp:7029:10: error: use of undeclared identifier 'ggml_tallocr_is_measure'
    if (!ggml_tallocr_is_measure(lctx.alloc)) {
         ^
/Users/ggml/work/llama.cpp/llama.cpp:10844:26: error: use of undeclared identifier 'ggml_backend_sched_get_tallocr'; did you mean 'ggml_backend_sched_reserve'?
            ctx->alloc = ggml_backend_sched_get_tallocr(ctx->sched, ctx->backend_cpu);
                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                         ggml_backend_sched_reserve
/Users/ggml/work/llama.cpp/./ggml-backend.h:163:36: note: 'ggml_backend_sched_reserve' declared here
    GGML_API bool                  ggml_backend_sched_reserve(ggml_backend_sched_t sched, struct ggml_cgraph * measure_graph);
                                   ^
/Users/ggml/work/llama.cpp/llama.cpp:10844:69: error: cannot initialize a parameter of type 'struct ggml_cgraph *' with an lvalue of type 'ggml_backend_t' (aka 'ggml_backend *')
            ctx->alloc = ggml_backend_sched_get_tallocr(ctx->sched, ctx->backend_cpu);
                                                                    ^~~~~~~~~~~~~~~~
/Users/ggml/work/llama.cpp/./ggml-alloc.h:11:16: note: 'ggml_backend' is not defined, but forward declared here; conversion would be valid if it was derived from 'ggml_cgraph'
typedef struct ggml_backend * ggml_backend_t;
               ^
/Users/ggml/work/llama.cpp/./ggml-backend.h:163:112: note: passing argument to parameter 'measure_graph' here
    GGML_API bool                  ggml_backend_sched_reserve(ggml_backend_sched_t sched, struct ggml_cgraph * measure_graph);
                                                                                                               ^
/Users/ggml/work/llama.cpp/llama.cpp:10853:13: error: use of undeclared identifier 'ggml_backend_sched_init_measure'
            ggml_backend_sched_init_measure(ctx->sched, gf);
            ^
/Users/ggml/work/llama.cpp/llama.cpp:10854:26: error: use of undeclared identifier 'ggml_backend_sched_get_tallocr'; did you mean 'ggml_backend_sched_reserve'?
            ctx->alloc = ggml_backend_sched_get_tallocr(ctx->sched, ctx->backend_cpu);
                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                         ggml_backend_sched_reserve
/Users/ggml/work/llama.cpp/./ggml-backend.h:163:36: note: 'ggml_backend_sched_reserve' declared here
    GGML_API bool                  ggml_backend_sched_reserve(ggml_backend_sched_t sched, struct ggml_cgraph * measure_graph);
                                   ^
/Users/ggml/work/llama.cpp/llama.cpp:10854:69: error: cannot initialize a parameter of type 'struct ggml_cgraph *' with an lvalue of type 'ggml_backend_t' (aka 'ggml_backend *')
            ctx->alloc = ggml_backend_sched_get_tallocr(ctx->sched, ctx->backend_cpu);
                                                                    ^~~~~~~~~~~~~~~~
/Users/ggml/work/llama.cpp/./ggml-alloc.h:11:16: note: 'ggml_backend' is not defined, but forward declared here; conversion would be valid if it was derived from 'ggml_cgraph'
typedef struct ggml_backend * ggml_backend_t;
               ^
/Users/ggml/work/llama.cpp/./ggml-backend.h:163:112: note: passing argument to parameter 'measure_graph' here
    GGML_API bool                  ggml_backend_sched_reserve(ggml_backend_sched_t sched, struct ggml_cgraph * measure_graph);
                                                                                                               ^
/Users/ggml/work/llama.cpp/llama.cpp:10857:45: error: use of undeclared identifier 'ggml_backend_sched_get_buffer'; did you mean 'ggml_backend_sched_get_buffer_size'?
                ggml_backend_buffer_t buf = ggml_backend_sched_get_buffer(ctx->sched, backend);
                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                                            ggml_backend_sched_get_buffer_size
/Users/ggml/work/llama.cpp/./ggml-backend.h:167:36: note: 'ggml_backend_sched_get_buffer_size' declared here
    GGML_API size_t                ggml_backend_sched_get_buffer_size(ggml_backend_sched_t sched, ggml_backend_t backend);
                                   ^
/Users/ggml/work/llama.cpp/llama.cpp:10857:39: error: cannot initialize a variable of type 'ggml_backend_buffer_t' (aka 'ggml_backend_buffer *') with an rvalue of type 'size_t' (aka 'unsigned long')
                ggml_backend_buffer_t buf = ggml_backend_sched_get_buffer(ctx->sched, backend);
                                      ^     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
9 errors generated.
make[2]: *** [CMakeFiles/llama.dir/llama.cpp.o] Error 1
make[1]: *** [CMakeFiles/llama.dir/all] Error 2
make: *** [all] Error 2
+ cur=2
+ echo 2
+ set +x
cat: /Users/ggml/results/llama.cpp/2f/04c6efe1e293cf963dc07e1da074692fe50362/ggml-100-m1/ctest_debug-ctest.log: No such file or directory
