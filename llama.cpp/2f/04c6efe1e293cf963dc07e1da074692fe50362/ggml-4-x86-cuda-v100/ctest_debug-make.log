+ make -j
[  1%] Generating build details from Git
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Built target build_info
/home/ggml/work/llama.cpp/ggml-backend.c: In function ‘ggml_backend_sched_alloc_splits’:
/home/ggml/work/llama.cpp/ggml-backend.c:1414:1: warning: control reaches end of non-void function [-Wreturn-type]
 1414 | }
      | ^
/home/ggml/work/llama.cpp/ggml-cuda.cu(645): warning #177-D: function "warp_reduce_sum(half2)" was declared but never referenced
                                   half2 warp_reduce_sum(half2 a) {
                                         ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/home/ggml/work/llama.cpp/ggml-cuda.cu(666): warning #177-D: function "warp_reduce_max(half2)" was declared but never referenced
                                   half2 warp_reduce_max(half2 x) {
                                         ^

/home/ggml/work/llama.cpp/ggml-cuda.cu(1696): warning #177-D: variable "ksigns64" was declared but never referenced
                         uint64_t ksigns64[128] = {
                                  ^

[  5%] Built target ggml
[  6%] Linking CUDA static library libggml_static.a
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Built target ggml_static
/home/ggml/work/llama.cpp/llama.cpp: In function ‘ggml_cgraph* llama_build_graph(llama_context&, const llama_batch&)’:
/home/ggml/work/llama.cpp/llama.cpp:7003:29: error: ‘ggml_tallocr_is_measure’ was not declared in this scope; did you mean ‘ggml_tallocr_free’?
 7003 |     const bool worst_case = ggml_tallocr_is_measure(lctx.alloc);
      |                             ^~~~~~~~~~~~~~~~~~~~~~~
      |                             ggml_tallocr_free
/home/ggml/work/llama.cpp/llama.cpp: In function ‘llama_context* llama_new_context_with_model(llama_model*, llama_context_params)’:
/home/ggml/work/llama.cpp/llama.cpp:10844:26: error: ‘ggml_backend_sched_get_tallocr’ was not declared in this scope; did you mean ‘ggml_backend_sched_get_n_splits’?
10844 |             ctx->alloc = ggml_backend_sched_get_tallocr(ctx->sched, ctx->backend_cpu);
      |                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      |                          ggml_backend_sched_get_n_splits
/home/ggml/work/llama.cpp/llama.cpp:10853:13: error: ‘ggml_backend_sched_init_measure’ was not declared in this scope; did you mean ‘ggml_backend_sched_reserve’?
10853 |             ggml_backend_sched_init_measure(ctx->sched, gf);
      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      |             ggml_backend_sched_reserve
/home/ggml/work/llama.cpp/llama.cpp:10857:45: error: ‘ggml_backend_sched_get_buffer’ was not declared in this scope; did you mean ‘ggml_backend_sched_get_buffer_size’?
10857 |                 ggml_backend_buffer_t buf = ggml_backend_sched_get_buffer(ctx->sched, backend);
      |                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      |                                             ggml_backend_sched_get_buffer_size
make[2]: *** [CMakeFiles/llama.dir/build.make:76: CMakeFiles/llama.dir/llama.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:772: CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m49.161s
user	0m50.516s
sys	0m1.836s
