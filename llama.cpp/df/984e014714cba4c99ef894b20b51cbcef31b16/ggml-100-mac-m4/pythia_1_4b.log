Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.594s
user	0m0.870s
sys	0m1.237s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-barrier
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-backend-ops
[ 63%] Built target test-autorelease
[ 63%] Built target test-quantize-fns
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Built target test-rope
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Built target llama-lookahead
[ 78%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup
[ 81%] Generating loading.html.hpp
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-cli
[ 82%] Built target llama-parallel
[ 82%] Built target llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-passkey
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-save-load-state
[ 93%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 93%] Built target llama-speculative
[ 93%] Built target llama-run
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.052s
user	0m6.150s
sys	0m9.473s

main: quantize time =  2857.49 ms
main:    total time =  2857.49 ms

main: quantize time =  1626.02 ms
main:    total time =  1626.02 ms

main: quantize time =  2539.59 ms
main:    total time =  2539.59 ms

main: quantize time =  3358.74 ms
main:    total time =  3358.74 ms

main: quantize time =  3094.15 ms
main:    total time =  3094.15 ms

main: quantize time =  5924.92 ms
main:    total time =  5924.92 ms

main: quantize time =  5556.16 ms
main:    total time =  5556.16 ms

main: quantize time =  6894.62 ms
main:    total time =  6894.62 ms

main: quantize time =  5830.58 ms
main:    total time =  5830.58 ms

main: quantize time =  4395.54 ms
main:    total time =  4395.54 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.141 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.328 I main: llama backend init
0.00.000.335 I main: load the model and apply lora adapter, if any
0.00.044.100 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.056.558 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.056.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.056.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.056.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.056.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.056.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.056.581 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.056.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.056.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.056.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.056.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.056.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.056.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.056.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.056.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.056.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.056.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.063.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.065.670 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.074.797 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.074.804 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.074.805 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.074.805 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.074.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.074.807 I llama_model_loader: - type  f32:  194 tensors
0.00.074.807 I llama_model_loader: - type  f16:   98 tensors
0.00.074.809 I print_info: file format = GGUF V3 (latest)
0.00.074.810 I print_info: file type   = all F32 (guessed)
0.00.074.812 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.814 I load: special tokens cache size = 25
0.00.114.406 I load: token to piece cache size = 0.2984 MB
0.00.114.410 I print_info: arch             = gptneox
0.00.114.410 I print_info: vocab_only       = 0
0.00.114.410 I print_info: n_ctx_train      = 2048
0.00.114.410 I print_info: n_embd           = 2048
0.00.114.410 I print_info: n_layer          = 24
0.00.114.414 I print_info: n_head           = 16
0.00.114.415 I print_info: n_head_kv        = 16
0.00.114.415 I print_info: n_rot            = 32
0.00.114.415 I print_info: n_swa            = 0
0.00.114.415 I print_info: n_embd_head_k    = 128
0.00.114.418 I print_info: n_embd_head_v    = 128
0.00.114.418 I print_info: n_gqa            = 1
0.00.114.419 I print_info: n_embd_k_gqa     = 2048
0.00.114.420 I print_info: n_embd_v_gqa     = 2048
0.00.114.420 I print_info: f_norm_eps       = 1.0e-05
0.00.114.421 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.114.421 I print_info: f_clamp_kqv      = 0.0e+00
0.00.114.421 I print_info: f_max_alibi_bias = 0.0e+00
0.00.114.421 I print_info: f_logit_scale    = 0.0e+00
0.00.114.422 I print_info: n_ff             = 8192
0.00.114.422 I print_info: n_expert         = 0
0.00.114.422 I print_info: n_expert_used    = 0
0.00.114.422 I print_info: causal attn      = 1
0.00.114.422 I print_info: pooling type     = 0
0.00.114.423 I print_info: rope type        = 2
0.00.114.423 I print_info: rope scaling     = linear
0.00.114.423 I print_info: freq_base_train  = 10000.0
0.00.114.423 I print_info: freq_scale_train = 1
0.00.114.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.114.424 I print_info: rope_finetuned   = unknown
0.00.114.424 I print_info: ssm_d_conv       = 0
0.00.114.424 I print_info: ssm_d_inner      = 0
0.00.114.424 I print_info: ssm_d_state      = 0
0.00.114.424 I print_info: ssm_dt_rank      = 0
0.00.114.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.114.425 I print_info: model type       = 1.4B
0.00.114.425 I print_info: model params     = 1.41 B
0.00.114.425 I print_info: general.name     = 1.4B
0.00.114.426 I print_info: vocab type       = BPE
0.00.114.426 I print_info: n_vocab          = 50304
0.00.114.426 I print_info: n_merges         = 50009
0.00.114.426 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.114.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.114.427 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.114.427 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.114.427 I print_info: LF token         = 128 'Ä'
0.00.114.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.114.428 I print_info: max token length = 1024
0.00.151.363 I load_tensors: offloading 24 repeating layers to GPU
0.00.151.367 I load_tensors: offloading output layer to GPU
0.00.151.367 I load_tensors: offloaded 25/25 layers to GPU
0.00.151.392 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.151.394 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.151.894 I llama_init_from_model: n_seq_max     = 1
0.00.151.895 I llama_init_from_model: n_ctx         = 2048
0.00.151.895 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.151.896 I llama_init_from_model: n_batch       = 2048
0.00.151.896 I llama_init_from_model: n_ubatch      = 512
0.00.151.896 I llama_init_from_model: flash_attn    = 0
0.00.151.896 I llama_init_from_model: freq_base     = 10000.0
0.00.151.896 I llama_init_from_model: freq_scale    = 1
0.00.151.897 I ggml_metal_init: allocating
0.00.151.922 I ggml_metal_init: found device: Apple M4
0.00.151.928 I ggml_metal_init: picking default device: Apple M4
0.00.152.504 I ggml_metal_init: using embedded metal library
0.00.319.640 I ggml_metal_init: GPU name:   Apple M4
0.00.319.660 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.319.661 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.319.662 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.319.663 I ggml_metal_init: simdgroup reduction   = true
0.00.319.663 I ggml_metal_init: simdgroup matrix mul. = true
0.00.319.663 I ggml_metal_init: has residency sets    = true
0.00.319.664 I ggml_metal_init: has bfloat            = true
0.00.319.664 I ggml_metal_init: use bfloat            = true
0.00.319.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.319.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.605 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.376.014 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.376.022 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.376.045 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.380.857 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.380.861 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.380.861 I llama_init_from_model: graph nodes  = 967
0.00.380.861 I llama_init_from_model: graph splits = 2
0.00.380.867 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.380.979 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.380.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.447.722 I main: llama threadpool init, n_threads = 4
0.00.447.762 I 
0.00.447.797 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.447.799 I 
0.00.448.039 I sampler seed: 1234
0.00.448.044 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.448.068 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.448.070 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.448.070 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.293.883 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.02.293.884 I llama_perf_context_print:        load time =     402.41 ms
0.02.293.885 I llama_perf_context_print: prompt eval time =      54.30 ms /     7 tokens (    7.76 ms per token,   128.92 tokens per second)
0.02.293.889 I llama_perf_context_print:        eval time =    1788.60 ms /    63 runs   (   28.39 ms per token,    35.22 tokens per second)
0.02.293.891 I llama_perf_context_print:       total time =    1847.37 ms /    70 tokens
0.02.294.123 I ggml_metal_free: deallocating

real	0m2.595s
user	0m0.152s
sys	0m0.136s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.014.485 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.777 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.790 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.792 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.792 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.793 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.795 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.795 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.795 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.703 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.743 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.585 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.586 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.586 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.587 I llama_model_loader: - type  f32:  194 tensors
0.00.038.588 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.588 I print_info: file format = GGUF V3 (latest)
0.00.038.589 I print_info: file type   = Q8_0
0.00.038.590 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.061.776 I load: special tokens cache size = 25
0.00.069.343 I load: token to piece cache size = 0.2984 MB
0.00.069.346 I print_info: arch             = gptneox
0.00.069.347 I print_info: vocab_only       = 0
0.00.069.347 I print_info: n_ctx_train      = 2048
0.00.069.347 I print_info: n_embd           = 2048
0.00.069.347 I print_info: n_layer          = 24
0.00.069.351 I print_info: n_head           = 16
0.00.069.352 I print_info: n_head_kv        = 16
0.00.069.352 I print_info: n_rot            = 32
0.00.069.354 I print_info: n_swa            = 0
0.00.069.355 I print_info: n_embd_head_k    = 128
0.00.069.355 I print_info: n_embd_head_v    = 128
0.00.069.355 I print_info: n_gqa            = 1
0.00.069.356 I print_info: n_embd_k_gqa     = 2048
0.00.069.357 I print_info: n_embd_v_gqa     = 2048
0.00.069.357 I print_info: f_norm_eps       = 1.0e-05
0.00.069.358 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.358 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.358 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.358 I print_info: f_logit_scale    = 0.0e+00
0.00.069.359 I print_info: n_ff             = 8192
0.00.069.359 I print_info: n_expert         = 0
0.00.069.359 I print_info: n_expert_used    = 0
0.00.069.359 I print_info: causal attn      = 1
0.00.069.359 I print_info: pooling type     = 0
0.00.069.360 I print_info: rope type        = 2
0.00.069.360 I print_info: rope scaling     = linear
0.00.069.360 I print_info: freq_base_train  = 10000.0
0.00.069.361 I print_info: freq_scale_train = 1
0.00.069.361 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.361 I print_info: rope_finetuned   = unknown
0.00.069.361 I print_info: ssm_d_conv       = 0
0.00.069.361 I print_info: ssm_d_inner      = 0
0.00.069.361 I print_info: ssm_d_state      = 0
0.00.069.361 I print_info: ssm_dt_rank      = 0
0.00.069.362 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.362 I print_info: model type       = 1.4B
0.00.069.362 I print_info: model params     = 1.41 B
0.00.069.362 I print_info: general.name     = 1.4B
0.00.069.363 I print_info: vocab type       = BPE
0.00.069.363 I print_info: n_vocab          = 50304
0.00.069.363 I print_info: n_merges         = 50009
0.00.069.363 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.363 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.364 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.364 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.364 I print_info: LF token         = 128 'Ä'
0.00.069.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.364 I print_info: max token length = 1024
0.01.279.197 I load_tensors: offloading 24 repeating layers to GPU
0.01.279.202 I load_tensors: offloading output layer to GPU
0.01.279.203 I load_tensors: offloaded 25/25 layers to GPU
0.01.279.229 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.279.231 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.280.486 I llama_init_from_model: n_seq_max     = 1
0.01.280.489 I llama_init_from_model: n_ctx         = 2048
0.01.280.489 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.280.489 I llama_init_from_model: n_batch       = 2048
0.01.280.490 I llama_init_from_model: n_ubatch      = 512
0.01.280.490 I llama_init_from_model: flash_attn    = 0
0.01.280.491 I llama_init_from_model: freq_base     = 10000.0
0.01.280.491 I llama_init_from_model: freq_scale    = 1
0.01.280.493 I ggml_metal_init: allocating
0.01.280.538 I ggml_metal_init: found device: Apple M4
0.01.280.546 I ggml_metal_init: picking default device: Apple M4
0.01.281.767 I ggml_metal_init: using embedded metal library
0.01.289.317 I ggml_metal_init: GPU name:   Apple M4
0.01.289.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.289.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.289.323 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.289.324 I ggml_metal_init: simdgroup reduction   = true
0.01.289.324 I ggml_metal_init: simdgroup matrix mul. = true
0.01.289.324 I ggml_metal_init: has residency sets    = true
0.01.289.325 I ggml_metal_init: has bfloat            = true
0.01.289.325 I ggml_metal_init: use bfloat            = true
0.01.289.326 I ggml_metal_init: hasUnifiedMemory      = true
0.01.289.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.305.313 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.353.925 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.353.931 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.353.952 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.358.091 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.358.093 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.358.093 I llama_init_from_model: graph nodes  = 967
0.01.358.093 I llama_init_from_model: graph splits = 2
0.01.358.100 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.358.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.358.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.412.699 I main: llama threadpool init, n_threads = 4
0.01.412.740 I 
0.01.412.764 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.412.764 I 
0.01.412.986 I sampler seed: 1234
0.01.412.990 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.413.031 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.413.033 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.413.033 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.518.536 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48464.16 tokens per second)
0.02.518.541 I llama_perf_context_print:        load time =    1397.34 ms
0.02.518.542 I llama_perf_context_print: prompt eval time =      49.33 ms /     7 tokens (    7.05 ms per token,   141.91 tokens per second)
0.02.518.544 I llama_perf_context_print:        eval time =    1053.67 ms /    63 runs   (   16.72 ms per token,    59.79 tokens per second)
0.02.518.545 I llama_perf_context_print:       total time =    1106.71 ms /    70 tokens
0.02.518.802 I ggml_metal_free: deallocating

real	0m2.539s
user	0m0.125s
sys	0m0.263s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.012.483 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.857 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.865 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.866 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.866 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.867 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.868 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.871 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.871 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.872 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.872 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.876 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.829 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.857 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.821 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.824 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.824 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.825 I llama_model_loader: - type  f32:  194 tensors
0.00.038.825 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.825 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.826 I print_info: file format = GGUF V3 (latest)
0.00.038.826 I print_info: file type   = Q4_0
0.00.038.827 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.241 I load: special tokens cache size = 25
0.00.069.063 I load: token to piece cache size = 0.2984 MB
0.00.069.066 I print_info: arch             = gptneox
0.00.069.067 I print_info: vocab_only       = 0
0.00.069.067 I print_info: n_ctx_train      = 2048
0.00.069.067 I print_info: n_embd           = 2048
0.00.069.067 I print_info: n_layer          = 24
0.00.069.071 I print_info: n_head           = 16
0.00.069.071 I print_info: n_head_kv        = 16
0.00.069.072 I print_info: n_rot            = 32
0.00.069.072 I print_info: n_swa            = 0
0.00.069.072 I print_info: n_embd_head_k    = 128
0.00.069.072 I print_info: n_embd_head_v    = 128
0.00.069.073 I print_info: n_gqa            = 1
0.00.069.074 I print_info: n_embd_k_gqa     = 2048
0.00.069.074 I print_info: n_embd_v_gqa     = 2048
0.00.069.075 I print_info: f_norm_eps       = 1.0e-05
0.00.069.075 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.077 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.077 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.077 I print_info: f_logit_scale    = 0.0e+00
0.00.069.078 I print_info: n_ff             = 8192
0.00.069.078 I print_info: n_expert         = 0
0.00.069.078 I print_info: n_expert_used    = 0
0.00.069.078 I print_info: causal attn      = 1
0.00.069.078 I print_info: pooling type     = 0
0.00.069.078 I print_info: rope type        = 2
0.00.069.080 I print_info: rope scaling     = linear
0.00.069.082 I print_info: freq_base_train  = 10000.0
0.00.069.082 I print_info: freq_scale_train = 1
0.00.069.082 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.082 I print_info: rope_finetuned   = unknown
0.00.069.083 I print_info: ssm_d_conv       = 0
0.00.069.083 I print_info: ssm_d_inner      = 0
0.00.069.083 I print_info: ssm_d_state      = 0
0.00.069.083 I print_info: ssm_dt_rank      = 0
0.00.069.083 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.083 I print_info: model type       = 1.4B
0.00.069.084 I print_info: model params     = 1.41 B
0.00.069.084 I print_info: general.name     = 1.4B
0.00.069.084 I print_info: vocab type       = BPE
0.00.069.085 I print_info: n_vocab          = 50304
0.00.069.086 I print_info: n_merges         = 50009
0.00.069.087 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.087 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.087 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.087 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.087 I print_info: LF token         = 128 'Ä'
0.00.069.088 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.089 I print_info: max token length = 1024
0.00.612.053 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.069 I load_tensors: offloading output layer to GPU
0.00.612.069 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.103 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.612.104 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.613.354 I llama_init_from_model: n_seq_max     = 1
0.00.613.359 I llama_init_from_model: n_ctx         = 2048
0.00.613.360 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.613.360 I llama_init_from_model: n_batch       = 2048
0.00.613.361 I llama_init_from_model: n_ubatch      = 512
0.00.613.361 I llama_init_from_model: flash_attn    = 0
0.00.613.363 I llama_init_from_model: freq_base     = 10000.0
0.00.613.363 I llama_init_from_model: freq_scale    = 1
0.00.613.366 I ggml_metal_init: allocating
0.00.613.445 I ggml_metal_init: found device: Apple M4
0.00.613.459 I ggml_metal_init: picking default device: Apple M4
0.00.615.238 I ggml_metal_init: using embedded metal library
0.00.621.368 I ggml_metal_init: GPU name:   Apple M4
0.00.621.373 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.374 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.375 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.376 I ggml_metal_init: simdgroup reduction   = true
0.00.621.376 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.376 I ggml_metal_init: has residency sets    = true
0.00.621.377 I ggml_metal_init: has bfloat            = true
0.00.621.377 I ggml_metal_init: use bfloat            = true
0.00.621.378 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.380 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.068 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.403 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.698.409 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.698.434 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.702.898 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.702.900 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.702.900 I llama_init_from_model: graph nodes  = 967
0.00.702.900 I llama_init_from_model: graph splits = 2
0.00.702.907 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.022 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.023 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.585 I main: llama threadpool init, n_threads = 4
0.00.757.625 I 
0.00.757.648 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.649 I 
0.00.757.852 I sampler seed: 1234
0.00.757.856 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.757.867 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.757.867 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.757.867 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.447.395 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.447.396 I llama_perf_context_print:        load time =     744.24 ms
0.01.447.397 I llama_perf_context_print: prompt eval time =      46.96 ms /     7 tokens (    6.71 ms per token,   149.05 tokens per second)
0.01.447.397 I llama_perf_context_print:        eval time =     639.67 ms /    63 runs   (   10.15 ms per token,    98.49 tokens per second)
0.01.447.398 I llama_perf_context_print:       total time =     690.67 ms /    70 tokens
0.01.447.668 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.128s
sys	0m0.195s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.723 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.939 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.945 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.946 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.946 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.947 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.951 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.952 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.952 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.953 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.953 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.954 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.954 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.957 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.957 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.887 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.887 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.888 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.888 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.888 I llama_model_loader: - type  f32:  194 tensors
0.00.029.889 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.889 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.889 I print_info: file format = GGUF V3 (latest)
0.00.029.890 I print_info: file type   = Q4_1
0.00.029.891 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.049.911 I load: special tokens cache size = 25
0.00.055.961 I load: token to piece cache size = 0.2984 MB
0.00.055.964 I print_info: arch             = gptneox
0.00.055.964 I print_info: vocab_only       = 0
0.00.055.965 I print_info: n_ctx_train      = 2048
0.00.055.965 I print_info: n_embd           = 2048
0.00.055.965 I print_info: n_layer          = 24
0.00.055.968 I print_info: n_head           = 16
0.00.055.969 I print_info: n_head_kv        = 16
0.00.055.969 I print_info: n_rot            = 32
0.00.055.969 I print_info: n_swa            = 0
0.00.055.969 I print_info: n_embd_head_k    = 128
0.00.055.969 I print_info: n_embd_head_v    = 128
0.00.055.970 I print_info: n_gqa            = 1
0.00.055.971 I print_info: n_embd_k_gqa     = 2048
0.00.055.972 I print_info: n_embd_v_gqa     = 2048
0.00.055.972 I print_info: f_norm_eps       = 1.0e-05
0.00.055.973 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.973 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.973 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.973 I print_info: f_logit_scale    = 0.0e+00
0.00.055.974 I print_info: n_ff             = 8192
0.00.055.974 I print_info: n_expert         = 0
0.00.055.974 I print_info: n_expert_used    = 0
0.00.055.974 I print_info: causal attn      = 1
0.00.055.974 I print_info: pooling type     = 0
0.00.055.976 I print_info: rope type        = 2
0.00.055.978 I print_info: rope scaling     = linear
0.00.055.979 I print_info: freq_base_train  = 10000.0
0.00.055.979 I print_info: freq_scale_train = 1
0.00.055.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.979 I print_info: rope_finetuned   = unknown
0.00.055.979 I print_info: ssm_d_conv       = 0
0.00.055.980 I print_info: ssm_d_inner      = 0
0.00.055.980 I print_info: ssm_d_state      = 0
0.00.055.980 I print_info: ssm_dt_rank      = 0
0.00.055.980 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.980 I print_info: model type       = 1.4B
0.00.055.981 I print_info: model params     = 1.41 B
0.00.055.981 I print_info: general.name     = 1.4B
0.00.055.981 I print_info: vocab type       = BPE
0.00.055.981 I print_info: n_vocab          = 50304
0.00.055.981 I print_info: n_merges         = 50009
0.00.055.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.982 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.982 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.982 I print_info: LF token         = 128 'Ä'
0.00.055.984 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.984 I print_info: max token length = 1024
0.00.681.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.681.310 I load_tensors: offloading output layer to GPU
0.00.681.310 I load_tensors: offloaded 25/25 layers to GPU
0.00.681.345 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.681.346 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.682.854 I llama_init_from_model: n_seq_max     = 1
0.00.682.860 I llama_init_from_model: n_ctx         = 2048
0.00.682.860 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.682.861 I llama_init_from_model: n_batch       = 2048
0.00.682.861 I llama_init_from_model: n_ubatch      = 512
0.00.682.862 I llama_init_from_model: flash_attn    = 0
0.00.682.863 I llama_init_from_model: freq_base     = 10000.0
0.00.682.864 I llama_init_from_model: freq_scale    = 1
0.00.682.871 I ggml_metal_init: allocating
0.00.682.956 I ggml_metal_init: found device: Apple M4
0.00.682.979 I ggml_metal_init: picking default device: Apple M4
0.00.685.875 I ggml_metal_init: using embedded metal library
0.00.691.407 I ggml_metal_init: GPU name:   Apple M4
0.00.691.411 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.412 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.414 I ggml_metal_init: simdgroup reduction   = true
0.00.691.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.414 I ggml_metal_init: has residency sets    = true
0.00.691.415 I ggml_metal_init: has bfloat            = true
0.00.691.415 I ggml_metal_init: use bfloat            = true
0.00.691.416 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.417 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.711.144 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.770.583 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.770.590 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.770.613 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.774.674 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.774.676 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.774.676 I llama_init_from_model: graph nodes  = 967
0.00.774.676 I llama_init_from_model: graph splits = 2
0.00.774.683 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.774.827 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.774.828 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.830.334 I main: llama threadpool init, n_threads = 4
0.00.830.373 I 
0.00.830.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.830.402 I 
0.00.830.630 I sampler seed: 1234
0.00.830.634 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.830.645 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.830.645 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.830.645 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.566.913 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.566.914 I llama_perf_context_print:        load time =     820.74 ms
0.01.566.914 I llama_perf_context_print: prompt eval time =      47.68 ms /     7 tokens (    6.81 ms per token,   146.82 tokens per second)
0.01.566.915 I llama_perf_context_print:        eval time =     685.71 ms /    63 runs   (   10.88 ms per token,    91.88 tokens per second)
0.01.566.915 I llama_perf_context_print:       total time =     737.44 ms /    70 tokens
0.01.567.136 I ggml_metal_free: deallocating

real	0m1.591s
user	0m0.124s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.768 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.214 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.024.218 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.223 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.224 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.225 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.225 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.226 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.227 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.227 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.228 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.228 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.230 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.230 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.230 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.914 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.752 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.754 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.754 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.754 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.755 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.755 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.032.755 I llama_model_loader: - type  f32:  194 tensors
0.00.032.756 I llama_model_loader: - type q5_0:   97 tensors
0.00.032.756 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.756 I print_info: file format = GGUF V3 (latest)
0.00.032.757 I print_info: file type   = Q5_0
0.00.032.758 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.054.056 I load: special tokens cache size = 25
0.00.060.310 I load: token to piece cache size = 0.2984 MB
0.00.060.314 I print_info: arch             = gptneox
0.00.060.314 I print_info: vocab_only       = 0
0.00.060.314 I print_info: n_ctx_train      = 2048
0.00.060.314 I print_info: n_embd           = 2048
0.00.060.314 I print_info: n_layer          = 24
0.00.060.317 I print_info: n_head           = 16
0.00.060.318 I print_info: n_head_kv        = 16
0.00.060.318 I print_info: n_rot            = 32
0.00.060.318 I print_info: n_swa            = 0
0.00.060.319 I print_info: n_embd_head_k    = 128
0.00.060.319 I print_info: n_embd_head_v    = 128
0.00.060.322 I print_info: n_gqa            = 1
0.00.060.323 I print_info: n_embd_k_gqa     = 2048
0.00.060.324 I print_info: n_embd_v_gqa     = 2048
0.00.060.324 I print_info: f_norm_eps       = 1.0e-05
0.00.060.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.325 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.325 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.325 I print_info: f_logit_scale    = 0.0e+00
0.00.060.326 I print_info: n_ff             = 8192
0.00.060.326 I print_info: n_expert         = 0
0.00.060.326 I print_info: n_expert_used    = 0
0.00.060.326 I print_info: causal attn      = 1
0.00.060.326 I print_info: pooling type     = 0
0.00.060.327 I print_info: rope type        = 2
0.00.060.327 I print_info: rope scaling     = linear
0.00.060.327 I print_info: freq_base_train  = 10000.0
0.00.060.328 I print_info: freq_scale_train = 1
0.00.060.328 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.328 I print_info: rope_finetuned   = unknown
0.00.060.328 I print_info: ssm_d_conv       = 0
0.00.060.328 I print_info: ssm_d_inner      = 0
0.00.060.329 I print_info: ssm_d_state      = 0
0.00.060.329 I print_info: ssm_dt_rank      = 0
0.00.060.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.329 I print_info: model type       = 1.4B
0.00.060.330 I print_info: model params     = 1.41 B
0.00.060.330 I print_info: general.name     = 1.4B
0.00.060.330 I print_info: vocab type       = BPE
0.00.060.330 I print_info: n_vocab          = 50304
0.00.060.331 I print_info: n_merges         = 50009
0.00.060.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.331 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.331 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.331 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.332 I print_info: LF token         = 128 'Ä'
0.00.060.334 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.334 I print_info: max token length = 1024
0.00.776.968 I load_tensors: offloading 24 repeating layers to GPU
0.00.776.978 I load_tensors: offloading output layer to GPU
0.00.776.979 I load_tensors: offloaded 25/25 layers to GPU
0.00.777.011 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.777.013 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.778.408 I llama_init_from_model: n_seq_max     = 1
0.00.778.413 I llama_init_from_model: n_ctx         = 2048
0.00.778.413 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.778.414 I llama_init_from_model: n_batch       = 2048
0.00.778.414 I llama_init_from_model: n_ubatch      = 512
0.00.778.415 I llama_init_from_model: flash_attn    = 0
0.00.778.417 I llama_init_from_model: freq_base     = 10000.0
0.00.778.418 I llama_init_from_model: freq_scale    = 1
0.00.778.424 I ggml_metal_init: allocating
0.00.778.490 I ggml_metal_init: found device: Apple M4
0.00.778.504 I ggml_metal_init: picking default device: Apple M4
0.00.780.283 I ggml_metal_init: using embedded metal library
0.00.787.006 I ggml_metal_init: GPU name:   Apple M4
0.00.787.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.787.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.787.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.787.013 I ggml_metal_init: simdgroup reduction   = true
0.00.787.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.787.013 I ggml_metal_init: has residency sets    = true
0.00.787.013 I ggml_metal_init: has bfloat            = true
0.00.787.013 I ggml_metal_init: use bfloat            = true
0.00.787.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.787.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.805.193 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.862.629 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.862.636 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.862.661 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.867.614 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.867.616 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.867.616 I llama_init_from_model: graph nodes  = 967
0.00.867.616 I llama_init_from_model: graph splits = 2
0.00.867.623 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.867.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.867.761 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.928.753 I main: llama threadpool init, n_threads = 4
0.00.928.796 I 
0.00.928.818 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.928.819 I 
0.00.929.040 I sampler seed: 1234
0.00.929.045 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.929.098 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.929.100 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.929.101 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.724.515 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51374.82 tokens per second)
0.01.724.515 I llama_perf_context_print:        load time =     919.03 ms
0.01.724.516 I llama_perf_context_print: prompt eval time =      47.91 ms /     7 tokens (    6.84 ms per token,   146.10 tokens per second)
0.01.724.517 I llama_perf_context_print:        eval time =     744.49 ms /    63 runs   (   11.82 ms per token,    84.62 tokens per second)
0.01.724.517 I llama_perf_context_print:       total time =     796.71 ms /    70 tokens
0.01.724.743 I ggml_metal_free: deallocating

real	0m1.741s
user	0m0.124s
sys	0m0.214s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.863 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.321 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.327 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.332 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.333 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.336 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.336 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.336 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.337 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.337 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.338 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.340 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.340 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.052 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.777 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.778 I llama_model_loader: - type  f32:  194 tensors
0.00.025.778 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.778 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.779 I print_info: file format = GGUF V3 (latest)
0.00.025.779 I print_info: file type   = Q5_1
0.00.025.784 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.544 I load: special tokens cache size = 25
0.00.050.514 I load: token to piece cache size = 0.2984 MB
0.00.050.516 I print_info: arch             = gptneox
0.00.050.517 I print_info: vocab_only       = 0
0.00.050.517 I print_info: n_ctx_train      = 2048
0.00.050.517 I print_info: n_embd           = 2048
0.00.050.517 I print_info: n_layer          = 24
0.00.050.520 I print_info: n_head           = 16
0.00.050.521 I print_info: n_head_kv        = 16
0.00.050.521 I print_info: n_rot            = 32
0.00.050.521 I print_info: n_swa            = 0
0.00.050.521 I print_info: n_embd_head_k    = 128
0.00.050.522 I print_info: n_embd_head_v    = 128
0.00.050.522 I print_info: n_gqa            = 1
0.00.050.525 I print_info: n_embd_k_gqa     = 2048
0.00.050.526 I print_info: n_embd_v_gqa     = 2048
0.00.050.527 I print_info: f_norm_eps       = 1.0e-05
0.00.050.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.527 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.528 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.529 I print_info: f_logit_scale    = 0.0e+00
0.00.050.530 I print_info: n_ff             = 8192
0.00.050.530 I print_info: n_expert         = 0
0.00.050.530 I print_info: n_expert_used    = 0
0.00.050.531 I print_info: causal attn      = 1
0.00.050.532 I print_info: pooling type     = 0
0.00.050.532 I print_info: rope type        = 2
0.00.050.532 I print_info: rope scaling     = linear
0.00.050.532 I print_info: freq_base_train  = 10000.0
0.00.050.533 I print_info: freq_scale_train = 1
0.00.050.533 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.533 I print_info: rope_finetuned   = unknown
0.00.050.533 I print_info: ssm_d_conv       = 0
0.00.050.533 I print_info: ssm_d_inner      = 0
0.00.050.534 I print_info: ssm_d_state      = 0
0.00.050.534 I print_info: ssm_dt_rank      = 0
0.00.050.534 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.534 I print_info: model type       = 1.4B
0.00.050.535 I print_info: model params     = 1.41 B
0.00.050.535 I print_info: general.name     = 1.4B
0.00.050.535 I print_info: vocab type       = BPE
0.00.050.535 I print_info: n_vocab          = 50304
0.00.050.536 I print_info: n_merges         = 50009
0.00.050.536 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.536 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.536 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.536 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.537 I print_info: LF token         = 128 'Ä'
0.00.050.537 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.537 I print_info: max token length = 1024
0.00.611.424 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.438 I load_tensors: offloading output layer to GPU
0.00.611.439 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.471 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.611.476 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.612.870 I llama_init_from_model: n_seq_max     = 1
0.00.612.873 I llama_init_from_model: n_ctx         = 2048
0.00.612.873 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.612.874 I llama_init_from_model: n_batch       = 2048
0.00.612.875 I llama_init_from_model: n_ubatch      = 512
0.00.612.875 I llama_init_from_model: flash_attn    = 0
0.00.612.876 I llama_init_from_model: freq_base     = 10000.0
0.00.612.877 I llama_init_from_model: freq_scale    = 1
0.00.612.878 I ggml_metal_init: allocating
0.00.612.895 I ggml_metal_init: found device: Apple M4
0.00.612.905 I ggml_metal_init: picking default device: Apple M4
0.00.614.337 I ggml_metal_init: using embedded metal library
0.00.620.565 I ggml_metal_init: GPU name:   Apple M4
0.00.620.569 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.570 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.571 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.572 I ggml_metal_init: simdgroup reduction   = true
0.00.620.572 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.572 I ggml_metal_init: has residency sets    = true
0.00.620.573 I ggml_metal_init: has bfloat            = true
0.00.620.573 I ggml_metal_init: use bfloat            = true
0.00.620.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.924 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.017 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.693.024 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.693.051 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.697.264 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.697.266 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.697.267 I llama_init_from_model: graph nodes  = 967
0.00.697.267 I llama_init_from_model: graph splits = 2
0.00.697.274 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.697.406 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.697.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.355 I main: llama threadpool init, n_threads = 4
0.00.756.403 I 
0.00.756.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.431 I 
0.00.756.647 I sampler seed: 1234
0.00.756.651 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.671 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.671 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.671 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.602.254 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52090.98 tokens per second)
0.01.602.255 I llama_perf_context_print:        load time =     745.55 ms
0.01.602.256 I llama_perf_context_print: prompt eval time =      47.98 ms /     7 tokens (    6.85 ms per token,   145.91 tokens per second)
0.01.602.257 I llama_perf_context_print:        eval time =     794.60 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.602.257 I llama_perf_context_print:       total time =     846.84 ms /    70 tokens
0.01.602.484 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.120s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.728 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.654 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.660 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.662 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.662 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.663 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.663 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.663 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.664 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.665 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.665 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.666 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.666 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.667 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.668 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.669 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.669 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.355 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.061 I llama_model_loader: - type  f32:  194 tensors
0.00.025.061 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.062 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.062 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.062 I print_info: file format = GGUF V3 (latest)
0.00.025.063 I print_info: file type   = Q2_K - Medium
0.00.025.064 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.624 I load: special tokens cache size = 25
0.00.050.672 I load: token to piece cache size = 0.2984 MB
0.00.050.675 I print_info: arch             = gptneox
0.00.050.675 I print_info: vocab_only       = 0
0.00.050.676 I print_info: n_ctx_train      = 2048
0.00.050.676 I print_info: n_embd           = 2048
0.00.050.676 I print_info: n_layer          = 24
0.00.050.679 I print_info: n_head           = 16
0.00.050.680 I print_info: n_head_kv        = 16
0.00.050.680 I print_info: n_rot            = 32
0.00.050.680 I print_info: n_swa            = 0
0.00.050.680 I print_info: n_embd_head_k    = 128
0.00.050.681 I print_info: n_embd_head_v    = 128
0.00.050.681 I print_info: n_gqa            = 1
0.00.050.682 I print_info: n_embd_k_gqa     = 2048
0.00.050.683 I print_info: n_embd_v_gqa     = 2048
0.00.050.684 I print_info: f_norm_eps       = 1.0e-05
0.00.050.684 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.684 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.684 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.685 I print_info: f_logit_scale    = 0.0e+00
0.00.050.685 I print_info: n_ff             = 8192
0.00.050.685 I print_info: n_expert         = 0
0.00.050.686 I print_info: n_expert_used    = 0
0.00.050.686 I print_info: causal attn      = 1
0.00.050.686 I print_info: pooling type     = 0
0.00.050.686 I print_info: rope type        = 2
0.00.050.686 I print_info: rope scaling     = linear
0.00.050.687 I print_info: freq_base_train  = 10000.0
0.00.050.687 I print_info: freq_scale_train = 1
0.00.050.687 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.687 I print_info: rope_finetuned   = unknown
0.00.050.687 I print_info: ssm_d_conv       = 0
0.00.050.688 I print_info: ssm_d_inner      = 0
0.00.050.688 I print_info: ssm_d_state      = 0
0.00.050.688 I print_info: ssm_dt_rank      = 0
0.00.050.688 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.688 I print_info: model type       = 1.4B
0.00.050.689 I print_info: model params     = 1.41 B
0.00.050.689 I print_info: general.name     = 1.4B
0.00.050.690 I print_info: vocab type       = BPE
0.00.050.690 I print_info: n_vocab          = 50304
0.00.050.690 I print_info: n_merges         = 50009
0.00.050.690 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.692 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.692 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.692 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.694 I print_info: LF token         = 128 'Ä'
0.00.050.694 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.694 I print_info: max token length = 1024
0.00.354.668 I load_tensors: offloading 24 repeating layers to GPU
0.00.354.685 I load_tensors: offloading output layer to GPU
0.00.354.686 I load_tensors: offloaded 25/25 layers to GPU
0.00.354.724 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.354.726 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.356.242 I llama_init_from_model: n_seq_max     = 1
0.00.356.247 I llama_init_from_model: n_ctx         = 2048
0.00.356.247 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.356.248 I llama_init_from_model: n_batch       = 2048
0.00.356.248 I llama_init_from_model: n_ubatch      = 512
0.00.356.248 I llama_init_from_model: flash_attn    = 0
0.00.356.250 I llama_init_from_model: freq_base     = 10000.0
0.00.356.251 I llama_init_from_model: freq_scale    = 1
0.00.356.255 I ggml_metal_init: allocating
0.00.356.364 I ggml_metal_init: found device: Apple M4
0.00.356.377 I ggml_metal_init: picking default device: Apple M4
0.00.358.266 I ggml_metal_init: using embedded metal library
0.00.363.830 I ggml_metal_init: GPU name:   Apple M4
0.00.363.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.363.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.363.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.363.846 I ggml_metal_init: simdgroup reduction   = true
0.00.363.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.363.846 I ggml_metal_init: has residency sets    = true
0.00.363.847 I ggml_metal_init: has bfloat            = true
0.00.363.847 I ggml_metal_init: use bfloat            = true
0.00.363.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.363.854 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.384.668 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.441.534 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.441.551 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.441.573 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.446.016 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.446.019 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.446.019 I llama_init_from_model: graph nodes  = 967
0.00.446.019 I llama_init_from_model: graph splits = 2
0.00.446.025 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.446.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.446.159 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.259 I main: llama threadpool init, n_threads = 4
0.00.504.303 I 
0.00.504.332 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.504.334 I 
0.00.504.563 I sampler seed: 1234
0.00.504.568 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.504.613 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.504.617 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.504.617 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.180.032 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.180.032 I llama_perf_context_print:        load time =     494.62 ms
0.01.180.033 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.67 tokens per second)
0.01.180.034 I llama_perf_context_print:        eval time =     636.80 ms /    63 runs   (   10.11 ms per token,    98.93 tokens per second)
0.01.180.034 I llama_perf_context_print:       total time =     676.68 ms /    70 tokens
0.01.180.266 I ggml_metal_free: deallocating

real	0m1.197s
user	0m0.122s
sys	0m0.162s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.508 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.096 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.096 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.097 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.097 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.097 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.099 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.099 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.100 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.100 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.100 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.101 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.102 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.102 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.103 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.849 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.540 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.543 I llama_model_loader: - type  f32:  194 tensors
0.00.024.543 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.543 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.543 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.544 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.544 I print_info: file format = GGUF V3 (latest)
0.00.024.545 I print_info: file type   = Q3_K - Medium
0.00.024.546 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.430 I load: special tokens cache size = 25
0.00.049.241 I load: token to piece cache size = 0.2984 MB
0.00.049.243 I print_info: arch             = gptneox
0.00.049.244 I print_info: vocab_only       = 0
0.00.049.244 I print_info: n_ctx_train      = 2048
0.00.049.244 I print_info: n_embd           = 2048
0.00.049.244 I print_info: n_layer          = 24
0.00.049.247 I print_info: n_head           = 16
0.00.049.248 I print_info: n_head_kv        = 16
0.00.049.248 I print_info: n_rot            = 32
0.00.049.248 I print_info: n_swa            = 0
0.00.049.249 I print_info: n_embd_head_k    = 128
0.00.049.249 I print_info: n_embd_head_v    = 128
0.00.049.251 I print_info: n_gqa            = 1
0.00.049.252 I print_info: n_embd_k_gqa     = 2048
0.00.049.252 I print_info: n_embd_v_gqa     = 2048
0.00.049.253 I print_info: f_norm_eps       = 1.0e-05
0.00.049.253 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.253 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.253 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.254 I print_info: f_logit_scale    = 0.0e+00
0.00.049.254 I print_info: n_ff             = 8192
0.00.049.255 I print_info: n_expert         = 0
0.00.049.255 I print_info: n_expert_used    = 0
0.00.049.255 I print_info: causal attn      = 1
0.00.049.255 I print_info: pooling type     = 0
0.00.049.255 I print_info: rope type        = 2
0.00.049.256 I print_info: rope scaling     = linear
0.00.049.257 I print_info: freq_base_train  = 10000.0
0.00.049.257 I print_info: freq_scale_train = 1
0.00.049.257 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.257 I print_info: rope_finetuned   = unknown
0.00.049.258 I print_info: ssm_d_conv       = 0
0.00.049.258 I print_info: ssm_d_inner      = 0
0.00.049.258 I print_info: ssm_d_state      = 0
0.00.049.260 I print_info: ssm_dt_rank      = 0
0.00.049.261 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.261 I print_info: model type       = 1.4B
0.00.049.261 I print_info: model params     = 1.41 B
0.00.049.261 I print_info: general.name     = 1.4B
0.00.049.262 I print_info: vocab type       = BPE
0.00.049.262 I print_info: n_vocab          = 50304
0.00.049.262 I print_info: n_merges         = 50009
0.00.049.262 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.263 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.263 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.263 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.263 I print_info: LF token         = 128 'Ä'
0.00.049.267 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.267 I print_info: max token length = 1024
0.00.448.003 I load_tensors: offloading 24 repeating layers to GPU
0.00.448.019 I load_tensors: offloading output layer to GPU
0.00.448.019 I load_tensors: offloaded 25/25 layers to GPU
0.00.448.054 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.448.055 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.449.674 I llama_init_from_model: n_seq_max     = 1
0.00.449.678 I llama_init_from_model: n_ctx         = 2048
0.00.449.678 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.449.679 I llama_init_from_model: n_batch       = 2048
0.00.449.679 I llama_init_from_model: n_ubatch      = 512
0.00.449.679 I llama_init_from_model: flash_attn    = 0
0.00.449.686 I llama_init_from_model: freq_base     = 10000.0
0.00.449.696 I llama_init_from_model: freq_scale    = 1
0.00.449.700 I ggml_metal_init: allocating
0.00.449.802 I ggml_metal_init: found device: Apple M4
0.00.449.817 I ggml_metal_init: picking default device: Apple M4
0.00.451.718 I ggml_metal_init: using embedded metal library
0.00.457.372 I ggml_metal_init: GPU name:   Apple M4
0.00.457.387 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.457.388 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.457.389 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.457.390 I ggml_metal_init: simdgroup reduction   = true
0.00.457.390 I ggml_metal_init: simdgroup matrix mul. = true
0.00.457.390 I ggml_metal_init: has residency sets    = true
0.00.457.391 I ggml_metal_init: has bfloat            = true
0.00.457.391 I ggml_metal_init: use bfloat            = true
0.00.457.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.457.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.477.585 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.535.796 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.535.803 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.535.829 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.540.040 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.540.042 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.540.043 I llama_init_from_model: graph nodes  = 967
0.00.540.043 I llama_init_from_model: graph splits = 2
0.00.540.051 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.540.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.540.183 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.599.009 I main: llama threadpool init, n_threads = 4
0.00.599.053 I 
0.00.599.079 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.599.079 I 
0.00.599.310 I sampler seed: 1234
0.00.599.315 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.599.325 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.599.326 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.599.326 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.346.666 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.346.667 I llama_perf_context_print:        load time =     588.62 ms
0.01.346.668 I llama_perf_context_print: prompt eval time =      45.37 ms /     7 tokens (    6.48 ms per token,   154.28 tokens per second)
0.01.346.668 I llama_perf_context_print:        eval time =     698.93 ms /    63 runs   (   11.09 ms per token,    90.14 tokens per second)
0.01.346.669 I llama_perf_context_print:       total time =     748.54 ms /    70 tokens
0.01.346.877 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.122s
sys	0m0.183s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.685 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.159 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.169 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.170 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.172 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.899 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.607 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.608 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.608 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.608 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.609 I llama_model_loader: - type  f32:  194 tensors
0.00.024.609 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.609 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.610 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.610 I print_info: file format = GGUF V3 (latest)
0.00.024.610 I print_info: file type   = Q4_K - Medium
0.00.024.611 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.529 I load: special tokens cache size = 25
0.00.049.364 I load: token to piece cache size = 0.2984 MB
0.00.049.366 I print_info: arch             = gptneox
0.00.049.367 I print_info: vocab_only       = 0
0.00.049.367 I print_info: n_ctx_train      = 2048
0.00.049.367 I print_info: n_embd           = 2048
0.00.049.367 I print_info: n_layer          = 24
0.00.049.370 I print_info: n_head           = 16
0.00.049.371 I print_info: n_head_kv        = 16
0.00.049.371 I print_info: n_rot            = 32
0.00.049.372 I print_info: n_swa            = 0
0.00.049.372 I print_info: n_embd_head_k    = 128
0.00.049.372 I print_info: n_embd_head_v    = 128
0.00.049.373 I print_info: n_gqa            = 1
0.00.049.373 I print_info: n_embd_k_gqa     = 2048
0.00.049.374 I print_info: n_embd_v_gqa     = 2048
0.00.049.375 I print_info: f_norm_eps       = 1.0e-05
0.00.049.375 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.375 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.375 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.375 I print_info: f_logit_scale    = 0.0e+00
0.00.049.376 I print_info: n_ff             = 8192
0.00.049.376 I print_info: n_expert         = 0
0.00.049.377 I print_info: n_expert_used    = 0
0.00.049.377 I print_info: causal attn      = 1
0.00.049.377 I print_info: pooling type     = 0
0.00.049.377 I print_info: rope type        = 2
0.00.049.377 I print_info: rope scaling     = linear
0.00.049.378 I print_info: freq_base_train  = 10000.0
0.00.049.378 I print_info: freq_scale_train = 1
0.00.049.378 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.378 I print_info: rope_finetuned   = unknown
0.00.049.379 I print_info: ssm_d_conv       = 0
0.00.049.381 I print_info: ssm_d_inner      = 0
0.00.049.381 I print_info: ssm_d_state      = 0
0.00.049.381 I print_info: ssm_dt_rank      = 0
0.00.049.381 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.381 I print_info: model type       = 1.4B
0.00.049.382 I print_info: model params     = 1.41 B
0.00.049.382 I print_info: general.name     = 1.4B
0.00.049.383 I print_info: vocab type       = BPE
0.00.049.383 I print_info: n_vocab          = 50304
0.00.049.383 I print_info: n_merges         = 50009
0.00.049.383 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.383 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.384 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.384 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.384 I print_info: LF token         = 128 'Ä'
0.00.049.384 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.385 I print_info: max token length = 1024
0.00.531.551 I load_tensors: offloading 24 repeating layers to GPU
0.00.531.567 I load_tensors: offloading output layer to GPU
0.00.531.568 I load_tensors: offloaded 25/25 layers to GPU
0.00.531.601 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.531.613 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.533.060 I llama_init_from_model: n_seq_max     = 1
0.00.533.066 I llama_init_from_model: n_ctx         = 2048
0.00.533.067 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.533.067 I llama_init_from_model: n_batch       = 2048
0.00.533.067 I llama_init_from_model: n_ubatch      = 512
0.00.533.068 I llama_init_from_model: flash_attn    = 0
0.00.533.069 I llama_init_from_model: freq_base     = 10000.0
0.00.533.070 I llama_init_from_model: freq_scale    = 1
0.00.533.071 I ggml_metal_init: allocating
0.00.533.167 I ggml_metal_init: found device: Apple M4
0.00.533.181 I ggml_metal_init: picking default device: Apple M4
0.00.534.951 I ggml_metal_init: using embedded metal library
0.00.541.365 I ggml_metal_init: GPU name:   Apple M4
0.00.541.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.541.372 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.541.373 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.541.373 I ggml_metal_init: simdgroup reduction   = true
0.00.541.374 I ggml_metal_init: simdgroup matrix mul. = true
0.00.541.374 I ggml_metal_init: has residency sets    = true
0.00.541.374 I ggml_metal_init: has bfloat            = true
0.00.541.375 I ggml_metal_init: use bfloat            = true
0.00.541.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.541.386 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.559.675 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.499 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.615.505 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.615.527 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.087 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.620.089 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.620.090 I llama_init_from_model: graph nodes  = 967
0.00.620.090 I llama_init_from_model: graph splits = 2
0.00.620.097 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.620.221 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.620.222 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.665 I main: llama threadpool init, n_threads = 4
0.00.680.713 I 
0.00.680.739 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.740 I 
0.00.680.966 I sampler seed: 1234
0.00.680.971 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.680.991 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.680.991 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.680.992 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.444.600 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47843.67 tokens per second)
0.01.444.602 I llama_perf_context_print:        load time =     670.09 ms
0.01.444.603 I llama_perf_context_print: prompt eval time =      51.96 ms /     7 tokens (    7.42 ms per token,   134.71 tokens per second)
0.01.444.604 I llama_perf_context_print:        eval time =     708.53 ms /    63 runs   (   11.25 ms per token,    88.92 tokens per second)
0.01.444.604 I llama_perf_context_print:       total time =     764.83 ms /    70 tokens
0.01.444.890 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.120s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.512 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.058 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.062 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.067 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.803 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.444 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.445 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.445 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.445 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.446 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.446 I llama_model_loader: - type  f32:  194 tensors
0.00.023.447 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.447 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.448 I print_info: file format = GGUF V3 (latest)
0.00.023.448 I print_info: file type   = Q5_K - Medium
0.00.023.449 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.042.211 I load: special tokens cache size = 25
0.00.048.362 I load: token to piece cache size = 0.2984 MB
0.00.048.365 I print_info: arch             = gptneox
0.00.048.365 I print_info: vocab_only       = 0
0.00.048.365 I print_info: n_ctx_train      = 2048
0.00.048.366 I print_info: n_embd           = 2048
0.00.048.366 I print_info: n_layer          = 24
0.00.048.369 I print_info: n_head           = 16
0.00.048.370 I print_info: n_head_kv        = 16
0.00.048.370 I print_info: n_rot            = 32
0.00.048.370 I print_info: n_swa            = 0
0.00.048.370 I print_info: n_embd_head_k    = 128
0.00.048.370 I print_info: n_embd_head_v    = 128
0.00.048.371 I print_info: n_gqa            = 1
0.00.048.372 I print_info: n_embd_k_gqa     = 2048
0.00.048.372 I print_info: n_embd_v_gqa     = 2048
0.00.048.373 I print_info: f_norm_eps       = 1.0e-05
0.00.048.373 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.374 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.374 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.374 I print_info: f_logit_scale    = 0.0e+00
0.00.048.375 I print_info: n_ff             = 8192
0.00.048.375 I print_info: n_expert         = 0
0.00.048.375 I print_info: n_expert_used    = 0
0.00.048.375 I print_info: causal attn      = 1
0.00.048.375 I print_info: pooling type     = 0
0.00.048.378 I print_info: rope type        = 2
0.00.048.380 I print_info: rope scaling     = linear
0.00.048.380 I print_info: freq_base_train  = 10000.0
0.00.048.380 I print_info: freq_scale_train = 1
0.00.048.381 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.381 I print_info: rope_finetuned   = unknown
0.00.048.381 I print_info: ssm_d_conv       = 0
0.00.048.381 I print_info: ssm_d_inner      = 0
0.00.048.381 I print_info: ssm_d_state      = 0
0.00.048.381 I print_info: ssm_dt_rank      = 0
0.00.048.381 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.382 I print_info: model type       = 1.4B
0.00.048.382 I print_info: model params     = 1.41 B
0.00.048.382 I print_info: general.name     = 1.4B
0.00.048.387 I print_info: vocab type       = BPE
0.00.048.387 I print_info: n_vocab          = 50304
0.00.048.387 I print_info: n_merges         = 50009
0.00.048.388 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.388 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.388 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.388 I print_info: LF token         = 128 'Ä'
0.00.048.390 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.390 I print_info: max token length = 1024
0.00.602.679 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.695 I load_tensors: offloading output layer to GPU
0.00.602.695 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.728 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.602.736 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.604.256 I llama_init_from_model: n_seq_max     = 1
0.00.604.260 I llama_init_from_model: n_ctx         = 2048
0.00.604.260 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.604.261 I llama_init_from_model: n_batch       = 2048
0.00.604.261 I llama_init_from_model: n_ubatch      = 512
0.00.604.261 I llama_init_from_model: flash_attn    = 0
0.00.604.262 I llama_init_from_model: freq_base     = 10000.0
0.00.604.263 I llama_init_from_model: freq_scale    = 1
0.00.604.268 I ggml_metal_init: allocating
0.00.604.293 I ggml_metal_init: found device: Apple M4
0.00.604.303 I ggml_metal_init: picking default device: Apple M4
0.00.605.760 I ggml_metal_init: using embedded metal library
0.00.611.894 I ggml_metal_init: GPU name:   Apple M4
0.00.611.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.900 I ggml_metal_init: simdgroup reduction   = true
0.00.611.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.901 I ggml_metal_init: has residency sets    = true
0.00.611.901 I ggml_metal_init: has bfloat            = true
0.00.611.901 I ggml_metal_init: use bfloat            = true
0.00.611.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.904 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.076 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.130 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.685.140 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.685.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.868 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.689.869 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.689.870 I llama_init_from_model: graph nodes  = 967
0.00.689.870 I llama_init_from_model: graph splits = 2
0.00.689.876 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.690.004 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.690.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.915 I main: llama threadpool init, n_threads = 4
0.00.754.957 I 
0.00.754.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.982 I 
0.00.755.210 I sampler seed: 1234
0.00.755.215 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.236 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.237 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.237 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.604.925 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.604.925 I llama_perf_context_print:        load time =     745.51 ms
0.01.604.926 I llama_perf_context_print: prompt eval time =      51.23 ms /     7 tokens (    7.32 ms per token,   136.64 tokens per second)
0.01.604.926 I llama_perf_context_print:        eval time =     795.53 ms /    63 runs   (   12.63 ms per token,    79.19 tokens per second)
0.01.604.927 I llama_perf_context_print:       total time =     850.90 ms /    70 tokens
0.01.605.191 I ggml_metal_free: deallocating

real	0m1.623s
user	0m0.119s
sys	0m0.212s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.298 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.054 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.059 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.065 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.066 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.066 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.067 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.069 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.070 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.070 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.071 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.833 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.889 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.734 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.735 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.735 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.736 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.736 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.736 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.737 I llama_model_loader: - type  f32:  194 tensors
0.00.024.737 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.738 I print_info: file format = GGUF V3 (latest)
0.00.024.738 I print_info: file type   = Q6_K
0.00.024.739 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.171 I load: special tokens cache size = 25
0.00.050.176 I load: token to piece cache size = 0.2984 MB
0.00.050.180 I print_info: arch             = gptneox
0.00.050.180 I print_info: vocab_only       = 0
0.00.050.181 I print_info: n_ctx_train      = 2048
0.00.050.181 I print_info: n_embd           = 2048
0.00.050.181 I print_info: n_layer          = 24
0.00.050.185 I print_info: n_head           = 16
0.00.050.186 I print_info: n_head_kv        = 16
0.00.050.186 I print_info: n_rot            = 32
0.00.050.186 I print_info: n_swa            = 0
0.00.050.186 I print_info: n_embd_head_k    = 128
0.00.050.187 I print_info: n_embd_head_v    = 128
0.00.050.187 I print_info: n_gqa            = 1
0.00.050.188 I print_info: n_embd_k_gqa     = 2048
0.00.050.191 I print_info: n_embd_v_gqa     = 2048
0.00.050.191 I print_info: f_norm_eps       = 1.0e-05
0.00.050.192 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.192 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.192 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.192 I print_info: f_logit_scale    = 0.0e+00
0.00.050.193 I print_info: n_ff             = 8192
0.00.050.193 I print_info: n_expert         = 0
0.00.050.193 I print_info: n_expert_used    = 0
0.00.050.193 I print_info: causal attn      = 1
0.00.050.193 I print_info: pooling type     = 0
0.00.050.193 I print_info: rope type        = 2
0.00.050.194 I print_info: rope scaling     = linear
0.00.050.195 I print_info: freq_base_train  = 10000.0
0.00.050.195 I print_info: freq_scale_train = 1
0.00.050.195 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.196 I print_info: rope_finetuned   = unknown
0.00.050.196 I print_info: ssm_d_conv       = 0
0.00.050.196 I print_info: ssm_d_inner      = 0
0.00.050.196 I print_info: ssm_d_state      = 0
0.00.050.196 I print_info: ssm_dt_rank      = 0
0.00.050.196 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.196 I print_info: model type       = 1.4B
0.00.050.197 I print_info: model params     = 1.41 B
0.00.050.197 I print_info: general.name     = 1.4B
0.00.050.198 I print_info: vocab type       = BPE
0.00.050.198 I print_info: n_vocab          = 50304
0.00.050.199 I print_info: n_merges         = 50009
0.00.050.199 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.199 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.199 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.199 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.199 I print_info: LF token         = 128 'Ä'
0.00.050.200 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.200 I print_info: max token length = 1024
0.00.642.946 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.951 I load_tensors: offloading output layer to GPU
0.00.642.952 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.972 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.642.974 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.643.770 I llama_init_from_model: n_seq_max     = 1
0.00.643.777 I llama_init_from_model: n_ctx         = 2048
0.00.643.777 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.778 I llama_init_from_model: n_batch       = 2048
0.00.643.778 I llama_init_from_model: n_ubatch      = 512
0.00.643.778 I llama_init_from_model: flash_attn    = 0
0.00.643.780 I llama_init_from_model: freq_base     = 10000.0
0.00.643.780 I llama_init_from_model: freq_scale    = 1
0.00.643.782 I ggml_metal_init: allocating
0.00.643.832 I ggml_metal_init: found device: Apple M4
0.00.643.846 I ggml_metal_init: picking default device: Apple M4
0.00.644.873 I ggml_metal_init: using embedded metal library
0.00.649.051 I ggml_metal_init: GPU name:   Apple M4
0.00.649.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.060 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.061 I ggml_metal_init: simdgroup reduction   = true
0.00.649.062 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.062 I ggml_metal_init: has residency sets    = true
0.00.649.062 I ggml_metal_init: has bfloat            = true
0.00.649.063 I ggml_metal_init: use bfloat            = true
0.00.649.064 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.066 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.818 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.029 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.694.036 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.694.101 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.655 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.657 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.657 I llama_init_from_model: graph nodes  = 967
0.00.698.658 I llama_init_from_model: graph splits = 2
0.00.698.664 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.698.798 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.698.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.872 I main: llama threadpool init, n_threads = 4
0.00.766.918 I 
0.00.766.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.943 I 
0.00.767.168 I sampler seed: 1234
0.00.767.172 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.183 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.184 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.184 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.648.081 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.648.082 I llama_perf_context_print:        load time =     756.71 ms
0.01.648.083 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.67 tokens per second)
0.01.648.083 I llama_perf_context_print:        eval time =     823.45 ms /    63 runs   (   13.07 ms per token,    76.51 tokens per second)
0.01.648.084 I llama_perf_context_print:       total time =     882.07 ms /    70 tokens
0.01.648.311 I ggml_metal_free: deallocating

real	0m1.665s
user	0m0.114s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.586 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.823 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.751 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.760 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.762 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.764 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.764 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.765 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.768 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.769 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.769 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.699 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.125 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.126 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.127 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.127 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.129 I llama_model_loader: - type  f32:  194 tensors
0.00.055.129 I llama_model_loader: - type  f16:   98 tensors
0.00.055.130 I print_info: file format = GGUF V3 (latest)
0.00.055.130 I print_info: file type   = all F32 (guessed)
0.00.055.133 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.231 I load: special tokens cache size = 25
0.00.087.625 I load: token to piece cache size = 0.2984 MB
0.00.087.628 I print_info: arch             = gptneox
0.00.087.628 I print_info: vocab_only       = 0
0.00.087.628 I print_info: n_ctx_train      = 2048
0.00.087.628 I print_info: n_embd           = 2048
0.00.087.629 I print_info: n_layer          = 24
0.00.087.631 I print_info: n_head           = 16
0.00.087.632 I print_info: n_head_kv        = 16
0.00.087.633 I print_info: n_rot            = 32
0.00.087.633 I print_info: n_swa            = 0
0.00.087.633 I print_info: n_embd_head_k    = 128
0.00.087.633 I print_info: n_embd_head_v    = 128
0.00.087.634 I print_info: n_gqa            = 1
0.00.087.635 I print_info: n_embd_k_gqa     = 2048
0.00.087.635 I print_info: n_embd_v_gqa     = 2048
0.00.087.636 I print_info: f_norm_eps       = 1.0e-05
0.00.087.636 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.636 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.636 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.636 I print_info: f_logit_scale    = 0.0e+00
0.00.087.637 I print_info: n_ff             = 8192
0.00.087.637 I print_info: n_expert         = 0
0.00.087.637 I print_info: n_expert_used    = 0
0.00.087.637 I print_info: causal attn      = 1
0.00.087.638 I print_info: pooling type     = 0
0.00.087.638 I print_info: rope type        = 2
0.00.087.638 I print_info: rope scaling     = linear
0.00.087.638 I print_info: freq_base_train  = 10000.0
0.00.087.639 I print_info: freq_scale_train = 1
0.00.087.639 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.639 I print_info: rope_finetuned   = unknown
0.00.087.639 I print_info: ssm_d_conv       = 0
0.00.087.639 I print_info: ssm_d_inner      = 0
0.00.087.639 I print_info: ssm_d_state      = 0
0.00.087.639 I print_info: ssm_dt_rank      = 0
0.00.087.640 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.640 I print_info: model type       = 1.4B
0.00.087.640 I print_info: model params     = 1.41 B
0.00.087.640 I print_info: general.name     = 1.4B
0.00.087.641 I print_info: vocab type       = BPE
0.00.087.642 I print_info: n_vocab          = 50304
0.00.087.642 I print_info: n_merges         = 50009
0.00.087.642 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.644 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.644 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.644 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.645 I print_info: LF token         = 128 'Ä'
0.00.087.645 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.645 I print_info: max token length = 1024
0.01.466.925 I load_tensors: offloading 24 repeating layers to GPU
0.01.466.930 I load_tensors: offloading output layer to GPU
0.01.466.931 I load_tensors: offloaded 25/25 layers to GPU
0.01.466.956 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.466.957 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.467.814 I llama_init_from_model: n_seq_max     = 1
0.01.467.815 I llama_init_from_model: n_ctx         = 128
0.01.467.815 I llama_init_from_model: n_ctx_per_seq = 128
0.01.467.816 I llama_init_from_model: n_batch       = 128
0.01.467.816 I llama_init_from_model: n_ubatch      = 128
0.01.467.816 I llama_init_from_model: flash_attn    = 0
0.01.467.817 I llama_init_from_model: freq_base     = 10000.0
0.01.467.817 I llama_init_from_model: freq_scale    = 1
0.01.467.817 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.467.822 I ggml_metal_init: allocating
0.01.467.883 I ggml_metal_init: found device: Apple M4
0.01.467.890 I ggml_metal_init: picking default device: Apple M4
0.01.468.941 I ggml_metal_init: using embedded metal library
0.01.472.882 I ggml_metal_init: GPU name:   Apple M4
0.01.472.885 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.472.885 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.472.886 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.472.886 I ggml_metal_init: simdgroup reduction   = true
0.01.472.886 I ggml_metal_init: simdgroup matrix mul. = true
0.01.472.886 I ggml_metal_init: has residency sets    = true
0.01.472.887 I ggml_metal_init: has bfloat            = true
0.01.472.887 I ggml_metal_init: use bfloat            = true
0.01.472.887 I ggml_metal_init: hasUnifiedMemory      = true
0.01.472.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.483.693 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.485.431 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.485.433 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.485.447 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.487.061 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.487.062 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.487.063 I llama_init_from_model: graph nodes  = 967
0.01.487.063 I llama_init_from_model: graph splits = 2
0.01.487.064 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.487.064 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.521.928 I 
0.01.521.970 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.521.990 I perplexity: tokenizing the input ..
0.01.531.606 I perplexity: tokenization took 9.613 ms
0.01.531.628 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.650.050 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.651.383 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.651.397 I llama_perf_context_print:        load time =    1498.09 ms
0.01.651.399 I llama_perf_context_print: prompt eval time =     118.12 ms /   128 tokens (    0.92 ms per token,  1083.69 tokens per second)
0.01.651.399 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.651.400 I llama_perf_context_print:       total time =     129.47 ms /   129 tokens
0.01.651.782 I ggml_metal_free: deallocating

real	0m1.839s
user	0m0.115s
sys	0m0.279s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.246 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.395 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.404 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.412 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.412 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.413 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.413 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.413 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.414 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.279 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.183 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.184 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.184 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.185 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.185 I llama_model_loader: - type  f32:  194 tensors
0.00.025.185 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.186 I print_info: file format = GGUF V3 (latest)
0.00.025.187 I print_info: file type   = Q8_0
0.00.025.188 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.680 I load: special tokens cache size = 25
0.00.055.302 I load: token to piece cache size = 0.2984 MB
0.00.055.304 I print_info: arch             = gptneox
0.00.055.305 I print_info: vocab_only       = 0
0.00.055.305 I print_info: n_ctx_train      = 2048
0.00.055.305 I print_info: n_embd           = 2048
0.00.055.305 I print_info: n_layer          = 24
0.00.055.309 I print_info: n_head           = 16
0.00.055.310 I print_info: n_head_kv        = 16
0.00.055.312 I print_info: n_rot            = 32
0.00.055.312 I print_info: n_swa            = 0
0.00.055.312 I print_info: n_embd_head_k    = 128
0.00.055.313 I print_info: n_embd_head_v    = 128
0.00.055.313 I print_info: n_gqa            = 1
0.00.055.314 I print_info: n_embd_k_gqa     = 2048
0.00.055.315 I print_info: n_embd_v_gqa     = 2048
0.00.055.315 I print_info: f_norm_eps       = 1.0e-05
0.00.055.316 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.316 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.316 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.316 I print_info: f_logit_scale    = 0.0e+00
0.00.055.317 I print_info: n_ff             = 8192
0.00.055.318 I print_info: n_expert         = 0
0.00.055.320 I print_info: n_expert_used    = 0
0.00.055.320 I print_info: causal attn      = 1
0.00.055.320 I print_info: pooling type     = 0
0.00.055.320 I print_info: rope type        = 2
0.00.055.321 I print_info: rope scaling     = linear
0.00.055.321 I print_info: freq_base_train  = 10000.0
0.00.055.321 I print_info: freq_scale_train = 1
0.00.055.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.322 I print_info: rope_finetuned   = unknown
0.00.055.325 I print_info: ssm_d_conv       = 0
0.00.055.325 I print_info: ssm_d_inner      = 0
0.00.055.325 I print_info: ssm_d_state      = 0
0.00.055.326 I print_info: ssm_dt_rank      = 0
0.00.055.326 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.326 I print_info: model type       = 1.4B
0.00.055.326 I print_info: model params     = 1.41 B
0.00.055.326 I print_info: general.name     = 1.4B
0.00.055.327 I print_info: vocab type       = BPE
0.00.055.327 I print_info: n_vocab          = 50304
0.00.055.327 I print_info: n_merges         = 50009
0.00.055.327 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.328 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.328 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.328 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.328 I print_info: LF token         = 128 'Ä'
0.00.055.328 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.329 I print_info: max token length = 1024
0.00.885.652 I load_tensors: offloading 24 repeating layers to GPU
0.00.885.656 I load_tensors: offloading output layer to GPU
0.00.885.657 I load_tensors: offloaded 25/25 layers to GPU
0.00.885.679 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.885.682 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.886.974 I llama_init_from_model: n_seq_max     = 1
0.00.886.976 I llama_init_from_model: n_ctx         = 128
0.00.886.977 I llama_init_from_model: n_ctx_per_seq = 128
0.00.886.977 I llama_init_from_model: n_batch       = 128
0.00.886.980 I llama_init_from_model: n_ubatch      = 128
0.00.886.981 I llama_init_from_model: flash_attn    = 0
0.00.886.982 I llama_init_from_model: freq_base     = 10000.0
0.00.886.984 I llama_init_from_model: freq_scale    = 1
0.00.886.984 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.886.985 I ggml_metal_init: allocating
0.00.887.016 I ggml_metal_init: found device: Apple M4
0.00.887.027 I ggml_metal_init: picking default device: Apple M4
0.00.888.244 I ggml_metal_init: using embedded metal library
0.00.893.653 I ggml_metal_init: GPU name:   Apple M4
0.00.893.656 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.893.657 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.893.657 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.893.658 I ggml_metal_init: simdgroup reduction   = true
0.00.893.658 I ggml_metal_init: simdgroup matrix mul. = true
0.00.893.658 I ggml_metal_init: has residency sets    = true
0.00.893.658 I ggml_metal_init: has bfloat            = true
0.00.893.659 I ggml_metal_init: use bfloat            = true
0.00.893.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.893.661 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.908.704 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.911.973 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.911.983 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.912.016 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.915.033 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.915.034 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.915.034 I llama_init_from_model: graph nodes  = 967
0.00.915.035 I llama_init_from_model: graph splits = 2
0.00.915.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.915.037 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.940.320 I 
0.00.940.393 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.940.414 I perplexity: tokenizing the input ..
0.00.949.090 I perplexity: tokenization took 8.674 ms
0.00.949.107 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.072.220 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.073.532 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.073.547 I llama_perf_context_print:        load time =     931.06 ms
0.01.073.548 I llama_perf_context_print: prompt eval time =     122.87 ms /   128 tokens (    0.96 ms per token,  1041.73 tokens per second)
0.01.073.549 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.073.549 I llama_perf_context_print:       total time =     133.23 ms /   129 tokens
0.01.073.914 I ggml_metal_free: deallocating

real	0m1.089s
user	0m0.093s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.117 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.558 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.563 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.565 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.565 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.566 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.566 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.566 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.567 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.568 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.568 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.568 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.569 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.569 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.570 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.571 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.572 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.572 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.315 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.068 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.068 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.069 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.069 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.069 I llama_model_loader: - type  f32:  194 tensors
0.00.026.070 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.070 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.071 I print_info: file format = GGUF V3 (latest)
0.00.026.071 I print_info: file type   = Q4_0
0.00.026.072 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.660 I load: special tokens cache size = 25
0.00.051.827 I load: token to piece cache size = 0.2984 MB
0.00.051.830 I print_info: arch             = gptneox
0.00.051.831 I print_info: vocab_only       = 0
0.00.051.831 I print_info: n_ctx_train      = 2048
0.00.051.831 I print_info: n_embd           = 2048
0.00.051.831 I print_info: n_layer          = 24
0.00.051.834 I print_info: n_head           = 16
0.00.051.835 I print_info: n_head_kv        = 16
0.00.051.835 I print_info: n_rot            = 32
0.00.051.838 I print_info: n_swa            = 0
0.00.051.838 I print_info: n_embd_head_k    = 128
0.00.051.838 I print_info: n_embd_head_v    = 128
0.00.051.839 I print_info: n_gqa            = 1
0.00.051.840 I print_info: n_embd_k_gqa     = 2048
0.00.051.841 I print_info: n_embd_v_gqa     = 2048
0.00.051.841 I print_info: f_norm_eps       = 1.0e-05
0.00.051.842 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.842 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.842 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.842 I print_info: f_logit_scale    = 0.0e+00
0.00.051.843 I print_info: n_ff             = 8192
0.00.051.843 I print_info: n_expert         = 0
0.00.051.843 I print_info: n_expert_used    = 0
0.00.051.843 I print_info: causal attn      = 1
0.00.051.844 I print_info: pooling type     = 0
0.00.051.844 I print_info: rope type        = 2
0.00.051.844 I print_info: rope scaling     = linear
0.00.051.844 I print_info: freq_base_train  = 10000.0
0.00.051.845 I print_info: freq_scale_train = 1
0.00.051.845 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.845 I print_info: rope_finetuned   = unknown
0.00.051.845 I print_info: ssm_d_conv       = 0
0.00.051.845 I print_info: ssm_d_inner      = 0
0.00.051.845 I print_info: ssm_d_state      = 0
0.00.051.845 I print_info: ssm_dt_rank      = 0
0.00.051.846 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.846 I print_info: model type       = 1.4B
0.00.051.846 I print_info: model params     = 1.41 B
0.00.051.846 I print_info: general.name     = 1.4B
0.00.051.847 I print_info: vocab type       = BPE
0.00.051.847 I print_info: n_vocab          = 50304
0.00.051.847 I print_info: n_merges         = 50009
0.00.051.847 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.848 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.848 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.848 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.848 I print_info: LF token         = 128 'Ä'
0.00.051.849 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.849 I print_info: max token length = 1024
0.00.609.390 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.405 I load_tensors: offloading output layer to GPU
0.00.609.406 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.441 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.609.442 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.610.896 I llama_init_from_model: n_seq_max     = 1
0.00.610.901 I llama_init_from_model: n_ctx         = 128
0.00.610.902 I llama_init_from_model: n_ctx_per_seq = 128
0.00.610.902 I llama_init_from_model: n_batch       = 128
0.00.610.903 I llama_init_from_model: n_ubatch      = 128
0.00.610.903 I llama_init_from_model: flash_attn    = 0
0.00.610.905 I llama_init_from_model: freq_base     = 10000.0
0.00.610.906 I llama_init_from_model: freq_scale    = 1
0.00.610.906 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.912 I ggml_metal_init: allocating
0.00.610.992 I ggml_metal_init: found device: Apple M4
0.00.611.006 I ggml_metal_init: picking default device: Apple M4
0.00.612.756 I ggml_metal_init: using embedded metal library
0.00.619.711 I ggml_metal_init: GPU name:   Apple M4
0.00.619.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.720 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.721 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.725 I ggml_metal_init: simdgroup reduction   = true
0.00.619.725 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.726 I ggml_metal_init: has residency sets    = true
0.00.619.726 I ggml_metal_init: has bfloat            = true
0.00.619.726 I ggml_metal_init: use bfloat            = true
0.00.619.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.636 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.110 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.641.114 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.641.139 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.644.452 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.644.454 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.644.454 I llama_init_from_model: graph nodes  = 967
0.00.644.455 I llama_init_from_model: graph splits = 2
0.00.644.458 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.644.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.522 I 
0.00.672.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.622 I perplexity: tokenizing the input ..
0.00.680.830 I perplexity: tokenization took 8.206 ms
0.00.680.843 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.802.540 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.803.871 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.803.884 I llama_perf_context_print:        load time =     662.40 ms
0.00.803.886 I llama_perf_context_print: prompt eval time =     121.47 ms /   128 tokens (    0.95 ms per token,  1053.80 tokens per second)
0.00.803.886 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.887 I llama_perf_context_print:       total time =     131.37 ms /   129 tokens
0.00.804.275 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.092s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.771 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.978 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.986 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.986 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.988 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.990 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.990 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.991 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.991 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.991 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.992 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.997 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.997 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.998 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.812 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.619 I llama_model_loader: - type  f32:  194 tensors
0.00.024.620 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.620 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.621 I print_info: file format = GGUF V3 (latest)
0.00.024.621 I print_info: file type   = Q4_1
0.00.024.622 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.164 I load: special tokens cache size = 25
0.00.049.952 I load: token to piece cache size = 0.2984 MB
0.00.049.955 I print_info: arch             = gptneox
0.00.049.955 I print_info: vocab_only       = 0
0.00.049.955 I print_info: n_ctx_train      = 2048
0.00.049.955 I print_info: n_embd           = 2048
0.00.049.956 I print_info: n_layer          = 24
0.00.049.958 I print_info: n_head           = 16
0.00.049.959 I print_info: n_head_kv        = 16
0.00.049.959 I print_info: n_rot            = 32
0.00.049.959 I print_info: n_swa            = 0
0.00.049.962 I print_info: n_embd_head_k    = 128
0.00.049.962 I print_info: n_embd_head_v    = 128
0.00.049.962 I print_info: n_gqa            = 1
0.00.049.963 I print_info: n_embd_k_gqa     = 2048
0.00.049.964 I print_info: n_embd_v_gqa     = 2048
0.00.049.964 I print_info: f_norm_eps       = 1.0e-05
0.00.049.965 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.965 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.968 I print_info: f_logit_scale    = 0.0e+00
0.00.049.968 I print_info: n_ff             = 8192
0.00.049.969 I print_info: n_expert         = 0
0.00.049.969 I print_info: n_expert_used    = 0
0.00.049.969 I print_info: causal attn      = 1
0.00.049.969 I print_info: pooling type     = 0
0.00.049.969 I print_info: rope type        = 2
0.00.049.971 I print_info: rope scaling     = linear
0.00.049.971 I print_info: freq_base_train  = 10000.0
0.00.049.976 I print_info: freq_scale_train = 1
0.00.049.976 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.976 I print_info: rope_finetuned   = unknown
0.00.049.977 I print_info: ssm_d_conv       = 0
0.00.049.977 I print_info: ssm_d_inner      = 0
0.00.049.978 I print_info: ssm_d_state      = 0
0.00.049.978 I print_info: ssm_dt_rank      = 0
0.00.049.978 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.978 I print_info: model type       = 1.4B
0.00.049.979 I print_info: model params     = 1.41 B
0.00.049.979 I print_info: general.name     = 1.4B
0.00.049.979 I print_info: vocab type       = BPE
0.00.049.979 I print_info: n_vocab          = 50304
0.00.049.979 I print_info: n_merges         = 50009
0.00.049.980 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.980 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.980 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.980 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.980 I print_info: LF token         = 128 'Ä'
0.00.049.981 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.985 I print_info: max token length = 1024
0.00.634.402 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.412 I load_tensors: offloading output layer to GPU
0.00.634.413 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.451 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.634.452 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.635.988 I llama_init_from_model: n_seq_max     = 1
0.00.635.992 I llama_init_from_model: n_ctx         = 128
0.00.635.993 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.993 I llama_init_from_model: n_batch       = 128
0.00.635.994 I llama_init_from_model: n_ubatch      = 128
0.00.635.994 I llama_init_from_model: flash_attn    = 0
0.00.635.996 I llama_init_from_model: freq_base     = 10000.0
0.00.635.997 I llama_init_from_model: freq_scale    = 1
0.00.635.997 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.999 I ggml_metal_init: allocating
0.00.636.066 I ggml_metal_init: found device: Apple M4
0.00.636.080 I ggml_metal_init: picking default device: Apple M4
0.00.637.820 I ggml_metal_init: using embedded metal library
0.00.644.394 I ggml_metal_init: GPU name:   Apple M4
0.00.644.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.399 I ggml_metal_init: simdgroup reduction   = true
0.00.644.400 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.400 I ggml_metal_init: has residency sets    = true
0.00.644.400 I ggml_metal_init: has bfloat            = true
0.00.644.400 I ggml_metal_init: use bfloat            = true
0.00.644.401 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.403 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.324 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.630 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.636 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.672 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.088 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.668.090 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.668.091 I llama_init_from_model: graph nodes  = 967
0.00.668.091 I llama_init_from_model: graph splits = 2
0.00.668.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.668.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.913 I 
0.00.693.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.006 I perplexity: tokenizing the input ..
0.00.704.196 I perplexity: tokenization took 10.188 ms
0.00.704.209 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.439 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.830.747 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.830.769 I llama_perf_context_print:        load time =     685.13 ms
0.00.830.770 I llama_perf_context_print: prompt eval time =     124.99 ms /   128 tokens (    0.98 ms per token,  1024.05 tokens per second)
0.00.830.771 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.830.771 I llama_perf_context_print:       total time =     136.86 ms /   129 tokens
0.00.831.181 I ggml_metal_free: deallocating

real	0m0.846s
user	0m0.093s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.673 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.674 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.675 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.679 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.682 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.683 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.683 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.684 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.687 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.687 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.341 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.341 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.342 I llama_model_loader: - type  f32:  194 tensors
0.00.024.342 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.343 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.343 I print_info: file format = GGUF V3 (latest)
0.00.024.344 I print_info: file type   = Q5_0
0.00.024.345 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.890 I load: special tokens cache size = 25
0.00.049.897 I load: token to piece cache size = 0.2984 MB
0.00.049.899 I print_info: arch             = gptneox
0.00.049.899 I print_info: vocab_only       = 0
0.00.049.900 I print_info: n_ctx_train      = 2048
0.00.049.900 I print_info: n_embd           = 2048
0.00.049.900 I print_info: n_layer          = 24
0.00.049.903 I print_info: n_head           = 16
0.00.049.904 I print_info: n_head_kv        = 16
0.00.049.904 I print_info: n_rot            = 32
0.00.049.904 I print_info: n_swa            = 0
0.00.049.904 I print_info: n_embd_head_k    = 128
0.00.049.905 I print_info: n_embd_head_v    = 128
0.00.049.905 I print_info: n_gqa            = 1
0.00.049.906 I print_info: n_embd_k_gqa     = 2048
0.00.049.907 I print_info: n_embd_v_gqa     = 2048
0.00.049.908 I print_info: f_norm_eps       = 1.0e-05
0.00.049.908 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.908 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.908 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.908 I print_info: f_logit_scale    = 0.0e+00
0.00.049.909 I print_info: n_ff             = 8192
0.00.049.909 I print_info: n_expert         = 0
0.00.049.909 I print_info: n_expert_used    = 0
0.00.049.909 I print_info: causal attn      = 1
0.00.049.910 I print_info: pooling type     = 0
0.00.049.910 I print_info: rope type        = 2
0.00.049.911 I print_info: rope scaling     = linear
0.00.049.911 I print_info: freq_base_train  = 10000.0
0.00.049.912 I print_info: freq_scale_train = 1
0.00.049.912 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.912 I print_info: rope_finetuned   = unknown
0.00.049.912 I print_info: ssm_d_conv       = 0
0.00.049.912 I print_info: ssm_d_inner      = 0
0.00.049.913 I print_info: ssm_d_state      = 0
0.00.049.913 I print_info: ssm_dt_rank      = 0
0.00.049.913 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.913 I print_info: model type       = 1.4B
0.00.049.914 I print_info: model params     = 1.41 B
0.00.049.914 I print_info: general.name     = 1.4B
0.00.049.914 I print_info: vocab type       = BPE
0.00.049.915 I print_info: n_vocab          = 50304
0.00.049.915 I print_info: n_merges         = 50009
0.00.049.915 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.915 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.915 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.917 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.917 I print_info: LF token         = 128 'Ä'
0.00.049.917 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.917 I print_info: max token length = 1024
0.00.705.938 I load_tensors: offloading 24 repeating layers to GPU
0.00.705.947 I load_tensors: offloading output layer to GPU
0.00.705.948 I load_tensors: offloaded 25/25 layers to GPU
0.00.705.977 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.705.979 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.707.425 I llama_init_from_model: n_seq_max     = 1
0.00.707.433 I llama_init_from_model: n_ctx         = 128
0.00.707.434 I llama_init_from_model: n_ctx_per_seq = 128
0.00.707.435 I llama_init_from_model: n_batch       = 128
0.00.707.435 I llama_init_from_model: n_ubatch      = 128
0.00.707.436 I llama_init_from_model: flash_attn    = 0
0.00.707.437 I llama_init_from_model: freq_base     = 10000.0
0.00.707.437 I llama_init_from_model: freq_scale    = 1
0.00.707.438 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.707.440 I ggml_metal_init: allocating
0.00.707.489 I ggml_metal_init: found device: Apple M4
0.00.707.502 I ggml_metal_init: picking default device: Apple M4
0.00.709.122 I ggml_metal_init: using embedded metal library
0.00.715.735 I ggml_metal_init: GPU name:   Apple M4
0.00.715.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.715.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.715.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.715.743 I ggml_metal_init: simdgroup reduction   = true
0.00.715.743 I ggml_metal_init: simdgroup matrix mul. = true
0.00.715.743 I ggml_metal_init: has residency sets    = true
0.00.715.744 I ggml_metal_init: has bfloat            = true
0.00.715.744 I ggml_metal_init: use bfloat            = true
0.00.715.745 I ggml_metal_init: hasUnifiedMemory      = true
0.00.715.746 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.732.896 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.593 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.736.601 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.736.627 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.201 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.740.203 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.740.203 I llama_init_from_model: graph nodes  = 967
0.00.740.203 I llama_init_from_model: graph splits = 2
0.00.740.207 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.740.207 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.783 I 
0.00.768.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.883 I perplexity: tokenizing the input ..
0.00.777.938 I perplexity: tokenization took 9.053 ms
0.00.777.950 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.912.199 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.913.622 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.913.636 I llama_perf_context_print:        load time =     759.95 ms
0.00.913.637 I llama_perf_context_print: prompt eval time =     134.00 ms /   128 tokens (    1.05 ms per token,   955.20 tokens per second)
0.00.913.638 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.913.638 I llama_perf_context_print:       total time =     144.86 ms /   129 tokens
0.00.914.039 I ggml_metal_free: deallocating

real	0m0.930s
user	0m0.092s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.992 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.626 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.627 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.628 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.629 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.631 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.632 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.188 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.190 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.191 I llama_model_loader: - type  f32:  194 tensors
0.00.026.191 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.192 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.192 I print_info: file format = GGUF V3 (latest)
0.00.026.193 I print_info: file type   = Q5_1
0.00.026.194 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.759 I load: special tokens cache size = 25
0.00.051.791 I load: token to piece cache size = 0.2984 MB
0.00.051.794 I print_info: arch             = gptneox
0.00.051.794 I print_info: vocab_only       = 0
0.00.051.794 I print_info: n_ctx_train      = 2048
0.00.051.794 I print_info: n_embd           = 2048
0.00.051.795 I print_info: n_layer          = 24
0.00.051.797 I print_info: n_head           = 16
0.00.051.798 I print_info: n_head_kv        = 16
0.00.051.798 I print_info: n_rot            = 32
0.00.051.798 I print_info: n_swa            = 0
0.00.051.799 I print_info: n_embd_head_k    = 128
0.00.051.799 I print_info: n_embd_head_v    = 128
0.00.051.799 I print_info: n_gqa            = 1
0.00.051.800 I print_info: n_embd_k_gqa     = 2048
0.00.051.801 I print_info: n_embd_v_gqa     = 2048
0.00.051.803 I print_info: f_norm_eps       = 1.0e-05
0.00.051.803 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.804 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.804 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.804 I print_info: f_logit_scale    = 0.0e+00
0.00.051.805 I print_info: n_ff             = 8192
0.00.051.805 I print_info: n_expert         = 0
0.00.051.805 I print_info: n_expert_used    = 0
0.00.051.805 I print_info: causal attn      = 1
0.00.051.805 I print_info: pooling type     = 0
0.00.051.805 I print_info: rope type        = 2
0.00.051.806 I print_info: rope scaling     = linear
0.00.051.806 I print_info: freq_base_train  = 10000.0
0.00.051.808 I print_info: freq_scale_train = 1
0.00.051.808 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.808 I print_info: rope_finetuned   = unknown
0.00.051.808 I print_info: ssm_d_conv       = 0
0.00.051.808 I print_info: ssm_d_inner      = 0
0.00.051.809 I print_info: ssm_d_state      = 0
0.00.051.809 I print_info: ssm_dt_rank      = 0
0.00.051.809 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.809 I print_info: model type       = 1.4B
0.00.051.810 I print_info: model params     = 1.41 B
0.00.051.810 I print_info: general.name     = 1.4B
0.00.051.810 I print_info: vocab type       = BPE
0.00.051.810 I print_info: n_vocab          = 50304
0.00.051.811 I print_info: n_merges         = 50009
0.00.051.816 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.818 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.818 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.819 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.819 I print_info: LF token         = 128 'Ä'
0.00.051.821 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.821 I print_info: max token length = 1024
0.00.613.370 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.375 I load_tensors: offloading output layer to GPU
0.00.613.376 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.400 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.613.401 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.614.804 I llama_init_from_model: n_seq_max     = 1
0.00.614.806 I llama_init_from_model: n_ctx         = 128
0.00.614.806 I llama_init_from_model: n_ctx_per_seq = 128
0.00.614.807 I llama_init_from_model: n_batch       = 128
0.00.614.807 I llama_init_from_model: n_ubatch      = 128
0.00.614.808 I llama_init_from_model: flash_attn    = 0
0.00.614.809 I llama_init_from_model: freq_base     = 10000.0
0.00.614.809 I llama_init_from_model: freq_scale    = 1
0.00.614.810 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.614.811 I ggml_metal_init: allocating
0.00.614.823 I ggml_metal_init: found device: Apple M4
0.00.614.831 I ggml_metal_init: picking default device: Apple M4
0.00.616.105 I ggml_metal_init: using embedded metal library
0.00.622.226 I ggml_metal_init: GPU name:   Apple M4
0.00.622.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.231 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.232 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.233 I ggml_metal_init: simdgroup reduction   = true
0.00.622.233 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.233 I ggml_metal_init: has residency sets    = true
0.00.622.233 I ggml_metal_init: has bfloat            = true
0.00.622.234 I ggml_metal_init: use bfloat            = true
0.00.622.235 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.969 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.559 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.563 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.590 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.739 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.645.741 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.645.741 I llama_init_from_model: graph nodes  = 967
0.00.645.741 I llama_init_from_model: graph splits = 2
0.00.645.744 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.052 I 
0.00.676.131 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.152 I perplexity: tokenizing the input ..
0.00.684.466 I perplexity: tokenization took 8.312 ms
0.00.684.478 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.356 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.819.691 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.819.711 I llama_perf_context_print:        load time =     665.05 ms
0.00.819.712 I llama_perf_context_print: prompt eval time =     133.65 ms /   128 tokens (    1.04 ms per token,   957.73 tokens per second)
0.00.819.712 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.713 I llama_perf_context_print:       total time =     143.66 ms /   129 tokens
0.00.820.096 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.090s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.718 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.724 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.725 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.726 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.726 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.727 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.727 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.728 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.728 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.731 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.731 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.731 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.445 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.446 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.184 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.185 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.186 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.186 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.187 I llama_model_loader: - type  f32:  194 tensors
0.00.025.187 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.187 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.187 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.188 I print_info: file format = GGUF V3 (latest)
0.00.025.189 I print_info: file type   = Q2_K - Medium
0.00.025.190 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.887 I load: special tokens cache size = 25
0.00.050.047 I load: token to piece cache size = 0.2984 MB
0.00.050.050 I print_info: arch             = gptneox
0.00.050.050 I print_info: vocab_only       = 0
0.00.050.050 I print_info: n_ctx_train      = 2048
0.00.050.051 I print_info: n_embd           = 2048
0.00.050.051 I print_info: n_layer          = 24
0.00.050.054 I print_info: n_head           = 16
0.00.050.055 I print_info: n_head_kv        = 16
0.00.050.055 I print_info: n_rot            = 32
0.00.050.055 I print_info: n_swa            = 0
0.00.050.055 I print_info: n_embd_head_k    = 128
0.00.050.055 I print_info: n_embd_head_v    = 128
0.00.050.056 I print_info: n_gqa            = 1
0.00.050.057 I print_info: n_embd_k_gqa     = 2048
0.00.050.058 I print_info: n_embd_v_gqa     = 2048
0.00.050.058 I print_info: f_norm_eps       = 1.0e-05
0.00.050.059 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.059 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.059 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.059 I print_info: f_logit_scale    = 0.0e+00
0.00.050.060 I print_info: n_ff             = 8192
0.00.050.060 I print_info: n_expert         = 0
0.00.050.060 I print_info: n_expert_used    = 0
0.00.050.060 I print_info: causal attn      = 1
0.00.050.060 I print_info: pooling type     = 0
0.00.050.061 I print_info: rope type        = 2
0.00.050.061 I print_info: rope scaling     = linear
0.00.050.061 I print_info: freq_base_train  = 10000.0
0.00.050.061 I print_info: freq_scale_train = 1
0.00.050.062 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.062 I print_info: rope_finetuned   = unknown
0.00.050.062 I print_info: ssm_d_conv       = 0
0.00.050.062 I print_info: ssm_d_inner      = 0
0.00.050.062 I print_info: ssm_d_state      = 0
0.00.050.063 I print_info: ssm_dt_rank      = 0
0.00.050.063 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.063 I print_info: model type       = 1.4B
0.00.050.063 I print_info: model params     = 1.41 B
0.00.050.063 I print_info: general.name     = 1.4B
0.00.050.064 I print_info: vocab type       = BPE
0.00.050.064 I print_info: n_vocab          = 50304
0.00.050.064 I print_info: n_merges         = 50009
0.00.050.065 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.065 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.065 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.065 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.066 I print_info: LF token         = 128 'Ä'
0.00.050.066 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.066 I print_info: max token length = 1024
0.00.351.688 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.699 I load_tensors: offloading output layer to GPU
0.00.351.700 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.733 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.734 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.353.258 I llama_init_from_model: n_seq_max     = 1
0.00.353.262 I llama_init_from_model: n_ctx         = 128
0.00.353.263 I llama_init_from_model: n_ctx_per_seq = 128
0.00.353.263 I llama_init_from_model: n_batch       = 128
0.00.353.264 I llama_init_from_model: n_ubatch      = 128
0.00.353.264 I llama_init_from_model: flash_attn    = 0
0.00.353.267 I llama_init_from_model: freq_base     = 10000.0
0.00.353.267 I llama_init_from_model: freq_scale    = 1
0.00.353.268 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.353.271 I ggml_metal_init: allocating
0.00.353.329 I ggml_metal_init: found device: Apple M4
0.00.353.344 I ggml_metal_init: picking default device: Apple M4
0.00.355.062 I ggml_metal_init: using embedded metal library
0.00.360.723 I ggml_metal_init: GPU name:   Apple M4
0.00.360.733 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.360.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.360.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.360.735 I ggml_metal_init: simdgroup reduction   = true
0.00.360.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.360.736 I ggml_metal_init: has residency sets    = true
0.00.360.736 I ggml_metal_init: has bfloat            = true
0.00.360.736 I ggml_metal_init: use bfloat            = true
0.00.360.738 I ggml_metal_init: hasUnifiedMemory      = true
0.00.360.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.382.215 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.385.819 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.385.829 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.385.890 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.389.265 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.389.267 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.389.267 I llama_init_from_model: graph nodes  = 967
0.00.389.268 I llama_init_from_model: graph splits = 2
0.00.389.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.389.271 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.417.470 I 
0.00.417.553 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.417.574 I perplexity: tokenizing the input ..
0.00.426.111 I perplexity: tokenization took 8.536 ms
0.00.426.126 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.557.196 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.558.580 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.558.593 I llama_perf_context_print:        load time =     407.73 ms
0.00.558.594 I llama_perf_context_print: prompt eval time =     130.84 ms /   128 tokens (    1.02 ms per token,   978.29 tokens per second)
0.00.558.594 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.558.595 I llama_perf_context_print:       total time =     141.13 ms /   129 tokens
0.00.558.941 I ggml_metal_free: deallocating

real	0m0.573s
user	0m0.093s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.789 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.400 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.408 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.410 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.154 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.931 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.932 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.933 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.933 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.934 I llama_model_loader: - type  f32:  194 tensors
0.00.023.934 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.934 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.935 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.935 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.935 I print_info: file format = GGUF V3 (latest)
0.00.023.936 I print_info: file type   = Q3_K - Medium
0.00.023.937 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.760 I load: special tokens cache size = 25
0.00.048.691 I load: token to piece cache size = 0.2984 MB
0.00.048.694 I print_info: arch             = gptneox
0.00.048.694 I print_info: vocab_only       = 0
0.00.048.694 I print_info: n_ctx_train      = 2048
0.00.048.694 I print_info: n_embd           = 2048
0.00.048.694 I print_info: n_layer          = 24
0.00.048.697 I print_info: n_head           = 16
0.00.048.698 I print_info: n_head_kv        = 16
0.00.048.698 I print_info: n_rot            = 32
0.00.048.699 I print_info: n_swa            = 0
0.00.048.699 I print_info: n_embd_head_k    = 128
0.00.048.699 I print_info: n_embd_head_v    = 128
0.00.048.700 I print_info: n_gqa            = 1
0.00.048.700 I print_info: n_embd_k_gqa     = 2048
0.00.048.701 I print_info: n_embd_v_gqa     = 2048
0.00.048.702 I print_info: f_norm_eps       = 1.0e-05
0.00.048.702 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.702 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.702 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.702 I print_info: f_logit_scale    = 0.0e+00
0.00.048.703 I print_info: n_ff             = 8192
0.00.048.703 I print_info: n_expert         = 0
0.00.048.703 I print_info: n_expert_used    = 0
0.00.048.703 I print_info: causal attn      = 1
0.00.048.703 I print_info: pooling type     = 0
0.00.048.704 I print_info: rope type        = 2
0.00.048.704 I print_info: rope scaling     = linear
0.00.048.704 I print_info: freq_base_train  = 10000.0
0.00.048.705 I print_info: freq_scale_train = 1
0.00.048.705 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.705 I print_info: rope_finetuned   = unknown
0.00.048.705 I print_info: ssm_d_conv       = 0
0.00.048.705 I print_info: ssm_d_inner      = 0
0.00.048.705 I print_info: ssm_d_state      = 0
0.00.048.706 I print_info: ssm_dt_rank      = 0
0.00.048.706 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.708 I print_info: model type       = 1.4B
0.00.048.708 I print_info: model params     = 1.41 B
0.00.048.709 I print_info: general.name     = 1.4B
0.00.048.709 I print_info: vocab type       = BPE
0.00.048.709 I print_info: n_vocab          = 50304
0.00.048.709 I print_info: n_merges         = 50009
0.00.048.710 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.710 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.710 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.710 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.710 I print_info: LF token         = 128 'Ä'
0.00.048.711 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.711 I print_info: max token length = 1024
0.00.467.131 I load_tensors: offloading 24 repeating layers to GPU
0.00.467.144 I load_tensors: offloading output layer to GPU
0.00.467.145 I load_tensors: offloaded 25/25 layers to GPU
0.00.467.172 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.467.174 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.468.435 I llama_init_from_model: n_seq_max     = 1
0.00.468.440 I llama_init_from_model: n_ctx         = 128
0.00.468.441 I llama_init_from_model: n_ctx_per_seq = 128
0.00.468.441 I llama_init_from_model: n_batch       = 128
0.00.468.442 I llama_init_from_model: n_ubatch      = 128
0.00.468.442 I llama_init_from_model: flash_attn    = 0
0.00.468.444 I llama_init_from_model: freq_base     = 10000.0
0.00.468.445 I llama_init_from_model: freq_scale    = 1
0.00.468.445 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.468.448 I ggml_metal_init: allocating
0.00.468.521 I ggml_metal_init: found device: Apple M4
0.00.468.535 I ggml_metal_init: picking default device: Apple M4
0.00.470.257 I ggml_metal_init: using embedded metal library
0.00.475.751 I ggml_metal_init: GPU name:   Apple M4
0.00.475.763 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.475.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.475.764 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.475.765 I ggml_metal_init: simdgroup reduction   = true
0.00.475.765 I ggml_metal_init: simdgroup matrix mul. = true
0.00.475.765 I ggml_metal_init: has residency sets    = true
0.00.475.765 I ggml_metal_init: has bfloat            = true
0.00.475.766 I ggml_metal_init: use bfloat            = true
0.00.475.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.475.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.496.567 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.500.208 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.500.215 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.500.248 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.503.634 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.503.636 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.503.636 I llama_init_from_model: graph nodes  = 967
0.00.503.637 I llama_init_from_model: graph splits = 2
0.00.503.640 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.503.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.534.660 I 
0.00.534.745 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.534.765 I perplexity: tokenizing the input ..
0.00.543.613 I perplexity: tokenization took 8.846 ms
0.00.543.626 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.674.684 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.676.027 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.676.044 I llama_perf_context_print:        load time =     525.86 ms
0.00.676.045 I llama_perf_context_print: prompt eval time =     130.83 ms /   128 tokens (    1.02 ms per token,   978.38 tokens per second)
0.00.676.048 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.676.048 I llama_perf_context_print:       total time =     141.39 ms /   129 tokens
0.00.676.449 I ggml_metal_free: deallocating

real	0m0.691s
user	0m0.092s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.621 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.611 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.612 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.612 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.612 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.615 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.615 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.615 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.616 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.618 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.618 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.618 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.414 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.426 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.245 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.246 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.246 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.247 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.247 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.247 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.248 I llama_model_loader: - type  f32:  194 tensors
0.00.025.248 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.248 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.249 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.249 I print_info: file format = GGUF V3 (latest)
0.00.025.250 I print_info: file type   = Q4_K - Medium
0.00.025.252 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.701 I load: special tokens cache size = 25
0.00.050.752 I load: token to piece cache size = 0.2984 MB
0.00.050.756 I print_info: arch             = gptneox
0.00.050.756 I print_info: vocab_only       = 0
0.00.050.756 I print_info: n_ctx_train      = 2048
0.00.050.756 I print_info: n_embd           = 2048
0.00.050.756 I print_info: n_layer          = 24
0.00.050.759 I print_info: n_head           = 16
0.00.050.760 I print_info: n_head_kv        = 16
0.00.050.760 I print_info: n_rot            = 32
0.00.050.760 I print_info: n_swa            = 0
0.00.050.761 I print_info: n_embd_head_k    = 128
0.00.050.761 I print_info: n_embd_head_v    = 128
0.00.050.762 I print_info: n_gqa            = 1
0.00.050.763 I print_info: n_embd_k_gqa     = 2048
0.00.050.763 I print_info: n_embd_v_gqa     = 2048
0.00.050.764 I print_info: f_norm_eps       = 1.0e-05
0.00.050.764 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.765 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.765 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.765 I print_info: f_logit_scale    = 0.0e+00
0.00.050.766 I print_info: n_ff             = 8192
0.00.050.766 I print_info: n_expert         = 0
0.00.050.766 I print_info: n_expert_used    = 0
0.00.050.766 I print_info: causal attn      = 1
0.00.050.766 I print_info: pooling type     = 0
0.00.050.766 I print_info: rope type        = 2
0.00.050.767 I print_info: rope scaling     = linear
0.00.050.767 I print_info: freq_base_train  = 10000.0
0.00.050.767 I print_info: freq_scale_train = 1
0.00.050.768 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.768 I print_info: rope_finetuned   = unknown
0.00.050.768 I print_info: ssm_d_conv       = 0
0.00.050.768 I print_info: ssm_d_inner      = 0
0.00.050.768 I print_info: ssm_d_state      = 0
0.00.050.770 I print_info: ssm_dt_rank      = 0
0.00.050.770 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.770 I print_info: model type       = 1.4B
0.00.050.771 I print_info: model params     = 1.41 B
0.00.050.771 I print_info: general.name     = 1.4B
0.00.050.771 I print_info: vocab type       = BPE
0.00.050.772 I print_info: n_vocab          = 50304
0.00.050.772 I print_info: n_merges         = 50009
0.00.050.772 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.772 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.772 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.772 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.773 I print_info: LF token         = 128 'Ä'
0.00.050.773 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.774 I print_info: max token length = 1024
0.00.535.486 I load_tensors: offloading 24 repeating layers to GPU
0.00.535.501 I load_tensors: offloading output layer to GPU
0.00.535.502 I load_tensors: offloaded 25/25 layers to GPU
0.00.535.536 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.535.538 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.537.116 I llama_init_from_model: n_seq_max     = 1
0.00.537.120 I llama_init_from_model: n_ctx         = 128
0.00.537.120 I llama_init_from_model: n_ctx_per_seq = 128
0.00.537.121 I llama_init_from_model: n_batch       = 128
0.00.537.121 I llama_init_from_model: n_ubatch      = 128
0.00.537.122 I llama_init_from_model: flash_attn    = 0
0.00.537.124 I llama_init_from_model: freq_base     = 10000.0
0.00.537.124 I llama_init_from_model: freq_scale    = 1
0.00.537.125 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.537.131 I ggml_metal_init: allocating
0.00.537.210 I ggml_metal_init: found device: Apple M4
0.00.537.224 I ggml_metal_init: picking default device: Apple M4
0.00.538.977 I ggml_metal_init: using embedded metal library
0.00.545.552 I ggml_metal_init: GPU name:   Apple M4
0.00.545.558 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.545.559 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.545.560 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.545.561 I ggml_metal_init: simdgroup reduction   = true
0.00.545.561 I ggml_metal_init: simdgroup matrix mul. = true
0.00.545.561 I ggml_metal_init: has residency sets    = true
0.00.545.562 I ggml_metal_init: has bfloat            = true
0.00.545.562 I ggml_metal_init: use bfloat            = true
0.00.545.564 I ggml_metal_init: hasUnifiedMemory      = true
0.00.545.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.563.297 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.566.869 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.566.873 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.566.898 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.570.014 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.570.016 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.570.016 I llama_init_from_model: graph nodes  = 967
0.00.570.017 I llama_init_from_model: graph splits = 2
0.00.570.020 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.570.020 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.083 I 
0.00.597.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.181 I perplexity: tokenizing the input ..
0.00.606.021 I perplexity: tokenization took 8.838 ms
0.00.606.033 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.739.477 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.740.820 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.740.835 I llama_perf_context_print:        load time =     587.45 ms
0.00.740.836 I llama_perf_context_print: prompt eval time =     133.21 ms /   128 tokens (    1.04 ms per token,   960.87 tokens per second)
0.00.740.837 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.740.838 I llama_perf_context_print:       total time =     143.76 ms /   129 tokens
0.00.741.273 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.092s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.749 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.646 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.655 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.657 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.659 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.662 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.266 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.267 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.267 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.268 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.269 I llama_model_loader: - type  f32:  194 tensors
0.00.024.269 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.270 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.270 I print_info: file format = GGUF V3 (latest)
0.00.024.271 I print_info: file type   = Q5_K - Medium
0.00.024.272 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.783 I load: special tokens cache size = 25
0.00.049.950 I load: token to piece cache size = 0.2984 MB
0.00.049.953 I print_info: arch             = gptneox
0.00.049.953 I print_info: vocab_only       = 0
0.00.049.953 I print_info: n_ctx_train      = 2048
0.00.049.953 I print_info: n_embd           = 2048
0.00.049.953 I print_info: n_layer          = 24
0.00.049.957 I print_info: n_head           = 16
0.00.049.957 I print_info: n_head_kv        = 16
0.00.049.958 I print_info: n_rot            = 32
0.00.049.958 I print_info: n_swa            = 0
0.00.049.958 I print_info: n_embd_head_k    = 128
0.00.049.958 I print_info: n_embd_head_v    = 128
0.00.049.960 I print_info: n_gqa            = 1
0.00.049.961 I print_info: n_embd_k_gqa     = 2048
0.00.049.961 I print_info: n_embd_v_gqa     = 2048
0.00.049.964 I print_info: f_norm_eps       = 1.0e-05
0.00.049.964 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.964 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.964 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.964 I print_info: f_logit_scale    = 0.0e+00
0.00.049.965 I print_info: n_ff             = 8192
0.00.049.965 I print_info: n_expert         = 0
0.00.049.965 I print_info: n_expert_used    = 0
0.00.049.966 I print_info: causal attn      = 1
0.00.049.966 I print_info: pooling type     = 0
0.00.049.966 I print_info: rope type        = 2
0.00.049.966 I print_info: rope scaling     = linear
0.00.049.968 I print_info: freq_base_train  = 10000.0
0.00.049.968 I print_info: freq_scale_train = 1
0.00.049.968 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.969 I print_info: rope_finetuned   = unknown
0.00.049.969 I print_info: ssm_d_conv       = 0
0.00.049.969 I print_info: ssm_d_inner      = 0
0.00.049.969 I print_info: ssm_d_state      = 0
0.00.049.969 I print_info: ssm_dt_rank      = 0
0.00.049.969 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.970 I print_info: model type       = 1.4B
0.00.049.970 I print_info: model params     = 1.41 B
0.00.049.970 I print_info: general.name     = 1.4B
0.00.049.971 I print_info: vocab type       = BPE
0.00.049.971 I print_info: n_vocab          = 50304
0.00.049.975 I print_info: n_merges         = 50009
0.00.049.975 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.975 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.976 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.976 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.976 I print_info: LF token         = 128 'Ä'
0.00.049.976 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.976 I print_info: max token length = 1024
0.00.603.140 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.154 I load_tensors: offloading output layer to GPU
0.00.603.155 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.186 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.603.188 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.604.553 I llama_init_from_model: n_seq_max     = 1
0.00.604.565 I llama_init_from_model: n_ctx         = 128
0.00.604.565 I llama_init_from_model: n_ctx_per_seq = 128
0.00.604.566 I llama_init_from_model: n_batch       = 128
0.00.604.566 I llama_init_from_model: n_ubatch      = 128
0.00.604.566 I llama_init_from_model: flash_attn    = 0
0.00.604.568 I llama_init_from_model: freq_base     = 10000.0
0.00.604.569 I llama_init_from_model: freq_scale    = 1
0.00.604.569 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.604.573 I ggml_metal_init: allocating
0.00.604.638 I ggml_metal_init: found device: Apple M4
0.00.604.655 I ggml_metal_init: picking default device: Apple M4
0.00.606.762 I ggml_metal_init: using embedded metal library
0.00.613.969 I ggml_metal_init: GPU name:   Apple M4
0.00.613.974 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.975 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.976 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.980 I ggml_metal_init: simdgroup reduction   = true
0.00.613.980 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.981 I ggml_metal_init: has residency sets    = true
0.00.613.981 I ggml_metal_init: has bfloat            = true
0.00.613.981 I ggml_metal_init: use bfloat            = true
0.00.613.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.991 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.416 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.636.061 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.636.064 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.636.096 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.639.367 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.639.368 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.639.369 I llama_init_from_model: graph nodes  = 967
0.00.639.369 I llama_init_from_model: graph splits = 2
0.00.639.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.639.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.881 I 
0.00.674.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.970 I perplexity: tokenizing the input ..
0.00.684.013 I perplexity: tokenization took 9.041 ms
0.00.684.026 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.260 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.825.701 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.825.720 I llama_perf_context_print:        load time =     666.12 ms
0.00.825.721 I llama_perf_context_print: prompt eval time =     139.98 ms /   128 tokens (    1.09 ms per token,   914.39 tokens per second)
0.00.825.722 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.723 I llama_perf_context_print:       total time =     150.84 ms /   129 tokens
0.00.826.156 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.093s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.382 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.415 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.420 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.428 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.428 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.430 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.430 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.431 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.431 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.432 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.432 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.432 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.433 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.440 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.239 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.096 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.097 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.098 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.098 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.098 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.098 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.099 I llama_model_loader: - type  f32:  194 tensors
0.00.025.099 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.099 I print_info: file format = GGUF V3 (latest)
0.00.025.100 I print_info: file type   = Q6_K
0.00.025.100 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.092 I load: special tokens cache size = 25
0.00.050.451 I load: token to piece cache size = 0.2984 MB
0.00.050.461 I print_info: arch             = gptneox
0.00.050.462 I print_info: vocab_only       = 0
0.00.050.462 I print_info: n_ctx_train      = 2048
0.00.050.462 I print_info: n_embd           = 2048
0.00.050.462 I print_info: n_layer          = 24
0.00.050.467 I print_info: n_head           = 16
0.00.050.468 I print_info: n_head_kv        = 16
0.00.050.468 I print_info: n_rot            = 32
0.00.050.468 I print_info: n_swa            = 0
0.00.050.468 I print_info: n_embd_head_k    = 128
0.00.050.469 I print_info: n_embd_head_v    = 128
0.00.050.469 I print_info: n_gqa            = 1
0.00.050.470 I print_info: n_embd_k_gqa     = 2048
0.00.050.471 I print_info: n_embd_v_gqa     = 2048
0.00.050.471 I print_info: f_norm_eps       = 1.0e-05
0.00.050.472 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.472 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.472 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.472 I print_info: f_logit_scale    = 0.0e+00
0.00.050.473 I print_info: n_ff             = 8192
0.00.050.475 I print_info: n_expert         = 0
0.00.050.476 I print_info: n_expert_used    = 0
0.00.050.477 I print_info: causal attn      = 1
0.00.050.477 I print_info: pooling type     = 0
0.00.050.477 I print_info: rope type        = 2
0.00.050.477 I print_info: rope scaling     = linear
0.00.050.478 I print_info: freq_base_train  = 10000.0
0.00.050.478 I print_info: freq_scale_train = 1
0.00.050.478 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.478 I print_info: rope_finetuned   = unknown
0.00.050.478 I print_info: ssm_d_conv       = 0
0.00.050.479 I print_info: ssm_d_inner      = 0
0.00.050.479 I print_info: ssm_d_state      = 0
0.00.050.479 I print_info: ssm_dt_rank      = 0
0.00.050.479 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.479 I print_info: model type       = 1.4B
0.00.050.480 I print_info: model params     = 1.41 B
0.00.050.480 I print_info: general.name     = 1.4B
0.00.050.481 I print_info: vocab type       = BPE
0.00.050.481 I print_info: n_vocab          = 50304
0.00.050.482 I print_info: n_merges         = 50009
0.00.050.482 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.483 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.483 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.483 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.483 I print_info: LF token         = 128 'Ä'
0.00.050.483 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.484 I print_info: max token length = 1024
0.00.630.621 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.627 I load_tensors: offloading output layer to GPU
0.00.630.628 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.645 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.630.646 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.631.394 I llama_init_from_model: n_seq_max     = 1
0.00.631.398 I llama_init_from_model: n_ctx         = 128
0.00.631.398 I llama_init_from_model: n_ctx_per_seq = 128
0.00.631.399 I llama_init_from_model: n_batch       = 128
0.00.631.402 I llama_init_from_model: n_ubatch      = 128
0.00.631.402 I llama_init_from_model: flash_attn    = 0
0.00.631.403 I llama_init_from_model: freq_base     = 10000.0
0.00.631.404 I llama_init_from_model: freq_scale    = 1
0.00.631.413 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.631.414 I ggml_metal_init: allocating
0.00.631.449 I ggml_metal_init: found device: Apple M4
0.00.631.459 I ggml_metal_init: picking default device: Apple M4
0.00.632.476 I ggml_metal_init: using embedded metal library
0.00.636.602 I ggml_metal_init: GPU name:   Apple M4
0.00.636.610 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.610 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.611 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.612 I ggml_metal_init: simdgroup reduction   = true
0.00.636.612 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.612 I ggml_metal_init: has residency sets    = true
0.00.636.612 I ggml_metal_init: has bfloat            = true
0.00.636.613 I ggml_metal_init: use bfloat            = true
0.00.636.614 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.616 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.180 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.777 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.651.779 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.651.794 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.653.335 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.653.336 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.653.336 I llama_init_from_model: graph nodes  = 967
0.00.653.336 I llama_init_from_model: graph splits = 2
0.00.653.338 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.653.338 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.287 I 
0.00.685.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.337 I perplexity: tokenizing the input ..
0.00.693.076 I perplexity: tokenization took 7.737 ms
0.00.693.089 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.348 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.833.754 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.833.769 I llama_perf_context_print:        load time =     675.90 ms
0.00.833.770 I llama_perf_context_print: prompt eval time =     139.02 ms /   128 tokens (    1.09 ms per token,   920.71 tokens per second)
0.00.833.771 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.773 I llama_perf_context_print:       total time =     148.48 ms /   129 tokens
0.00.834.124 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.083s
sys	0m0.116s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.355 I build: 4565 (df984e01) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.436 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.932 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.943 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.944 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.944 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.944 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.945 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.946 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.947 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.947 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.948 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.948 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.951 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.951 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.661 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.391 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.392 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.393 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.393 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.393 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.394 I llama_model_loader: - type  f32:  194 tensors
0.00.037.394 I llama_model_loader: - type  f16:   98 tensors
0.00.037.395 I print_info: file format = GGUF V3 (latest)
0.00.037.396 I print_info: file type   = all F32 (guessed)
0.00.037.397 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.056.886 I load: special tokens cache size = 25
0.00.063.092 I load: token to piece cache size = 0.2984 MB
0.00.063.095 I print_info: arch             = gptneox
0.00.063.096 I print_info: vocab_only       = 0
0.00.063.096 I print_info: n_ctx_train      = 2048
0.00.063.096 I print_info: n_embd           = 2048
0.00.063.096 I print_info: n_layer          = 24
0.00.063.099 I print_info: n_head           = 16
0.00.063.100 I print_info: n_head_kv        = 16
0.00.063.100 I print_info: n_rot            = 32
0.00.063.100 I print_info: n_swa            = 0
0.00.063.101 I print_info: n_embd_head_k    = 128
0.00.063.101 I print_info: n_embd_head_v    = 128
0.00.063.101 I print_info: n_gqa            = 1
0.00.063.102 I print_info: n_embd_k_gqa     = 2048
0.00.063.103 I print_info: n_embd_v_gqa     = 2048
0.00.063.104 I print_info: f_norm_eps       = 1.0e-05
0.00.063.104 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.104 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.104 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.105 I print_info: f_logit_scale    = 0.0e+00
0.00.063.105 I print_info: n_ff             = 8192
0.00.063.106 I print_info: n_expert         = 0
0.00.063.106 I print_info: n_expert_used    = 0
0.00.063.106 I print_info: causal attn      = 1
0.00.063.106 I print_info: pooling type     = 0
0.00.063.106 I print_info: rope type        = 2
0.00.063.106 I print_info: rope scaling     = linear
0.00.063.106 I print_info: freq_base_train  = 10000.0
0.00.063.107 I print_info: freq_scale_train = 1
0.00.063.107 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.107 I print_info: rope_finetuned   = unknown
0.00.063.107 I print_info: ssm_d_conv       = 0
0.00.063.107 I print_info: ssm_d_inner      = 0
0.00.063.107 I print_info: ssm_d_state      = 0
0.00.063.108 I print_info: ssm_dt_rank      = 0
0.00.063.108 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.108 I print_info: model type       = 1.4B
0.00.063.108 I print_info: model params     = 1.41 B
0.00.063.108 I print_info: general.name     = 1.4B
0.00.063.109 I print_info: vocab type       = BPE
0.00.063.109 I print_info: n_vocab          = 50304
0.00.063.109 I print_info: n_merges         = 50009
0.00.063.110 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.110 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.110 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.110 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.112 I print_info: LF token         = 128 'Ä'
0.00.063.112 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.113 I print_info: max token length = 1024
0.01.388.715 I load_tensors: offloading 24 repeating layers to GPU
0.01.388.722 I load_tensors: offloading output layer to GPU
0.01.388.722 I load_tensors: offloaded 25/25 layers to GPU
0.01.388.757 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.388.759 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.389.869 I llama_init_from_model: n_seq_max     = 1
0.01.389.870 I llama_init_from_model: n_ctx         = 128
0.01.389.870 I llama_init_from_model: n_ctx_per_seq = 128
0.01.389.870 I llama_init_from_model: n_batch       = 128
0.01.389.871 I llama_init_from_model: n_ubatch      = 128
0.01.389.871 I llama_init_from_model: flash_attn    = 0
0.01.389.871 I llama_init_from_model: freq_base     = 10000.0
0.01.389.872 I llama_init_from_model: freq_scale    = 1
0.01.389.872 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.389.873 I ggml_metal_init: allocating
0.01.390.000 I ggml_metal_init: found device: Apple M4
0.01.390.008 I ggml_metal_init: picking default device: Apple M4
0.01.391.268 I ggml_metal_init: using embedded metal library
0.01.394.928 I ggml_metal_init: GPU name:   Apple M4
0.01.394.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.394.931 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.394.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.394.931 I ggml_metal_init: simdgroup reduction   = true
0.01.394.931 I ggml_metal_init: simdgroup matrix mul. = true
0.01.394.932 I ggml_metal_init: has residency sets    = true
0.01.394.932 I ggml_metal_init: has bfloat            = true
0.01.394.932 I ggml_metal_init: use bfloat            = true
0.01.394.932 I ggml_metal_init: hasUnifiedMemory      = true
0.01.394.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.404.997 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.406.718 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.406.722 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.406.739 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.408.378 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.408.380 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.408.380 I llama_init_from_model: graph nodes  = 967
0.01.408.380 I llama_init_from_model: graph splits = 2
0.01.408.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.408.382 I 
0.01.408.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.408.420 I compute_imatrix: tokenizing the input ..
0.01.415.877 I compute_imatrix: tokenization took 7.456 ms
0.01.415.878 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.685.071 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.687.685 I llama_perf_context_print:        load time =    1667.63 ms
0.01.687.686 I llama_perf_context_print: prompt eval time =     267.45 ms /   128 tokens (    2.09 ms per token,   478.59 tokens per second)
0.01.687.687 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.687.687 I llama_perf_context_print:       total time =    1670.24 ms /   129 tokens
0.01.688.264 I ggml_metal_free: deallocating

real	0m1.872s
user	0m0.123s
sys	0m0.251s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4565 (df984e01)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149c05e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149c064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149c06930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149c09620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149c09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149c09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149c0a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149c0aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149c0b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149c0b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149c0ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149c0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149c0ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149c0d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149c0d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149c0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149c0e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149c0ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149c0f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149c0fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149c10560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149c10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149c113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149c11c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149c12360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149c12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149c12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149c138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149c13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149c140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149c14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149c14800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149c15090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149c155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149c15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149c15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149c161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149c16670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149c16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149c16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149c17450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149c178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149c17d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149c18230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149c184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149c18b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149c19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149c19a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149c1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149c1a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149c1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149c1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149c1b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149c1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149c1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149c1cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149c1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149c1d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149c1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149c1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149c1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149c1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149c1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149c1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149c1f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149c1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149c1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149c203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149c20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149c20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149c21180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149c21620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149c21ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149c22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149c22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149c22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149c23000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149c23550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149c23aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149c23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149c24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149c24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149c24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149c25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149c25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149c25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149c26520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149c26a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149c26fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149c27510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149c27a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149c27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149c28500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149c28a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149c28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149c294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149c29a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149c19720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149c29eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149c2a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149c2abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149c2b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149c2b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149c2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149c2c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149c2c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149c2cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149c2d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149c2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149c2db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149c2e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149c2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149c2eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149c2f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149c2f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149c2f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149c2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149c30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149c30730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149c30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149c31070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149c31510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149c319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149c31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149c322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149c32790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149c32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149c330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149c33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149c33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149c33eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149c34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149c347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149c34c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149c35130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149c355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149c35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149c35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149c363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149c36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149c36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149c37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149c37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149c37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149c37f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149c38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149c388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149c38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149c391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149c39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149c39b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149c39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149c3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149c3a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149c3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149c3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149c3b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149c3bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149c3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149c3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149c3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149c3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149c3d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149c3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149c3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149c3e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149c3e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149c3e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149c3ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149c3f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149c3f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149c3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149c400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149c40590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149c40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149c40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149c41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149c41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149c41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149c42150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149c425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149c42a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149c42f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149c433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149c43870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149c43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149c441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149c44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149c44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149c44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149c45430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149c458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149c45d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149c462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149c46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149c46d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149c472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149c47570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149c47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149c48190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149c487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149c48f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149c49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149c496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149c49d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149c4a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149c4ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149c4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149c4b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149c4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149c4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149c4c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149c4cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149c4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149c4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149c4db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149c4e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149c4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149c4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149c4f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149c4f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149c4fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149c50050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149c505a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149c50af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149c51040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149c51590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149c51ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149c52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149c52580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149c52ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149c53020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149c53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149c53ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149c54010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149c54560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149c54ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149c55000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149c55550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149c55aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149c55ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149c56540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149c56a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149c56fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149c57530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149c57a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149c57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149c58520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149c58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149c58fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149c59510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149c59a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149c59fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149c5a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149c5aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149c5afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149c5b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149c5ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149c5bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149c5c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149c5ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149c5cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149c5d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149c5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149c5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149c5e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149c5ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149c5eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149c5f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149c5f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149c5fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149c60130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149c605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149c60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149c60f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149c613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149c61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149c61cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149c62190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149c62630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149c62ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149c62f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149c634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149c63be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149c64300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149c64a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149c65140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149c65400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149c65bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149c65eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149c664c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.712.297 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.712.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149b06210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149b06680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149b06af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149b06f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149b073d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149b07840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149b07cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149b08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149b08590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149b08b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149b08f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149b09600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149b0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149b0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149b0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149b0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149b0bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149b0c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149b0cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149b0d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149b0dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149b0e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149b0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149b0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149b0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149b0fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149b0fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149b102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149b10730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149b10ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149b110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149b115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149b11a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149b11ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149b12150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149b125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149b12b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149b13020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149b13520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149b13a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149b13f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149b14420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149b14920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149b14e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149b15320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149b15790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149b15c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149b16070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149b164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149b16950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149b16dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149b17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149b176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149b17b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149b17f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149b18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149b18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149b18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149b194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149b19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149b1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149b1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149b1aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149b1af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149b1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149b1b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149b1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149b1c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149b1c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149b1caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149b1cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149b1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149b1d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149b1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149b1e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149b1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149b1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149b1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149b1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149b1fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149b20350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149b208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149b20df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149b21340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149b21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149b21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149b22330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149b22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149b22dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149b23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149b23870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149b23dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149b24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149b24860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149b24db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149b25300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149b25850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149b25da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149b262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149b26840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149b26d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149b272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149b27830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149b27d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149b282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149b28820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149b28d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149b292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149b29810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149b29d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149b2a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149b2a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149b2ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149b2b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149b2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149b2bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149b2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149b2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149b2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149b2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149b2d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149b2d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149b2db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149b2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149b2e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149b2e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149b2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149b2f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149b2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149b2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149b30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149b30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149b309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149b30e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149b31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149b317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149b31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149b320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149b32590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149b32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149b32ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149b33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149b33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149b33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149b34150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149b345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149b34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149b34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149b353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149b35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149b35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149b361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149b36650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149b36af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149b36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149b37430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149b378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149b37d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149b38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149b386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149b38b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149b38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149b39490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149b39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149b39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149b3a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149b3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149b3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149b3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149b3b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149b3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149b3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149b3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149b3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149b3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149b3d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149b3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149b3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149b3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149b3e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149b3e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149b3ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149b3f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149b3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149b3fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149b3fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149b40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149b40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149b40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149b41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149b41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149b41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149b41f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149b424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149b429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149b42f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149b43490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149b43750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149b43d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149b44370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149b44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149b45170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149b45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149b458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149b45ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149b464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149b46ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149b47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149b47620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149b47ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149b48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149b487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149b48d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149b49260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149b497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149b49d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149b4a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149b4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149b4acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149b4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149b4b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149b4bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149b4c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149b4c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149b4ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149b4d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149b4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149b4dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149b4e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149b4e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149b4ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149b4f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149b4f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149b4fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149b501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149b50740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149b50c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149b511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149b51730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149b51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149b521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149b52720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149b52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149b531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149b53710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149b53c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149b541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149b54700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149b54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149b551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149b556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149b55c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149b56190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149b566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149b56c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149b57180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149b576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149b57c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149b58170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149b586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149b58c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149b59160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149b596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149b59c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149b5a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149b5a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149b5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149b5b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149b5b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149b5b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149b5be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149b5c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149b5c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149b5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149b5d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149b5d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149b5da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149b5ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149b5e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149b5e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149b5ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149b5f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149b5f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149b5fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149b604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149b60c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149b61320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149b615e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149b61dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149b62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149b626a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149d04760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149d04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149d05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149d054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149d05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149d05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149d06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149d06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149d06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149d06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149d073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149d07ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149d08600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149d08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149d095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149d09ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149d0a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149d0ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149d0b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149d0b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149d0c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149d0c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149d0ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149d0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149d0dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149d0dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149d0e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149d0e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149d0eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149d0efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149d0f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149d0f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149d0fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149d100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149d10520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149d10990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149d10e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149d11270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149d116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149d11b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149d11fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149d12430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149d128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149d12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149d13180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149d135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149d13a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149d13ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149d14340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149d147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149d14c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149d15090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149d15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149d15970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149d15de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149d16250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149d167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149d16cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149d17130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149d175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149d17a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149d17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149d182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149d18760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149d18bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149d19040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149d194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149d19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149d19d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149d1a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149d1a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149d1aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149d1af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149d1b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149d1b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149d1bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149d1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149d1c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149d1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149d1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149d1d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149d1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149d1dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149d1e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149d1e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149d1e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149d1ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149d1f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149d1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149d1fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149d1ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149d203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149d20810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149d20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149d210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149d21560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149d219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149d21e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149d222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149d22720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149d22b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149d23000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149d23470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149d23d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149d23fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149d24430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149d248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149d24d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149d25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149d255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149d25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149d25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149d26340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149d267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149d26c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149d27090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149d27500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149d27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149d27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149d28250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149d286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149d28b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149d28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149d29410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149d29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149d29cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149d2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149d2a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149d2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149d2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149d2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149d2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149d2bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149d2c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149d2c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149d2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149d2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149d2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149d2d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149d2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149d2df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149d2e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149d2e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149d2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149d2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149d2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149d2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149d2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149d30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149d30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149d30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149d31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149d314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149d31930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149d31da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149d32210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149d32680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149d32af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149d32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149d333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149d33840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149d33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149d34120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149d34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149d34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149d34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149d352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149d35750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149d35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149d36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149d364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149d36910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149d36d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149d371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149d37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149d37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149d37f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149d383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149d38820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149d38c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149d39100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149d39570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149d399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149d39e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149d3a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149d3a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149d3aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149d3b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149d3b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149d3b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149d3bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149d3c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149d3c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149d3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149d3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149d3d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149d3d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149d3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149d3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149d3e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149d3e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149d3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149d3f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149d3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149d3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149d3fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149d40460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149d408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149d40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149d411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149d41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149d41ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149d422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149d42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149d42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149d43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149d43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149d438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149d43d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149d441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149d44630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149d44aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149d44f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149d45380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149d457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149d45c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149d460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149d46540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149d469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149d46e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149d47290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149d47700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149d47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149d47fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149d48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149d488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149d48d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149d491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149d49610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149d49a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149d49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149d4a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149d4a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149d4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149d4b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149d4b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149d4b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149d4be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149d4c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149d4c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149d4cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149d4cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149d4d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149d4d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149d4dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149d4e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149d4e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149d4ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149d4eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149d4f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149d4f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149d4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149d50090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149d50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149d50970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149d50de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149d51250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149d516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149d51b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149d51fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149d52410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149d52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149d52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149d53160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149d535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149d53a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149d53eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149d54320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149d54790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149d54c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149d55070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149d554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149d55950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149d563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149d56ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149d57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149d57920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149d57be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149d58050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149d58650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149d58c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.765s
user	0m0.287s
sys	0m0.304s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4565 (df984e01)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c00a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c00aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c00b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c00b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c00bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c00c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c00c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c00cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c00d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c00d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c00dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c00e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c00ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c00f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c00fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c0103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c010ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c011200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c0120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c012f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c013650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c013ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c014610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c0148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c014ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c015b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c016090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c016350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c0167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c016ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c017340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c017880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c017b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c017fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c018480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c018920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c018dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c019260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c019700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c019ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c01a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c01a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c01a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c01adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c01b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c01bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c01c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c01c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c01cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c01d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c01db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c01e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c01e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c01edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c01f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c01f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c01fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c020330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c0205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c020a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c020f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c0213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c021870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c021d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c0221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c022650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c022af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c022f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c023430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c0238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c023d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c0242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c024810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c024d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c0252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c025800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c025d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c0262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c0267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c026d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c027290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c0277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c027d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c028280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c0287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c028d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c029270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c0297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c029d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c02a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c02a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c02ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c02b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c02b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c02bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c01b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c02c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c02c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c02ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c02d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c02d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c02de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c02e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c02e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c02ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c02f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c02f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c02fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c030380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c0308d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c0312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c031760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c031c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c0320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c032540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c0329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c032e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c033320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c0337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c033c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c034100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c0345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c034a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c034ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c035380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c035820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c035cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c036160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c036600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c036aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c036f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c0373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c037880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c037d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c0381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c038660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c038b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c038fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c039440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c0398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c039d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c03a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c03a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c03ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c03b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c03b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c03b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c03bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c03c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c03c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c03cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c03d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c03d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c03d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c03de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c03e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c03e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c03ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c03f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c03f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c03fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c03fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c040340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c0407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c040c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c041120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c0415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c041a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c041f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c0423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c042840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c042ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c043180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c043620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c043ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c043f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c044400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c0448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c044d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c0451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c045680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c045b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c045fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c046460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c046900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c046da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c047240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c0476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c047b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c048020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c048570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c048ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c049560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c049820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c049e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c04a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c04aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c04b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c04b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c04b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c04bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c04c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c04cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c04d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c04d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c04db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c04e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c04e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c04ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c04f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c04f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c04fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c050320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c050870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c050dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c051310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c051860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c051db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c052300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c052850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c052da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c0532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c053840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c053d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c0542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c054830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c054d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c0552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c055820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c055d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c0562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c056810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c056d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c0572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c057800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c057d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c0582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c0587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c058d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c059290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c0597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c059d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c05a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c05a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c05ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c05b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c05b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c05bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c05c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c05c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c05cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c05d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c05d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c05dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c05e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c05e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c05ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c05f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c05f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c05fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c060220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c060770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c060cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c061160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c061600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c061aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c061f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c0623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c062880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c062d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c0631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c063660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c063b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c063fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c064440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c0648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c064d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c065220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c065770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c065e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c0665b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c066cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c0673f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c0676b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c067ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c068160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c068770 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.110.567 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a705690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a705b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a705f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a7063e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a706850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a706cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a707130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a7075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a707a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a707e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a7082f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a7089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a7094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a709c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a70a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a70abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a70b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a70b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a70c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a70c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a70d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a70d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a70de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a70e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a70ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a70ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a70f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a70f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a70fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a70ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a7103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a7108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a710d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a711020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a711490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a711900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a711d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a7121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a712650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a712ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a712f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a7133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a713810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a713c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a7140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a714560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a7149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a714e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a7152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a715720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a715b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a716000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a716470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a7168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a716d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a7171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a717730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a717c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a7180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a718510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a718980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a718df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a719260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a7196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a719b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a719fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a71a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a71a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a71ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a71b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a71b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a71ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a71bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a71c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a71c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a71cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a71d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a71d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a71d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a71ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a71e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a71e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a71eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a71ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a71f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a71f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a71fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a720150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a7205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a720a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a720ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a721310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a721780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a721bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a722060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a7224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a722940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a722db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a723220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a723690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a723b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a723f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a7243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a724850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a724cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a725130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a7255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a725a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a725e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a7262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a726760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a726bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a727040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a7274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a727920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a727d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a728200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a728670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a728ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a728f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a7293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a729830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a729ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a72a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a72a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a72a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a72ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a72b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a72b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a72bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a72c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a72c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a72c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a72cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a72d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a72d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a72dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a72df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a72e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a72e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a72ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a72f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a72f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a72f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a72fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a7302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a730720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a730b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a731000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a731470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a7318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a731d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a7321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a732630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a732aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a732f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a733380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a7337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a733c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a7340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a734540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a7349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a734e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a735290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a735700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a735b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a7367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a736a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a736d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a737190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a737600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a737a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a737ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a738350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a7387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a738c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a7390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a739510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a739980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a739df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a73a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a73a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a73ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a73afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a73b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a73b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a73bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a73c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a73c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a73ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a73cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a73d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a73d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a73dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a73e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a73e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a73e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a73edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a73f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a73f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a73fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a73ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a7404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a740a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a740e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a7412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a741750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a741bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a7420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a7425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a743160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a743420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a7439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a743fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a744560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a744b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a7450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a7456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a745c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a746220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a7467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a746da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a747360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a747920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a747ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a7484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a748a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a749020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a7495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a749ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a74a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a74a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a74ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a74b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a74b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a74be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a74c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a74c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a74cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a74d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a74dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a74e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a74e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a74ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a74f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a74f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a74fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a750320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a7508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a750ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a751460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a751a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a751fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a7525a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a752b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a753120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a7536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a753ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a754260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a754820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a754de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a7553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a755960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a755f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a7564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a756aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a757060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a757620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a757b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a758020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a758520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a758a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a758f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a759420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a759920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a759e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a75a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a75a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a75ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a75b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a75b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a75bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a75c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a75cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a75d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a75d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a75e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a75e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a75eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a75ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a75f410 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a60aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a60aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a60b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a60b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a60bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a60c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a60c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a60c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a60cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a60d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a60d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a60de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a60e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a60f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a60f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a610050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a610770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a610e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a6115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a611d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a6124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a612bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a6132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a613a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a614120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a6143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a6146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a614b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a614f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a6153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a615860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a615d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a6164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a616930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a617680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a617af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a617f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a6183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a618840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a618cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a619590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a619a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a619e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a61a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a61a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a61abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a61b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a61b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a61b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a61bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a61c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a61c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a61cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a61d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a61d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a61d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a61de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a61e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a61e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a61eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a61efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a61f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a61f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a61fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a6201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a620610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a620ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a621360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a6217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a6220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a622e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a6236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a623b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a623fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a624430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a6248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a624d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a6255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a625a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a626340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a6267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a626c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a627500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a627970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a6286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a628fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a629410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a629880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a62a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a62a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a62a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a62acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a62b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a62b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a62ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a62be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a62c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a62c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a62cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a62d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a62d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a62d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a62dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a62e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a62e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a62ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a62ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a62f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a62f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a62fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a630100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a630570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a6309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a630e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a6312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a631730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a631ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a632010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a632480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a6328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a632d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a6331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a633640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a633f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a634390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a634800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a634c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a6350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a635550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a6359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a635e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a6362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a636b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a636ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a637460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a6378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a637d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a6381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a638620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a638a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a638f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a639370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a6397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a639c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a63a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a63a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a63a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a63ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a63b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a63b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a63bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a63bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a63c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a63c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a63cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a63d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a63d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a63da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a63dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a63e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a63e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a63ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a63f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a63f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a63f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a63fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a640260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a6406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a640fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a641420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a641d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a642170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a6425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a642a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a642ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a643330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a6437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a643c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a644080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a6444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a644960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a644dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a645240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a6456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a646400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a646870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a646ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a647150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a6475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a648140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a648400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a6486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a648b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a648fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a649410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a649880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a649cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a64a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a64a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a64aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a64aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a64b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a64b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a64bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a64c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a64c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a64c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a64cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a64d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a64d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a64db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a64df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a64e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a64e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a64ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a64f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a64f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a64fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a64fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a650300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a650770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a650be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a651050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a6514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a651930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a651da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a652210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a652680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a652af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a652f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a6533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a653840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a653cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a654120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a654590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a654a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a654e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a6552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a655750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a655bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a656030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a6564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a656910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a656d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a6571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a657660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a657ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a657f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a6583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a658820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a658c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a659100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a659570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a6599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a659e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a65a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a65a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a65aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a65b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a65b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a65b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a65bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a65c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a65cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a65d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a65dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a65dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a65e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a65ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a65f070 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.973s
user	0m0.246s
sys	0m0.190s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
