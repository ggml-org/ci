### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.25 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.14 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.46 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.30 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.67 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.65 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.24 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.20 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.64 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.27 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.91 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.97 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.62 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.79 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 165.60 sec*proc (29 tests)

Total Test time (real) = 165.61 sec

real	2m45.674s
user	4m41.080s
sys	0m5.703s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.04 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.04 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.81 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.20 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.37 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.31 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.30 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.03 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.06 sec*proc (29 tests)

Total Test time (real) =  48.07 sec

real	0m48.081s
user	0m54.482s
sys	0m5.169s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.096 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.963 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.665 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.674 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.684 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.686 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.687 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.687 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.689 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.689 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.690 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.691 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.691 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.695 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.696 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.697 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.697 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.698 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.699 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.702 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.534 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.536 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.537 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.537 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.538 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.538 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.539 I llama_model_loader: - type  f32:  124 tensors
0.00.027.539 I llama_model_loader: - type  f16:   73 tensors
0.00.027.540 I print_info: file format = GGUF V3 (latest)
0.00.027.541 I print_info: file type   = F16
0.00.027.541 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.586 I load: special tokens cache size = 5
0.00.033.844 I load: token to piece cache size = 0.2032 MB
0.00.033.848 I print_info: arch             = bert
0.00.033.849 I print_info: vocab_only       = 0
0.00.033.849 I print_info: n_ctx_train      = 512
0.00.033.849 I print_info: n_embd           = 384
0.00.033.849 I print_info: n_layer          = 12
0.00.033.852 I print_info: n_head           = 12
0.00.033.853 I print_info: n_head_kv        = 12
0.00.033.853 I print_info: n_rot            = 32
0.00.033.854 I print_info: n_swa            = 0
0.00.033.854 I print_info: n_embd_head_k    = 32
0.00.033.854 I print_info: n_embd_head_v    = 32
0.00.033.857 I print_info: n_gqa            = 1
0.00.033.858 I print_info: n_embd_k_gqa     = 384
0.00.033.859 I print_info: n_embd_v_gqa     = 384
0.00.033.860 I print_info: f_norm_eps       = 1.0e-12
0.00.033.860 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.861 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.862 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.864 I print_info: f_logit_scale    = 0.0e+00
0.00.033.865 I print_info: n_ff             = 1536
0.00.033.866 I print_info: n_expert         = 0
0.00.033.866 I print_info: n_expert_used    = 0
0.00.033.866 I print_info: causal attn      = 0
0.00.033.868 I print_info: pooling type     = 2
0.00.033.869 I print_info: rope type        = 2
0.00.033.869 I print_info: rope scaling     = linear
0.00.033.870 I print_info: freq_base_train  = 10000.0
0.00.033.870 I print_info: freq_scale_train = 1
0.00.033.876 I print_info: n_ctx_orig_yarn  = 512
0.00.033.876 I print_info: rope_finetuned   = unknown
0.00.033.877 I print_info: ssm_d_conv       = 0
0.00.033.877 I print_info: ssm_d_inner      = 0
0.00.033.877 I print_info: ssm_d_state      = 0
0.00.033.877 I print_info: ssm_dt_rank      = 0
0.00.033.877 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.878 I print_info: model type       = 33M
0.00.033.878 I print_info: model params     = 33.21 M
0.00.033.879 I print_info: general.name     = Bge Small
0.00.033.879 I print_info: vocab type       = WPM
0.00.033.880 I print_info: n_vocab          = 30522
0.00.033.880 I print_info: n_merges         = 0
0.00.033.880 I print_info: BOS token        = 101 '[CLS]'
0.00.033.880 I print_info: UNK token        = 100 '[UNK]'
0.00.033.881 I print_info: SEP token        = 102 '[SEP]'
0.00.033.881 I print_info: PAD token        = 0 '[PAD]'
0.00.033.881 I print_info: MASK token       = 103 '[MASK]'
0.00.033.881 I print_info: LF token         = 0 '[PAD]'
0.00.033.882 I print_info: max token length = 21
0.00.033.882 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.037.457 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.459 I load_tensors: offloading output layer to GPU
0.00.037.459 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.483 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.484 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.813 I llama_init_from_model: n_seq_max     = 1
0.00.037.814 I llama_init_from_model: n_ctx         = 512
0.00.037.815 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.815 I llama_init_from_model: n_batch       = 2048
0.00.037.815 I llama_init_from_model: n_ubatch      = 2048
0.00.037.815 I llama_init_from_model: flash_attn    = 0
0.00.037.816 I llama_init_from_model: freq_base     = 10000.0
0.00.037.816 I llama_init_from_model: freq_scale    = 1
0.00.037.817 I ggml_metal_init: allocating
0.00.037.829 I ggml_metal_init: found device: Apple M4
0.00.037.835 I ggml_metal_init: picking default device: Apple M4
0.00.038.589 I ggml_metal_init: using embedded metal library
0.00.042.703 I ggml_metal_init: GPU name:   Apple M4
0.00.042.706 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.706 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.707 I ggml_metal_init: simdgroup reduction   = true
0.00.042.707 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.707 I ggml_metal_init: has residency sets    = true
0.00.042.708 I ggml_metal_init: has bfloat            = true
0.00.042.708 I ggml_metal_init: use bfloat            = true
0.00.042.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.731 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.055.456 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.459 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.482 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.743 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.744 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.745 I llama_init_from_model: graph nodes  = 429
0.00.056.745 I llama_init_from_model: graph splits = 2
0.00.056.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.063.081 I 
0.00.063.111 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.063.806 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.069.129 I llama_perf_context_print:        load time =      47.11 ms
0.00.069.130 I llama_perf_context_print: prompt eval time =       5.17 ms /     9 tokens (    0.57 ms per token,  1740.81 tokens per second)
0.00.069.131 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.069.132 I llama_perf_context_print:       total time =       6.05 ms /    10 tokens
0.00.069.347 I ggml_metal_free: deallocating

real	0m0.246s
user	0m0.048s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.043 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.180 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.013.032 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.013.036 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.037 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.013.038 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.038 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.013.039 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.013.039 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.013.042 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.013.043 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.013.043 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.013.043 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.013.044 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.013.046 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.013.046 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.013.047 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.013.047 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.013.047 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.013.048 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.615 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.300 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.301 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.302 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.302 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.303 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.303 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.016.303 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.016.304 I llama_model_loader: - type  f32:  124 tensors
0.00.016.304 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.304 I print_info: file format = GGUF V3 (latest)
0.00.016.305 I print_info: file type   = Q8_0
0.00.016.306 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.879 I load: special tokens cache size = 5
0.00.020.231 I load: token to piece cache size = 0.2032 MB
0.00.020.234 I print_info: arch             = bert
0.00.020.235 I print_info: vocab_only       = 0
0.00.020.235 I print_info: n_ctx_train      = 512
0.00.020.235 I print_info: n_embd           = 384
0.00.020.235 I print_info: n_layer          = 12
0.00.020.239 I print_info: n_head           = 12
0.00.020.240 I print_info: n_head_kv        = 12
0.00.020.240 I print_info: n_rot            = 32
0.00.020.240 I print_info: n_swa            = 0
0.00.020.240 I print_info: n_embd_head_k    = 32
0.00.020.240 I print_info: n_embd_head_v    = 32
0.00.020.241 I print_info: n_gqa            = 1
0.00.020.242 I print_info: n_embd_k_gqa     = 384
0.00.020.242 I print_info: n_embd_v_gqa     = 384
0.00.020.243 I print_info: f_norm_eps       = 1.0e-12
0.00.020.243 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.020.244 I print_info: f_clamp_kqv      = 0.0e+00
0.00.020.244 I print_info: f_max_alibi_bias = 0.0e+00
0.00.020.244 I print_info: f_logit_scale    = 0.0e+00
0.00.020.245 I print_info: n_ff             = 1536
0.00.020.247 I print_info: n_expert         = 0
0.00.020.247 I print_info: n_expert_used    = 0
0.00.020.247 I print_info: causal attn      = 0
0.00.020.247 I print_info: pooling type     = 2
0.00.020.247 I print_info: rope type        = 2
0.00.020.247 I print_info: rope scaling     = linear
0.00.020.248 I print_info: freq_base_train  = 10000.0
0.00.020.248 I print_info: freq_scale_train = 1
0.00.020.248 I print_info: n_ctx_orig_yarn  = 512
0.00.020.248 I print_info: rope_finetuned   = unknown
0.00.020.248 I print_info: ssm_d_conv       = 0
0.00.020.248 I print_info: ssm_d_inner      = 0
0.00.020.249 I print_info: ssm_d_state      = 0
0.00.020.249 I print_info: ssm_dt_rank      = 0
0.00.020.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.020.249 I print_info: model type       = 33M
0.00.020.249 I print_info: model params     = 33.21 M
0.00.020.249 I print_info: general.name     = Bge Small
0.00.020.250 I print_info: vocab type       = WPM
0.00.020.250 I print_info: n_vocab          = 30522
0.00.020.250 I print_info: n_merges         = 0
0.00.020.250 I print_info: BOS token        = 101 '[CLS]'
0.00.020.251 I print_info: UNK token        = 100 '[UNK]'
0.00.020.251 I print_info: SEP token        = 102 '[SEP]'
0.00.020.251 I print_info: PAD token        = 0 '[PAD]'
0.00.020.251 I print_info: MASK token       = 103 '[MASK]'
0.00.020.251 I print_info: LF token         = 0 '[PAD]'
0.00.020.252 I print_info: max token length = 21
0.00.020.256 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.022.131 I load_tensors: offloading 12 repeating layers to GPU
0.00.022.132 I load_tensors: offloading output layer to GPU
0.00.022.133 I load_tensors: offloaded 13/13 layers to GPU
0.00.022.139 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.022.140 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.022.369 I llama_init_from_model: n_seq_max     = 1
0.00.022.370 I llama_init_from_model: n_ctx         = 512
0.00.022.370 I llama_init_from_model: n_ctx_per_seq = 512
0.00.022.370 I llama_init_from_model: n_batch       = 2048
0.00.022.370 I llama_init_from_model: n_ubatch      = 2048
0.00.022.370 I llama_init_from_model: flash_attn    = 0
0.00.022.374 I llama_init_from_model: freq_base     = 10000.0
0.00.022.374 I llama_init_from_model: freq_scale    = 1
0.00.022.375 I ggml_metal_init: allocating
0.00.022.391 I ggml_metal_init: found device: Apple M4
0.00.022.396 I ggml_metal_init: picking default device: Apple M4
0.00.023.026 I ggml_metal_init: using embedded metal library
0.00.025.738 I ggml_metal_init: GPU name:   Apple M4
0.00.025.740 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.740 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.740 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.740 I ggml_metal_init: simdgroup reduction   = true
0.00.025.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.742 I ggml_metal_init: has residency sets    = true
0.00.025.742 I ggml_metal_init: has bfloat            = true
0.00.025.742 I ggml_metal_init: use bfloat            = true
0.00.025.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.216 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.909 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.911 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.925 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.037.145 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.037.146 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.037.146 I llama_init_from_model: graph nodes  = 429
0.00.037.147 I llama_init_from_model: graph splits = 2
0.00.037.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.037.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.524 I 
0.00.041.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.042.094 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.560 I llama_perf_context_print:        load time =      31.34 ms
0.00.046.561 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2072.30 tokens per second)
0.00.046.562 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.562 I llama_perf_context_print:       total time =       5.04 ms /    10 tokens
0.00.046.914 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.257 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.503 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.946 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.951 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.953 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.957 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.957 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.958 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.959 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.960 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.961 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.962 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.965 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.966 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.969 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.970 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.971 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.971 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.972 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.388 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.411 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.758 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.760 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.761 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.761 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.761 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.762 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.762 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.762 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.763 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.763 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.764 I llama_model_loader: - type  f32:   40 tensors
0.00.048.766 I llama_model_loader: - type  f16:   30 tensors
0.00.048.766 I print_info: file format = GGUF V3 (latest)
0.00.048.767 I print_info: file type   = F16
0.00.048.768 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.982 W load: empty token at index 5
0.00.058.329 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.910 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.944 I load: special tokens cache size = 5
0.00.322.609 I load: token to piece cache size = 1.5060 MB
0.00.322.614 I print_info: arch             = jina-bert-v2
0.00.322.614 I print_info: vocab_only       = 0
0.00.322.614 I print_info: n_ctx_train      = 8192
0.00.322.615 I print_info: n_embd           = 384
0.00.322.615 I print_info: n_layer          = 4
0.00.322.619 I print_info: n_head           = 12
0.00.322.621 I print_info: n_head_kv        = 12
0.00.322.622 I print_info: n_rot            = 32
0.00.322.622 I print_info: n_swa            = 0
0.00.322.622 I print_info: n_embd_head_k    = 32
0.00.322.622 I print_info: n_embd_head_v    = 32
0.00.322.622 I print_info: n_gqa            = 1
0.00.322.623 I print_info: n_embd_k_gqa     = 384
0.00.322.624 I print_info: n_embd_v_gqa     = 384
0.00.322.624 I print_info: f_norm_eps       = 1.0e-12
0.00.322.624 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.322.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.322.625 I print_info: f_max_alibi_bias = 8.0e+00
0.00.322.625 I print_info: f_logit_scale    = 0.0e+00
0.00.322.626 I print_info: n_ff             = 1536
0.00.322.626 I print_info: n_expert         = 0
0.00.322.626 I print_info: n_expert_used    = 0
0.00.322.626 I print_info: causal attn      = 0
0.00.322.626 I print_info: pooling type     = -1
0.00.322.626 I print_info: rope type        = -1
0.00.322.627 I print_info: rope scaling     = linear
0.00.322.627 I print_info: freq_base_train  = 10000.0
0.00.322.627 I print_info: freq_scale_train = 1
0.00.322.627 I print_info: n_ctx_orig_yarn  = 8192
0.00.322.634 I print_info: rope_finetuned   = unknown
0.00.322.636 I print_info: ssm_d_conv       = 0
0.00.322.637 I print_info: ssm_d_inner      = 0
0.00.322.637 I print_info: ssm_d_state      = 0
0.00.322.637 I print_info: ssm_dt_rank      = 0
0.00.322.637 I print_info: ssm_dt_b_c_rms   = 0
0.00.322.637 I print_info: model type       = 33M
0.00.322.638 I print_info: model params     = 32.90 M
0.00.322.638 I print_info: general.name     = Jina Bert Implementation
0.00.322.640 I print_info: vocab type       = BPE
0.00.322.641 I print_info: n_vocab          = 61056
0.00.322.641 I print_info: n_merges         = 39382
0.00.322.644 I print_info: BOS token        = 0 '<s>'
0.00.322.644 I print_info: EOS token        = 2 '</s>'
0.00.322.644 I print_info: UNK token        = 3 '<unk>'
0.00.322.644 I print_info: SEP token        = 2 '</s>'
0.00.322.644 I print_info: PAD token        = 1 '<pad>'
0.00.322.645 I print_info: MASK token       = 4 '<mask>'
0.00.322.645 I print_info: EOG token        = 2 '</s>'
0.00.322.645 I print_info: max token length = 45
0.00.322.645 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.324.332 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.333 I load_tensors: offloading output layer to GPU
0.00.324.333 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.357 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.358 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.683 I llama_init_from_model: n_seq_max     = 1
0.00.324.683 I llama_init_from_model: n_ctx         = 8192
0.00.324.684 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.684 I llama_init_from_model: n_batch       = 2048
0.00.324.684 I llama_init_from_model: n_ubatch      = 2048
0.00.324.684 I llama_init_from_model: flash_attn    = 0
0.00.324.684 I llama_init_from_model: freq_base     = 10000.0
0.00.324.685 I llama_init_from_model: freq_scale    = 1
0.00.324.685 I ggml_metal_init: allocating
0.00.324.689 I ggml_metal_init: found device: Apple M4
0.00.324.692 I ggml_metal_init: picking default device: Apple M4
0.00.325.401 I ggml_metal_init: using embedded metal library
0.00.328.353 I ggml_metal_init: GPU name:   Apple M4
0.00.328.354 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.328.355 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.328.355 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.328.356 I ggml_metal_init: simdgroup reduction   = true
0.00.328.356 I ggml_metal_init: simdgroup matrix mul. = true
0.00.328.356 I ggml_metal_init: has residency sets    = true
0.00.328.356 I ggml_metal_init: has bfloat            = true
0.00.328.356 I ggml_metal_init: use bfloat            = true
0.00.328.357 I ggml_metal_init: hasUnifiedMemory      = true
0.00.328.359 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.338.162 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.341.258 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.341.259 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.341.279 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.348.564 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.348.566 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.348.566 I llama_init_from_model: graph nodes  = 154
0.00.348.566 I llama_init_from_model: graph splits = 2
0.00.348.568 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.568 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.126 I 
0.00.356.157 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.494 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.356.495 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.356.504 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.356.504 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.356.510 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.356.510 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.357.032 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.360.838 I llama_perf_context_print:        load time =     333.62 ms
0.00.360.839 I llama_perf_context_print: prompt eval time =       3.80 ms /    62 tokens (    0.06 ms per token, 16315.79 tokens per second)
0.00.360.840 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.360.840 I llama_perf_context_print:       total time =       4.71 ms /    63 tokens
0.00.361.063 I ggml_metal_free: deallocating

real	0m1.057s
user	0m0.329s
sys	0m0.051s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.171 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.368 I main: llama backend init
0.00.000.373 I main: load the model and apply lora adapter, if any
0.00.050.277 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.062.701 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.062.714 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.062.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.062.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.062.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.062.723 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.062.726 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.062.729 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.062.729 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.062.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.062.731 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.062.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.062.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.062.733 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.062.738 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.062.739 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.062.739 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.069.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.071.715 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.450 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.078.459 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.460 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.461 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.462 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.463 I llama_model_loader: - type  f32:  194 tensors
0.00.078.463 I llama_model_loader: - type  f16:   98 tensors
0.00.078.465 I print_info: file format = GGUF V3 (latest)
0.00.078.467 I print_info: file type   = all F32 (guessed)
0.00.078.471 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.730 I load: special tokens cache size = 25
0.00.107.218 I load: token to piece cache size = 0.2984 MB
0.00.107.224 I print_info: arch             = gptneox
0.00.107.225 I print_info: vocab_only       = 0
0.00.107.225 I print_info: n_ctx_train      = 2048
0.00.107.225 I print_info: n_embd           = 2048
0.00.107.225 I print_info: n_layer          = 24
0.00.107.231 I print_info: n_head           = 16
0.00.107.233 I print_info: n_head_kv        = 16
0.00.107.234 I print_info: n_rot            = 32
0.00.107.234 I print_info: n_swa            = 0
0.00.107.234 I print_info: n_embd_head_k    = 128
0.00.107.234 I print_info: n_embd_head_v    = 128
0.00.107.235 I print_info: n_gqa            = 1
0.00.107.236 I print_info: n_embd_k_gqa     = 2048
0.00.107.239 I print_info: n_embd_v_gqa     = 2048
0.00.107.240 I print_info: f_norm_eps       = 1.0e-05
0.00.107.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.107.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.107.242 I print_info: f_max_alibi_bias = 0.0e+00
0.00.107.242 I print_info: f_logit_scale    = 0.0e+00
0.00.107.243 I print_info: n_ff             = 8192
0.00.107.244 I print_info: n_expert         = 0
0.00.107.244 I print_info: n_expert_used    = 0
0.00.107.244 I print_info: causal attn      = 1
0.00.107.244 I print_info: pooling type     = 0
0.00.107.244 I print_info: rope type        = 2
0.00.107.245 I print_info: rope scaling     = linear
0.00.107.245 I print_info: freq_base_train  = 10000.0
0.00.107.245 I print_info: freq_scale_train = 1
0.00.107.246 I print_info: n_ctx_orig_yarn  = 2048
0.00.107.246 I print_info: rope_finetuned   = unknown
0.00.107.246 I print_info: ssm_d_conv       = 0
0.00.107.246 I print_info: ssm_d_inner      = 0
0.00.107.247 I print_info: ssm_d_state      = 0
0.00.107.247 I print_info: ssm_dt_rank      = 0
0.00.107.247 I print_info: ssm_dt_b_c_rms   = 0
0.00.107.247 I print_info: model type       = 1.4B
0.00.107.252 I print_info: model params     = 1.41 B
0.00.107.252 I print_info: general.name     = 1.4B
0.00.107.253 I print_info: vocab type       = BPE
0.00.107.254 I print_info: n_vocab          = 50304
0.00.107.254 I print_info: n_merges         = 50009
0.00.107.254 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.107.255 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.107.255 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.107.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.107.255 I print_info: LF token         = 187 ''
0.00.107.256 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.107.256 I print_info: max token length = 1024
0.00.107.257 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.179.750 I load_tensors: offloading 24 repeating layers to GPU
0.00.179.754 I load_tensors: offloading output layer to GPU
0.00.179.754 I load_tensors: offloaded 25/25 layers to GPU
0.00.179.782 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.179.783 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.180.395 I llama_init_from_model: n_seq_max     = 1
0.00.180.397 I llama_init_from_model: n_ctx         = 2048
0.00.180.397 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.180.397 I llama_init_from_model: n_batch       = 2048
0.00.180.397 I llama_init_from_model: n_ubatch      = 512
0.00.180.397 I llama_init_from_model: flash_attn    = 0
0.00.180.398 I llama_init_from_model: freq_base     = 10000.0
0.00.180.398 I llama_init_from_model: freq_scale    = 1
0.00.180.400 I ggml_metal_init: allocating
0.00.180.440 I ggml_metal_init: found device: Apple M4
0.00.180.446 I ggml_metal_init: picking default device: Apple M4
0.00.181.108 I ggml_metal_init: using embedded metal library
0.00.272.002 I ggml_metal_init: GPU name:   Apple M4
0.00.272.009 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.272.009 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.272.010 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.272.010 I ggml_metal_init: simdgroup reduction   = true
0.00.272.010 I ggml_metal_init: simdgroup matrix mul. = true
0.00.272.010 I ggml_metal_init: has residency sets    = true
0.00.272.010 I ggml_metal_init: has bfloat            = true
0.00.272.011 I ggml_metal_init: use bfloat            = true
0.00.272.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.272.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.298.752 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.329.182 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.329.189 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.329.232 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.332.940 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.332.942 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.332.942 I llama_init_from_model: graph nodes  = 967
0.00.332.943 I llama_init_from_model: graph splits = 2
0.00.332.948 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.333.081 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.333.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.400.699 I main: llama threadpool init, n_threads = 4
0.00.400.744 I 
0.00.400.774 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.400.775 I 
0.00.400.956 I sampler seed: 1234
0.00.400.960 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.400.985 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.400.987 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.400.987 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.225.296 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60631.94 tokens per second)
0.02.225.296 I llama_perf_context_print:        load time =     349.54 ms
0.02.225.297 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.20 tokens per second)
0.02.225.299 I llama_perf_context_print:        eval time =    1777.75 ms /    63 runs   (   28.22 ms per token,    35.44 tokens per second)
0.02.225.299 I llama_perf_context_print:       total time =    1825.47 ms /    70 tokens
0.02.225.525 I ggml_metal_free: deallocating

real	0m2.535s
user	0m0.135s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.525 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.320 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.325 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.327 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.328 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.328 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.329 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.329 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.331 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.331 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.332 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.332 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.332 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.334 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.335 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.337 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.337 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.338 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.404 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.212 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.672 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.674 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.674 I llama_model_loader: - type  f32:  194 tensors
0.00.052.675 I llama_model_loader: - type  f16:   98 tensors
0.00.052.675 I print_info: file format = GGUF V3 (latest)
0.00.052.676 I print_info: file type   = all F32 (guessed)
0.00.052.678 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.535 I load: special tokens cache size = 25
0.00.072.260 I load: token to piece cache size = 0.2984 MB
0.00.072.263 I print_info: arch             = gptneox
0.00.072.264 I print_info: vocab_only       = 0
0.00.072.264 I print_info: n_ctx_train      = 2048
0.00.072.264 I print_info: n_embd           = 2048
0.00.072.264 I print_info: n_layer          = 24
0.00.072.267 I print_info: n_head           = 16
0.00.072.268 I print_info: n_head_kv        = 16
0.00.072.269 I print_info: n_rot            = 32
0.00.072.269 I print_info: n_swa            = 0
0.00.072.269 I print_info: n_embd_head_k    = 128
0.00.072.269 I print_info: n_embd_head_v    = 128
0.00.072.270 I print_info: n_gqa            = 1
0.00.072.271 I print_info: n_embd_k_gqa     = 2048
0.00.072.271 I print_info: n_embd_v_gqa     = 2048
0.00.072.272 I print_info: f_norm_eps       = 1.0e-05
0.00.072.272 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.272 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.273 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.273 I print_info: f_logit_scale    = 0.0e+00
0.00.072.275 I print_info: n_ff             = 8192
0.00.072.275 I print_info: n_expert         = 0
0.00.072.275 I print_info: n_expert_used    = 0
0.00.072.276 I print_info: causal attn      = 1
0.00.072.276 I print_info: pooling type     = 0
0.00.072.276 I print_info: rope type        = 2
0.00.072.276 I print_info: rope scaling     = linear
0.00.072.277 I print_info: freq_base_train  = 10000.0
0.00.072.277 I print_info: freq_scale_train = 1
0.00.072.277 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.277 I print_info: rope_finetuned   = unknown
0.00.072.278 I print_info: ssm_d_conv       = 0
0.00.072.278 I print_info: ssm_d_inner      = 0
0.00.072.278 I print_info: ssm_d_state      = 0
0.00.072.278 I print_info: ssm_dt_rank      = 0
0.00.072.278 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.278 I print_info: model type       = 1.4B
0.00.072.279 I print_info: model params     = 1.41 B
0.00.072.281 I print_info: general.name     = 1.4B
0.00.072.281 I print_info: vocab type       = BPE
0.00.072.281 I print_info: n_vocab          = 50304
0.00.072.282 I print_info: n_merges         = 50009
0.00.072.282 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.283 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.284 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.284 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.284 I print_info: LF token         = 187 ''
0.00.072.284 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.284 I print_info: max token length = 1024
0.00.072.285 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.383.594 I load_tensors: offloading 24 repeating layers to GPU
0.01.383.599 I load_tensors: offloading output layer to GPU
0.01.383.599 I load_tensors: offloaded 25/25 layers to GPU
0.01.383.629 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.383.631 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.384.691 I llama_init_from_model: n_seq_max     = 1
0.01.384.692 I llama_init_from_model: n_ctx         = 128
0.01.384.693 I llama_init_from_model: n_ctx_per_seq = 128
0.01.384.693 I llama_init_from_model: n_batch       = 128
0.01.384.693 I llama_init_from_model: n_ubatch      = 128
0.01.384.693 I llama_init_from_model: flash_attn    = 0
0.01.384.694 I llama_init_from_model: freq_base     = 10000.0
0.01.384.694 I llama_init_from_model: freq_scale    = 1
0.01.384.695 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.384.698 I ggml_metal_init: allocating
0.01.384.774 I ggml_metal_init: found device: Apple M4
0.01.384.783 I ggml_metal_init: picking default device: Apple M4
0.01.385.975 I ggml_metal_init: using embedded metal library
0.01.389.993 I ggml_metal_init: GPU name:   Apple M4
0.01.389.995 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.389.996 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.389.996 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.389.996 I ggml_metal_init: simdgroup reduction   = true
0.01.389.997 I ggml_metal_init: simdgroup matrix mul. = true
0.01.389.997 I ggml_metal_init: has residency sets    = true
0.01.389.997 I ggml_metal_init: has bfloat            = true
0.01.389.997 I ggml_metal_init: use bfloat            = true
0.01.389.998 I ggml_metal_init: hasUnifiedMemory      = true
0.01.389.998 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.401.000 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.402.692 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.402.694 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.402.721 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.404.371 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.404.372 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.404.372 I llama_init_from_model: graph nodes  = 967
0.01.404.372 I llama_init_from_model: graph splits = 2
0.01.404.374 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.404.374 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.438.087 I 
0.01.438.119 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.438.137 I perplexity: tokenizing the input ..
0.01.443.108 I perplexity: tokenization took 4.969 ms
0.01.443.128 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.562.237 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.565.022 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.565.064 I llama_perf_context_print:        load time =    1416.14 ms
0.01.565.066 I llama_perf_context_print: prompt eval time =     118.84 ms /   128 tokens (    0.93 ms per token,  1077.06 tokens per second)
0.01.565.067 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.565.067 I llama_perf_context_print:       total time =     126.98 ms /   129 tokens
0.01.565.777 I ggml_metal_free: deallocating

real	0m1.782s
user	0m0.103s
sys	0m0.251s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.009.890 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.731 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.739 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.740 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.742 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.742 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.743 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.743 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.746 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.748 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.443 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.445 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.445 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.447 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.448 I llama_model_loader: - type  f32:  194 tensors
0.00.028.448 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.449 I print_info: file format = GGUF V3 (latest)
0.00.028.450 I print_info: file type   = Q8_0
0.00.028.451 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.036.623 I load: special tokens cache size = 25
0.00.042.626 I load: token to piece cache size = 0.2984 MB
0.00.042.631 I print_info: arch             = gptneox
0.00.042.631 I print_info: vocab_only       = 0
0.00.042.632 I print_info: n_ctx_train      = 2048
0.00.042.634 I print_info: n_embd           = 2048
0.00.042.634 I print_info: n_layer          = 24
0.00.042.640 I print_info: n_head           = 16
0.00.042.641 I print_info: n_head_kv        = 16
0.00.042.641 I print_info: n_rot            = 32
0.00.042.642 I print_info: n_swa            = 0
0.00.042.642 I print_info: n_embd_head_k    = 128
0.00.042.642 I print_info: n_embd_head_v    = 128
0.00.042.643 I print_info: n_gqa            = 1
0.00.042.644 I print_info: n_embd_k_gqa     = 2048
0.00.042.644 I print_info: n_embd_v_gqa     = 2048
0.00.042.645 I print_info: f_norm_eps       = 1.0e-05
0.00.042.645 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.646 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.646 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.646 I print_info: f_logit_scale    = 0.0e+00
0.00.042.647 I print_info: n_ff             = 8192
0.00.042.647 I print_info: n_expert         = 0
0.00.042.647 I print_info: n_expert_used    = 0
0.00.042.650 I print_info: causal attn      = 1
0.00.042.651 I print_info: pooling type     = 0
0.00.042.651 I print_info: rope type        = 2
0.00.042.651 I print_info: rope scaling     = linear
0.00.042.651 I print_info: freq_base_train  = 10000.0
0.00.042.652 I print_info: freq_scale_train = 1
0.00.042.652 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.652 I print_info: rope_finetuned   = unknown
0.00.042.652 I print_info: ssm_d_conv       = 0
0.00.042.652 I print_info: ssm_d_inner      = 0
0.00.042.653 I print_info: ssm_d_state      = 0
0.00.042.653 I print_info: ssm_dt_rank      = 0
0.00.042.653 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.653 I print_info: model type       = 1.4B
0.00.042.653 I print_info: model params     = 1.41 B
0.00.042.653 I print_info: general.name     = 1.4B
0.00.042.654 I print_info: vocab type       = BPE
0.00.042.654 I print_info: n_vocab          = 50304
0.00.042.654 I print_info: n_merges         = 50009
0.00.042.655 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.655 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.655 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.655 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.655 I print_info: LF token         = 187 ''
0.00.042.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.656 I print_info: max token length = 1024
0.00.042.656 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.222.726 I load_tensors: offloading 24 repeating layers to GPU
0.01.222.733 I load_tensors: offloading output layer to GPU
0.01.222.734 I load_tensors: offloaded 25/25 layers to GPU
0.01.222.760 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.222.763 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.223.580 I llama_init_from_model: n_seq_max     = 1
0.01.223.582 I llama_init_from_model: n_ctx         = 2048
0.01.223.582 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.223.582 I llama_init_from_model: n_batch       = 2048
0.01.223.583 I llama_init_from_model: n_ubatch      = 512
0.01.223.583 I llama_init_from_model: flash_attn    = 0
0.01.223.584 I llama_init_from_model: freq_base     = 10000.0
0.01.223.584 I llama_init_from_model: freq_scale    = 1
0.01.223.585 I ggml_metal_init: allocating
0.01.223.604 I ggml_metal_init: found device: Apple M4
0.01.223.612 I ggml_metal_init: picking default device: Apple M4
0.01.224.763 I ggml_metal_init: using embedded metal library
0.01.229.980 I ggml_metal_init: GPU name:   Apple M4
0.01.229.983 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.229.984 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.229.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.229.985 I ggml_metal_init: simdgroup reduction   = true
0.01.229.985 I ggml_metal_init: simdgroup matrix mul. = true
0.01.229.985 I ggml_metal_init: has residency sets    = true
0.01.229.986 I ggml_metal_init: has bfloat            = true
0.01.229.986 I ggml_metal_init: use bfloat            = true
0.01.229.987 I ggml_metal_init: hasUnifiedMemory      = true
0.01.229.988 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.246.741 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.297.302 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.297.309 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.297.344 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.301.481 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.301.483 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.301.483 I llama_init_from_model: graph nodes  = 967
0.01.301.484 I llama_init_from_model: graph splits = 2
0.01.301.487 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.301.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.301.617 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.356.906 I main: llama threadpool init, n_threads = 4
0.01.356.960 I 
0.01.356.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.356.982 I 
0.01.357.134 I sampler seed: 1234
0.01.357.138 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.357.148 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.357.150 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.357.150 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.443.367 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.02.443.369 I llama_perf_context_print:        load time =    1346.26 ms
0.02.443.370 I llama_perf_context_print: prompt eval time =      49.06 ms /     7 tokens (    7.01 ms per token,   142.68 tokens per second)
0.02.443.371 I llama_perf_context_print:        eval time =    1034.33 ms /    63 runs   (   16.42 ms per token,    60.91 tokens per second)
0.02.443.371 I llama_perf_context_print:       total time =    1087.22 ms /    70 tokens
0.02.443.660 I ggml_metal_free: deallocating

real	0m2.463s
user	0m0.108s
sys	0m0.287s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.276 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.059 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.130 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.131 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.132 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.134 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.015 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.833 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.834 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.834 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.835 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.835 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.835 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.836 I llama_model_loader: - type  f32:  194 tensors
0.00.025.836 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.837 I print_info: file format = GGUF V3 (latest)
0.00.025.837 I print_info: file type   = Q8_0
0.00.025.838 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.029 I load: special tokens cache size = 25
0.00.040.034 I load: token to piece cache size = 0.2984 MB
0.00.040.039 I print_info: arch             = gptneox
0.00.040.039 I print_info: vocab_only       = 0
0.00.040.039 I print_info: n_ctx_train      = 2048
0.00.040.039 I print_info: n_embd           = 2048
0.00.040.039 I print_info: n_layer          = 24
0.00.040.043 I print_info: n_head           = 16
0.00.040.044 I print_info: n_head_kv        = 16
0.00.040.044 I print_info: n_rot            = 32
0.00.040.044 I print_info: n_swa            = 0
0.00.040.045 I print_info: n_embd_head_k    = 128
0.00.040.046 I print_info: n_embd_head_v    = 128
0.00.040.047 I print_info: n_gqa            = 1
0.00.040.048 I print_info: n_embd_k_gqa     = 2048
0.00.040.049 I print_info: n_embd_v_gqa     = 2048
0.00.040.049 I print_info: f_norm_eps       = 1.0e-05
0.00.040.049 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.049 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.050 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.050 I print_info: f_logit_scale    = 0.0e+00
0.00.040.050 I print_info: n_ff             = 8192
0.00.040.051 I print_info: n_expert         = 0
0.00.040.051 I print_info: n_expert_used    = 0
0.00.040.051 I print_info: causal attn      = 1
0.00.040.051 I print_info: pooling type     = 0
0.00.040.051 I print_info: rope type        = 2
0.00.040.051 I print_info: rope scaling     = linear
0.00.040.052 I print_info: freq_base_train  = 10000.0
0.00.040.052 I print_info: freq_scale_train = 1
0.00.040.052 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.053 I print_info: rope_finetuned   = unknown
0.00.040.054 I print_info: ssm_d_conv       = 0
0.00.040.054 I print_info: ssm_d_inner      = 0
0.00.040.054 I print_info: ssm_d_state      = 0
0.00.040.054 I print_info: ssm_dt_rank      = 0
0.00.040.054 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.055 I print_info: model type       = 1.4B
0.00.040.055 I print_info: model params     = 1.41 B
0.00.040.057 I print_info: general.name     = 1.4B
0.00.040.057 I print_info: vocab type       = BPE
0.00.040.059 I print_info: n_vocab          = 50304
0.00.040.059 I print_info: n_merges         = 50009
0.00.040.059 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.060 I print_info: LF token         = 187 ''
0.00.040.060 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.060 I print_info: max token length = 1024
0.00.040.060 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.929.066 I load_tensors: offloading 24 repeating layers to GPU
0.00.929.071 I load_tensors: offloading output layer to GPU
0.00.929.071 I load_tensors: offloaded 25/25 layers to GPU
0.00.929.100 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.929.102 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.930.624 I llama_init_from_model: n_seq_max     = 1
0.00.930.626 I llama_init_from_model: n_ctx         = 128
0.00.930.626 I llama_init_from_model: n_ctx_per_seq = 128
0.00.930.626 I llama_init_from_model: n_batch       = 128
0.00.930.627 I llama_init_from_model: n_ubatch      = 128
0.00.930.627 I llama_init_from_model: flash_attn    = 0
0.00.930.628 I llama_init_from_model: freq_base     = 10000.0
0.00.930.628 I llama_init_from_model: freq_scale    = 1
0.00.930.629 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.930.630 I ggml_metal_init: allocating
0.00.930.680 I ggml_metal_init: found device: Apple M4
0.00.930.690 I ggml_metal_init: picking default device: Apple M4
0.00.932.100 I ggml_metal_init: using embedded metal library
0.00.937.448 I ggml_metal_init: GPU name:   Apple M4
0.00.937.451 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.937.452 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.937.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.937.453 I ggml_metal_init: simdgroup reduction   = true
0.00.937.453 I ggml_metal_init: simdgroup matrix mul. = true
0.00.937.453 I ggml_metal_init: has residency sets    = true
0.00.937.453 I ggml_metal_init: has bfloat            = true
0.00.937.453 I ggml_metal_init: use bfloat            = true
0.00.937.454 I ggml_metal_init: hasUnifiedMemory      = true
0.00.937.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.953.396 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.956.827 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.956.836 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.956.888 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.960.082 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.960.083 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.960.084 I llama_init_from_model: graph nodes  = 967
0.00.960.084 I llama_init_from_model: graph splits = 2
0.00.960.087 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.960.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.988.980 I 
0.00.989.052 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.989.071 I perplexity: tokenizing the input ..
0.00.996.495 I perplexity: tokenization took 7.421 ms
0.00.996.515 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.135.281 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.136.624 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.136.637 I llama_perf_context_print:        load time =     978.91 ms
0.01.136.638 I llama_perf_context_print: prompt eval time =     137.82 ms /   128 tokens (    1.08 ms per token,   928.75 tokens per second)
0.01.136.639 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.136.639 I llama_perf_context_print:       total time =     147.66 ms /   129 tokens
0.01.137.014 I ggml_metal_free: deallocating

real	0m1.154s
user	0m0.078s
sys	0m0.190s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.015.902 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.845 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.850 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.853 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.853 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.856 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.856 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.856 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.857 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.071 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.073 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.073 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.074 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.074 I llama_model_loader: - type  f32:  194 tensors
0.00.035.075 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.075 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.075 I print_info: file format = GGUF V3 (latest)
0.00.035.076 I print_info: file type   = Q4_0
0.00.035.077 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.275 I load: special tokens cache size = 25
0.00.053.790 I load: token to piece cache size = 0.2984 MB
0.00.053.794 I print_info: arch             = gptneox
0.00.053.794 I print_info: vocab_only       = 0
0.00.053.794 I print_info: n_ctx_train      = 2048
0.00.053.795 I print_info: n_embd           = 2048
0.00.053.795 I print_info: n_layer          = 24
0.00.053.798 I print_info: n_head           = 16
0.00.053.799 I print_info: n_head_kv        = 16
0.00.053.799 I print_info: n_rot            = 32
0.00.053.799 I print_info: n_swa            = 0
0.00.053.800 I print_info: n_embd_head_k    = 128
0.00.053.800 I print_info: n_embd_head_v    = 128
0.00.053.804 I print_info: n_gqa            = 1
0.00.053.805 I print_info: n_embd_k_gqa     = 2048
0.00.053.807 I print_info: n_embd_v_gqa     = 2048
0.00.053.807 I print_info: f_norm_eps       = 1.0e-05
0.00.053.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.808 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.809 I print_info: f_logit_scale    = 0.0e+00
0.00.053.809 I print_info: n_ff             = 8192
0.00.053.810 I print_info: n_expert         = 0
0.00.053.810 I print_info: n_expert_used    = 0
0.00.053.810 I print_info: causal attn      = 1
0.00.053.810 I print_info: pooling type     = 0
0.00.053.810 I print_info: rope type        = 2
0.00.053.811 I print_info: rope scaling     = linear
0.00.053.811 I print_info: freq_base_train  = 10000.0
0.00.053.811 I print_info: freq_scale_train = 1
0.00.053.812 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.812 I print_info: rope_finetuned   = unknown
0.00.053.812 I print_info: ssm_d_conv       = 0
0.00.053.812 I print_info: ssm_d_inner      = 0
0.00.053.812 I print_info: ssm_d_state      = 0
0.00.053.812 I print_info: ssm_dt_rank      = 0
0.00.053.813 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.813 I print_info: model type       = 1.4B
0.00.053.813 I print_info: model params     = 1.41 B
0.00.053.813 I print_info: general.name     = 1.4B
0.00.053.814 I print_info: vocab type       = BPE
0.00.053.814 I print_info: n_vocab          = 50304
0.00.053.814 I print_info: n_merges         = 50009
0.00.053.815 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.815 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.815 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.816 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.816 I print_info: LF token         = 187 ''
0.00.053.816 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.818 I print_info: max token length = 1024
0.00.053.818 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.612 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.622 I load_tensors: offloading output layer to GPU
0.00.628.623 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.655 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.628.657 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.629.985 I llama_init_from_model: n_seq_max     = 1
0.00.629.987 I llama_init_from_model: n_ctx         = 2048
0.00.629.987 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.629.988 I llama_init_from_model: n_batch       = 2048
0.00.629.989 I llama_init_from_model: n_ubatch      = 512
0.00.629.989 I llama_init_from_model: flash_attn    = 0
0.00.629.991 I llama_init_from_model: freq_base     = 10000.0
0.00.629.992 I llama_init_from_model: freq_scale    = 1
0.00.629.994 I ggml_metal_init: allocating
0.00.630.070 I ggml_metal_init: found device: Apple M4
0.00.630.083 I ggml_metal_init: picking default device: Apple M4
0.00.631.917 I ggml_metal_init: using embedded metal library
0.00.637.758 I ggml_metal_init: GPU name:   Apple M4
0.00.637.763 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.765 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.766 I ggml_metal_init: simdgroup reduction   = true
0.00.637.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.767 I ggml_metal_init: has residency sets    = true
0.00.637.767 I ggml_metal_init: has bfloat            = true
0.00.637.767 I ggml_metal_init: use bfloat            = true
0.00.637.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.108 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.180 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.710.190 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.237 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.394 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.714.396 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.714.396 I llama_init_from_model: graph nodes  = 967
0.00.714.396 I llama_init_from_model: graph splits = 2
0.00.714.402 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.527 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.712 I main: llama threadpool init, n_threads = 4
0.00.768.755 I 
0.00.768.779 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.781 I 
0.00.768.933 I sampler seed: 1234
0.00.768.938 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.962 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.963 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.964 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.445.339 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50605.84 tokens per second)
0.01.445.340 I llama_perf_context_print:        load time =     752.07 ms
0.01.445.341 I llama_perf_context_print: prompt eval time =      44.03 ms /     7 tokens (    6.29 ms per token,   158.99 tokens per second)
0.01.445.341 I llama_perf_context_print:        eval time =     629.52 ms /    63 runs   (    9.99 ms per token,   100.08 tokens per second)
0.01.445.342 I llama_perf_context_print:       total time =     677.37 ms /    70 tokens
0.01.445.575 I ggml_metal_free: deallocating

real	0m1.473s
user	0m0.117s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.281 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.697 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.740 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.755 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.756 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.757 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.757 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.757 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.758 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.760 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.661 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.449 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.451 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.451 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.452 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.452 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.453 I llama_model_loader: - type  f32:  194 tensors
0.00.025.453 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.454 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.454 I print_info: file format = GGUF V3 (latest)
0.00.025.455 I print_info: file type   = Q4_0
0.00.025.456 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.470 I load: special tokens cache size = 25
0.00.039.590 I load: token to piece cache size = 0.2984 MB
0.00.039.594 I print_info: arch             = gptneox
0.00.039.594 I print_info: vocab_only       = 0
0.00.039.595 I print_info: n_ctx_train      = 2048
0.00.039.595 I print_info: n_embd           = 2048
0.00.039.595 I print_info: n_layer          = 24
0.00.039.600 I print_info: n_head           = 16
0.00.039.600 I print_info: n_head_kv        = 16
0.00.039.601 I print_info: n_rot            = 32
0.00.039.601 I print_info: n_swa            = 0
0.00.039.601 I print_info: n_embd_head_k    = 128
0.00.039.601 I print_info: n_embd_head_v    = 128
0.00.039.602 I print_info: n_gqa            = 1
0.00.039.603 I print_info: n_embd_k_gqa     = 2048
0.00.039.604 I print_info: n_embd_v_gqa     = 2048
0.00.039.605 I print_info: f_norm_eps       = 1.0e-05
0.00.039.605 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.605 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.607 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.608 I print_info: f_logit_scale    = 0.0e+00
0.00.039.608 I print_info: n_ff             = 8192
0.00.039.608 I print_info: n_expert         = 0
0.00.039.608 I print_info: n_expert_used    = 0
0.00.039.609 I print_info: causal attn      = 1
0.00.039.609 I print_info: pooling type     = 0
0.00.039.609 I print_info: rope type        = 2
0.00.039.609 I print_info: rope scaling     = linear
0.00.039.609 I print_info: freq_base_train  = 10000.0
0.00.039.610 I print_info: freq_scale_train = 1
0.00.039.610 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.610 I print_info: rope_finetuned   = unknown
0.00.039.610 I print_info: ssm_d_conv       = 0
0.00.039.610 I print_info: ssm_d_inner      = 0
0.00.039.610 I print_info: ssm_d_state      = 0
0.00.039.610 I print_info: ssm_dt_rank      = 0
0.00.039.611 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.612 I print_info: model type       = 1.4B
0.00.039.612 I print_info: model params     = 1.41 B
0.00.039.613 I print_info: general.name     = 1.4B
0.00.039.613 I print_info: vocab type       = BPE
0.00.039.613 I print_info: n_vocab          = 50304
0.00.039.613 I print_info: n_merges         = 50009
0.00.039.613 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: LF token         = 187 ''
0.00.039.614 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: max token length = 1024
0.00.039.615 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.570.261 I load_tensors: offloading 24 repeating layers to GPU
0.00.570.275 I load_tensors: offloading output layer to GPU
0.00.570.276 I load_tensors: offloaded 25/25 layers to GPU
0.00.570.317 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.570.318 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.572.127 I llama_init_from_model: n_seq_max     = 1
0.00.572.130 I llama_init_from_model: n_ctx         = 128
0.00.572.131 I llama_init_from_model: n_ctx_per_seq = 128
0.00.572.131 I llama_init_from_model: n_batch       = 128
0.00.572.132 I llama_init_from_model: n_ubatch      = 128
0.00.572.132 I llama_init_from_model: flash_attn    = 0
0.00.572.134 I llama_init_from_model: freq_base     = 10000.0
0.00.572.135 I llama_init_from_model: freq_scale    = 1
0.00.572.135 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.572.152 I ggml_metal_init: allocating
0.00.572.275 I ggml_metal_init: found device: Apple M4
0.00.572.297 I ggml_metal_init: picking default device: Apple M4
0.00.574.201 I ggml_metal_init: using embedded metal library
0.00.580.936 I ggml_metal_init: GPU name:   Apple M4
0.00.580.945 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.580.946 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.580.947 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.580.947 I ggml_metal_init: simdgroup reduction   = true
0.00.580.947 I ggml_metal_init: simdgroup matrix mul. = true
0.00.580.948 I ggml_metal_init: has residency sets    = true
0.00.580.948 I ggml_metal_init: has bfloat            = true
0.00.580.948 I ggml_metal_init: use bfloat            = true
0.00.580.949 I ggml_metal_init: hasUnifiedMemory      = true
0.00.580.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.599.392 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.603.002 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.603.007 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.603.052 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.606.252 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.606.253 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.606.254 I llama_init_from_model: graph nodes  = 967
0.00.606.254 I llama_init_from_model: graph splits = 2
0.00.606.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.606.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.101 I 
0.00.633.178 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.196 I perplexity: tokenizing the input ..
0.00.639.989 I perplexity: tokenization took 6.789 ms
0.00.640.010 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.768.079 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.769.387 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.769.412 I llama_perf_context_print:        load time =     623.39 ms
0.00.769.413 I llama_perf_context_print: prompt eval time =     127.38 ms /   128 tokens (    1.00 ms per token,  1004.84 tokens per second)
0.00.769.414 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.769.415 I llama_perf_context_print:       total time =     136.32 ms /   129 tokens
0.00.769.852 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.080s
sys	0m0.120s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.756 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.204 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.030.208 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.214 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.216 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.218 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.198 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.263 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.160 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.161 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.162 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.162 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.162 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.163 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.039.163 I llama_model_loader: - type  f32:  194 tensors
0.00.039.163 I llama_model_loader: - type q4_1:   97 tensors
0.00.039.164 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.164 I print_info: file format = GGUF V3 (latest)
0.00.039.165 I print_info: file type   = Q4_1
0.00.039.165 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.870 I load: special tokens cache size = 25
0.00.054.918 I load: token to piece cache size = 0.2984 MB
0.00.054.921 I print_info: arch             = gptneox
0.00.054.921 I print_info: vocab_only       = 0
0.00.054.921 I print_info: n_ctx_train      = 2048
0.00.054.921 I print_info: n_embd           = 2048
0.00.054.921 I print_info: n_layer          = 24
0.00.054.924 I print_info: n_head           = 16
0.00.054.925 I print_info: n_head_kv        = 16
0.00.054.927 I print_info: n_rot            = 32
0.00.054.927 I print_info: n_swa            = 0
0.00.054.927 I print_info: n_embd_head_k    = 128
0.00.054.927 I print_info: n_embd_head_v    = 128
0.00.054.928 I print_info: n_gqa            = 1
0.00.054.929 I print_info: n_embd_k_gqa     = 2048
0.00.054.929 I print_info: n_embd_v_gqa     = 2048
0.00.054.930 I print_info: f_norm_eps       = 1.0e-05
0.00.054.930 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.931 I print_info: f_logit_scale    = 0.0e+00
0.00.054.931 I print_info: n_ff             = 8192
0.00.054.931 I print_info: n_expert         = 0
0.00.054.931 I print_info: n_expert_used    = 0
0.00.054.932 I print_info: causal attn      = 1
0.00.054.932 I print_info: pooling type     = 0
0.00.054.932 I print_info: rope type        = 2
0.00.054.937 I print_info: rope scaling     = linear
0.00.054.939 I print_info: freq_base_train  = 10000.0
0.00.054.939 I print_info: freq_scale_train = 1
0.00.054.940 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.943 I print_info: rope_finetuned   = unknown
0.00.054.943 I print_info: ssm_d_conv       = 0
0.00.054.943 I print_info: ssm_d_inner      = 0
0.00.054.943 I print_info: ssm_d_state      = 0
0.00.054.944 I print_info: ssm_dt_rank      = 0
0.00.054.944 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.944 I print_info: model type       = 1.4B
0.00.054.945 I print_info: model params     = 1.41 B
0.00.054.945 I print_info: general.name     = 1.4B
0.00.054.945 I print_info: vocab type       = BPE
0.00.054.945 I print_info: n_vocab          = 50304
0.00.054.946 I print_info: n_merges         = 50009
0.00.054.946 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.946 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.946 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.946 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.947 I print_info: LF token         = 187 ''
0.00.054.947 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.947 I print_info: max token length = 1024
0.00.054.947 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.770.286 I load_tensors: offloading 24 repeating layers to GPU
0.00.770.300 I load_tensors: offloading output layer to GPU
0.00.770.301 I load_tensors: offloaded 25/25 layers to GPU
0.00.770.335 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.770.337 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.771.846 I llama_init_from_model: n_seq_max     = 1
0.00.771.848 I llama_init_from_model: n_ctx         = 2048
0.00.771.849 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.771.849 I llama_init_from_model: n_batch       = 2048
0.00.771.850 I llama_init_from_model: n_ubatch      = 512
0.00.771.851 I llama_init_from_model: flash_attn    = 0
0.00.771.853 I llama_init_from_model: freq_base     = 10000.0
0.00.771.854 I llama_init_from_model: freq_scale    = 1
0.00.771.856 I ggml_metal_init: allocating
0.00.771.929 I ggml_metal_init: found device: Apple M4
0.00.771.943 I ggml_metal_init: picking default device: Apple M4
0.00.773.823 I ggml_metal_init: using embedded metal library
0.00.779.748 I ggml_metal_init: GPU name:   Apple M4
0.00.779.753 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.779.754 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.779.755 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.779.755 I ggml_metal_init: simdgroup reduction   = true
0.00.779.756 I ggml_metal_init: simdgroup matrix mul. = true
0.00.779.756 I ggml_metal_init: has residency sets    = true
0.00.779.756 I ggml_metal_init: has bfloat            = true
0.00.779.756 I ggml_metal_init: use bfloat            = true
0.00.779.758 I ggml_metal_init: hasUnifiedMemory      = true
0.00.779.759 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.798.924 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.852.810 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.852.819 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.852.858 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.857.912 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.857.914 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.857.914 I llama_init_from_model: graph nodes  = 967
0.00.857.915 I llama_init_from_model: graph splits = 2
0.00.857.919 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.858.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.858.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.908.740 I main: llama threadpool init, n_threads = 4
0.00.908.780 I 
0.00.908.800 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.908.800 I 
0.00.908.913 I sampler seed: 1234
0.00.908.917 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.908.935 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.908.935 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.908.935 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.646.233 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.646.234 I llama_perf_context_print:        load time =     899.29 ms
0.01.646.235 I llama_perf_context_print: prompt eval time =      49.63 ms /     7 tokens (    7.09 ms per token,   141.05 tokens per second)
0.01.646.235 I llama_perf_context_print:        eval time =     685.00 ms /    63 runs   (   10.87 ms per token,    91.97 tokens per second)
0.01.646.236 I llama_perf_context_print:       total time =     738.18 ms /    70 tokens
0.01.646.515 I ggml_metal_free: deallocating

real	0m1.663s
user	0m0.111s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.982 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.212 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.213 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.213 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.214 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.215 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.220 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.222 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.057 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.954 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.955 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.956 I llama_model_loader: - type  f32:  194 tensors
0.00.024.956 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.957 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.957 I print_info: file format = GGUF V3 (latest)
0.00.024.958 I print_info: file type   = Q4_1
0.00.024.959 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.295 I load: special tokens cache size = 25
0.00.039.456 I load: token to piece cache size = 0.2984 MB
0.00.039.460 I print_info: arch             = gptneox
0.00.039.460 I print_info: vocab_only       = 0
0.00.039.460 I print_info: n_ctx_train      = 2048
0.00.039.461 I print_info: n_embd           = 2048
0.00.039.461 I print_info: n_layer          = 24
0.00.039.465 I print_info: n_head           = 16
0.00.039.466 I print_info: n_head_kv        = 16
0.00.039.466 I print_info: n_rot            = 32
0.00.039.466 I print_info: n_swa            = 0
0.00.039.467 I print_info: n_embd_head_k    = 128
0.00.039.467 I print_info: n_embd_head_v    = 128
0.00.039.467 I print_info: n_gqa            = 1
0.00.039.468 I print_info: n_embd_k_gqa     = 2048
0.00.039.469 I print_info: n_embd_v_gqa     = 2048
0.00.039.470 I print_info: f_norm_eps       = 1.0e-05
0.00.039.470 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.470 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.470 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.470 I print_info: f_logit_scale    = 0.0e+00
0.00.039.471 I print_info: n_ff             = 8192
0.00.039.471 I print_info: n_expert         = 0
0.00.039.471 I print_info: n_expert_used    = 0
0.00.039.471 I print_info: causal attn      = 1
0.00.039.471 I print_info: pooling type     = 0
0.00.039.472 I print_info: rope type        = 2
0.00.039.472 I print_info: rope scaling     = linear
0.00.039.472 I print_info: freq_base_train  = 10000.0
0.00.039.472 I print_info: freq_scale_train = 1
0.00.039.472 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.473 I print_info: rope_finetuned   = unknown
0.00.039.473 I print_info: ssm_d_conv       = 0
0.00.039.473 I print_info: ssm_d_inner      = 0
0.00.039.473 I print_info: ssm_d_state      = 0
0.00.039.473 I print_info: ssm_dt_rank      = 0
0.00.039.476 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.476 I print_info: model type       = 1.4B
0.00.039.476 I print_info: model params     = 1.41 B
0.00.039.477 I print_info: general.name     = 1.4B
0.00.039.478 I print_info: vocab type       = BPE
0.00.039.478 I print_info: n_vocab          = 50304
0.00.039.478 I print_info: n_merges         = 50009
0.00.039.478 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.479 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.479 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.479 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.479 I print_info: LF token         = 187 ''
0.00.039.479 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.481 I print_info: max token length = 1024
0.00.039.481 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.890 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.907 I load_tensors: offloading output layer to GPU
0.00.613.907 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.945 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.613.947 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.615.624 I llama_init_from_model: n_seq_max     = 1
0.00.615.626 I llama_init_from_model: n_ctx         = 128
0.00.615.627 I llama_init_from_model: n_ctx_per_seq = 128
0.00.615.627 I llama_init_from_model: n_batch       = 128
0.00.615.628 I llama_init_from_model: n_ubatch      = 128
0.00.615.628 I llama_init_from_model: flash_attn    = 0
0.00.615.631 I llama_init_from_model: freq_base     = 10000.0
0.00.615.631 I llama_init_from_model: freq_scale    = 1
0.00.615.632 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.615.634 I ggml_metal_init: allocating
0.00.615.717 I ggml_metal_init: found device: Apple M4
0.00.615.732 I ggml_metal_init: picking default device: Apple M4
0.00.617.472 I ggml_metal_init: using embedded metal library
0.00.623.979 I ggml_metal_init: GPU name:   Apple M4
0.00.623.988 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.989 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.989 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.990 I ggml_metal_init: simdgroup reduction   = true
0.00.623.990 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.991 I ggml_metal_init: has residency sets    = true
0.00.623.991 I ggml_metal_init: has bfloat            = true
0.00.623.991 I ggml_metal_init: use bfloat            = true
0.00.623.993 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.569 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.250 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.259 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.331 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.650.785 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.650.786 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.650.787 I llama_init_from_model: graph nodes  = 967
0.00.650.787 I llama_init_from_model: graph splits = 2
0.00.650.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.237 I 
0.00.679.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.343 I perplexity: tokenizing the input ..
0.00.686.297 I perplexity: tokenization took 6.95 ms
0.00.686.318 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.810 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.823.164 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.823.179 I llama_perf_context_print:        load time =     670.25 ms
0.00.823.180 I llama_perf_context_print: prompt eval time =     135.15 ms /   128 tokens (    1.06 ms per token,   947.07 tokens per second)
0.00.823.181 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.183 I llama_perf_context_print:       total time =     143.95 ms /   129 tokens
0.00.823.565 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.080s
sys	0m0.117s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.064 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.474 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.478 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.480 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.481 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.484 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.485 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.489 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.489 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.490 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.490 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.490 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.278 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.966 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.966 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.967 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.968 I llama_model_loader: - type  f32:  194 tensors
0.00.025.968 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.969 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.969 I print_info: file format = GGUF V3 (latest)
0.00.025.970 I print_info: file type   = Q5_0
0.00.025.971 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.126 I load: special tokens cache size = 25
0.00.040.209 I load: token to piece cache size = 0.2984 MB
0.00.040.212 I print_info: arch             = gptneox
0.00.040.212 I print_info: vocab_only       = 0
0.00.040.212 I print_info: n_ctx_train      = 2048
0.00.040.212 I print_info: n_embd           = 2048
0.00.040.213 I print_info: n_layer          = 24
0.00.040.215 I print_info: n_head           = 16
0.00.040.216 I print_info: n_head_kv        = 16
0.00.040.218 I print_info: n_rot            = 32
0.00.040.218 I print_info: n_swa            = 0
0.00.040.218 I print_info: n_embd_head_k    = 128
0.00.040.219 I print_info: n_embd_head_v    = 128
0.00.040.219 I print_info: n_gqa            = 1
0.00.040.220 I print_info: n_embd_k_gqa     = 2048
0.00.040.221 I print_info: n_embd_v_gqa     = 2048
0.00.040.221 I print_info: f_norm_eps       = 1.0e-05
0.00.040.222 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.222 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.222 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.222 I print_info: f_logit_scale    = 0.0e+00
0.00.040.223 I print_info: n_ff             = 8192
0.00.040.223 I print_info: n_expert         = 0
0.00.040.223 I print_info: n_expert_used    = 0
0.00.040.223 I print_info: causal attn      = 1
0.00.040.224 I print_info: pooling type     = 0
0.00.040.225 I print_info: rope type        = 2
0.00.040.227 I print_info: rope scaling     = linear
0.00.040.227 I print_info: freq_base_train  = 10000.0
0.00.040.227 I print_info: freq_scale_train = 1
0.00.040.228 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.228 I print_info: rope_finetuned   = unknown
0.00.040.228 I print_info: ssm_d_conv       = 0
0.00.040.230 I print_info: ssm_d_inner      = 0
0.00.040.230 I print_info: ssm_d_state      = 0
0.00.040.230 I print_info: ssm_dt_rank      = 0
0.00.040.230 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.230 I print_info: model type       = 1.4B
0.00.040.231 I print_info: model params     = 1.41 B
0.00.040.231 I print_info: general.name     = 1.4B
0.00.040.231 I print_info: vocab type       = BPE
0.00.040.231 I print_info: n_vocab          = 50304
0.00.040.232 I print_info: n_merges         = 50009
0.00.040.232 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.233 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.233 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.233 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.233 I print_info: LF token         = 187 ''
0.00.040.233 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.234 I print_info: max token length = 1024
0.00.040.234 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.667.155 I load_tensors: offloading 24 repeating layers to GPU
0.00.667.171 I load_tensors: offloading output layer to GPU
0.00.667.172 I load_tensors: offloaded 25/25 layers to GPU
0.00.667.205 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.667.212 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.668.973 I llama_init_from_model: n_seq_max     = 1
0.00.668.976 I llama_init_from_model: n_ctx         = 2048
0.00.668.976 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.668.977 I llama_init_from_model: n_batch       = 2048
0.00.668.977 I llama_init_from_model: n_ubatch      = 512
0.00.668.978 I llama_init_from_model: flash_attn    = 0
0.00.668.979 I llama_init_from_model: freq_base     = 10000.0
0.00.668.979 I llama_init_from_model: freq_scale    = 1
0.00.668.980 I ggml_metal_init: allocating
0.00.669.002 I ggml_metal_init: found device: Apple M4
0.00.669.014 I ggml_metal_init: picking default device: Apple M4
0.00.670.544 I ggml_metal_init: using embedded metal library
0.00.677.123 I ggml_metal_init: GPU name:   Apple M4
0.00.677.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.677.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.677.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.677.129 I ggml_metal_init: simdgroup reduction   = true
0.00.677.129 I ggml_metal_init: simdgroup matrix mul. = true
0.00.677.129 I ggml_metal_init: has residency sets    = true
0.00.677.130 I ggml_metal_init: has bfloat            = true
0.00.677.130 I ggml_metal_init: use bfloat            = true
0.00.677.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.677.132 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.695.027 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.030 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.746.037 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.746.071 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.410 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.412 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.412 I llama_init_from_model: graph nodes  = 967
0.00.750.413 I llama_init_from_model: graph splits = 2
0.00.750.419 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.542 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.764 I main: llama threadpool init, n_threads = 4
0.00.807.810 I 
0.00.807.833 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.835 I 
0.00.808.003 I sampler seed: 1234
0.00.808.008 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.808.028 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.808.029 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.808.029 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.607.495 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.607.496 I llama_perf_context_print:        load time =     796.99 ms
0.01.607.497 I llama_perf_context_print: prompt eval time =      53.12 ms /     7 tokens (    7.59 ms per token,   131.78 tokens per second)
0.01.607.497 I llama_perf_context_print:        eval time =     743.47 ms /    63 runs   (   11.80 ms per token,    84.74 tokens per second)
0.01.607.498 I llama_perf_context_print:       total time =     800.44 ms /    70 tokens
0.01.607.754 I ggml_metal_free: deallocating

real	0m1.626s
user	0m0.109s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.983 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.892 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.899 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.900 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.900 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.901 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.902 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.902 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.903 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.903 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.904 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.906 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.907 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.816 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.649 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.650 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.650 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.650 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.651 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.651 I llama_model_loader: - type  f32:  194 tensors
0.00.025.652 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.652 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.653 I print_info: file format = GGUF V3 (latest)
0.00.025.653 I print_info: file type   = Q5_0
0.00.025.654 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.015 I load: special tokens cache size = 25
0.00.039.952 I load: token to piece cache size = 0.2984 MB
0.00.039.956 I print_info: arch             = gptneox
0.00.039.956 I print_info: vocab_only       = 0
0.00.039.956 I print_info: n_ctx_train      = 2048
0.00.039.956 I print_info: n_embd           = 2048
0.00.039.957 I print_info: n_layer          = 24
0.00.039.961 I print_info: n_head           = 16
0.00.039.962 I print_info: n_head_kv        = 16
0.00.039.962 I print_info: n_rot            = 32
0.00.039.962 I print_info: n_swa            = 0
0.00.039.962 I print_info: n_embd_head_k    = 128
0.00.039.963 I print_info: n_embd_head_v    = 128
0.00.039.963 I print_info: n_gqa            = 1
0.00.039.964 I print_info: n_embd_k_gqa     = 2048
0.00.039.965 I print_info: n_embd_v_gqa     = 2048
0.00.039.966 I print_info: f_norm_eps       = 1.0e-05
0.00.039.967 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.967 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.967 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.967 I print_info: f_logit_scale    = 0.0e+00
0.00.039.968 I print_info: n_ff             = 8192
0.00.039.968 I print_info: n_expert         = 0
0.00.039.968 I print_info: n_expert_used    = 0
0.00.039.968 I print_info: causal attn      = 1
0.00.039.968 I print_info: pooling type     = 0
0.00.039.968 I print_info: rope type        = 2
0.00.039.970 I print_info: rope scaling     = linear
0.00.039.970 I print_info: freq_base_train  = 10000.0
0.00.039.971 I print_info: freq_scale_train = 1
0.00.039.971 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.971 I print_info: rope_finetuned   = unknown
0.00.039.971 I print_info: ssm_d_conv       = 0
0.00.039.971 I print_info: ssm_d_inner      = 0
0.00.039.971 I print_info: ssm_d_state      = 0
0.00.039.972 I print_info: ssm_dt_rank      = 0
0.00.039.972 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.972 I print_info: model type       = 1.4B
0.00.039.972 I print_info: model params     = 1.41 B
0.00.039.972 I print_info: general.name     = 1.4B
0.00.039.973 I print_info: vocab type       = BPE
0.00.039.973 I print_info: n_vocab          = 50304
0.00.039.973 I print_info: n_merges         = 50009
0.00.039.974 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.974 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.974 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.974 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.974 I print_info: LF token         = 187 ''
0.00.039.975 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.975 I print_info: max token length = 1024
0.00.039.975 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.704.581 I load_tensors: offloading 24 repeating layers to GPU
0.00.704.596 I load_tensors: offloading output layer to GPU
0.00.704.597 I load_tensors: offloaded 25/25 layers to GPU
0.00.704.632 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.704.634 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.706.389 I llama_init_from_model: n_seq_max     = 1
0.00.706.392 I llama_init_from_model: n_ctx         = 128
0.00.706.392 I llama_init_from_model: n_ctx_per_seq = 128
0.00.706.393 I llama_init_from_model: n_batch       = 128
0.00.706.393 I llama_init_from_model: n_ubatch      = 128
0.00.706.393 I llama_init_from_model: flash_attn    = 0
0.00.706.395 I llama_init_from_model: freq_base     = 10000.0
0.00.706.397 I llama_init_from_model: freq_scale    = 1
0.00.706.397 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.706.399 I ggml_metal_init: allocating
0.00.706.511 I ggml_metal_init: found device: Apple M4
0.00.706.542 I ggml_metal_init: picking default device: Apple M4
0.00.707.976 I ggml_metal_init: using embedded metal library
0.00.714.451 I ggml_metal_init: GPU name:   Apple M4
0.00.714.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.458 I ggml_metal_init: simdgroup reduction   = true
0.00.714.458 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.458 I ggml_metal_init: has residency sets    = true
0.00.714.458 I ggml_metal_init: has bfloat            = true
0.00.714.459 I ggml_metal_init: use bfloat            = true
0.00.714.460 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.463 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.731.526 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.968 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.734.975 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.735.029 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.340 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.738.342 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.738.343 I llama_init_from_model: graph nodes  = 967
0.00.738.343 I llama_init_from_model: graph splits = 2
0.00.738.347 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.738.347 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.744 I 
0.00.765.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.843 I perplexity: tokenizing the input ..
0.00.773.490 I perplexity: tokenization took 7.644 ms
0.00.773.509 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.909.355 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.910.687 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.910.705 I llama_perf_context_print:        load time =     755.75 ms
0.00.910.706 I llama_perf_context_print: prompt eval time =     134.90 ms /   128 tokens (    1.05 ms per token,   948.83 tokens per second)
0.00.910.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.910.709 I llama_perf_context_print:       total time =     144.97 ms /   129 tokens
0.00.911.111 I ggml_metal_free: deallocating

real	0m0.927s
user	0m0.080s
sys	0m0.148s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.643 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.988 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.992 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.996 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.997 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.997 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.997 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.998 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.998 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.999 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.999 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.002 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.002 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.002 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.780 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.524 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.524 I llama_model_loader: - type  f32:  194 tensors
0.00.024.524 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.525 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.525 I print_info: file format = GGUF V3 (latest)
0.00.024.526 I print_info: file type   = Q5_1
0.00.024.527 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.370 I load: special tokens cache size = 25
0.00.038.397 I load: token to piece cache size = 0.2984 MB
0.00.038.400 I print_info: arch             = gptneox
0.00.038.401 I print_info: vocab_only       = 0
0.00.038.401 I print_info: n_ctx_train      = 2048
0.00.038.401 I print_info: n_embd           = 2048
0.00.038.401 I print_info: n_layer          = 24
0.00.038.404 I print_info: n_head           = 16
0.00.038.404 I print_info: n_head_kv        = 16
0.00.038.405 I print_info: n_rot            = 32
0.00.038.405 I print_info: n_swa            = 0
0.00.038.406 I print_info: n_embd_head_k    = 128
0.00.038.406 I print_info: n_embd_head_v    = 128
0.00.038.407 I print_info: n_gqa            = 1
0.00.038.408 I print_info: n_embd_k_gqa     = 2048
0.00.038.409 I print_info: n_embd_v_gqa     = 2048
0.00.038.409 I print_info: f_norm_eps       = 1.0e-05
0.00.038.410 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.410 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.410 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.410 I print_info: f_logit_scale    = 0.0e+00
0.00.038.411 I print_info: n_ff             = 8192
0.00.038.411 I print_info: n_expert         = 0
0.00.038.411 I print_info: n_expert_used    = 0
0.00.038.411 I print_info: causal attn      = 1
0.00.038.411 I print_info: pooling type     = 0
0.00.038.413 I print_info: rope type        = 2
0.00.038.415 I print_info: rope scaling     = linear
0.00.038.415 I print_info: freq_base_train  = 10000.0
0.00.038.415 I print_info: freq_scale_train = 1
0.00.038.416 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.416 I print_info: rope_finetuned   = unknown
0.00.038.416 I print_info: ssm_d_conv       = 0
0.00.038.416 I print_info: ssm_d_inner      = 0
0.00.038.416 I print_info: ssm_d_state      = 0
0.00.038.416 I print_info: ssm_dt_rank      = 0
0.00.038.417 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.417 I print_info: model type       = 1.4B
0.00.038.417 I print_info: model params     = 1.41 B
0.00.038.417 I print_info: general.name     = 1.4B
0.00.038.418 I print_info: vocab type       = BPE
0.00.038.418 I print_info: n_vocab          = 50304
0.00.038.418 I print_info: n_merges         = 50009
0.00.038.418 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.419 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.419 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.419 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.419 I print_info: LF token         = 187 ''
0.00.038.419 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.420 I print_info: max token length = 1024
0.00.038.425 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.603.339 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.354 I load_tensors: offloading output layer to GPU
0.00.603.355 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.389 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.603.390 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.604.952 I llama_init_from_model: n_seq_max     = 1
0.00.604.955 I llama_init_from_model: n_ctx         = 2048
0.00.604.955 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.604.955 I llama_init_from_model: n_batch       = 2048
0.00.604.956 I llama_init_from_model: n_ubatch      = 512
0.00.604.956 I llama_init_from_model: flash_attn    = 0
0.00.604.958 I llama_init_from_model: freq_base     = 10000.0
0.00.604.958 I llama_init_from_model: freq_scale    = 1
0.00.604.959 I ggml_metal_init: allocating
0.00.604.977 I ggml_metal_init: found device: Apple M4
0.00.604.986 I ggml_metal_init: picking default device: Apple M4
0.00.606.474 I ggml_metal_init: using embedded metal library
0.00.612.623 I ggml_metal_init: GPU name:   Apple M4
0.00.612.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.628 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.628 I ggml_metal_init: simdgroup reduction   = true
0.00.612.629 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.629 I ggml_metal_init: has residency sets    = true
0.00.612.629 I ggml_metal_init: has bfloat            = true
0.00.612.629 I ggml_metal_init: use bfloat            = true
0.00.612.630 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.632 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.863 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.289 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.295 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.330 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.587 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.684.590 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.684.590 I llama_init_from_model: graph nodes  = 967
0.00.684.590 I llama_init_from_model: graph splits = 2
0.00.684.595 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.712 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.177 I main: llama threadpool init, n_threads = 4
0.00.745.222 I 
0.00.745.244 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.246 I 
0.00.745.414 I sampler seed: 1234
0.00.745.418 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.429 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.429 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.429 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.590.934 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.01.590.935 I llama_perf_context_print:        load time =     735.82 ms
0.01.590.936 I llama_perf_context_print: prompt eval time =      51.87 ms /     7 tokens (    7.41 ms per token,   134.95 tokens per second)
0.01.590.936 I llama_perf_context_print:        eval time =     790.84 ms /    63 runs   (   12.55 ms per token,    79.66 tokens per second)
0.01.590.937 I llama_perf_context_print:       total time =     846.47 ms /    70 tokens
0.01.591.167 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.108s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.079 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.837 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.852 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.597 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.637 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.539 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.540 I llama_model_loader: - type  f32:  194 tensors
0.00.024.540 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.541 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.541 I print_info: file format = GGUF V3 (latest)
0.00.024.544 I print_info: file type   = Q5_1
0.00.024.545 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.500 I load: special tokens cache size = 25
0.00.038.625 I load: token to piece cache size = 0.2984 MB
0.00.038.630 I print_info: arch             = gptneox
0.00.038.630 I print_info: vocab_only       = 0
0.00.038.630 I print_info: n_ctx_train      = 2048
0.00.038.630 I print_info: n_embd           = 2048
0.00.038.630 I print_info: n_layer          = 24
0.00.038.635 I print_info: n_head           = 16
0.00.038.636 I print_info: n_head_kv        = 16
0.00.038.636 I print_info: n_rot            = 32
0.00.038.636 I print_info: n_swa            = 0
0.00.038.636 I print_info: n_embd_head_k    = 128
0.00.038.636 I print_info: n_embd_head_v    = 128
0.00.038.637 I print_info: n_gqa            = 1
0.00.038.638 I print_info: n_embd_k_gqa     = 2048
0.00.038.640 I print_info: n_embd_v_gqa     = 2048
0.00.038.642 I print_info: f_norm_eps       = 1.0e-05
0.00.038.642 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.642 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.643 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.644 I print_info: f_logit_scale    = 0.0e+00
0.00.038.645 I print_info: n_ff             = 8192
0.00.038.645 I print_info: n_expert         = 0
0.00.038.645 I print_info: n_expert_used    = 0
0.00.038.645 I print_info: causal attn      = 1
0.00.038.645 I print_info: pooling type     = 0
0.00.038.645 I print_info: rope type        = 2
0.00.038.646 I print_info: rope scaling     = linear
0.00.038.647 I print_info: freq_base_train  = 10000.0
0.00.038.647 I print_info: freq_scale_train = 1
0.00.038.647 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.647 I print_info: rope_finetuned   = unknown
0.00.038.647 I print_info: ssm_d_conv       = 0
0.00.038.649 I print_info: ssm_d_inner      = 0
0.00.038.649 I print_info: ssm_d_state      = 0
0.00.038.649 I print_info: ssm_dt_rank      = 0
0.00.038.649 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.649 I print_info: model type       = 1.4B
0.00.038.650 I print_info: model params     = 1.41 B
0.00.038.650 I print_info: general.name     = 1.4B
0.00.038.650 I print_info: vocab type       = BPE
0.00.038.650 I print_info: n_vocab          = 50304
0.00.038.650 I print_info: n_merges         = 50009
0.00.038.651 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.651 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.651 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.651 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.651 I print_info: LF token         = 187 ''
0.00.038.652 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: max token length = 1024
0.00.038.652 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.597.640 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.657 I load_tensors: offloading output layer to GPU
0.00.597.658 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.698 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.597.700 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.599.171 I llama_init_from_model: n_seq_max     = 1
0.00.599.174 I llama_init_from_model: n_ctx         = 128
0.00.599.174 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.174 I llama_init_from_model: n_batch       = 128
0.00.599.175 I llama_init_from_model: n_ubatch      = 128
0.00.599.175 I llama_init_from_model: flash_attn    = 0
0.00.599.177 I llama_init_from_model: freq_base     = 10000.0
0.00.599.178 I llama_init_from_model: freq_scale    = 1
0.00.599.178 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.181 I ggml_metal_init: allocating
0.00.599.284 I ggml_metal_init: found device: Apple M4
0.00.599.297 I ggml_metal_init: picking default device: Apple M4
0.00.600.885 I ggml_metal_init: using embedded metal library
0.00.607.144 I ggml_metal_init: GPU name:   Apple M4
0.00.607.148 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.149 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.150 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.150 I ggml_metal_init: simdgroup reduction   = true
0.00.607.151 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.151 I ggml_metal_init: has residency sets    = true
0.00.607.151 I ggml_metal_init: has bfloat            = true
0.00.607.152 I ggml_metal_init: use bfloat            = true
0.00.607.153 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.303 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.760 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.627.766 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.627.808 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.630.967 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.630.969 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.630.969 I llama_init_from_model: graph nodes  = 967
0.00.630.970 I llama_init_from_model: graph splits = 2
0.00.630.972 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.630.972 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.948 I 
0.00.664.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.058 I perplexity: tokenizing the input ..
0.00.671.125 I perplexity: tokenization took 7.062 ms
0.00.671.145 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.213 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.821.566 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.821.582 I llama_perf_context_print:        load time =     654.86 ms
0.00.821.582 I llama_perf_context_print: prompt eval time =     148.14 ms /   128 tokens (    1.16 ms per token,   864.03 tokens per second)
0.00.821.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.583 I llama_perf_context_print:       total time =     157.64 ms /   129 tokens
0.00.821.985 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.079s
sys	0m0.139s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.449 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.271 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.277 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.279 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.279 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.279 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.281 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.282 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.283 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.283 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.284 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.284 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.284 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.285 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.286 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.286 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.287 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.125 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.125 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.962 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.963 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.964 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.964 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.964 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.965 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.965 I llama_model_loader: - type  f32:  194 tensors
0.00.024.965 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.966 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.966 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.967 I print_info: file format = GGUF V3 (latest)
0.00.024.967 I print_info: file type   = Q2_K - Medium
0.00.024.968 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.177 I load: special tokens cache size = 25
0.00.039.251 I load: token to piece cache size = 0.2984 MB
0.00.039.253 I print_info: arch             = gptneox
0.00.039.254 I print_info: vocab_only       = 0
0.00.039.254 I print_info: n_ctx_train      = 2048
0.00.039.254 I print_info: n_embd           = 2048
0.00.039.254 I print_info: n_layer          = 24
0.00.039.257 I print_info: n_head           = 16
0.00.039.257 I print_info: n_head_kv        = 16
0.00.039.258 I print_info: n_rot            = 32
0.00.039.258 I print_info: n_swa            = 0
0.00.039.260 I print_info: n_embd_head_k    = 128
0.00.039.260 I print_info: n_embd_head_v    = 128
0.00.039.261 I print_info: n_gqa            = 1
0.00.039.262 I print_info: n_embd_k_gqa     = 2048
0.00.039.267 I print_info: n_embd_v_gqa     = 2048
0.00.039.267 I print_info: f_norm_eps       = 1.0e-05
0.00.039.268 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.269 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.269 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.269 I print_info: f_logit_scale    = 0.0e+00
0.00.039.270 I print_info: n_ff             = 8192
0.00.039.270 I print_info: n_expert         = 0
0.00.039.273 I print_info: n_expert_used    = 0
0.00.039.273 I print_info: causal attn      = 1
0.00.039.273 I print_info: pooling type     = 0
0.00.039.274 I print_info: rope type        = 2
0.00.039.274 I print_info: rope scaling     = linear
0.00.039.275 I print_info: freq_base_train  = 10000.0
0.00.039.275 I print_info: freq_scale_train = 1
0.00.039.275 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.275 I print_info: rope_finetuned   = unknown
0.00.039.275 I print_info: ssm_d_conv       = 0
0.00.039.275 I print_info: ssm_d_inner      = 0
0.00.039.275 I print_info: ssm_d_state      = 0
0.00.039.276 I print_info: ssm_dt_rank      = 0
0.00.039.276 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.276 I print_info: model type       = 1.4B
0.00.039.276 I print_info: model params     = 1.41 B
0.00.039.276 I print_info: general.name     = 1.4B
0.00.039.277 I print_info: vocab type       = BPE
0.00.039.277 I print_info: n_vocab          = 50304
0.00.039.277 I print_info: n_merges         = 50009
0.00.039.279 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.279 I print_info: LF token         = 187 ''
0.00.039.279 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.280 I print_info: max token length = 1024
0.00.039.280 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.349.687 I load_tensors: offloading 24 repeating layers to GPU
0.00.349.698 I load_tensors: offloading output layer to GPU
0.00.349.698 I load_tensors: offloaded 25/25 layers to GPU
0.00.349.731 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.349.732 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.351.183 I llama_init_from_model: n_seq_max     = 1
0.00.351.186 I llama_init_from_model: n_ctx         = 2048
0.00.351.187 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.351.187 I llama_init_from_model: n_batch       = 2048
0.00.351.187 I llama_init_from_model: n_ubatch      = 512
0.00.351.188 I llama_init_from_model: flash_attn    = 0
0.00.351.191 I llama_init_from_model: freq_base     = 10000.0
0.00.351.191 I llama_init_from_model: freq_scale    = 1
0.00.351.193 I ggml_metal_init: allocating
0.00.351.280 I ggml_metal_init: found device: Apple M4
0.00.351.292 I ggml_metal_init: picking default device: Apple M4
0.00.353.254 I ggml_metal_init: using embedded metal library
0.00.359.074 I ggml_metal_init: GPU name:   Apple M4
0.00.359.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.359.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.359.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.359.100 I ggml_metal_init: simdgroup reduction   = true
0.00.359.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.359.100 I ggml_metal_init: has residency sets    = true
0.00.359.101 I ggml_metal_init: has bfloat            = true
0.00.359.101 I ggml_metal_init: use bfloat            = true
0.00.359.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.359.111 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.380.861 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.429.631 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.429.646 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.429.681 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.433.854 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.433.856 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.433.856 I llama_init_from_model: graph nodes  = 967
0.00.433.856 I llama_init_from_model: graph splits = 2
0.00.433.863 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.433.992 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.433.993 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.808 I main: llama threadpool init, n_threads = 4
0.00.491.852 I 
0.00.491.871 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.491.872 I 
0.00.492.042 I sampler seed: 1234
0.00.492.046 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.492.081 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.492.085 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.492.085 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.156.751 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50283.29 tokens per second)
0.01.156.752 I llama_perf_context_print:        load time =     481.58 ms
0.01.156.753 I llama_perf_context_print: prompt eval time =      35.87 ms /     7 tokens (    5.12 ms per token,   195.14 tokens per second)
0.01.156.754 I llama_perf_context_print:        eval time =     626.39 ms /    63 runs   (    9.94 ms per token,   100.58 tokens per second)
0.01.156.754 I llama_perf_context_print:       total time =     665.72 ms /    70 tokens
0.01.156.991 I ggml_metal_free: deallocating

real	0m1.175s
user	0m0.112s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.497 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.350 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.364 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.365 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.367 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.368 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.368 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.369 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.151 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.147 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.915 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.917 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.917 I llama_model_loader: - type  f32:  194 tensors
0.00.024.917 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.918 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.918 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.919 I print_info: file format = GGUF V3 (latest)
0.00.024.919 I print_info: file type   = Q2_K - Medium
0.00.024.920 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.303 I load: special tokens cache size = 25
0.00.039.029 I load: token to piece cache size = 0.2984 MB
0.00.039.034 I print_info: arch             = gptneox
0.00.039.034 I print_info: vocab_only       = 0
0.00.039.035 I print_info: n_ctx_train      = 2048
0.00.039.035 I print_info: n_embd           = 2048
0.00.039.035 I print_info: n_layer          = 24
0.00.039.039 I print_info: n_head           = 16
0.00.039.040 I print_info: n_head_kv        = 16
0.00.039.040 I print_info: n_rot            = 32
0.00.039.040 I print_info: n_swa            = 0
0.00.039.040 I print_info: n_embd_head_k    = 128
0.00.039.040 I print_info: n_embd_head_v    = 128
0.00.039.044 I print_info: n_gqa            = 1
0.00.039.045 I print_info: n_embd_k_gqa     = 2048
0.00.039.045 I print_info: n_embd_v_gqa     = 2048
0.00.039.046 I print_info: f_norm_eps       = 1.0e-05
0.00.039.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.046 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.046 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.046 I print_info: f_logit_scale    = 0.0e+00
0.00.039.047 I print_info: n_ff             = 8192
0.00.039.047 I print_info: n_expert         = 0
0.00.039.047 I print_info: n_expert_used    = 0
0.00.039.048 I print_info: causal attn      = 1
0.00.039.048 I print_info: pooling type     = 0
0.00.039.048 I print_info: rope type        = 2
0.00.039.048 I print_info: rope scaling     = linear
0.00.039.048 I print_info: freq_base_train  = 10000.0
0.00.039.049 I print_info: freq_scale_train = 1
0.00.039.049 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.049 I print_info: rope_finetuned   = unknown
0.00.039.049 I print_info: ssm_d_conv       = 0
0.00.039.049 I print_info: ssm_d_inner      = 0
0.00.039.049 I print_info: ssm_d_state      = 0
0.00.039.050 I print_info: ssm_dt_rank      = 0
0.00.039.050 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.050 I print_info: model type       = 1.4B
0.00.039.050 I print_info: model params     = 1.41 B
0.00.039.050 I print_info: general.name     = 1.4B
0.00.039.051 I print_info: vocab type       = BPE
0.00.039.051 I print_info: n_vocab          = 50304
0.00.039.051 I print_info: n_merges         = 50009
0.00.039.053 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.053 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.053 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.053 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.054 I print_info: LF token         = 187 ''
0.00.039.054 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.054 I print_info: max token length = 1024
0.00.039.054 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.334.647 I load_tensors: offloading 24 repeating layers to GPU
0.00.334.663 I load_tensors: offloading output layer to GPU
0.00.334.664 I load_tensors: offloaded 25/25 layers to GPU
0.00.334.696 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.334.698 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.336.393 I llama_init_from_model: n_seq_max     = 1
0.00.336.396 I llama_init_from_model: n_ctx         = 128
0.00.336.396 I llama_init_from_model: n_ctx_per_seq = 128
0.00.336.397 I llama_init_from_model: n_batch       = 128
0.00.336.397 I llama_init_from_model: n_ubatch      = 128
0.00.336.397 I llama_init_from_model: flash_attn    = 0
0.00.336.400 I llama_init_from_model: freq_base     = 10000.0
0.00.336.400 I llama_init_from_model: freq_scale    = 1
0.00.336.401 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.336.410 I ggml_metal_init: allocating
0.00.336.491 I ggml_metal_init: found device: Apple M4
0.00.336.503 I ggml_metal_init: picking default device: Apple M4
0.00.338.336 I ggml_metal_init: using embedded metal library
0.00.343.764 I ggml_metal_init: GPU name:   Apple M4
0.00.343.784 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.343.785 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.343.786 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.343.786 I ggml_metal_init: simdgroup reduction   = true
0.00.343.787 I ggml_metal_init: simdgroup matrix mul. = true
0.00.343.787 I ggml_metal_init: has residency sets    = true
0.00.343.787 I ggml_metal_init: has bfloat            = true
0.00.343.788 I ggml_metal_init: use bfloat            = true
0.00.343.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.343.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.365.356 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.369.030 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.369.036 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.369.082 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.372.596 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.372.598 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.372.598 I llama_init_from_model: graph nodes  = 967
0.00.372.599 I llama_init_from_model: graph splits = 2
0.00.372.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.372.603 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.400.026 I 
0.00.400.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.400.119 I perplexity: tokenizing the input ..
0.00.407.138 I perplexity: tokenization took 7.016 ms
0.00.407.156 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.540.199 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.541.537 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.541.559 I llama_perf_context_print:        load time =     390.52 ms
0.00.541.561 I llama_perf_context_print: prompt eval time =     132.07 ms /   128 tokens (    1.03 ms per token,   969.15 tokens per second)
0.00.541.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.541.566 I llama_perf_context_print:       total time =     141.53 ms /   129 tokens
0.00.541.954 I ggml_metal_free: deallocating

real	0m0.558s
user	0m0.081s
sys	0m0.088s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.180 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.841 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.847 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.849 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.850 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.851 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.851 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.851 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.852 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.853 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.853 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.854 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.854 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.854 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.855 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.858 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.858 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.858 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.745 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.774 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.680 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.682 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.682 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.682 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.683 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.684 I llama_model_loader: - type  f32:  194 tensors
0.00.027.684 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.685 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.685 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.685 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.691 I print_info: file format = GGUF V3 (latest)
0.00.027.691 I print_info: file type   = Q3_K - Medium
0.00.027.693 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.036.313 I load: special tokens cache size = 25
0.00.042.490 I load: token to piece cache size = 0.2984 MB
0.00.042.493 I print_info: arch             = gptneox
0.00.042.494 I print_info: vocab_only       = 0
0.00.042.494 I print_info: n_ctx_train      = 2048
0.00.042.494 I print_info: n_embd           = 2048
0.00.042.499 I print_info: n_layer          = 24
0.00.042.502 I print_info: n_head           = 16
0.00.042.503 I print_info: n_head_kv        = 16
0.00.042.503 I print_info: n_rot            = 32
0.00.042.503 I print_info: n_swa            = 0
0.00.042.504 I print_info: n_embd_head_k    = 128
0.00.042.504 I print_info: n_embd_head_v    = 128
0.00.042.504 I print_info: n_gqa            = 1
0.00.042.505 I print_info: n_embd_k_gqa     = 2048
0.00.042.506 I print_info: n_embd_v_gqa     = 2048
0.00.042.506 I print_info: f_norm_eps       = 1.0e-05
0.00.042.508 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.510 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.510 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.510 I print_info: f_logit_scale    = 0.0e+00
0.00.042.511 I print_info: n_ff             = 8192
0.00.042.511 I print_info: n_expert         = 0
0.00.042.511 I print_info: n_expert_used    = 0
0.00.042.511 I print_info: causal attn      = 1
0.00.042.511 I print_info: pooling type     = 0
0.00.042.511 I print_info: rope type        = 2
0.00.042.512 I print_info: rope scaling     = linear
0.00.042.512 I print_info: freq_base_train  = 10000.0
0.00.042.512 I print_info: freq_scale_train = 1
0.00.042.512 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.513 I print_info: rope_finetuned   = unknown
0.00.042.513 I print_info: ssm_d_conv       = 0
0.00.042.513 I print_info: ssm_d_inner      = 0
0.00.042.513 I print_info: ssm_d_state      = 0
0.00.042.513 I print_info: ssm_dt_rank      = 0
0.00.042.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.513 I print_info: model type       = 1.4B
0.00.042.514 I print_info: model params     = 1.41 B
0.00.042.514 I print_info: general.name     = 1.4B
0.00.042.514 I print_info: vocab type       = BPE
0.00.042.514 I print_info: n_vocab          = 50304
0.00.042.515 I print_info: n_merges         = 50009
0.00.042.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.515 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.515 I print_info: LF token         = 187 ''
0.00.042.516 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.516 I print_info: max token length = 1024
0.00.042.516 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.423.213 I load_tensors: offloading 24 repeating layers to GPU
0.00.423.219 I load_tensors: offloading output layer to GPU
0.00.423.219 I load_tensors: offloaded 25/25 layers to GPU
0.00.423.237 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.423.239 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.424.136 I llama_init_from_model: n_seq_max     = 1
0.00.424.140 I llama_init_from_model: n_ctx         = 2048
0.00.424.141 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.424.141 I llama_init_from_model: n_batch       = 2048
0.00.424.141 I llama_init_from_model: n_ubatch      = 512
0.00.424.142 I llama_init_from_model: flash_attn    = 0
0.00.424.143 I llama_init_from_model: freq_base     = 10000.0
0.00.424.143 I llama_init_from_model: freq_scale    = 1
0.00.424.144 I ggml_metal_init: allocating
0.00.424.171 I ggml_metal_init: found device: Apple M4
0.00.424.182 I ggml_metal_init: picking default device: Apple M4
0.00.425.243 I ggml_metal_init: using embedded metal library
0.00.431.045 I ggml_metal_init: GPU name:   Apple M4
0.00.431.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.431.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.431.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.431.054 I ggml_metal_init: simdgroup reduction   = true
0.00.431.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.431.054 I ggml_metal_init: has residency sets    = true
0.00.431.055 I ggml_metal_init: has bfloat            = true
0.00.431.055 I ggml_metal_init: use bfloat            = true
0.00.431.056 I ggml_metal_init: hasUnifiedMemory      = true
0.00.431.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.451.266 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.484.205 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.484.212 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.484.247 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.488.487 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.488.490 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.488.490 I llama_init_from_model: graph nodes  = 967
0.00.488.490 I llama_init_from_model: graph splits = 2
0.00.488.494 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.488.632 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.488.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.546.422 I main: llama threadpool init, n_threads = 4
0.00.546.467 I 
0.00.546.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.546.489 I 
0.00.546.655 I sampler seed: 1234
0.00.546.660 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.546.670 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.546.670 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.546.670 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.289.700 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.01.289.701 I llama_perf_context_print:        load time =     535.51 ms
0.01.289.701 I llama_perf_context_print: prompt eval time =      48.79 ms /     7 tokens (    6.97 ms per token,   143.46 tokens per second)
0.01.289.705 I llama_perf_context_print:        eval time =     691.18 ms /    63 runs   (   10.97 ms per token,    91.15 tokens per second)
0.01.289.706 I llama_perf_context_print:       total time =     744.01 ms /    70 tokens
0.01.289.915 I ggml_metal_free: deallocating

real	0m1.307s
user	0m0.109s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.858 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.145 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.152 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.153 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.154 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.154 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.155 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.155 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.156 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.156 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.157 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.157 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.161 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.980 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.025 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.818 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.818 I llama_model_loader: - type  f32:  194 tensors
0.00.024.819 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.819 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.819 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.819 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.820 I print_info: file format = GGUF V3 (latest)
0.00.024.822 I print_info: file type   = Q3_K - Medium
0.00.024.823 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.105 I load: special tokens cache size = 25
0.00.039.207 I load: token to piece cache size = 0.2984 MB
0.00.039.212 I print_info: arch             = gptneox
0.00.039.212 I print_info: vocab_only       = 0
0.00.039.212 I print_info: n_ctx_train      = 2048
0.00.039.212 I print_info: n_embd           = 2048
0.00.039.213 I print_info: n_layer          = 24
0.00.039.217 I print_info: n_head           = 16
0.00.039.218 I print_info: n_head_kv        = 16
0.00.039.218 I print_info: n_rot            = 32
0.00.039.218 I print_info: n_swa            = 0
0.00.039.218 I print_info: n_embd_head_k    = 128
0.00.039.219 I print_info: n_embd_head_v    = 128
0.00.039.219 I print_info: n_gqa            = 1
0.00.039.220 I print_info: n_embd_k_gqa     = 2048
0.00.039.221 I print_info: n_embd_v_gqa     = 2048
0.00.039.221 I print_info: f_norm_eps       = 1.0e-05
0.00.039.222 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.222 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.222 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.222 I print_info: f_logit_scale    = 0.0e+00
0.00.039.223 I print_info: n_ff             = 8192
0.00.039.223 I print_info: n_expert         = 0
0.00.039.223 I print_info: n_expert_used    = 0
0.00.039.223 I print_info: causal attn      = 1
0.00.039.223 I print_info: pooling type     = 0
0.00.039.223 I print_info: rope type        = 2
0.00.039.223 I print_info: rope scaling     = linear
0.00.039.224 I print_info: freq_base_train  = 10000.0
0.00.039.224 I print_info: freq_scale_train = 1
0.00.039.224 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.224 I print_info: rope_finetuned   = unknown
0.00.039.224 I print_info: ssm_d_conv       = 0
0.00.039.225 I print_info: ssm_d_inner      = 0
0.00.039.227 I print_info: ssm_d_state      = 0
0.00.039.227 I print_info: ssm_dt_rank      = 0
0.00.039.227 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.228 I print_info: model type       = 1.4B
0.00.039.228 I print_info: model params     = 1.41 B
0.00.039.228 I print_info: general.name     = 1.4B
0.00.039.229 I print_info: vocab type       = BPE
0.00.039.229 I print_info: n_vocab          = 50304
0.00.039.230 I print_info: n_merges         = 50009
0.00.039.230 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.230 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: LF token         = 187 ''
0.00.039.231 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.231 I print_info: max token length = 1024
0.00.039.232 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.460.002 I load_tensors: offloading 24 repeating layers to GPU
0.00.460.018 I load_tensors: offloading output layer to GPU
0.00.460.019 I load_tensors: offloaded 25/25 layers to GPU
0.00.460.054 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.460.056 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.461.644 I llama_init_from_model: n_seq_max     = 1
0.00.461.650 I llama_init_from_model: n_ctx         = 128
0.00.461.651 I llama_init_from_model: n_ctx_per_seq = 128
0.00.461.651 I llama_init_from_model: n_batch       = 128
0.00.461.652 I llama_init_from_model: n_ubatch      = 128
0.00.461.652 I llama_init_from_model: flash_attn    = 0
0.00.461.653 I llama_init_from_model: freq_base     = 10000.0
0.00.461.654 I llama_init_from_model: freq_scale    = 1
0.00.461.654 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.461.660 I ggml_metal_init: allocating
0.00.461.765 I ggml_metal_init: found device: Apple M4
0.00.461.780 I ggml_metal_init: picking default device: Apple M4
0.00.463.582 I ggml_metal_init: using embedded metal library
0.00.469.869 I ggml_metal_init: GPU name:   Apple M4
0.00.469.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.469.885 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.469.886 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.469.886 I ggml_metal_init: simdgroup reduction   = true
0.00.469.887 I ggml_metal_init: simdgroup matrix mul. = true
0.00.469.887 I ggml_metal_init: has residency sets    = true
0.00.469.887 I ggml_metal_init: has bfloat            = true
0.00.469.888 I ggml_metal_init: use bfloat            = true
0.00.469.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.469.893 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.491.393 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.495.117 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.495.129 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.495.186 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.498.555 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.498.557 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.498.558 I llama_init_from_model: graph nodes  = 967
0.00.498.558 I llama_init_from_model: graph splits = 2
0.00.498.561 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.498.561 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.525.337 I 
0.00.525.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.525.451 I perplexity: tokenizing the input ..
0.00.532.332 I perplexity: tokenization took 6.878 ms
0.00.532.350 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.663.640 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.665.069 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.665.084 I llama_perf_context_print:        load time =     516.47 ms
0.00.665.085 I llama_perf_context_print: prompt eval time =     130.92 ms /   128 tokens (    1.02 ms per token,   977.73 tokens per second)
0.00.665.085 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.665.086 I llama_perf_context_print:       total time =     139.75 ms /   129 tokens
0.00.665.439 I ggml_metal_free: deallocating

real	0m0.679s
user	0m0.080s
sys	0m0.121s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.271 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.276 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.278 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.279 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.279 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.279 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.280 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.281 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.281 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.281 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.282 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.282 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.282 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.283 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.286 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.286 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.286 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.113 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.118 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.882 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.884 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.884 I llama_model_loader: - type  f32:  194 tensors
0.00.024.884 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.885 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.885 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.885 I print_info: file format = GGUF V3 (latest)
0.00.024.886 I print_info: file type   = Q4_K - Medium
0.00.024.892 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.053 I load: special tokens cache size = 25
0.00.039.031 I load: token to piece cache size = 0.2984 MB
0.00.039.034 I print_info: arch             = gptneox
0.00.039.034 I print_info: vocab_only       = 0
0.00.039.034 I print_info: n_ctx_train      = 2048
0.00.039.034 I print_info: n_embd           = 2048
0.00.039.034 I print_info: n_layer          = 24
0.00.039.037 I print_info: n_head           = 16
0.00.039.038 I print_info: n_head_kv        = 16
0.00.039.038 I print_info: n_rot            = 32
0.00.039.038 I print_info: n_swa            = 0
0.00.039.039 I print_info: n_embd_head_k    = 128
0.00.039.039 I print_info: n_embd_head_v    = 128
0.00.039.039 I print_info: n_gqa            = 1
0.00.039.041 I print_info: n_embd_k_gqa     = 2048
0.00.039.042 I print_info: n_embd_v_gqa     = 2048
0.00.039.042 I print_info: f_norm_eps       = 1.0e-05
0.00.039.043 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.043 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.043 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.043 I print_info: f_logit_scale    = 0.0e+00
0.00.039.044 I print_info: n_ff             = 8192
0.00.039.044 I print_info: n_expert         = 0
0.00.039.044 I print_info: n_expert_used    = 0
0.00.039.044 I print_info: causal attn      = 1
0.00.039.046 I print_info: pooling type     = 0
0.00.039.048 I print_info: rope type        = 2
0.00.039.048 I print_info: rope scaling     = linear
0.00.039.048 I print_info: freq_base_train  = 10000.0
0.00.039.049 I print_info: freq_scale_train = 1
0.00.039.049 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.049 I print_info: rope_finetuned   = unknown
0.00.039.049 I print_info: ssm_d_conv       = 0
0.00.039.049 I print_info: ssm_d_inner      = 0
0.00.039.050 I print_info: ssm_d_state      = 0
0.00.039.050 I print_info: ssm_dt_rank      = 0
0.00.039.050 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.050 I print_info: model type       = 1.4B
0.00.039.051 I print_info: model params     = 1.41 B
0.00.039.051 I print_info: general.name     = 1.4B
0.00.039.051 I print_info: vocab type       = BPE
0.00.039.051 I print_info: n_vocab          = 50304
0.00.039.051 I print_info: n_merges         = 50009
0.00.039.052 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.052 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.052 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.052 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.056 I print_info: LF token         = 187 ''
0.00.039.056 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.056 I print_info: max token length = 1024
0.00.039.057 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.513.464 I load_tensors: offloading 24 repeating layers to GPU
0.00.513.480 I load_tensors: offloading output layer to GPU
0.00.513.480 I load_tensors: offloaded 25/25 layers to GPU
0.00.513.515 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.513.517 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.515.184 I llama_init_from_model: n_seq_max     = 1
0.00.515.187 I llama_init_from_model: n_ctx         = 2048
0.00.515.187 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.515.188 I llama_init_from_model: n_batch       = 2048
0.00.515.188 I llama_init_from_model: n_ubatch      = 512
0.00.515.189 I llama_init_from_model: flash_attn    = 0
0.00.515.192 I llama_init_from_model: freq_base     = 10000.0
0.00.515.192 I llama_init_from_model: freq_scale    = 1
0.00.515.195 I ggml_metal_init: allocating
0.00.515.268 I ggml_metal_init: found device: Apple M4
0.00.515.281 I ggml_metal_init: picking default device: Apple M4
0.00.517.220 I ggml_metal_init: using embedded metal library
0.00.523.478 I ggml_metal_init: GPU name:   Apple M4
0.00.523.484 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.523.485 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.523.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.523.486 I ggml_metal_init: simdgroup reduction   = true
0.00.523.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.523.487 I ggml_metal_init: has residency sets    = true
0.00.523.488 I ggml_metal_init: has bfloat            = true
0.00.523.488 I ggml_metal_init: use bfloat            = true
0.00.523.489 I ggml_metal_init: hasUnifiedMemory      = true
0.00.523.491 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.271 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.596.138 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.596.146 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.596.188 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.601.123 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.601.125 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.601.125 I llama_init_from_model: graph nodes  = 967
0.00.601.125 I llama_init_from_model: graph splits = 2
0.00.601.131 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.601.255 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.601.256 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.218 I main: llama threadpool init, n_threads = 4
0.00.650.259 I 
0.00.650.280 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.280 I 
0.00.650.388 I sampler seed: 1234
0.00.650.393 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.650.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.650.434 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.650.434 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.424.282 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49374.13 tokens per second)
0.01.424.283 I llama_perf_context_print:        load time =     640.66 ms
0.01.424.283 I llama_perf_context_print: prompt eval time =      59.13 ms /     7 tokens (    8.45 ms per token,   118.38 tokens per second)
0.01.424.284 I llama_perf_context_print:        eval time =     711.75 ms /    63 runs   (   11.30 ms per token,    88.51 tokens per second)
0.01.424.284 I llama_perf_context_print:       total time =     774.77 ms /    70 tokens
0.01.424.565 I ggml_metal_free: deallocating

real	0m1.441s
user	0m0.110s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.815 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.030 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.032 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.032 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.033 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.033 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.033 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.034 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.035 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.035 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.036 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.037 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.038 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.039 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.041 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.041 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.041 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.903 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.866 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.868 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.868 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.869 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.869 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.869 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.870 I llama_model_loader: - type  f32:  194 tensors
0.00.024.871 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.871 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.871 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.872 I print_info: file format = GGUF V3 (latest)
0.00.024.872 I print_info: file type   = Q4_K - Medium
0.00.024.874 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.426 I load: special tokens cache size = 25
0.00.039.445 I load: token to piece cache size = 0.2984 MB
0.00.039.451 I print_info: arch             = gptneox
0.00.039.452 I print_info: vocab_only       = 0
0.00.039.454 I print_info: n_ctx_train      = 2048
0.00.039.455 I print_info: n_embd           = 2048
0.00.039.455 I print_info: n_layer          = 24
0.00.039.460 I print_info: n_head           = 16
0.00.039.460 I print_info: n_head_kv        = 16
0.00.039.460 I print_info: n_rot            = 32
0.00.039.461 I print_info: n_swa            = 0
0.00.039.461 I print_info: n_embd_head_k    = 128
0.00.039.461 I print_info: n_embd_head_v    = 128
0.00.039.461 I print_info: n_gqa            = 1
0.00.039.462 I print_info: n_embd_k_gqa     = 2048
0.00.039.463 I print_info: n_embd_v_gqa     = 2048
0.00.039.463 I print_info: f_norm_eps       = 1.0e-05
0.00.039.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.464 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.492 I print_info: f_logit_scale    = 0.0e+00
0.00.039.495 I print_info: n_ff             = 8192
0.00.039.495 I print_info: n_expert         = 0
0.00.039.495 I print_info: n_expert_used    = 0
0.00.039.495 I print_info: causal attn      = 1
0.00.039.495 I print_info: pooling type     = 0
0.00.039.495 I print_info: rope type        = 2
0.00.039.496 I print_info: rope scaling     = linear
0.00.039.498 I print_info: freq_base_train  = 10000.0
0.00.039.498 I print_info: freq_scale_train = 1
0.00.039.498 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.498 I print_info: rope_finetuned   = unknown
0.00.039.499 I print_info: ssm_d_conv       = 0
0.00.039.499 I print_info: ssm_d_inner      = 0
0.00.039.499 I print_info: ssm_d_state      = 0
0.00.039.499 I print_info: ssm_dt_rank      = 0
0.00.039.499 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.499 I print_info: model type       = 1.4B
0.00.039.500 I print_info: model params     = 1.41 B
0.00.039.500 I print_info: general.name     = 1.4B
0.00.039.500 I print_info: vocab type       = BPE
0.00.039.501 I print_info: n_vocab          = 50304
0.00.039.501 I print_info: n_merges         = 50009
0.00.039.501 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.501 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.501 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.501 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.502 I print_info: LF token         = 187 ''
0.00.039.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.502 I print_info: max token length = 1024
0.00.039.502 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.493.691 I load_tensors: offloading 24 repeating layers to GPU
0.00.493.710 I load_tensors: offloading output layer to GPU
0.00.493.711 I load_tensors: offloaded 25/25 layers to GPU
0.00.493.745 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.493.746 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.495.040 I llama_init_from_model: n_seq_max     = 1
0.00.495.046 I llama_init_from_model: n_ctx         = 128
0.00.495.047 I llama_init_from_model: n_ctx_per_seq = 128
0.00.495.047 I llama_init_from_model: n_batch       = 128
0.00.495.047 I llama_init_from_model: n_ubatch      = 128
0.00.495.048 I llama_init_from_model: flash_attn    = 0
0.00.495.049 I llama_init_from_model: freq_base     = 10000.0
0.00.495.050 I llama_init_from_model: freq_scale    = 1
0.00.495.050 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.495.052 I ggml_metal_init: allocating
0.00.495.107 I ggml_metal_init: found device: Apple M4
0.00.495.121 I ggml_metal_init: picking default device: Apple M4
0.00.496.505 I ggml_metal_init: using embedded metal library
0.00.500.692 I ggml_metal_init: GPU name:   Apple M4
0.00.500.697 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.500.697 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.500.698 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.500.698 I ggml_metal_init: simdgroup reduction   = true
0.00.500.698 I ggml_metal_init: simdgroup matrix mul. = true
0.00.500.698 I ggml_metal_init: has residency sets    = true
0.00.500.698 I ggml_metal_init: has bfloat            = true
0.00.500.698 I ggml_metal_init: use bfloat            = true
0.00.500.699 I ggml_metal_init: hasUnifiedMemory      = true
0.00.500.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.511.094 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.512.866 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.512.868 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.512.895 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.514.638 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.514.639 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.514.639 I llama_init_from_model: graph nodes  = 967
0.00.514.640 I llama_init_from_model: graph splits = 2
0.00.514.641 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.514.641 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.539.952 I 
0.00.539.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.539.997 I perplexity: tokenizing the input ..
0.00.544.062 I perplexity: tokenization took 4.064 ms
0.00.544.077 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.689.550 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.690.882 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.690.894 I llama_perf_context_print:        load time =     531.13 ms
0.00.690.896 I llama_perf_context_print: prompt eval time =     145.25 ms /   128 tokens (    1.13 ms per token,   881.27 tokens per second)
0.00.690.896 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.690.897 I llama_perf_context_print:       total time =     150.94 ms /   129 tokens
0.00.691.245 I ggml_metal_free: deallocating

real	0m0.707s
user	0m0.067s
sys	0m0.094s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.606 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.164 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.169 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.176 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.182 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.084 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.869 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.870 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.871 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.871 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.871 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.871 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.872 I llama_model_loader: - type  f32:  194 tensors
0.00.026.872 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.872 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.873 I print_info: file format = GGUF V3 (latest)
0.00.026.873 I print_info: file type   = Q5_K - Medium
0.00.026.874 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.747 I load: special tokens cache size = 25
0.00.040.794 I load: token to piece cache size = 0.2984 MB
0.00.040.797 I print_info: arch             = gptneox
0.00.040.797 I print_info: vocab_only       = 0
0.00.040.797 I print_info: n_ctx_train      = 2048
0.00.040.797 I print_info: n_embd           = 2048
0.00.040.797 I print_info: n_layer          = 24
0.00.040.801 I print_info: n_head           = 16
0.00.040.801 I print_info: n_head_kv        = 16
0.00.040.803 I print_info: n_rot            = 32
0.00.040.803 I print_info: n_swa            = 0
0.00.040.804 I print_info: n_embd_head_k    = 128
0.00.040.804 I print_info: n_embd_head_v    = 128
0.00.040.804 I print_info: n_gqa            = 1
0.00.040.805 I print_info: n_embd_k_gqa     = 2048
0.00.040.810 I print_info: n_embd_v_gqa     = 2048
0.00.040.812 I print_info: f_norm_eps       = 1.0e-05
0.00.040.812 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.812 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.812 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.813 I print_info: f_logit_scale    = 0.0e+00
0.00.040.813 I print_info: n_ff             = 8192
0.00.040.814 I print_info: n_expert         = 0
0.00.040.814 I print_info: n_expert_used    = 0
0.00.040.814 I print_info: causal attn      = 1
0.00.040.814 I print_info: pooling type     = 0
0.00.040.814 I print_info: rope type        = 2
0.00.040.814 I print_info: rope scaling     = linear
0.00.040.816 I print_info: freq_base_train  = 10000.0
0.00.040.816 I print_info: freq_scale_train = 1
0.00.040.816 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.817 I print_info: rope_finetuned   = unknown
0.00.040.817 I print_info: ssm_d_conv       = 0
0.00.040.817 I print_info: ssm_d_inner      = 0
0.00.040.817 I print_info: ssm_d_state      = 0
0.00.040.817 I print_info: ssm_dt_rank      = 0
0.00.040.817 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.817 I print_info: model type       = 1.4B
0.00.040.818 I print_info: model params     = 1.41 B
0.00.040.818 I print_info: general.name     = 1.4B
0.00.040.819 I print_info: vocab type       = BPE
0.00.040.819 I print_info: n_vocab          = 50304
0.00.040.819 I print_info: n_merges         = 50009
0.00.040.819 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.819 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.819 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.820 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.820 I print_info: LF token         = 187 ''
0.00.040.820 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.824 I print_info: max token length = 1024
0.00.040.825 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.592.895 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.909 I load_tensors: offloading output layer to GPU
0.00.592.910 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.943 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.592.944 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.594.435 I llama_init_from_model: n_seq_max     = 1
0.00.594.437 I llama_init_from_model: n_ctx         = 2048
0.00.594.438 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.594.439 I llama_init_from_model: n_batch       = 2048
0.00.594.439 I llama_init_from_model: n_ubatch      = 512
0.00.594.439 I llama_init_from_model: flash_attn    = 0
0.00.594.441 I llama_init_from_model: freq_base     = 10000.0
0.00.594.442 I llama_init_from_model: freq_scale    = 1
0.00.594.444 I ggml_metal_init: allocating
0.00.594.522 I ggml_metal_init: found device: Apple M4
0.00.594.533 I ggml_metal_init: picking default device: Apple M4
0.00.596.046 I ggml_metal_init: using embedded metal library
0.00.602.537 I ggml_metal_init: GPU name:   Apple M4
0.00.602.541 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.543 I ggml_metal_init: simdgroup reduction   = true
0.00.602.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.543 I ggml_metal_init: has residency sets    = true
0.00.602.544 I ggml_metal_init: has bfloat            = true
0.00.602.544 I ggml_metal_init: use bfloat            = true
0.00.602.545 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.435 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.671.134 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.671.141 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.671.176 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.442 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.676.445 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.676.445 I llama_init_from_model: graph nodes  = 967
0.00.676.445 I llama_init_from_model: graph splits = 2
0.00.676.452 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.676.576 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.676.576 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.296 I main: llama threadpool init, n_threads = 4
0.00.737.340 I 
0.00.737.362 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.362 I 
0.00.737.518 I sampler seed: 1234
0.00.737.522 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.737.543 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.737.543 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.737.543 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.588.602 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.588.602 I llama_perf_context_print:        load time =     725.97 ms
0.01.588.603 I llama_perf_context_print: prompt eval time =      52.61 ms /     7 tokens (    7.52 ms per token,   133.06 tokens per second)
0.01.588.607 I llama_perf_context_print:        eval time =     795.49 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.588.607 I llama_perf_context_print:       total time =     852.02 ms /    70 tokens
0.01.588.832 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.109s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.914 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.929 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.935 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.937 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.938 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.941 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.941 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.947 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.948 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.741 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.816 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.617 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.617 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.618 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.618 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.619 I llama_model_loader: - type  f32:  194 tensors
0.00.025.619 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.619 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.620 I print_info: file format = GGUF V3 (latest)
0.00.025.621 I print_info: file type   = Q5_K - Medium
0.00.025.622 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.633 I load: special tokens cache size = 25
0.00.039.584 I load: token to piece cache size = 0.2984 MB
0.00.039.587 I print_info: arch             = gptneox
0.00.039.588 I print_info: vocab_only       = 0
0.00.039.588 I print_info: n_ctx_train      = 2048
0.00.039.588 I print_info: n_embd           = 2048
0.00.039.588 I print_info: n_layer          = 24
0.00.039.592 I print_info: n_head           = 16
0.00.039.596 I print_info: n_head_kv        = 16
0.00.039.596 I print_info: n_rot            = 32
0.00.039.596 I print_info: n_swa            = 0
0.00.039.596 I print_info: n_embd_head_k    = 128
0.00.039.596 I print_info: n_embd_head_v    = 128
0.00.039.597 I print_info: n_gqa            = 1
0.00.039.598 I print_info: n_embd_k_gqa     = 2048
0.00.039.599 I print_info: n_embd_v_gqa     = 2048
0.00.039.599 I print_info: f_norm_eps       = 1.0e-05
0.00.039.607 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.610 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.610 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.610 I print_info: f_logit_scale    = 0.0e+00
0.00.039.619 I print_info: n_ff             = 8192
0.00.039.619 I print_info: n_expert         = 0
0.00.039.619 I print_info: n_expert_used    = 0
0.00.039.619 I print_info: causal attn      = 1
0.00.039.619 I print_info: pooling type     = 0
0.00.039.620 I print_info: rope type        = 2
0.00.039.620 I print_info: rope scaling     = linear
0.00.039.620 I print_info: freq_base_train  = 10000.0
0.00.039.620 I print_info: freq_scale_train = 1
0.00.039.622 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.622 I print_info: rope_finetuned   = unknown
0.00.039.622 I print_info: ssm_d_conv       = 0
0.00.039.622 I print_info: ssm_d_inner      = 0
0.00.039.622 I print_info: ssm_d_state      = 0
0.00.039.622 I print_info: ssm_dt_rank      = 0
0.00.039.622 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.623 I print_info: model type       = 1.4B
0.00.039.623 I print_info: model params     = 1.41 B
0.00.039.625 I print_info: general.name     = 1.4B
0.00.039.625 I print_info: vocab type       = BPE
0.00.039.625 I print_info: n_vocab          = 50304
0.00.039.626 I print_info: n_merges         = 50009
0.00.039.626 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.626 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.626 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.626 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.627 I print_info: LF token         = 187 ''
0.00.039.627 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: max token length = 1024
0.00.039.628 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.454 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.468 I load_tensors: offloading output layer to GPU
0.00.587.469 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.502 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.504 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.589.218 I llama_init_from_model: n_seq_max     = 1
0.00.589.221 I llama_init_from_model: n_ctx         = 128
0.00.589.222 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.222 I llama_init_from_model: n_batch       = 128
0.00.589.222 I llama_init_from_model: n_ubatch      = 128
0.00.589.223 I llama_init_from_model: flash_attn    = 0
0.00.589.224 I llama_init_from_model: freq_base     = 10000.0
0.00.589.225 I llama_init_from_model: freq_scale    = 1
0.00.589.226 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.227 I ggml_metal_init: allocating
0.00.589.249 I ggml_metal_init: found device: Apple M4
0.00.589.259 I ggml_metal_init: picking default device: Apple M4
0.00.590.713 I ggml_metal_init: using embedded metal library
0.00.597.020 I ggml_metal_init: GPU name:   Apple M4
0.00.597.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.026 I ggml_metal_init: simdgroup reduction   = true
0.00.597.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.026 I ggml_metal_init: has residency sets    = true
0.00.597.027 I ggml_metal_init: has bfloat            = true
0.00.597.027 I ggml_metal_init: use bfloat            = true
0.00.597.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.029 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.916 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.402 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.407 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.455 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.694 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.696 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.696 I llama_init_from_model: graph nodes  = 967
0.00.621.697 I llama_init_from_model: graph splits = 2
0.00.621.699 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.699 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.187 I 
0.00.653.251 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.267 I perplexity: tokenizing the input ..
0.00.659.240 I perplexity: tokenization took 5.972 ms
0.00.659.256 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.576 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.995 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.797.010 I llama_perf_context_print:        load time =     643.27 ms
0.00.797.011 I llama_perf_context_print: prompt eval time =     136.08 ms /   128 tokens (    1.06 ms per token,   940.63 tokens per second)
0.00.797.012 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.012 I llama_perf_context_print:       total time =     143.82 ms /   129 tokens
0.00.797.357 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.077s
sys	0m0.138s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.695 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.736 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.741 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.742 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.743 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.743 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.744 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.744 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.746 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.747 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.747 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.748 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.748 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.751 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.751 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.751 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.588 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.631 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.457 I llama_model_loader: - type  f32:  194 tensors
0.00.025.457 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.458 I print_info: file format = GGUF V3 (latest)
0.00.025.458 I print_info: file type   = Q6_K
0.00.025.459 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.298 I load: special tokens cache size = 25
0.00.039.222 I load: token to piece cache size = 0.2984 MB
0.00.039.225 I print_info: arch             = gptneox
0.00.039.225 I print_info: vocab_only       = 0
0.00.039.225 I print_info: n_ctx_train      = 2048
0.00.039.225 I print_info: n_embd           = 2048
0.00.039.226 I print_info: n_layer          = 24
0.00.039.229 I print_info: n_head           = 16
0.00.039.230 I print_info: n_head_kv        = 16
0.00.039.230 I print_info: n_rot            = 32
0.00.039.230 I print_info: n_swa            = 0
0.00.039.230 I print_info: n_embd_head_k    = 128
0.00.039.230 I print_info: n_embd_head_v    = 128
0.00.039.231 I print_info: n_gqa            = 1
0.00.039.232 I print_info: n_embd_k_gqa     = 2048
0.00.039.233 I print_info: n_embd_v_gqa     = 2048
0.00.039.235 I print_info: f_norm_eps       = 1.0e-05
0.00.039.235 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.235 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.235 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.237 I print_info: f_logit_scale    = 0.0e+00
0.00.039.238 I print_info: n_ff             = 8192
0.00.039.238 I print_info: n_expert         = 0
0.00.039.238 I print_info: n_expert_used    = 0
0.00.039.239 I print_info: causal attn      = 1
0.00.039.239 I print_info: pooling type     = 0
0.00.039.239 I print_info: rope type        = 2
0.00.039.241 I print_info: rope scaling     = linear
0.00.039.241 I print_info: freq_base_train  = 10000.0
0.00.039.241 I print_info: freq_scale_train = 1
0.00.039.241 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.242 I print_info: rope_finetuned   = unknown
0.00.039.242 I print_info: ssm_d_conv       = 0
0.00.039.242 I print_info: ssm_d_inner      = 0
0.00.039.242 I print_info: ssm_d_state      = 0
0.00.039.242 I print_info: ssm_dt_rank      = 0
0.00.039.242 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.243 I print_info: model type       = 1.4B
0.00.039.245 I print_info: model params     = 1.41 B
0.00.039.245 I print_info: general.name     = 1.4B
0.00.039.246 I print_info: vocab type       = BPE
0.00.039.246 I print_info: n_vocab          = 50304
0.00.039.246 I print_info: n_merges         = 50009
0.00.039.250 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.250 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.250 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.251 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.251 I print_info: LF token         = 187 ''
0.00.039.251 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.251 I print_info: max token length = 1024
0.00.039.252 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.967 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.971 I load_tensors: offloading output layer to GPU
0.00.632.973 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.996 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.632.998 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.634.428 I llama_init_from_model: n_seq_max     = 1
0.00.634.429 I llama_init_from_model: n_ctx         = 2048
0.00.634.430 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.634.430 I llama_init_from_model: n_batch       = 2048
0.00.634.431 I llama_init_from_model: n_ubatch      = 512
0.00.634.431 I llama_init_from_model: flash_attn    = 0
0.00.634.432 I llama_init_from_model: freq_base     = 10000.0
0.00.634.433 I llama_init_from_model: freq_scale    = 1
0.00.634.434 I ggml_metal_init: allocating
0.00.634.450 I ggml_metal_init: found device: Apple M4
0.00.634.460 I ggml_metal_init: picking default device: Apple M4
0.00.635.827 I ggml_metal_init: using embedded metal library
0.00.641.588 I ggml_metal_init: GPU name:   Apple M4
0.00.641.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.593 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.593 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.594 I ggml_metal_init: simdgroup reduction   = true
0.00.641.594 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.594 I ggml_metal_init: has residency sets    = true
0.00.641.594 I ggml_metal_init: has bfloat            = true
0.00.641.595 I ggml_metal_init: use bfloat            = true
0.00.641.596 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.597 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.167 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.263 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.707.269 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.707.306 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.711.324 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.711.326 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.711.327 I llama_init_from_model: graph nodes  = 967
0.00.711.327 I llama_init_from_model: graph splits = 2
0.00.711.332 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.711.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.711.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.049 I main: llama threadpool init, n_threads = 4
0.00.779.094 I 
0.00.779.115 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.118 I 
0.00.779.265 I sampler seed: 1234
0.00.779.270 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.290 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.290 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.290 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.665.152 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.665.153 I llama_perf_context_print:        load time =     769.55 ms
0.01.665.154 I llama_perf_context_print: prompt eval time =      57.55 ms /     7 tokens (    8.22 ms per token,   121.63 tokens per second)
0.01.665.154 I llama_perf_context_print:        eval time =     825.41 ms /    63 runs   (   13.10 ms per token,    76.33 tokens per second)
0.01.665.155 I llama_perf_context_print:       total time =     886.91 ms /    70 tokens
0.01.665.414 I ggml_metal_free: deallocating

real	0m1.682s
user	0m0.107s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4818 (dfd6b2c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.161 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.094 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.099 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.104 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.108 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.110 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.904 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.740 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.742 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.742 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.743 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.744 I llama_model_loader: - type  f32:  194 tensors
0.00.024.744 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.745 I print_info: file format = GGUF V3 (latest)
0.00.024.746 I print_info: file type   = Q6_K
0.00.024.746 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.757 I load: special tokens cache size = 25
0.00.038.600 I load: token to piece cache size = 0.2984 MB
0.00.038.604 I print_info: arch             = gptneox
0.00.038.604 I print_info: vocab_only       = 0
0.00.038.605 I print_info: n_ctx_train      = 2048
0.00.038.605 I print_info: n_embd           = 2048
0.00.038.605 I print_info: n_layer          = 24
0.00.038.610 I print_info: n_head           = 16
0.00.038.610 I print_info: n_head_kv        = 16
0.00.038.611 I print_info: n_rot            = 32
0.00.038.611 I print_info: n_swa            = 0
0.00.038.611 I print_info: n_embd_head_k    = 128
0.00.038.611 I print_info: n_embd_head_v    = 128
0.00.038.612 I print_info: n_gqa            = 1
0.00.038.613 I print_info: n_embd_k_gqa     = 2048
0.00.038.614 I print_info: n_embd_v_gqa     = 2048
0.00.038.614 I print_info: f_norm_eps       = 1.0e-05
0.00.038.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.615 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.615 I print_info: f_logit_scale    = 0.0e+00
0.00.038.615 I print_info: n_ff             = 8192
0.00.038.616 I print_info: n_expert         = 0
0.00.038.616 I print_info: n_expert_used    = 0
0.00.038.616 I print_info: causal attn      = 1
0.00.038.616 I print_info: pooling type     = 0
0.00.038.616 I print_info: rope type        = 2
0.00.038.616 I print_info: rope scaling     = linear
0.00.038.617 I print_info: freq_base_train  = 10000.0
0.00.038.617 I print_info: freq_scale_train = 1
0.00.038.617 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.617 I print_info: rope_finetuned   = unknown
0.00.038.617 I print_info: ssm_d_conv       = 0
0.00.038.617 I print_info: ssm_d_inner      = 0
0.00.038.618 I print_info: ssm_d_state      = 0
0.00.038.618 I print_info: ssm_dt_rank      = 0
0.00.038.620 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.621 I print_info: model type       = 1.4B
0.00.038.621 I print_info: model params     = 1.41 B
0.00.038.621 I print_info: general.name     = 1.4B
0.00.038.622 I print_info: vocab type       = BPE
0.00.038.622 I print_info: n_vocab          = 50304
0.00.038.622 I print_info: n_merges         = 50009
0.00.038.622 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.622 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.624 I print_info: LF token         = 187 ''
0.00.038.624 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.625 I print_info: max token length = 1024
0.00.038.625 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.181 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.194 I load_tensors: offloading output layer to GPU
0.00.594.195 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.229 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.594.231 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.595.908 I llama_init_from_model: n_seq_max     = 1
0.00.595.910 I llama_init_from_model: n_ctx         = 128
0.00.595.911 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.911 I llama_init_from_model: n_batch       = 128
0.00.595.911 I llama_init_from_model: n_ubatch      = 128
0.00.595.912 I llama_init_from_model: flash_attn    = 0
0.00.595.914 I llama_init_from_model: freq_base     = 10000.0
0.00.595.914 I llama_init_from_model: freq_scale    = 1
0.00.595.915 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.917 I ggml_metal_init: allocating
0.00.595.963 I ggml_metal_init: found device: Apple M4
0.00.595.976 I ggml_metal_init: picking default device: Apple M4
0.00.597.443 I ggml_metal_init: using embedded metal library
0.00.604.033 I ggml_metal_init: GPU name:   Apple M4
0.00.604.037 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.038 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.041 I ggml_metal_init: simdgroup reduction   = true
0.00.604.041 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.041 I ggml_metal_init: has residency sets    = true
0.00.604.042 I ggml_metal_init: has bfloat            = true
0.00.604.042 I ggml_metal_init: use bfloat            = true
0.00.604.043 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.724 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.140 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.624.144 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.191 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.468 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.470 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.470 I llama_init_from_model: graph nodes  = 967
0.00.627.471 I llama_init_from_model: graph splits = 2
0.00.627.473 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.420 I 
0.00.661.501 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.521 I perplexity: tokenizing the input ..
0.00.668.636 I perplexity: tokenization took 7.112 ms
0.00.668.661 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.575 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.801.884 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.801.898 I llama_perf_context_print:        load time =     652.25 ms
0.00.801.899 I llama_perf_context_print: prompt eval time =     131.03 ms /   128 tokens (    1.02 ms per token,   976.88 tokens per second)
0.00.801.900 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.901 I llama_perf_context_print:       total time =     140.49 ms /   129 tokens
0.00.802.291 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.078s
sys	0m0.137s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4818 (dfd6b2c0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129b05100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129b05770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129b05be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129b06050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129b064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129b06930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129b06da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129b07210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129b077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129b07cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129b081c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129b086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129b091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129b09990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129b0a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129b0a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129b0afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129b0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129b0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129b0c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129b0cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129b0d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129b0db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129b0e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129b0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129b0edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129b0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129b10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129b10590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129b10850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129b10cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129b10fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129b11840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129b11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129b12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129b124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129b12980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129b12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129b132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129b13760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129b13c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129b140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129b14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129b149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129b14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129b152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129b158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129b161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129b167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129b16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129b17410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129b17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129b18030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129b18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129b18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129b192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129b19770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129b19a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129b1a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129b1a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129b1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129b1af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129b1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129b1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129b1bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129b1c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129b1c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129b1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129b1cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129b1d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129b1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129b1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129b1e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129b1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129b1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129b1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129b1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129b1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129b20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129b207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129b20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129b21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129b21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129b21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129b22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129b22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129b22cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129b23220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129b23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129b23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129b24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129b24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129b24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129b25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129b25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129b25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129b261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129b15ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129b26660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129b26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129b27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129b278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129b27e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129b28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129b288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129b28df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129b29340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129b29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129b29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129b2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129b2a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129b2add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129b2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129b2b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129b2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129b2c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129b2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129b2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129b2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129b2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129b2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129b2dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129b2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129b2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129b2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129b2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129b2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129b2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129b2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129b301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129b30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129b30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129b30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129b31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129b318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129b31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129b32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129b326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129b32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129b33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129b334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129b33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129b33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129b34280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129b34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129b34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129b35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129b35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129b359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129b35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129b362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129b36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129b36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129b370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129b37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129b37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129b37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129b38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129b387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129b38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129b39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129b395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129b39a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129b39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129b3a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129b3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129b3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129b3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129b3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129b3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129b3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129b3c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129b3c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129b3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129b3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129b3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129b3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129b3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129b3e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129b3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129b3eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129b3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129b3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129b3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129b40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129b404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129b40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129b40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129b412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129b41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129b41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129b42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129b42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129b42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129b42fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129b43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129b43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129b43d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129b44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129b44940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129b44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129b45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129b45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129b45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129b464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129b46ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129b472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129b47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129b47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129b48090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129b48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129b48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129b492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129b49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129b49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129b4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129b4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129b4ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129b4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129b4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129b4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129b4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129b4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129b4cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129b4d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129b4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129b4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129b4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129b4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129b4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129b4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129b4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129b4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129b50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129b507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129b50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129b51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129b517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129b51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129b52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129b527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129b52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129b53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129b53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129b53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129b54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129b54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129b54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129b55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129b55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129b55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129b56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129b56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129b56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129b57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129b57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129b57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129b581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129b58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129b58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129b591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129b59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129b59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129b5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129b5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129b5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129b5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129b5b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129b5bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129b5bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129b5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129b5c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129b5cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129b5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129b5d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129b5db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129b5e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129b5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129b5e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129b5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129b5f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129b5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x129b5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x129b60060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x129b60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x129b609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x129b60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x129b612e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x129b61780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x129b61c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x129b620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x129b62560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129b62ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129b631d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129b638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129b64010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129b64730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129b649f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129b651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129b654a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129b65ab0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.709.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118e08f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118e09400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x118e09870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118e09ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118e0a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x118e0a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118e0aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x118e0aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x118e0b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x118e0b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x118e0bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x118e0c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x118e0cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x118e0d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x118e0dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x118e0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x118e0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118e0f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x118e0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118e101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118e10900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118e11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118e11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118e11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118e12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x118e12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118e12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118e12f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118e133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118e13850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118e13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118e14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118e146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118e14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118e14e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118e15270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118e157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118e15cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x118e161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118e166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118e16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118e170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118e175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118e17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118e17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118e18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x118e188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118e18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118e19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x118e19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118e19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118e1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118e1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118e1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x118e1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x118e1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118e1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118e1c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x118e1c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x118e1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118e1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x118e1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x118e1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x118e1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118e1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x118e1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x118e1ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118e1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118e1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x118e1fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x118e200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118e20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x118e20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x118e21020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x118e21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x118e21ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x118e22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118e22560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x118e22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x118e23000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118e23550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118e23aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118e23ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118e24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118e24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118e24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118e25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118e25a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118e25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118e26520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118e26a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118e26fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118e27510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118e27a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118e27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118e28500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x118e28a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118e28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x118e294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118e29a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118e29f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x118e2a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118e2aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118e2af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x118e2b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118e2ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118e2bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118e2c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118e2ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x118e2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118e2d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118e2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118e2dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x118e2e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118e2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118e2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118e2f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118e2f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118e2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118e2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118e303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118e30840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118e30ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118e31180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118e31620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118e31ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118e31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118e32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118e328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x118e32d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118e331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118e33680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118e33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118e33fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118e34460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118e34900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118e34da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118e35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118e356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118e35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118e36020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118e364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118e36960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118e36e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118e372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118e37740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118e37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118e38080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118e38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118e389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118e38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118e39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118e397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x118e39c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118e3a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118e3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118e3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118e3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x118e3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x118e3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x118e3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118e3c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118e3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x118e3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x118e3cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x118e3d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x118e3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x118e3dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x118e3e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x118e3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118e3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x118e3ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118e3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118e3f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118e3fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118e40200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118e406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x118e40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118e40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118e41480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x118e41920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118e42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118e42700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x118e42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118e43040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118e434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118e43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118e43e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118e442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118e44760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118e44c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118e45150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118e456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118e45bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118e46140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118e46400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118e46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118e47020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118e47630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118e47e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118e482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118e48580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x118e48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x118e491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x118e49990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118e49e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118e4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118e4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118e4af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118e4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118e4b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118e4bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118e4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x118e4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118e4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118e4d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118e4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x118e4def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x118e4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118e4e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118e4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118e4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118e4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118e4fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118e50420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118e50970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118e50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118e51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118e51960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118e51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118e52400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118e52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x118e52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118e533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118e53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x118e53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118e543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118e54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x118e54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118e553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118e55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118e55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x118e563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118e56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x118e56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118e573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x118e57900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118e57e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118e583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x118e588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x118e58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x118e59390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x118e598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x118e59e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118e5a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x118e5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118e5ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118e5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x118e5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118e5be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x118e5c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118e5c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118e5ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118e5d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118e5d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x118e5dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x118e5e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118e5e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118e5eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118e5efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118e5f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118e5f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x118e5fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118e60240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118e606e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118e60b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118e61020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118e614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118e61960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118e61e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x118e622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x118e62740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x118e62be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x118e63080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x118e63520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x118e639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x118e63e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x118e64300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x118e647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x118e64c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118e65190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118e658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x118e65fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118e666f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118e66e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118e670d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x118e678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x118e67b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118e68190 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105f06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105f07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105f078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105f083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105f08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105f09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105f09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x105f0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x105f0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x105f0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x105f0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105f0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105f0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105f0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105f0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105f0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105f0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105f0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105f0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105f0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105f0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105f0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105f0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105f10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105f107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105f10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105f110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105f11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105f119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105f11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105f12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105f12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105f12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105f13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105f138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105f141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105f14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105f14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105f14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105f15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105f157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105f15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105f160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105f16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105f16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105f16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105f17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105f17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105f17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105f18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105f185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105f18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105f18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105f19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105f19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105f1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x105f1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x105f1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105f1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129b46160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129b445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129b65760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129b43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129b44c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129b17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129b176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129b0f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129b15b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129b164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129b16ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129b15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129b14f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129b182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129b170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129b0e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129b08980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129b46ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129b18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129b1a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129b26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129b64cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129b113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129b45210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129b0f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129b0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129b0fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129b65f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129b661d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129b66490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129b66750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129b66a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129b66cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129b66f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129b67250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129b67510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129b677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129b67a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129b67d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129b68010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129b682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129b68590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129b68850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129b68b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129b68dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129b69090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129b69350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129b69610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129b698d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129b69b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129b69e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129b6a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129b6a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129b6a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129b6a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129b6ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129b6aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129b6b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129b6b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129b6b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129b6b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129b6bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129b6bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129b6c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129b6c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129b6c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129b6ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129b6cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129b6cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129b6d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129b6d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129b6d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129b6dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129b6dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129b6e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129b6e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129b6e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129b6e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129b6eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129b6ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129b6f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129b6f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129b6f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129b6f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129b6fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129b6fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129b70150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129b70410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129b706d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129b70990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129b70c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129b70f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129b711d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129b71490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129b71750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129b71a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129b71cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129b71f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129b72250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129b72510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129b727d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129b72a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129b72d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129b73010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129b732d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129b73590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129b73850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129b73b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129b73dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129b74090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129b74350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129b74610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129b748d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129b74b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129b74e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129b75110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129b753d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129b75690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129b75950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129b75c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129b75ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129b76190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129b76450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129b76710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129b769d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129b76c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129b76f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129b77210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129b774d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129b77790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129b77a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129b77d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129b77fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129b78290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129b78550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129b78810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129b78ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129b790a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129b79360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129b79620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129b798e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129b79ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129b79e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129b7a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129b7a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129b7a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129b7a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129b7ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129b7aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129b7b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129b7b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129b7b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129b7b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129b7bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129b7bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129b7c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129b7c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129b7c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129b7ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129b7cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129b7cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129b7d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129b7d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129b7d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129b7dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129b7dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129b7e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129b7e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129b7e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129b7e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129b7eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129b7ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129b7f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129b7f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129b7f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129b7f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129b7fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129b7fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129b80160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129b80420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129b806e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129b809a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129b80c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129b80f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129b811e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129b814a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129b81760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129b81a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129b81ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129b81fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129b82260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129b82520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129b827e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129b82aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129b82d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129b83020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129b832e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129b835a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129b83860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129b83b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129b83de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129b840a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129b84360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129b84620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129b848e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129b84ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129b84e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129b85120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129b853e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x129b856a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x129b85960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x129b85c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x129b85ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x129b861a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x129b86460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x129b86720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x129b869e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x129b86ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x129b86f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129b87220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129b874e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129b877a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129b87a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129b87d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129b87fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129b882a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129b88560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129b88820 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.731s
user	0m0.277s
sys	0m0.307s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4818 (dfd6b2c0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154f107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154f10ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154f114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154f11a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154f12000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154f125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154f12b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x154f13110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154f136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154f13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154f140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154f145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154f150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154f160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154f167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154f16ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x154f17600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x154f17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154f184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154f18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154f19330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154f19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154f1a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154f1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154f1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154f1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154f1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154f1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154f1cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154f1ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154f1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154f1dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154f1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154f1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154f1ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156004230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1560046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156004b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156004f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1560053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156005860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156005cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156006140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1560065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156006a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156007210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156007680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x156007af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156007f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1560083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156008840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156008cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156009120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156009680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156009b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156009ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15600a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15600a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15600ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15600b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15600b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15600ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15600bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15600c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15600c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15600cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15600d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15600d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15600d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15600de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15600e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15600e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15600eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15600efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15600f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15600f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15600fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156010190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156010600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156010e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156011380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156011930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156011ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156012490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156012a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156012ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1560135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156013b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156014100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1560146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156014c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156015210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1560157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156015d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156006ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1560164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156016940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156016db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156017360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156017910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156017ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156018470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156018a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156018fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156019580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156019b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15601a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15601a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15601ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15601b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15601b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15601bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15601c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15601c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15601cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15601d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15601d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15601daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15601dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15601e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15601e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15601eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15601f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15601f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15601fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1560202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1560207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156020ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1560211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1560216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156021ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1560220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1560225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156022aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156022fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1560234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1560239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156023ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1560243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1560248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156024da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1560252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1560257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156025ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1560261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1560266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156026ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1560270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1560275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156027aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156027fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1560284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1560289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156028ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1560293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1560298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156029da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15602a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15602a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15602aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15602b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15602b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15602bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15602c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15602c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15602caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15602cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15602d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15602d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15602dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15602e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15602e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15602eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15602f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15602f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15602fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1560301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1560306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156030ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1560310a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1560315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156031fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1560324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1560329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156032ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1560333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1560338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156033da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1560342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1560347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156034d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156035300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1560358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156035e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156036470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156036a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156037090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156037880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156037d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156037fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1560385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156038c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1560393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156039890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156039d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15603a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15603a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15603aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15603b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15603b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15603bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15603c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15603c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15603ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15603d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15603d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15603dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15603e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15603e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15603ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15603f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15603f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15603fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1560403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156040920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156040e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1560413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156041910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156041e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1560423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156042900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156042e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1560433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1560438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156043e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156044390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1560448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156044e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156045380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1560458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156045e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156046370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1560468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156046e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156047360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1560478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156047e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156048350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1560488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156048df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156049340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156049890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156049de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15604a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15604a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15604add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15604b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15604b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15604bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15604c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15604c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15604cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15604d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15604d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15604dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15604e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15604e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15604ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15604eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15604f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15604f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15604fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156050140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1560505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156050a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156050f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1560513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156051860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x156051d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1560521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x156052640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x156052ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x156052f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x156053420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1560538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x156053d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x156054200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1560546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156054bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156055310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156055a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156056150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156056870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156056b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156057320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1560575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156057bf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.752 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144d04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144d05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144d054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144d05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144d05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144d06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144d06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144d06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144d06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144d073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144d07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144d07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144d08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144d091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144d09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144d0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144d0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144d0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144d0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144d0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144d0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144d0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144d0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144d0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144d0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144d0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144d0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144d0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144d0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144d0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144d0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144d0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144d10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144d104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144d10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144d10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144d11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144d116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144d11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144d11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144d12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144d12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144d12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144d13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144d135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144d13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144d13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144d14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144d14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144d14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144d15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144d154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144d15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144d15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144d16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144d16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144d16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144d17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144d17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144d179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144d17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144d182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144d18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144d18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144d19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144d19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144d198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144d19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144d1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144d1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144d1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144d1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144d1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144d1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144d1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144d1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144d1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144d1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144d1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144d1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144d1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144d1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144d1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144d1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144d1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144d1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144d1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144d1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144d1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144d1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144d20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144d207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144d20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144d210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144d21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144d219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144d21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144d22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144d226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144d22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x144d22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144d23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144d238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144d23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144d24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144d24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144d24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144d24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144d25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144d257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144d25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144d260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144d26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144d26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144d26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144d27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144d276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144d27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144d27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144d28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144d28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144d28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144d29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144d295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144d29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144d29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144d2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144d2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144d2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144d2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144d2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144d2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144d2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144d2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144d2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144d2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144d2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144d2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144d2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144d2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144d2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144d2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144d2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144d2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144d2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144d2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144d2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144d30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144d304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144d30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144d30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144d31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144d31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144d31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144d31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144d323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144d32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144d32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144d33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144d335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144d33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144d33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144d342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144d34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144d34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144d35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144d35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144d35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144d361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144d36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144d36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144d36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144d373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144d37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144d37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144d38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144d38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144d389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144d38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144d392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144d39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144d39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144d3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144d3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144d3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144d3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144d3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144d3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144d3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144d3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144d3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144d3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144d3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144d3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144d3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144d3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144d3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144d3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144d3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144d3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144d3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144d3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144d3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144d3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144d40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144d407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144d40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144d41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144d415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144d41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144d42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144d428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144d42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144d43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144d43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144d43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144d445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144d44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144d45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144d456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144d45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144d46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144d46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144d46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144d473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144d47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144d47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144d484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144d48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144d49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144d49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144d49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144d4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144d4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144d4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144d4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144d4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144d4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144d4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144d4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144d4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144d4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144d4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144d4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144d4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144d4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144d4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144d4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144d4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144d50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144d50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144d50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144d514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144d51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144d52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144d525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x144d52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144d53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144d53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144d53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144d542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144d54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144d54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144d553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144d559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144d55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144d56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144d56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144d56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144d574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144d579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144d57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144d583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144d588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144d58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144d592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144d597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144d59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144d5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144d5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144d5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144d5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x144d5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x144d5baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x144d5bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x144d5c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x144d5c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x144d5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x144d5d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x144d5d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x144d5ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x144d5e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144d5e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144d5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144d5f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144d60040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144d60760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144d60a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144d61210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144d614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144d61ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154f19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154f1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154f1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154f1f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154f1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154f1fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154f201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x154f20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154f20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154f20ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154f21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154f21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154f22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154f22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154f23500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154f23c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154f24340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x154f24a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x154f25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154f25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154f26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154f26790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154f26eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154f275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154f27cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154f27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154f28270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154f286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154f28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154f28fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154f29430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154f29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154f29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154f2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154f2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154f2a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154f2ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154f2b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154f2b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154f2bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154f2bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154f2c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154f2c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154f2ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154f2d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154f2d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154f2da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154f2deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154f2e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x154f2e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x154f2ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x154f2f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x154f2f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x154f2f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x154f2fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154f30230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x154f307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x154f30ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x154f31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x154f31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154f319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x154f31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x154f322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154f32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x154f32bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154f33020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154f33490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154f33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154f33d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154f341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x154f34650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154f34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154f34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154f353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x154f35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154f35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154f360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x154f36560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154f369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154f36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x154f372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154f37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154f37b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x154f38000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154f38470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154f388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154f38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x154f391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154f39630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x154f39aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x154f39f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154f3a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154f3a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154f3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154f3b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154f3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154f3b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154f3be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154f3c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x154f3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154f3d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x154f3d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x154f3dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154f3e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154f3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x154f3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x154f3f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154f3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x154f3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154f40480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x154f40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x154f41590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x154f41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154f42040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154f42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154f42a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x154f42f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154f43440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154f43940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154f43e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154f44340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154f44840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154f44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154f45240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154f45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154f45c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154f46140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154f46640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154f46b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154f47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154f47540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154f47a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154f47f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154f48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154f48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154f49340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154f49840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154f49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154f4a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154f4a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154f4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154f4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154f4b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154f4bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154f4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154f4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154f4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154f4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154f4d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154f4d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x154f4de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x154f4e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154f4e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154f4ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x154f4f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x154f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x154f4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x154f50140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x154f50640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x154f50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x154f51040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x154f51540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x154f51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x154f51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x154f52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x154f52940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x154f52e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154f53340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154f53840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154f53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154f54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154f54740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154f54c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154f55140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154f55640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154f55b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154f56040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154f56540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154f5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154f5b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154f5bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154f5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154f5c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154f5ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154f5d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x154f5dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154f5e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154f5e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x154f5e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154f5efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154f5f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x154f5fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154f600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154f60570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154f60d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x154f61270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154f617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154f61d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154f62260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154f627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154f62d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154f63250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154f637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154f63cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154f64240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154f64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154f64ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154f65230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154f65780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154f65cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154f66220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154f66770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154f66cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154f67210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154f67760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154f67cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154f68200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154f68750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154f68ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154f691f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154f69740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154f69c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154f6a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154f6a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154f6ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154f6b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154f6b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154f6bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154f6c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154f6c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154f6cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154f6d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154f6d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154f6dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154f6e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154f6e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154f6ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154f6f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154f6f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x154f6fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x154f70180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154f706d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154f70c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154f71170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154f716c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154f71c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154f72160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154f726b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154f72c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154f73150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154f736a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154f73b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154f73fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154f74480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154f74920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154f74dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154f75260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154f75700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154f75ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154f76040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154f764e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154f76980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154f76e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154f772c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154f77760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154f77c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x154f780a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x154f78540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x154f789e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x154f78e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x154f79320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x154f797c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x154f79c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x154f7a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x154f7a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x154f7aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154f7af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154f7b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154f7bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154f7c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154f7cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154f7ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x154f7d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x154f7d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154f7df90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.957s
user	0m0.231s
sys	0m0.189s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.38 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.81 sec*proc (2 tests)

Total Test time (real) =   1.83 sec
        1.85 real         0.50 user         0.23 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.13 user         0.08 sys
```
