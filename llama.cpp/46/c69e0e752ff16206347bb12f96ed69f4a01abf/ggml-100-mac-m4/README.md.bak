### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.38 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.75 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.21 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.28 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.14 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.22 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.24 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.20 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  173.86 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.91 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.94 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.34 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.21 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 216.26 sec*proc (27 tests)

Total Test time (real) = 216.27 sec

real	3m36.285s
user	7m25.364s
sys	0m5.549s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.95 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.18 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   28.08 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.28 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.02 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.11 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  49.75 sec*proc (27 tests)

Total Test time (real) =  49.76 sec

real	0m49.767s
user	1m9.530s
sys	0m4.869s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.134 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.315 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.451 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.460 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.462 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.462 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.463 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.464 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.468 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.469 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.469 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.470 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.474 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.474 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.475 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.476 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.476 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.477 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.477 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.507 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.031.728 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.730 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.031.731 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.031.731 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.031.732 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.031.732 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.031.733 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.031.733 I llama_model_loader: - type  f32:  124 tensors
0.00.031.734 I llama_model_loader: - type  f16:   73 tensors
0.00.036.047 I llm_load_vocab: special tokens cache size = 5
0.00.038.269 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.038.272 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.038.273 I llm_load_print_meta: arch             = bert
0.00.038.273 I llm_load_print_meta: vocab type       = WPM
0.00.038.274 I llm_load_print_meta: n_vocab          = 30522
0.00.038.274 I llm_load_print_meta: n_merges         = 0
0.00.038.274 I llm_load_print_meta: vocab_only       = 0
0.00.038.274 I llm_load_print_meta: n_ctx_train      = 512
0.00.038.275 I llm_load_print_meta: n_embd           = 384
0.00.038.275 I llm_load_print_meta: n_layer          = 12
0.00.038.278 I llm_load_print_meta: n_head           = 12
0.00.038.279 I llm_load_print_meta: n_head_kv        = 12
0.00.038.279 I llm_load_print_meta: n_rot            = 32
0.00.038.280 I llm_load_print_meta: n_swa            = 0
0.00.038.280 I llm_load_print_meta: n_embd_head_k    = 32
0.00.038.280 I llm_load_print_meta: n_embd_head_v    = 32
0.00.038.281 I llm_load_print_meta: n_gqa            = 1
0.00.038.282 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.038.283 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.038.284 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.038.284 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.038.284 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.038.285 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.038.285 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.038.286 I llm_load_print_meta: n_ff             = 1536
0.00.038.286 I llm_load_print_meta: n_expert         = 0
0.00.038.287 I llm_load_print_meta: n_expert_used    = 0
0.00.038.287 I llm_load_print_meta: causal attn      = 0
0.00.038.287 I llm_load_print_meta: pooling type     = 2
0.00.038.287 I llm_load_print_meta: rope type        = 2
0.00.038.288 I llm_load_print_meta: rope scaling     = linear
0.00.038.288 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.038.289 I llm_load_print_meta: freq_scale_train = 1
0.00.038.289 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.038.289 I llm_load_print_meta: rope_finetuned   = unknown
0.00.038.289 I llm_load_print_meta: ssm_d_conv       = 0
0.00.038.290 I llm_load_print_meta: ssm_d_inner      = 0
0.00.038.290 I llm_load_print_meta: ssm_d_state      = 0
0.00.038.290 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.038.290 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.038.304 I llm_load_print_meta: model type       = 33M
0.00.038.305 I llm_load_print_meta: model ftype      = F16
0.00.038.306 I llm_load_print_meta: model params     = 33.21 M
0.00.038.306 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.038.307 I llm_load_print_meta: general.name     = Bge Small
0.00.038.307 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.038.308 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.038.308 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.038.308 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.038.309 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.038.309 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.038.309 I llm_load_print_meta: max token length = 21
0.00.040.483 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.040.484 I llm_load_tensors: offloading output layer to GPU
0.00.040.485 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.040.511 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.513 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.094 I llama_new_context_with_model: n_seq_max     = 1
0.00.041.096 I llama_new_context_with_model: n_ctx         = 512
0.00.041.096 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.041.096 I llama_new_context_with_model: n_batch       = 2048
0.00.041.097 I llama_new_context_with_model: n_ubatch      = 2048
0.00.041.097 I llama_new_context_with_model: flash_attn    = 0
0.00.041.098 I llama_new_context_with_model: freq_base     = 10000.0
0.00.041.098 I llama_new_context_with_model: freq_scale    = 1
0.00.041.099 I ggml_metal_init: allocating
0.00.041.103 I ggml_metal_init: found device: Apple M4
0.00.041.109 I ggml_metal_init: picking default device: Apple M4
0.00.041.955 I ggml_metal_init: using embedded metal library
0.00.045.627 I ggml_metal_init: GPU name:   Apple M4
0.00.045.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.630 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.631 I ggml_metal_init: simdgroup reduction   = true
0.00.045.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.631 I ggml_metal_init: has bfloat            = true
0.00.045.631 I ggml_metal_init: use bfloat            = true
0.00.045.632 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.632 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.056.714 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.056.716 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.056.717 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.057.519 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.057.521 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.057.521 I llama_new_context_with_model: graph nodes  = 429
0.00.057.521 I llama_new_context_with_model: graph splits = 2
0.00.057.543 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.063.966 I 
0.00.063.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.064.675 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.069.887 I llama_perf_context_print:        load time =      42.64 ms
0.00.069.888 I llama_perf_context_print: prompt eval time =       5.06 ms /     9 tokens (    0.56 ms per token,  1779.01 tokens per second)
0.00.069.889 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.069.889 I llama_perf_context_print:       total time =       5.92 ms /    10 tokens
0.00.070.026 I ggml_metal_free: deallocating

real	0m0.263s
user	0m0.050s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.040 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.689 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.760 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.763 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.765 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.767 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.767 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.768 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.769 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.769 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.769 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.770 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.770 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.772 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.772 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.772 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.773 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.773 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.773 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.774 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.269 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.936 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.937 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.937 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.937 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.938 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.938 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.938 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.939 I llama_model_loader: - type  f32:  124 tensors
0.00.014.939 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.398 I llm_load_vocab: special tokens cache size = 5
0.00.018.655 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.657 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.657 I llm_load_print_meta: arch             = bert
0.00.018.658 I llm_load_print_meta: vocab type       = WPM
0.00.018.658 I llm_load_print_meta: n_vocab          = 30522
0.00.018.658 I llm_load_print_meta: n_merges         = 0
0.00.018.658 I llm_load_print_meta: vocab_only       = 0
0.00.018.658 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.659 I llm_load_print_meta: n_embd           = 384
0.00.018.659 I llm_load_print_meta: n_layer          = 12
0.00.018.661 I llm_load_print_meta: n_head           = 12
0.00.018.661 I llm_load_print_meta: n_head_kv        = 12
0.00.018.663 I llm_load_print_meta: n_rot            = 32
0.00.018.663 I llm_load_print_meta: n_swa            = 0
0.00.018.663 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.663 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.664 I llm_load_print_meta: n_gqa            = 1
0.00.018.664 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.665 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.665 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.665 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.666 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.666 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.666 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.666 I llm_load_print_meta: n_ff             = 1536
0.00.018.667 I llm_load_print_meta: n_expert         = 0
0.00.018.667 I llm_load_print_meta: n_expert_used    = 0
0.00.018.667 I llm_load_print_meta: causal attn      = 0
0.00.018.667 I llm_load_print_meta: pooling type     = 2
0.00.018.667 I llm_load_print_meta: rope type        = 2
0.00.018.667 I llm_load_print_meta: rope scaling     = linear
0.00.018.668 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.668 I llm_load_print_meta: freq_scale_train = 1
0.00.018.668 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.668 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.668 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.669 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.669 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.669 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.669 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.675 I llm_load_print_meta: model type       = 33M
0.00.018.675 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.676 I llm_load_print_meta: model params     = 33.21 M
0.00.018.676 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.676 I llm_load_print_meta: general.name     = Bge Small
0.00.018.677 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.677 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.677 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.678 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.678 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.679 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.680 I llm_load_print_meta: max token length = 21
0.00.019.988 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.988 I llm_load_tensors: offloading output layer to GPU
0.00.019.988 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.995 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.996 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.342 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.343 I llama_new_context_with_model: n_ctx         = 512
0.00.020.343 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.343 I llama_new_context_with_model: n_batch       = 2048
0.00.020.343 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.344 I llama_new_context_with_model: flash_attn    = 0
0.00.020.344 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.344 I llama_new_context_with_model: freq_scale    = 1
0.00.020.345 I ggml_metal_init: allocating
0.00.020.348 I ggml_metal_init: found device: Apple M4
0.00.020.350 I ggml_metal_init: picking default device: Apple M4
0.00.020.835 I ggml_metal_init: using embedded metal library
0.00.022.855 I ggml_metal_init: GPU name:   Apple M4
0.00.022.857 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.857 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.858 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.858 I ggml_metal_init: simdgroup reduction   = true
0.00.022.858 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.858 I ggml_metal_init: has bfloat            = true
0.00.022.858 I ggml_metal_init: use bfloat            = true
0.00.022.859 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.859 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.868 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.031.871 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.031.872 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.498 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.500 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.500 I llama_new_context_with_model: graph nodes  = 429
0.00.032.500 I llama_new_context_with_model: graph splits = 2
0.00.032.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.036.935 I 
0.00.036.954 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.037.493 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.010 I llama_perf_context_print:        load time =      27.24 ms
0.00.042.013 I llama_perf_context_print: prompt eval time =       4.36 ms /     9 tokens (    0.48 ms per token,  2066.59 tokens per second)
0.00.042.014 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.014 I llama_perf_context_print:       total time =       5.08 ms /    10 tokens
0.00.042.189 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.181 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.790 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.611 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.615 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.618 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.030.618 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.626 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.030.627 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.030.628 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.030.629 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.030.630 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.030.630 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.030.631 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.030.631 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.030.634 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.635 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.636 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.030.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.637 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.038.166 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.040.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.699 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.044.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.044.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.044.702 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.044.702 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.044.702 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.044.703 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.044.703 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.044.703 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.044.704 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.044.704 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.044.705 I llama_model_loader: - type  f32:   41 tensors
0.00.044.705 I llama_model_loader: - type  f16:   29 tensors
0.00.062.258 W llm_load_vocab: empty token at index 5
0.00.066.762 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.068.054 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.068.078 I llm_load_vocab: special tokens cache size = 5
0.00.312.000 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.312.004 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.312.005 I llm_load_print_meta: arch             = jina-bert-v2
0.00.312.005 I llm_load_print_meta: vocab type       = BPE
0.00.312.006 I llm_load_print_meta: n_vocab          = 61056
0.00.312.006 I llm_load_print_meta: n_merges         = 39382
0.00.312.006 I llm_load_print_meta: vocab_only       = 0
0.00.312.006 I llm_load_print_meta: n_ctx_train      = 8192
0.00.312.006 I llm_load_print_meta: n_embd           = 384
0.00.312.007 I llm_load_print_meta: n_layer          = 4
0.00.312.011 I llm_load_print_meta: n_head           = 12
0.00.312.012 I llm_load_print_meta: n_head_kv        = 12
0.00.312.012 I llm_load_print_meta: n_rot            = 32
0.00.312.013 I llm_load_print_meta: n_swa            = 0
0.00.312.013 I llm_load_print_meta: n_embd_head_k    = 32
0.00.312.013 I llm_load_print_meta: n_embd_head_v    = 32
0.00.312.013 I llm_load_print_meta: n_gqa            = 1
0.00.312.014 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.312.015 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.312.015 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.312.016 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.312.016 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.312.016 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.312.017 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.312.017 I llm_load_print_meta: n_ff             = 1536
0.00.312.017 I llm_load_print_meta: n_expert         = 0
0.00.312.017 I llm_load_print_meta: n_expert_used    = 0
0.00.312.018 I llm_load_print_meta: causal attn      = 0
0.00.312.018 I llm_load_print_meta: pooling type     = -1
0.00.312.018 I llm_load_print_meta: rope type        = -1
0.00.312.018 I llm_load_print_meta: rope scaling     = linear
0.00.312.019 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.312.019 I llm_load_print_meta: freq_scale_train = 1
0.00.312.019 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.312.020 I llm_load_print_meta: rope_finetuned   = unknown
0.00.312.020 I llm_load_print_meta: ssm_d_conv       = 0
0.00.312.020 I llm_load_print_meta: ssm_d_inner      = 0
0.00.312.020 I llm_load_print_meta: ssm_d_state      = 0
0.00.312.020 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.312.021 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.312.043 I llm_load_print_meta: model type       = 33M
0.00.312.044 I llm_load_print_meta: model ftype      = F16
0.00.312.044 I llm_load_print_meta: model params     = 32.90 M
0.00.312.044 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.312.045 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.312.045 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.312.045 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.312.045 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.312.045 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.312.046 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.312.046 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.312.046 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.312.046 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.312.046 I llm_load_print_meta: max token length = 45
0.00.312.900 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.312.900 I llm_load_tensors: offloading output layer to GPU
0.00.312.900 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.312.922 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.312.923 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.313.611 I llama_new_context_with_model: n_seq_max     = 1
0.00.313.612 I llama_new_context_with_model: n_ctx         = 8192
0.00.313.612 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.313.612 I llama_new_context_with_model: n_batch       = 2048
0.00.313.613 I llama_new_context_with_model: n_ubatch      = 2048
0.00.313.613 I llama_new_context_with_model: flash_attn    = 0
0.00.313.613 I llama_new_context_with_model: freq_base     = 10000.0
0.00.313.613 I llama_new_context_with_model: freq_scale    = 1
0.00.313.614 I ggml_metal_init: allocating
0.00.313.617 I ggml_metal_init: found device: Apple M4
0.00.313.619 I ggml_metal_init: picking default device: Apple M4
0.00.314.333 I ggml_metal_init: using embedded metal library
0.00.317.111 I ggml_metal_init: GPU name:   Apple M4
0.00.317.113 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.317.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.317.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.317.114 I ggml_metal_init: simdgroup reduction   = true
0.00.317.114 I ggml_metal_init: simdgroup matrix mul. = true
0.00.317.114 I ggml_metal_init: has bfloat            = true
0.00.317.114 I ggml_metal_init: use bfloat            = true
0.00.317.115 I ggml_metal_init: hasUnifiedMemory      = true
0.00.317.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.327.431 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.327.435 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.327.436 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.328.017 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.328.018 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.328.018 I llama_new_context_with_model: graph nodes  = 154
0.00.328.018 I llama_new_context_with_model: graph splits = 2
0.00.328.036 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.339.967 I 
0.00.339.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.340.138 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.340.139 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.340.143 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.340.143 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.340.148 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.340.148 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.340.648 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.344.473 I llama_perf_context_print:        load time =     319.17 ms
0.00.344.474 I llama_perf_context_print: prompt eval time =       3.82 ms /    62 tokens (    0.06 ms per token, 16247.38 tokens per second)
0.00.344.475 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.344.475 I llama_perf_context_print:       total time =       4.51 ms /    63 tokens
0.00.344.640 I ggml_metal_free: deallocating

real	0m1.038s
user	0m0.321s
sys	0m0.043s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.170 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.336 I main: llama backend init
0.00.000.383 I main: load the model and apply lora adapter, if any
0.00.034.091 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.046.771 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.790 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.791 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.794 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.795 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.797 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.055.804 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.065.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.065.199 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.065.200 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.065.200 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.065.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.065.201 I llama_model_loader: - type  f32:  194 tensors
0.00.065.202 I llama_model_loader: - type  f16:   98 tensors
0.00.094.306 I llm_load_vocab: special tokens cache size = 25
0.00.101.168 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.101.171 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.101.172 I llm_load_print_meta: arch             = gptneox
0.00.101.172 I llm_load_print_meta: vocab type       = BPE
0.00.101.172 I llm_load_print_meta: n_vocab          = 50304
0.00.101.172 I llm_load_print_meta: n_merges         = 50009
0.00.101.172 I llm_load_print_meta: vocab_only       = 0
0.00.101.173 I llm_load_print_meta: n_ctx_train      = 2048
0.00.101.173 I llm_load_print_meta: n_embd           = 2048
0.00.101.173 I llm_load_print_meta: n_layer          = 24
0.00.101.175 I llm_load_print_meta: n_head           = 16
0.00.101.176 I llm_load_print_meta: n_head_kv        = 16
0.00.101.176 I llm_load_print_meta: n_rot            = 32
0.00.101.176 I llm_load_print_meta: n_swa            = 0
0.00.101.177 I llm_load_print_meta: n_embd_head_k    = 128
0.00.101.177 I llm_load_print_meta: n_embd_head_v    = 128
0.00.101.177 I llm_load_print_meta: n_gqa            = 1
0.00.101.178 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.101.179 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.101.179 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.101.180 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.101.182 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.101.182 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.101.183 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.101.183 I llm_load_print_meta: n_ff             = 8192
0.00.101.183 I llm_load_print_meta: n_expert         = 0
0.00.101.185 I llm_load_print_meta: n_expert_used    = 0
0.00.101.185 I llm_load_print_meta: causal attn      = 1
0.00.101.185 I llm_load_print_meta: pooling type     = 0
0.00.101.185 I llm_load_print_meta: rope type        = 2
0.00.101.185 I llm_load_print_meta: rope scaling     = linear
0.00.101.186 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.101.186 I llm_load_print_meta: freq_scale_train = 1
0.00.101.186 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.101.187 I llm_load_print_meta: rope_finetuned   = unknown
0.00.101.187 I llm_load_print_meta: ssm_d_conv       = 0
0.00.101.187 I llm_load_print_meta: ssm_d_inner      = 0
0.00.101.187 I llm_load_print_meta: ssm_d_state      = 0
0.00.101.187 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.101.187 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.101.199 I llm_load_print_meta: model type       = 1.4B
0.00.101.200 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.101.200 I llm_load_print_meta: model params     = 1.41 B
0.00.101.201 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.101.201 I llm_load_print_meta: general.name     = 1.4B
0.00.101.201 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.101.201 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.101.201 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.101.201 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.101.202 I llm_load_print_meta: LF token         = 128 ''
0.00.101.202 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.101.202 I llm_load_print_meta: max token length = 1024
0.00.103.651 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.103.651 I llm_load_tensors: offloading output layer to GPU
0.00.103.651 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.103.668 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.103.669 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.104.573 I llama_new_context_with_model: n_seq_max     = 1
0.00.104.574 I llama_new_context_with_model: n_ctx         = 2048
0.00.104.574 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.104.574 I llama_new_context_with_model: n_batch       = 2048
0.00.104.574 I llama_new_context_with_model: n_ubatch      = 512
0.00.104.574 I llama_new_context_with_model: flash_attn    = 0
0.00.104.575 I llama_new_context_with_model: freq_base     = 10000.0
0.00.104.575 I llama_new_context_with_model: freq_scale    = 1
0.00.104.575 I ggml_metal_init: allocating
0.00.104.578 I ggml_metal_init: found device: Apple M4
0.00.104.580 I ggml_metal_init: picking default device: Apple M4
0.00.105.193 I ggml_metal_init: using embedded metal library
0.00.113.731 I ggml_metal_init: GPU name:   Apple M4
0.00.113.732 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.733 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.733 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.733 I ggml_metal_init: simdgroup reduction   = true
0.00.113.733 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.734 I ggml_metal_init: has bfloat            = true
0.00.113.734 I ggml_metal_init: use bfloat            = true
0.00.113.734 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.735 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.147.120 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.147.125 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.147.141 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.148.028 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.148.029 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.148.029 I llama_new_context_with_model: graph nodes  = 967
0.00.148.029 I llama_new_context_with_model: graph splits = 2
0.00.148.068 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.221.552 I main: llama threadpool init, n_threads = 4
0.00.221.584 I 
0.00.221.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.221.604 I 
0.00.221.681 I sampler seed: 1234
0.00.221.686 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.221.708 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.221.710 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.221.710 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.084.814 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.02.084.815 I llama_perf_context_print:        load time =     187.45 ms
0.02.084.815 I llama_perf_context_print: prompt eval time =      37.51 ms /     7 tokens (    5.36 ms per token,   186.62 tokens per second)
0.02.084.816 I llama_perf_context_print:        eval time =    1822.53 ms /    63 runs   (   28.93 ms per token,    34.57 tokens per second)
0.02.084.817 I llama_perf_context_print:       total time =    1863.26 ms /    70 tokens
0.02.085.000 I ggml_metal_free: deallocating

real	0m2.379s
user	0m0.144s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.572 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.027.018 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.088 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.113 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.424 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.057.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.026 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.027 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.027 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.028 I llama_model_loader: - type  f32:  194 tensors
0.00.057.028 I llama_model_loader: - type  f16:   98 tensors
0.00.085.139 I llm_load_vocab: special tokens cache size = 25
0.00.091.615 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.618 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.618 I llm_load_print_meta: arch             = gptneox
0.00.091.618 I llm_load_print_meta: vocab type       = BPE
0.00.091.619 I llm_load_print_meta: n_vocab          = 50304
0.00.091.619 I llm_load_print_meta: n_merges         = 50009
0.00.091.619 I llm_load_print_meta: vocab_only       = 0
0.00.091.619 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.619 I llm_load_print_meta: n_embd           = 2048
0.00.091.619 I llm_load_print_meta: n_layer          = 24
0.00.091.623 I llm_load_print_meta: n_head           = 16
0.00.091.623 I llm_load_print_meta: n_head_kv        = 16
0.00.091.623 I llm_load_print_meta: n_rot            = 32
0.00.091.623 I llm_load_print_meta: n_swa            = 0
0.00.091.624 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.624 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.624 I llm_load_print_meta: n_gqa            = 1
0.00.091.625 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.625 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.626 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.626 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.626 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.626 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.627 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.629 I llm_load_print_meta: n_ff             = 8192
0.00.091.630 I llm_load_print_meta: n_expert         = 0
0.00.091.630 I llm_load_print_meta: n_expert_used    = 0
0.00.091.630 I llm_load_print_meta: causal attn      = 1
0.00.091.630 I llm_load_print_meta: pooling type     = 0
0.00.091.630 I llm_load_print_meta: rope type        = 2
0.00.091.630 I llm_load_print_meta: rope scaling     = linear
0.00.091.631 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.631 I llm_load_print_meta: freq_scale_train = 1
0.00.091.631 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.632 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.633 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.633 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.633 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.633 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.633 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.645 I llm_load_print_meta: model type       = 1.4B
0.00.091.645 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.646 I llm_load_print_meta: model params     = 1.41 B
0.00.091.646 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.646 I llm_load_print_meta: general.name     = 1.4B
0.00.091.647 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.647 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.647 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.647 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.647 I llm_load_print_meta: LF token         = 128 ''
0.00.091.648 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.648 I llm_load_print_meta: max token length = 1024
0.00.094.128 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.128 I llm_load_tensors: offloading output layer to GPU
0.00.094.129 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.139 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.141 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.131 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.132 I llama_new_context_with_model: n_ctx         = 128
0.00.095.132 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.133 I llama_new_context_with_model: n_batch       = 128
0.00.095.133 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.133 I llama_new_context_with_model: flash_attn    = 0
0.00.095.133 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.134 I llama_new_context_with_model: freq_scale    = 1
0.00.095.134 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.134 I ggml_metal_init: allocating
0.00.095.140 I ggml_metal_init: found device: Apple M4
0.00.095.142 I ggml_metal_init: picking default device: Apple M4
0.00.095.690 I ggml_metal_init: using embedded metal library
0.00.097.749 I ggml_metal_init: GPU name:   Apple M4
0.00.097.750 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.751 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.751 I ggml_metal_init: simdgroup reduction   = true
0.00.097.751 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.752 I ggml_metal_init: has bfloat            = true
0.00.097.752 I ggml_metal_init: use bfloat            = true
0.00.097.752 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.753 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.125 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.129 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.021 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.022 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.023 I llama_new_context_with_model: graph nodes  = 967
0.00.109.023 I llama_new_context_with_model: graph splits = 2
0.00.109.035 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.060.769 I 
0.01.060.800 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.060.806 I perplexity: tokenizing the input ..
0.01.073.875 I perplexity: tokenization took 13.063 ms
0.01.073.917 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.195.130 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.196.806 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.196.830 I llama_perf_context_print:        load time =    1033.73 ms
0.01.196.831 I llama_perf_context_print: prompt eval time =     120.28 ms /   128 tokens (    0.94 ms per token,  1064.21 tokens per second)
0.01.196.833 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.196.838 I llama_perf_context_print:       total time =     136.06 ms /   129 tokens
0.01.197.389 I ggml_metal_free: deallocating

real	0m1.385s
user	0m0.123s
sys	0m0.229s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.025 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.566 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.573 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.575 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.575 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.576 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.500 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.622 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.706 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.707 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.709 I llama_model_loader: - type  f32:  194 tensors
0.00.036.710 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.322 I llm_load_vocab: special tokens cache size = 25
0.00.066.677 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.681 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.681 I llm_load_print_meta: arch             = gptneox
0.00.066.681 I llm_load_print_meta: vocab type       = BPE
0.00.066.681 I llm_load_print_meta: n_vocab          = 50304
0.00.066.681 I llm_load_print_meta: n_merges         = 50009
0.00.066.682 I llm_load_print_meta: vocab_only       = 0
0.00.066.682 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.684 I llm_load_print_meta: n_embd           = 2048
0.00.066.684 I llm_load_print_meta: n_layer          = 24
0.00.066.688 I llm_load_print_meta: n_head           = 16
0.00.066.689 I llm_load_print_meta: n_head_kv        = 16
0.00.066.689 I llm_load_print_meta: n_rot            = 32
0.00.066.689 I llm_load_print_meta: n_swa            = 0
0.00.066.689 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.690 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.690 I llm_load_print_meta: n_gqa            = 1
0.00.066.691 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.691 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.692 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.692 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.692 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.692 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.693 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.693 I llm_load_print_meta: n_ff             = 8192
0.00.066.693 I llm_load_print_meta: n_expert         = 0
0.00.066.694 I llm_load_print_meta: n_expert_used    = 0
0.00.066.694 I llm_load_print_meta: causal attn      = 1
0.00.066.694 I llm_load_print_meta: pooling type     = 0
0.00.066.694 I llm_load_print_meta: rope type        = 2
0.00.066.694 I llm_load_print_meta: rope scaling     = linear
0.00.066.695 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.695 I llm_load_print_meta: freq_scale_train = 1
0.00.066.696 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.696 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.696 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.696 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.697 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.697 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.697 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.710 I llm_load_print_meta: model type       = 1.4B
0.00.066.710 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.711 I llm_load_print_meta: model params     = 1.41 B
0.00.066.711 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.711 I llm_load_print_meta: general.name     = 1.4B
0.00.066.711 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.712 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.712 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.712 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.712 I llm_load_print_meta: LF token         = 128 ''
0.00.066.713 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.713 I llm_load_print_meta: max token length = 1024
0.00.069.179 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.179 I llm_load_tensors: offloading output layer to GPU
0.00.069.179 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.190 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.191 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.189 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.190 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.191 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.191 I llama_new_context_with_model: n_batch       = 2048
0.00.070.191 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.191 I llama_new_context_with_model: flash_attn    = 0
0.00.070.192 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.192 I llama_new_context_with_model: freq_scale    = 1
0.00.070.192 I ggml_metal_init: allocating
0.00.070.199 I ggml_metal_init: found device: Apple M4
0.00.070.201 I ggml_metal_init: picking default device: Apple M4
0.00.070.933 I ggml_metal_init: using embedded metal library
0.00.073.055 I ggml_metal_init: GPU name:   Apple M4
0.00.073.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.058 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.058 I ggml_metal_init: simdgroup reduction   = true
0.00.073.058 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.058 I ggml_metal_init: has bfloat            = true
0.00.073.058 I ggml_metal_init: use bfloat            = true
0.00.073.059 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.854 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.867 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.891 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.073 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.108.075 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.108.075 I llama_new_context_with_model: graph nodes  = 967
0.00.108.075 I llama_new_context_with_model: graph splits = 2
0.00.108.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.348.757 I main: llama threadpool init, n_threads = 4
0.01.348.801 I 
0.01.348.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.348.834 I 
0.01.349.229 I sampler seed: 1234
0.01.349.235 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.349.292 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.349.297 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.349.297 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.448.079 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.02.448.080 I llama_perf_context_print:        load time =    1338.73 ms
0.02.448.080 I llama_perf_context_print: prompt eval time =      42.12 ms /     7 tokens (    6.02 ms per token,   166.21 tokens per second)
0.02.448.081 I llama_perf_context_print:        eval time =    1053.52 ms /    63 runs   (   16.72 ms per token,    59.80 tokens per second)
0.02.448.082 I llama_perf_context_print:       total time =    1099.32 ms /    70 tokens
0.02.448.260 I ggml_metal_free: deallocating

real	0m2.466s
user	0m0.123s
sys	0m0.253s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.282 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.360 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.673 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.684 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.684 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.685 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.687 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.566 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.599 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.601 I llama_model_loader: - type  f32:  194 tensors
0.00.031.602 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.665 I llm_load_vocab: special tokens cache size = 25
0.00.063.155 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.158 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.158 I llm_load_print_meta: arch             = gptneox
0.00.063.159 I llm_load_print_meta: vocab type       = BPE
0.00.063.159 I llm_load_print_meta: n_vocab          = 50304
0.00.063.159 I llm_load_print_meta: n_merges         = 50009
0.00.063.159 I llm_load_print_meta: vocab_only       = 0
0.00.063.159 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.160 I llm_load_print_meta: n_embd           = 2048
0.00.063.160 I llm_load_print_meta: n_layer          = 24
0.00.063.162 I llm_load_print_meta: n_head           = 16
0.00.063.163 I llm_load_print_meta: n_head_kv        = 16
0.00.063.163 I llm_load_print_meta: n_rot            = 32
0.00.063.163 I llm_load_print_meta: n_swa            = 0
0.00.063.163 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.164 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.164 I llm_load_print_meta: n_gqa            = 1
0.00.063.165 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.166 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.166 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.167 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.167 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.167 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.167 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.168 I llm_load_print_meta: n_ff             = 8192
0.00.063.168 I llm_load_print_meta: n_expert         = 0
0.00.063.168 I llm_load_print_meta: n_expert_used    = 0
0.00.063.168 I llm_load_print_meta: causal attn      = 1
0.00.063.168 I llm_load_print_meta: pooling type     = 0
0.00.063.169 I llm_load_print_meta: rope type        = 2
0.00.063.169 I llm_load_print_meta: rope scaling     = linear
0.00.063.169 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.170 I llm_load_print_meta: freq_scale_train = 1
0.00.063.170 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.170 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.170 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.170 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.170 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.171 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.171 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.182 I llm_load_print_meta: model type       = 1.4B
0.00.063.183 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.183 I llm_load_print_meta: model params     = 1.41 B
0.00.063.183 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.184 I llm_load_print_meta: general.name     = 1.4B
0.00.063.184 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.184 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.184 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.184 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.185 I llm_load_print_meta: LF token         = 128 ''
0.00.063.185 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.185 I llm_load_print_meta: max token length = 1024
0.00.065.371 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.371 I llm_load_tensors: offloading output layer to GPU
0.00.065.371 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.381 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.382 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.326 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.326 I llama_new_context_with_model: n_ctx         = 128
0.00.066.327 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.327 I llama_new_context_with_model: n_batch       = 128
0.00.066.327 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.327 I llama_new_context_with_model: flash_attn    = 0
0.00.066.327 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.328 I llama_new_context_with_model: freq_scale    = 1
0.00.066.328 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.328 I ggml_metal_init: allocating
0.00.066.332 I ggml_metal_init: found device: Apple M4
0.00.066.334 I ggml_metal_init: picking default device: Apple M4
0.00.066.906 I ggml_metal_init: using embedded metal library
0.00.068.981 I ggml_metal_init: GPU name:   Apple M4
0.00.068.983 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.983 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.984 I ggml_metal_init: simdgroup reduction   = true
0.00.068.984 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.984 I ggml_metal_init: has bfloat            = true
0.00.068.984 I ggml_metal_init: use bfloat            = true
0.00.068.985 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.985 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.319 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.321 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.335 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.300 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.302 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.302 I llama_new_context_with_model: graph nodes  = 967
0.00.079.302 I llama_new_context_with_model: graph splits = 2
0.00.079.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.834.888 I 
0.00.834.912 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.834.921 I perplexity: tokenizing the input ..
0.00.842.483 I perplexity: tokenization took 7.564 ms
0.00.842.501 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.964.620 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.965.834 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.965.847 I llama_perf_context_print:        load time =     823.52 ms
0.00.965.848 I llama_perf_context_print: prompt eval time =     121.89 ms /   128 tokens (    0.95 ms per token,  1050.13 tokens per second)
0.00.965.849 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.965.849 I llama_perf_context_print:       total time =     130.96 ms /   129 tokens
0.00.966.249 I ggml_metal_free: deallocating

real	0m0.986s
user	0m0.090s
sys	0m0.142s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.642 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.305 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.306 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.365 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.274 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.274 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.275 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.275 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.276 I llama_model_loader: - type  f32:  194 tensors
0.00.026.276 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.277 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.851 I llm_load_vocab: special tokens cache size = 25
0.00.052.929 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.933 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.933 I llm_load_print_meta: arch             = gptneox
0.00.052.934 I llm_load_print_meta: vocab type       = BPE
0.00.052.934 I llm_load_print_meta: n_vocab          = 50304
0.00.052.934 I llm_load_print_meta: n_merges         = 50009
0.00.052.934 I llm_load_print_meta: vocab_only       = 0
0.00.052.938 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.938 I llm_load_print_meta: n_embd           = 2048
0.00.052.938 I llm_load_print_meta: n_layer          = 24
0.00.052.943 I llm_load_print_meta: n_head           = 16
0.00.052.944 I llm_load_print_meta: n_head_kv        = 16
0.00.052.944 I llm_load_print_meta: n_rot            = 32
0.00.052.946 I llm_load_print_meta: n_swa            = 0
0.00.052.946 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.946 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.947 I llm_load_print_meta: n_gqa            = 1
0.00.052.948 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.948 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.949 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.949 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.950 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.950 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.950 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.952 I llm_load_print_meta: n_ff             = 8192
0.00.052.952 I llm_load_print_meta: n_expert         = 0
0.00.052.952 I llm_load_print_meta: n_expert_used    = 0
0.00.052.952 I llm_load_print_meta: causal attn      = 1
0.00.052.952 I llm_load_print_meta: pooling type     = 0
0.00.052.953 I llm_load_print_meta: rope type        = 2
0.00.052.953 I llm_load_print_meta: rope scaling     = linear
0.00.052.953 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.954 I llm_load_print_meta: freq_scale_train = 1
0.00.052.954 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.954 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.954 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.954 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.954 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.954 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.954 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.967 I llm_load_print_meta: model type       = 1.4B
0.00.052.969 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.969 I llm_load_print_meta: model params     = 1.41 B
0.00.052.969 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.970 I llm_load_print_meta: general.name     = 1.4B
0.00.052.970 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.970 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.970 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.970 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.971 I llm_load_print_meta: LF token         = 128 ''
0.00.052.971 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.971 I llm_load_print_meta: max token length = 1024
0.00.055.212 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.212 I llm_load_tensors: offloading output layer to GPU
0.00.055.213 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.223 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.224 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.215 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.216 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.216 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.216 I llama_new_context_with_model: n_batch       = 2048
0.00.056.216 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.216 I llama_new_context_with_model: flash_attn    = 0
0.00.056.217 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.217 I llama_new_context_with_model: freq_scale    = 1
0.00.056.218 I ggml_metal_init: allocating
0.00.056.220 I ggml_metal_init: found device: Apple M4
0.00.056.222 I ggml_metal_init: picking default device: Apple M4
0.00.056.887 I ggml_metal_init: using embedded metal library
0.00.059.024 I ggml_metal_init: GPU name:   Apple M4
0.00.059.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.027 I ggml_metal_init: simdgroup reduction   = true
0.00.059.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.027 I ggml_metal_init: has bfloat            = true
0.00.059.027 I ggml_metal_init: use bfloat            = true
0.00.059.028 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.028 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.910 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.919 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.943 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.153 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.155 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.155 I llama_new_context_with_model: graph nodes  = 967
0.00.093.155 I llama_new_context_with_model: graph splits = 2
0.00.093.181 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.817 I main: llama threadpool init, n_threads = 4
0.00.678.852 I 
0.00.678.871 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.678.871 I 
0.00.679.105 I sampler seed: 1234
0.00.679.109 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.120 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.121 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.121 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.355.439 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.355.439 I llama_perf_context_print:        load time =     668.17 ms
0.01.355.440 I llama_perf_context_print: prompt eval time =      35.92 ms /     7 tokens (    5.13 ms per token,   194.90 tokens per second)
0.01.355.441 I llama_perf_context_print:        eval time =     637.43 ms /    63 runs   (   10.12 ms per token,    98.83 tokens per second)
0.01.355.441 I llama_perf_context_print:       total time =     676.62 ms /    70 tokens
0.01.355.608 I ggml_metal_free: deallocating

real	0m1.372s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.254 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.832 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.737 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.742 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.743 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.749 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.750 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.751 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.751 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.752 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.752 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.753 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.754 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.589 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.661 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.553 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.554 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.555 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.555 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.555 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.555 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.556 I llama_model_loader: - type  f32:  194 tensors
0.00.024.556 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.557 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.577 I llm_load_vocab: special tokens cache size = 25
0.00.051.549 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.552 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.552 I llm_load_print_meta: arch             = gptneox
0.00.051.553 I llm_load_print_meta: vocab type       = BPE
0.00.051.553 I llm_load_print_meta: n_vocab          = 50304
0.00.051.553 I llm_load_print_meta: n_merges         = 50009
0.00.051.553 I llm_load_print_meta: vocab_only       = 0
0.00.051.553 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.554 I llm_load_print_meta: n_embd           = 2048
0.00.051.554 I llm_load_print_meta: n_layer          = 24
0.00.051.557 I llm_load_print_meta: n_head           = 16
0.00.051.557 I llm_load_print_meta: n_head_kv        = 16
0.00.051.558 I llm_load_print_meta: n_rot            = 32
0.00.051.558 I llm_load_print_meta: n_swa            = 0
0.00.051.558 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.558 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.559 I llm_load_print_meta: n_gqa            = 1
0.00.051.560 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.561 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.561 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.561 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.562 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.562 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.562 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.563 I llm_load_print_meta: n_ff             = 8192
0.00.051.563 I llm_load_print_meta: n_expert         = 0
0.00.051.563 I llm_load_print_meta: n_expert_used    = 0
0.00.051.563 I llm_load_print_meta: causal attn      = 1
0.00.051.563 I llm_load_print_meta: pooling type     = 0
0.00.051.564 I llm_load_print_meta: rope type        = 2
0.00.051.564 I llm_load_print_meta: rope scaling     = linear
0.00.051.564 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.564 I llm_load_print_meta: freq_scale_train = 1
0.00.051.565 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.565 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.565 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.565 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.565 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.566 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.566 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.578 I llm_load_print_meta: model type       = 1.4B
0.00.051.578 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.578 I llm_load_print_meta: model params     = 1.41 B
0.00.051.579 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.579 I llm_load_print_meta: general.name     = 1.4B
0.00.051.579 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.579 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.579 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.579 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.580 I llm_load_print_meta: LF token         = 128 ''
0.00.051.581 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.582 I llm_load_print_meta: max token length = 1024
0.00.053.581 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.582 I llm_load_tensors: offloading output layer to GPU
0.00.053.582 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.592 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.593 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.546 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.546 I llama_new_context_with_model: n_ctx         = 128
0.00.054.547 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.547 I llama_new_context_with_model: n_batch       = 128
0.00.054.547 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.547 I llama_new_context_with_model: flash_attn    = 0
0.00.054.548 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.548 I llama_new_context_with_model: freq_scale    = 1
0.00.054.548 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.548 I ggml_metal_init: allocating
0.00.054.552 I ggml_metal_init: found device: Apple M4
0.00.054.554 I ggml_metal_init: picking default device: Apple M4
0.00.055.099 I ggml_metal_init: using embedded metal library
0.00.057.057 I ggml_metal_init: GPU name:   Apple M4
0.00.057.058 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.059 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.059 I ggml_metal_init: simdgroup reduction   = true
0.00.057.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.060 I ggml_metal_init: has bfloat            = true
0.00.057.060 I ggml_metal_init: use bfloat            = true
0.00.057.060 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.062 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.596 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.598 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.611 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.556 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.557 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.558 I llama_new_context_with_model: graph nodes  = 967
0.00.067.558 I llama_new_context_with_model: graph splits = 2
0.00.067.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.734 I 
0.00.623.764 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.623.783 I perplexity: tokenizing the input ..
0.00.631.253 I perplexity: tokenization took 7.469 ms
0.00.631.265 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.753.277 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.754.484 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.754.500 I llama_perf_context_print:        load time =     613.90 ms
0.00.754.501 I llama_perf_context_print: prompt eval time =     121.78 ms /   128 tokens (    0.95 ms per token,  1051.11 tokens per second)
0.00.754.502 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.502 I llama_perf_context_print:       total time =     130.77 ms /   129 tokens
0.00.754.943 I ggml_metal_free: deallocating

real	0m0.772s
user	0m0.078s
sys	0m0.114s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.660 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.243 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.253 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.253 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.254 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.255 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.255 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.256 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.256 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.256 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.259 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.259 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.058 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.122 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.910 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.910 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.910 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.911 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.911 I llama_model_loader: - type  f32:  194 tensors
0.00.023.912 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.912 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.481 I llm_load_vocab: special tokens cache size = 25
0.00.050.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.604 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.604 I llm_load_print_meta: arch             = gptneox
0.00.050.605 I llm_load_print_meta: vocab type       = BPE
0.00.050.605 I llm_load_print_meta: n_vocab          = 50304
0.00.050.605 I llm_load_print_meta: n_merges         = 50009
0.00.050.605 I llm_load_print_meta: vocab_only       = 0
0.00.050.605 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.605 I llm_load_print_meta: n_embd           = 2048
0.00.050.606 I llm_load_print_meta: n_layer          = 24
0.00.050.608 I llm_load_print_meta: n_head           = 16
0.00.050.610 I llm_load_print_meta: n_head_kv        = 16
0.00.050.610 I llm_load_print_meta: n_rot            = 32
0.00.050.610 I llm_load_print_meta: n_swa            = 0
0.00.050.611 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.611 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.611 I llm_load_print_meta: n_gqa            = 1
0.00.050.612 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.613 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.617 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.618 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.618 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.618 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.619 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.620 I llm_load_print_meta: n_ff             = 8192
0.00.050.620 I llm_load_print_meta: n_expert         = 0
0.00.050.620 I llm_load_print_meta: n_expert_used    = 0
0.00.050.620 I llm_load_print_meta: causal attn      = 1
0.00.050.620 I llm_load_print_meta: pooling type     = 0
0.00.050.621 I llm_load_print_meta: rope type        = 2
0.00.050.622 I llm_load_print_meta: rope scaling     = linear
0.00.050.623 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.623 I llm_load_print_meta: freq_scale_train = 1
0.00.050.623 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.623 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.624 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.624 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.624 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.624 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.624 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.636 I llm_load_print_meta: model type       = 1.4B
0.00.050.637 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.637 I llm_load_print_meta: model params     = 1.41 B
0.00.050.638 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.638 I llm_load_print_meta: general.name     = 1.4B
0.00.050.639 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: LF token         = 128 ''
0.00.050.641 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.641 I llm_load_print_meta: max token length = 1024
0.00.052.655 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.656 I llm_load_tensors: offloading output layer to GPU
0.00.052.656 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.666 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.667 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.544 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.545 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.545 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.545 I llama_new_context_with_model: n_batch       = 2048
0.00.053.546 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.546 I llama_new_context_with_model: flash_attn    = 0
0.00.053.546 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.546 I llama_new_context_with_model: freq_scale    = 1
0.00.053.547 I ggml_metal_init: allocating
0.00.053.550 I ggml_metal_init: found device: Apple M4
0.00.053.552 I ggml_metal_init: picking default device: Apple M4
0.00.054.098 I ggml_metal_init: using embedded metal library
0.00.056.034 I ggml_metal_init: GPU name:   Apple M4
0.00.056.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.036 I ggml_metal_init: simdgroup reduction   = true
0.00.056.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.036 I ggml_metal_init: has bfloat            = true
0.00.056.036 I ggml_metal_init: use bfloat            = true
0.00.056.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.037 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.145 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.154 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.174 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.127 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.129 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.129 I llama_new_context_with_model: graph nodes  = 967
0.00.084.129 I llama_new_context_with_model: graph splits = 2
0.00.084.142 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.270 I main: llama threadpool init, n_threads = 4
0.00.679.305 I 
0.00.679.323 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.679.324 I 
0.00.679.474 I sampler seed: 1234
0.00.679.478 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.513 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.514 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.514 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.406.995 I llama_perf_sampler_print:    sampling time =       1.03 ms /    71 runs   (    0.01 ms per token, 68999.03 tokens per second)
0.01.406.996 I llama_perf_context_print:        load time =     670.61 ms
0.01.406.997 I llama_perf_context_print: prompt eval time =      36.54 ms /     7 tokens (    5.22 ms per token,   191.55 tokens per second)
0.01.406.998 I llama_perf_context_print:        eval time =     688.09 ms /    63 runs   (   10.92 ms per token,    91.56 tokens per second)
0.01.406.998 I llama_perf_context_print:       total time =     727.73 ms /    70 tokens
0.01.407.173 I ggml_metal_free: deallocating

real	0m1.426s
user	0m0.109s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.400 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.304 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.306 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.312 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.317 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.205 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.107 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.107 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.107 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.108 I llama_model_loader: - type  f32:  194 tensors
0.00.023.108 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.109 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.854 I llm_load_vocab: special tokens cache size = 25
0.00.049.931 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.934 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.934 I llm_load_print_meta: arch             = gptneox
0.00.049.934 I llm_load_print_meta: vocab type       = BPE
0.00.049.935 I llm_load_print_meta: n_vocab          = 50304
0.00.049.935 I llm_load_print_meta: n_merges         = 50009
0.00.049.935 I llm_load_print_meta: vocab_only       = 0
0.00.049.935 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.935 I llm_load_print_meta: n_embd           = 2048
0.00.049.936 I llm_load_print_meta: n_layer          = 24
0.00.049.939 I llm_load_print_meta: n_head           = 16
0.00.049.939 I llm_load_print_meta: n_head_kv        = 16
0.00.049.939 I llm_load_print_meta: n_rot            = 32
0.00.049.940 I llm_load_print_meta: n_swa            = 0
0.00.049.942 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.942 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.943 I llm_load_print_meta: n_gqa            = 1
0.00.049.943 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.944 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.945 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.945 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.945 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.945 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.945 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.946 I llm_load_print_meta: n_ff             = 8192
0.00.049.946 I llm_load_print_meta: n_expert         = 0
0.00.049.946 I llm_load_print_meta: n_expert_used    = 0
0.00.049.947 I llm_load_print_meta: causal attn      = 1
0.00.049.947 I llm_load_print_meta: pooling type     = 0
0.00.049.947 I llm_load_print_meta: rope type        = 2
0.00.049.947 I llm_load_print_meta: rope scaling     = linear
0.00.049.951 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.951 I llm_load_print_meta: freq_scale_train = 1
0.00.049.952 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.952 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.952 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.952 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.952 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.953 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.953 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.964 I llm_load_print_meta: model type       = 1.4B
0.00.049.964 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.965 I llm_load_print_meta: model params     = 1.41 B
0.00.049.965 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.965 I llm_load_print_meta: general.name     = 1.4B
0.00.049.965 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.966 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.967 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.968 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.968 I llm_load_print_meta: LF token         = 128 ''
0.00.049.968 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.968 I llm_load_print_meta: max token length = 1024
0.00.051.557 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.557 I llm_load_tensors: offloading output layer to GPU
0.00.051.558 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.567 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.568 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.390 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.390 I llama_new_context_with_model: n_ctx         = 128
0.00.052.390 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.391 I llama_new_context_with_model: n_batch       = 128
0.00.052.391 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.391 I llama_new_context_with_model: flash_attn    = 0
0.00.052.391 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.392 I llama_new_context_with_model: freq_scale    = 1
0.00.052.392 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.392 I ggml_metal_init: allocating
0.00.052.396 I ggml_metal_init: found device: Apple M4
0.00.052.398 I ggml_metal_init: picking default device: Apple M4
0.00.052.922 I ggml_metal_init: using embedded metal library
0.00.054.871 I ggml_metal_init: GPU name:   Apple M4
0.00.054.873 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.873 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.874 I ggml_metal_init: simdgroup reduction   = true
0.00.054.874 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.874 I ggml_metal_init: has bfloat            = true
0.00.054.874 I ggml_metal_init: use bfloat            = true
0.00.054.875 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.290 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.292 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.307 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.198 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.199 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.200 I llama_new_context_with_model: graph nodes  = 967
0.00.065.200 I llama_new_context_with_model: graph splits = 2
0.00.065.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.549 I 
0.00.643.579 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.643.587 I perplexity: tokenizing the input ..
0.00.651.145 I perplexity: tokenization took 7.556 ms
0.00.651.157 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.773.351 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.774.622 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.774.636 I llama_perf_context_print:        load time =     635.14 ms
0.00.774.637 I llama_perf_context_print: prompt eval time =     121.96 ms /   128 tokens (    0.95 ms per token,  1049.54 tokens per second)
0.00.774.638 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.774.638 I llama_perf_context_print:       total time =     131.09 ms /   129 tokens
0.00.775.020 I ggml_metal_free: deallocating

real	0m0.787s
user	0m0.077s
sys	0m0.111s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.012.783 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.638 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.643 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.644 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.645 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.645 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.646 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.646 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.647 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.647 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.647 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.648 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.648 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.654 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.654 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.532 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.654 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.481 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.483 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.483 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.484 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.484 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.484 I llama_model_loader: - type  f32:  194 tensors
0.00.028.485 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.485 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.569 I llm_load_vocab: special tokens cache size = 25
0.00.055.679 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.682 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.682 I llm_load_print_meta: arch             = gptneox
0.00.055.683 I llm_load_print_meta: vocab type       = BPE
0.00.055.683 I llm_load_print_meta: n_vocab          = 50304
0.00.055.683 I llm_load_print_meta: n_merges         = 50009
0.00.055.683 I llm_load_print_meta: vocab_only       = 0
0.00.055.684 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.684 I llm_load_print_meta: n_embd           = 2048
0.00.055.684 I llm_load_print_meta: n_layer          = 24
0.00.055.686 I llm_load_print_meta: n_head           = 16
0.00.055.687 I llm_load_print_meta: n_head_kv        = 16
0.00.055.687 I llm_load_print_meta: n_rot            = 32
0.00.055.688 I llm_load_print_meta: n_swa            = 0
0.00.055.688 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.688 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.689 I llm_load_print_meta: n_gqa            = 1
0.00.055.689 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.690 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.691 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.691 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.691 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.692 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.692 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.692 I llm_load_print_meta: n_ff             = 8192
0.00.055.693 I llm_load_print_meta: n_expert         = 0
0.00.055.693 I llm_load_print_meta: n_expert_used    = 0
0.00.055.694 I llm_load_print_meta: causal attn      = 1
0.00.055.696 I llm_load_print_meta: pooling type     = 0
0.00.055.696 I llm_load_print_meta: rope type        = 2
0.00.055.696 I llm_load_print_meta: rope scaling     = linear
0.00.055.697 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.697 I llm_load_print_meta: freq_scale_train = 1
0.00.055.697 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.697 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.697 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.698 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.698 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.698 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.698 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.705 I llm_load_print_meta: model type       = 1.4B
0.00.055.705 I llm_load_print_meta: model ftype      = Q5_0
0.00.055.706 I llm_load_print_meta: model params     = 1.41 B
0.00.055.706 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.055.706 I llm_load_print_meta: general.name     = 1.4B
0.00.055.707 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.707 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.707 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.708 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.708 I llm_load_print_meta: LF token         = 128 ''
0.00.055.708 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.709 I llm_load_print_meta: max token length = 1024
0.00.057.510 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.510 I llm_load_tensors: offloading output layer to GPU
0.00.057.511 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.515 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.057.516 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.058.496 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.496 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.497 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.497 I llama_new_context_with_model: n_batch       = 2048
0.00.058.497 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.497 I llama_new_context_with_model: flash_attn    = 0
0.00.058.498 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.498 I llama_new_context_with_model: freq_scale    = 1
0.00.058.498 I ggml_metal_init: allocating
0.00.058.501 I ggml_metal_init: found device: Apple M4
0.00.058.504 I ggml_metal_init: picking default device: Apple M4
0.00.059.063 I ggml_metal_init: using embedded metal library
0.00.060.976 I ggml_metal_init: GPU name:   Apple M4
0.00.060.977 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.978 I ggml_metal_init: simdgroup reduction   = true
0.00.060.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.979 I ggml_metal_init: has bfloat            = true
0.00.060.979 I ggml_metal_init: use bfloat            = true
0.00.060.979 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.982 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.864 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.869 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.888 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.877 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.878 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.878 I llama_new_context_with_model: graph nodes  = 967
0.00.089.878 I llama_new_context_with_model: graph splits = 2
0.00.089.891 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.218 I main: llama threadpool init, n_threads = 4
0.00.803.255 I 
0.00.803.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.803.273 I 
0.00.803.505 I sampler seed: 1234
0.00.803.509 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.533 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.535 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.535 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.593.693 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.593.693 I llama_perf_context_print:        load time =     790.43 ms
0.01.593.694 I llama_perf_context_print: prompt eval time =      36.65 ms /     7 tokens (    5.24 ms per token,   190.99 tokens per second)
0.01.593.695 I llama_perf_context_print:        eval time =     750.37 ms /    63 runs   (   11.91 ms per token,    83.96 tokens per second)
0.01.593.695 I llama_perf_context_print:       total time =     790.48 ms /    70 tokens
0.01.593.870 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.109s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.617 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.400 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.408 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.409 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.199 I llama_model_loader: - type  f32:  194 tensors
0.00.024.199 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.199 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.388 I llm_load_vocab: special tokens cache size = 25
0.00.050.413 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.416 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.416 I llm_load_print_meta: arch             = gptneox
0.00.050.416 I llm_load_print_meta: vocab type       = BPE
0.00.050.417 I llm_load_print_meta: n_vocab          = 50304
0.00.050.417 I llm_load_print_meta: n_merges         = 50009
0.00.050.417 I llm_load_print_meta: vocab_only       = 0
0.00.050.417 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.417 I llm_load_print_meta: n_embd           = 2048
0.00.050.418 I llm_load_print_meta: n_layer          = 24
0.00.050.420 I llm_load_print_meta: n_head           = 16
0.00.050.421 I llm_load_print_meta: n_head_kv        = 16
0.00.050.421 I llm_load_print_meta: n_rot            = 32
0.00.050.421 I llm_load_print_meta: n_swa            = 0
0.00.050.421 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.421 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.422 I llm_load_print_meta: n_gqa            = 1
0.00.050.423 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.424 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.424 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.424 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.426 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.426 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.426 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.427 I llm_load_print_meta: n_ff             = 8192
0.00.050.427 I llm_load_print_meta: n_expert         = 0
0.00.050.427 I llm_load_print_meta: n_expert_used    = 0
0.00.050.427 I llm_load_print_meta: causal attn      = 1
0.00.050.428 I llm_load_print_meta: pooling type     = 0
0.00.050.428 I llm_load_print_meta: rope type        = 2
0.00.050.428 I llm_load_print_meta: rope scaling     = linear
0.00.050.428 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.429 I llm_load_print_meta: freq_scale_train = 1
0.00.050.429 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.429 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.429 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.430 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.430 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.430 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.430 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.442 I llm_load_print_meta: model type       = 1.4B
0.00.050.443 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.443 I llm_load_print_meta: model params     = 1.41 B
0.00.050.444 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.444 I llm_load_print_meta: general.name     = 1.4B
0.00.050.444 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.444 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.444 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: LF token         = 128 ''
0.00.050.445 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.445 I llm_load_print_meta: max token length = 1024
0.00.052.312 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.313 I llm_load_tensors: offloading output layer to GPU
0.00.052.313 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.317 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.317 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.199 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.200 I llama_new_context_with_model: n_ctx         = 128
0.00.053.200 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.201 I llama_new_context_with_model: n_batch       = 128
0.00.053.201 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.201 I llama_new_context_with_model: flash_attn    = 0
0.00.053.201 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.202 I llama_new_context_with_model: freq_scale    = 1
0.00.053.202 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.202 I ggml_metal_init: allocating
0.00.053.205 I ggml_metal_init: found device: Apple M4
0.00.053.207 I ggml_metal_init: picking default device: Apple M4
0.00.053.734 I ggml_metal_init: using embedded metal library
0.00.055.619 I ggml_metal_init: GPU name:   Apple M4
0.00.055.620 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.621 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.621 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.621 I ggml_metal_init: simdgroup reduction   = true
0.00.055.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.621 I ggml_metal_init: has bfloat            = true
0.00.055.622 I ggml_metal_init: use bfloat            = true
0.00.055.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.906 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.908 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.921 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.783 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.784 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.785 I llama_new_context_with_model: graph nodes  = 967
0.00.065.785 I llama_new_context_with_model: graph splits = 2
0.00.065.792 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.532 I 
0.00.751.559 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.751.562 I perplexity: tokenizing the input ..
0.00.759.077 I perplexity: tokenization took 7.513 ms
0.00.759.088 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.894.389 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.895.621 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.895.639 I llama_perf_context_print:        load time =     741.91 ms
0.00.895.640 I llama_perf_context_print: prompt eval time =     135.07 ms /   128 tokens (    1.06 ms per token,   947.64 tokens per second)
0.00.895.640 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.895.641 I llama_perf_context_print:       total time =     144.11 ms /   129 tokens
0.00.896.122 I ggml_metal_free: deallocating

real	0m0.913s
user	0m0.076s
sys	0m0.130s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.698 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.453 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.306 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.306 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.307 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.307 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.308 I llama_model_loader: - type  f32:  194 tensors
0.00.024.308 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.308 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.573 I llm_load_vocab: special tokens cache size = 25
0.00.051.820 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.823 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.823 I llm_load_print_meta: arch             = gptneox
0.00.051.824 I llm_load_print_meta: vocab type       = BPE
0.00.051.824 I llm_load_print_meta: n_vocab          = 50304
0.00.051.824 I llm_load_print_meta: n_merges         = 50009
0.00.051.824 I llm_load_print_meta: vocab_only       = 0
0.00.051.824 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.824 I llm_load_print_meta: n_embd           = 2048
0.00.051.825 I llm_load_print_meta: n_layer          = 24
0.00.051.827 I llm_load_print_meta: n_head           = 16
0.00.051.828 I llm_load_print_meta: n_head_kv        = 16
0.00.051.828 I llm_load_print_meta: n_rot            = 32
0.00.051.828 I llm_load_print_meta: n_swa            = 0
0.00.051.828 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.828 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.829 I llm_load_print_meta: n_gqa            = 1
0.00.051.830 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.830 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.831 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.831 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.832 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.832 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.832 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.833 I llm_load_print_meta: n_ff             = 8192
0.00.051.833 I llm_load_print_meta: n_expert         = 0
0.00.051.833 I llm_load_print_meta: n_expert_used    = 0
0.00.051.833 I llm_load_print_meta: causal attn      = 1
0.00.051.833 I llm_load_print_meta: pooling type     = 0
0.00.051.833 I llm_load_print_meta: rope type        = 2
0.00.051.834 I llm_load_print_meta: rope scaling     = linear
0.00.051.835 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.836 I llm_load_print_meta: freq_scale_train = 1
0.00.051.836 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.836 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.836 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.836 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.837 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.837 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.837 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.844 I llm_load_print_meta: model type       = 1.4B
0.00.051.844 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.845 I llm_load_print_meta: model params     = 1.41 B
0.00.051.845 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.845 I llm_load_print_meta: general.name     = 1.4B
0.00.051.846 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.846 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.846 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.846 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.847 I llm_load_print_meta: LF token         = 128 ''
0.00.051.847 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.847 I llm_load_print_meta: max token length = 1024
0.00.053.709 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.709 I llm_load_tensors: offloading output layer to GPU
0.00.053.709 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.714 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.715 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.659 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.660 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.660 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.660 I llama_new_context_with_model: n_batch       = 2048
0.00.054.660 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.661 I llama_new_context_with_model: flash_attn    = 0
0.00.054.661 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.661 I llama_new_context_with_model: freq_scale    = 1
0.00.054.662 I ggml_metal_init: allocating
0.00.054.669 I ggml_metal_init: found device: Apple M4
0.00.054.672 I ggml_metal_init: picking default device: Apple M4
0.00.055.277 I ggml_metal_init: using embedded metal library
0.00.057.207 I ggml_metal_init: GPU name:   Apple M4
0.00.057.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.209 I ggml_metal_init: simdgroup reduction   = true
0.00.057.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.210 I ggml_metal_init: has bfloat            = true
0.00.057.210 I ggml_metal_init: use bfloat            = true
0.00.057.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.006 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.015 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.034 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.985 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.986 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.986 I llama_new_context_with_model: graph nodes  = 967
0.00.084.987 I llama_new_context_with_model: graph splits = 2
0.00.084.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.476 I main: llama threadpool init, n_threads = 4
0.00.821.510 I 
0.00.821.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.821.528 I 
0.00.821.757 I sampler seed: 1234
0.00.821.761 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.821.781 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.821.781 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.821.781 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.663.426 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.663.427 I llama_perf_context_print:        load time =     812.77 ms
0.01.663.427 I llama_perf_context_print: prompt eval time =      36.81 ms /     7 tokens (    5.26 ms per token,   190.19 tokens per second)
0.01.663.428 I llama_perf_context_print:        eval time =     801.82 ms /    63 runs   (   12.73 ms per token,    78.57 tokens per second)
0.01.663.428 I llama_perf_context_print:       total time =     841.95 ms /    70 tokens
0.01.663.610 I ggml_metal_free: deallocating

real	0m1.680s
user	0m0.109s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.625 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.431 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.435 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.439 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.439 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.440 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.440 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.441 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.441 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.441 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.442 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.442 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.443 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.446 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.446 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.446 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.298 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.256 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.257 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.257 I llama_model_loader: - type  f32:  194 tensors
0.00.023.258 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.258 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.269 I llm_load_vocab: special tokens cache size = 25
0.00.050.379 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.383 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.384 I llm_load_print_meta: arch             = gptneox
0.00.050.384 I llm_load_print_meta: vocab type       = BPE
0.00.050.384 I llm_load_print_meta: n_vocab          = 50304
0.00.050.390 I llm_load_print_meta: n_merges         = 50009
0.00.050.390 I llm_load_print_meta: vocab_only       = 0
0.00.050.390 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.390 I llm_load_print_meta: n_embd           = 2048
0.00.050.391 I llm_load_print_meta: n_layer          = 24
0.00.050.394 I llm_load_print_meta: n_head           = 16
0.00.050.395 I llm_load_print_meta: n_head_kv        = 16
0.00.050.395 I llm_load_print_meta: n_rot            = 32
0.00.050.396 I llm_load_print_meta: n_swa            = 0
0.00.050.396 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.396 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.397 I llm_load_print_meta: n_gqa            = 1
0.00.050.398 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.398 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.399 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.399 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.399 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.400 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.400 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.400 I llm_load_print_meta: n_ff             = 8192
0.00.050.401 I llm_load_print_meta: n_expert         = 0
0.00.050.401 I llm_load_print_meta: n_expert_used    = 0
0.00.050.401 I llm_load_print_meta: causal attn      = 1
0.00.050.401 I llm_load_print_meta: pooling type     = 0
0.00.050.401 I llm_load_print_meta: rope type        = 2
0.00.050.402 I llm_load_print_meta: rope scaling     = linear
0.00.050.404 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.405 I llm_load_print_meta: freq_scale_train = 1
0.00.050.405 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.405 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.405 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.406 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.406 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.406 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.406 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.418 I llm_load_print_meta: model type       = 1.4B
0.00.050.418 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.418 I llm_load_print_meta: model params     = 1.41 B
0.00.050.419 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.419 I llm_load_print_meta: general.name     = 1.4B
0.00.050.419 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.419 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.419 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.420 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.420 I llm_load_print_meta: LF token         = 128 ''
0.00.050.420 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.420 I llm_load_print_meta: max token length = 1024
0.00.052.468 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.468 I llm_load_tensors: offloading output layer to GPU
0.00.052.468 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.478 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.480 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.504 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.505 I llama_new_context_with_model: n_ctx         = 128
0.00.053.505 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.505 I llama_new_context_with_model: n_batch       = 128
0.00.053.505 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.506 I llama_new_context_with_model: flash_attn    = 0
0.00.053.506 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.506 I llama_new_context_with_model: freq_scale    = 1
0.00.053.507 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.507 I ggml_metal_init: allocating
0.00.053.510 I ggml_metal_init: found device: Apple M4
0.00.053.512 I ggml_metal_init: picking default device: Apple M4
0.00.054.060 I ggml_metal_init: using embedded metal library
0.00.055.996 I ggml_metal_init: GPU name:   Apple M4
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.998 I ggml_metal_init: simdgroup reduction   = true
0.00.055.998 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.998 I ggml_metal_init: has bfloat            = true
0.00.055.998 I ggml_metal_init: use bfloat            = true
0.00.056.000 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.463 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.466 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.481 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.438 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.439 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.439 I llama_new_context_with_model: graph nodes  = 967
0.00.066.440 I llama_new_context_with_model: graph splits = 2
0.00.066.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.726 I 
0.00.698.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.698.747 I perplexity: tokenizing the input ..
0.00.706.064 I perplexity: tokenization took 7.316 ms
0.00.706.075 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.133 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.841.293 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.841.311 I llama_perf_context_print:        load time =     690.10 ms
0.00.841.312 I llama_perf_context_print: prompt eval time =     133.83 ms /   128 tokens (    1.05 ms per token,   956.41 tokens per second)
0.00.841.313 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.313 I llama_perf_context_print:       total time =     142.58 ms /   129 tokens
0.00.841.704 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.078s
sys	0m0.120s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.595 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.081 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.086 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.091 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.092 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.092 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.093 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.096 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.099 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.099 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.739 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.740 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.741 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.741 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.742 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.742 I llama_model_loader: - type  f32:  194 tensors
0.00.023.742 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.743 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.743 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.802 I llm_load_vocab: special tokens cache size = 25
0.00.049.965 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.968 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.968 I llm_load_print_meta: arch             = gptneox
0.00.049.969 I llm_load_print_meta: vocab type       = BPE
0.00.049.969 I llm_load_print_meta: n_vocab          = 50304
0.00.049.969 I llm_load_print_meta: n_merges         = 50009
0.00.049.969 I llm_load_print_meta: vocab_only       = 0
0.00.049.969 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.970 I llm_load_print_meta: n_embd           = 2048
0.00.049.970 I llm_load_print_meta: n_layer          = 24
0.00.049.973 I llm_load_print_meta: n_head           = 16
0.00.049.974 I llm_load_print_meta: n_head_kv        = 16
0.00.049.974 I llm_load_print_meta: n_rot            = 32
0.00.049.974 I llm_load_print_meta: n_swa            = 0
0.00.049.974 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.977 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.978 I llm_load_print_meta: n_gqa            = 1
0.00.049.978 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.979 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.980 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.980 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.980 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.981 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.981 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.982 I llm_load_print_meta: n_ff             = 8192
0.00.049.982 I llm_load_print_meta: n_expert         = 0
0.00.049.982 I llm_load_print_meta: n_expert_used    = 0
0.00.049.982 I llm_load_print_meta: causal attn      = 1
0.00.049.982 I llm_load_print_meta: pooling type     = 0
0.00.049.982 I llm_load_print_meta: rope type        = 2
0.00.049.982 I llm_load_print_meta: rope scaling     = linear
0.00.049.983 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.983 I llm_load_print_meta: freq_scale_train = 1
0.00.049.983 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.984 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.984 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.984 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.984 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.984 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.984 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.996 I llm_load_print_meta: model type       = 1.4B
0.00.049.997 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.997 I llm_load_print_meta: model params     = 1.41 B
0.00.049.997 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.998 I llm_load_print_meta: general.name     = 1.4B
0.00.049.998 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.998 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.998 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.998 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.999 I llm_load_print_meta: LF token         = 128 ''
0.00.049.999 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.999 I llm_load_print_meta: max token length = 1024
0.00.051.520 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.520 I llm_load_tensors: offloading output layer to GPU
0.00.051.521 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.530 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.531 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.352 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.353 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.353 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.353 I llama_new_context_with_model: n_batch       = 2048
0.00.052.353 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.354 I llama_new_context_with_model: flash_attn    = 0
0.00.052.354 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.354 I llama_new_context_with_model: freq_scale    = 1
0.00.052.355 I ggml_metal_init: allocating
0.00.052.358 I ggml_metal_init: found device: Apple M4
0.00.052.360 I ggml_metal_init: picking default device: Apple M4
0.00.052.935 I ggml_metal_init: using embedded metal library
0.00.054.867 I ggml_metal_init: GPU name:   Apple M4
0.00.054.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.869 I ggml_metal_init: simdgroup reduction   = true
0.00.054.869 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.869 I ggml_metal_init: has bfloat            = true
0.00.054.870 I ggml_metal_init: use bfloat            = true
0.00.054.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.194 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.200 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.218 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.251 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.253 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.253 I llama_new_context_with_model: graph nodes  = 967
0.00.083.254 I llama_new_context_with_model: graph splits = 2
0.00.083.275 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.519.708 I main: llama threadpool init, n_threads = 4
0.00.519.752 I 
0.00.519.776 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.519.776 I 
0.00.520.015 I sampler seed: 1234
0.00.520.020 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.520.062 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.520.067 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.520.067 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.204.390 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.204.390 I llama_perf_context_print:        load time =     510.11 ms
0.01.204.391 I llama_perf_context_print: prompt eval time =      39.73 ms /     7 tokens (    5.68 ms per token,   176.18 tokens per second)
0.01.204.392 I llama_perf_context_print:        eval time =     641.58 ms /    63 runs   (   10.18 ms per token,    98.20 tokens per second)
0.01.204.392 I llama_perf_context_print:       total time =     684.69 ms /    70 tokens
0.01.204.567 I ggml_metal_free: deallocating

real	0m1.223s
user	0m0.109s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.132 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.674 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.679 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.683 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.684 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.684 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.684 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.687 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.449 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.479 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.246 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.248 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.248 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.249 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.249 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.249 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.250 I llama_model_loader: - type  f32:  194 tensors
0.00.023.250 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.250 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.250 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.416 I llm_load_vocab: special tokens cache size = 25
0.00.049.402 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.404 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.405 I llm_load_print_meta: arch             = gptneox
0.00.049.405 I llm_load_print_meta: vocab type       = BPE
0.00.049.405 I llm_load_print_meta: n_vocab          = 50304
0.00.049.406 I llm_load_print_meta: n_merges         = 50009
0.00.049.406 I llm_load_print_meta: vocab_only       = 0
0.00.049.406 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.406 I llm_load_print_meta: n_embd           = 2048
0.00.049.406 I llm_load_print_meta: n_layer          = 24
0.00.049.409 I llm_load_print_meta: n_head           = 16
0.00.049.410 I llm_load_print_meta: n_head_kv        = 16
0.00.049.410 I llm_load_print_meta: n_rot            = 32
0.00.049.410 I llm_load_print_meta: n_swa            = 0
0.00.049.410 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.410 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.411 I llm_load_print_meta: n_gqa            = 1
0.00.049.412 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.412 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.413 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.413 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.413 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.414 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.414 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.414 I llm_load_print_meta: n_ff             = 8192
0.00.049.415 I llm_load_print_meta: n_expert         = 0
0.00.049.415 I llm_load_print_meta: n_expert_used    = 0
0.00.049.415 I llm_load_print_meta: causal attn      = 1
0.00.049.415 I llm_load_print_meta: pooling type     = 0
0.00.049.415 I llm_load_print_meta: rope type        = 2
0.00.049.415 I llm_load_print_meta: rope scaling     = linear
0.00.049.416 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.416 I llm_load_print_meta: freq_scale_train = 1
0.00.049.416 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.417 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.417 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.417 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.417 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.417 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.417 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.428 I llm_load_print_meta: model type       = 1.4B
0.00.049.429 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.429 I llm_load_print_meta: model params     = 1.41 B
0.00.049.429 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.430 I llm_load_print_meta: general.name     = 1.4B
0.00.049.430 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.430 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.431 I llm_load_print_meta: LF token         = 128 ''
0.00.049.431 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.431 I llm_load_print_meta: max token length = 1024
0.00.050.972 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.972 I llm_load_tensors: offloading output layer to GPU
0.00.050.972 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.981 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.982 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.860 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.861 I llama_new_context_with_model: n_ctx         = 128
0.00.051.861 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.861 I llama_new_context_with_model: n_batch       = 128
0.00.051.861 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.861 I llama_new_context_with_model: flash_attn    = 0
0.00.051.862 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.862 I llama_new_context_with_model: freq_scale    = 1
0.00.051.862 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.863 I ggml_metal_init: allocating
0.00.051.869 I ggml_metal_init: found device: Apple M4
0.00.051.871 I ggml_metal_init: picking default device: Apple M4
0.00.052.434 I ggml_metal_init: using embedded metal library
0.00.054.386 I ggml_metal_init: GPU name:   Apple M4
0.00.054.388 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.388 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.388 I ggml_metal_init: simdgroup reduction   = true
0.00.054.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.389 I ggml_metal_init: has bfloat            = true
0.00.054.389 I ggml_metal_init: use bfloat            = true
0.00.054.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.390 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.414 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.418 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.434 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.325 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.326 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.327 I llama_new_context_with_model: graph nodes  = 967
0.00.064.327 I llama_new_context_with_model: graph splits = 2
0.00.064.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.468.314 I 
0.00.468.336 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.468.340 I perplexity: tokenizing the input ..
0.00.475.907 I perplexity: tokenization took 7.567 ms
0.00.475.921 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.607.765 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.608.929 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.608.943 I llama_perf_context_print:        load time =     459.18 ms
0.00.608.944 I llama_perf_context_print: prompt eval time =     131.62 ms /   128 tokens (    1.03 ms per token,   972.47 tokens per second)
0.00.608.945 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.608.945 I llama_perf_context_print:       total time =     140.63 ms /   129 tokens
0.00.609.326 I ggml_metal_free: deallocating

real	0m0.625s
user	0m0.076s
sys	0m0.081s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.539 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.124 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.131 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.131 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.132 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.133 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.133 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.134 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.134 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.136 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.136 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.987 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.110 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.926 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.927 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.927 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.928 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.928 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.928 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.929 I llama_model_loader: - type  f32:  194 tensors
0.00.023.929 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.929 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.929 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.930 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.528 I llm_load_vocab: special tokens cache size = 25
0.00.050.542 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.544 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.545 I llm_load_print_meta: arch             = gptneox
0.00.050.545 I llm_load_print_meta: vocab type       = BPE
0.00.050.545 I llm_load_print_meta: n_vocab          = 50304
0.00.050.546 I llm_load_print_meta: n_merges         = 50009
0.00.050.546 I llm_load_print_meta: vocab_only       = 0
0.00.050.546 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.546 I llm_load_print_meta: n_embd           = 2048
0.00.050.546 I llm_load_print_meta: n_layer          = 24
0.00.050.551 I llm_load_print_meta: n_head           = 16
0.00.050.551 I llm_load_print_meta: n_head_kv        = 16
0.00.050.554 I llm_load_print_meta: n_rot            = 32
0.00.050.554 I llm_load_print_meta: n_swa            = 0
0.00.050.554 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.554 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.555 I llm_load_print_meta: n_gqa            = 1
0.00.050.555 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.556 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.557 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.557 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.557 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.557 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.557 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.558 I llm_load_print_meta: n_ff             = 8192
0.00.050.558 I llm_load_print_meta: n_expert         = 0
0.00.050.559 I llm_load_print_meta: n_expert_used    = 0
0.00.050.559 I llm_load_print_meta: causal attn      = 1
0.00.050.559 I llm_load_print_meta: pooling type     = 0
0.00.050.559 I llm_load_print_meta: rope type        = 2
0.00.050.559 I llm_load_print_meta: rope scaling     = linear
0.00.050.560 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.560 I llm_load_print_meta: freq_scale_train = 1
0.00.050.560 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.560 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.562 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.562 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.562 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.562 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.562 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.574 I llm_load_print_meta: model type       = 1.4B
0.00.050.575 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.575 I llm_load_print_meta: model params     = 1.41 B
0.00.050.575 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.576 I llm_load_print_meta: general.name     = 1.4B
0.00.050.576 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.576 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.576 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.576 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.577 I llm_load_print_meta: LF token         = 128 ''
0.00.050.577 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.577 I llm_load_print_meta: max token length = 1024
0.00.052.559 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.559 I llm_load_tensors: offloading output layer to GPU
0.00.052.559 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.569 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.570 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.517 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.518 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.518 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.519 I llama_new_context_with_model: n_batch       = 2048
0.00.053.519 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.519 I llama_new_context_with_model: flash_attn    = 0
0.00.053.519 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.520 I llama_new_context_with_model: freq_scale    = 1
0.00.053.520 I ggml_metal_init: allocating
0.00.053.527 I ggml_metal_init: found device: Apple M4
0.00.053.529 I ggml_metal_init: picking default device: Apple M4
0.00.054.072 I ggml_metal_init: using embedded metal library
0.00.056.020 I ggml_metal_init: GPU name:   Apple M4
0.00.056.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.023 I ggml_metal_init: simdgroup reduction   = true
0.00.056.023 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.023 I ggml_metal_init: has bfloat            = true
0.00.056.023 I ggml_metal_init: use bfloat            = true
0.00.056.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.240 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.248 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.275 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.278 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.279 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.280 I llama_new_context_with_model: graph nodes  = 967
0.00.085.280 I llama_new_context_with_model: graph splits = 2
0.00.085.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.406 I main: llama threadpool init, n_threads = 4
0.00.613.447 I 
0.00.613.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.613.468 I 
0.00.613.692 I sampler seed: 1234
0.00.613.697 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.613.735 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.613.739 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.613.739 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.361.246 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.361.247 I llama_perf_context_print:        load time =     603.86 ms
0.01.361.248 I llama_perf_context_print: prompt eval time =      35.71 ms /     7 tokens (    5.10 ms per token,   196.00 tokens per second)
0.01.361.250 I llama_perf_context_print:        eval time =     708.72 ms /    63 runs   (   11.25 ms per token,    88.89 tokens per second)
0.01.361.250 I llama_perf_context_print:       total time =     747.85 ms /    70 tokens
0.01.361.428 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.109s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.742 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.436 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.440 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.443 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.443 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.443 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.444 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.445 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.446 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.446 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.447 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.354 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.319 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.320 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.320 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.321 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.321 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.321 I llama_model_loader: - type  f32:  194 tensors
0.00.023.322 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.322 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.322 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.322 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.120 I llm_load_vocab: special tokens cache size = 25
0.00.050.285 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.288 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.288 I llm_load_print_meta: arch             = gptneox
0.00.050.288 I llm_load_print_meta: vocab type       = BPE
0.00.050.289 I llm_load_print_meta: n_vocab          = 50304
0.00.050.289 I llm_load_print_meta: n_merges         = 50009
0.00.050.289 I llm_load_print_meta: vocab_only       = 0
0.00.050.289 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.289 I llm_load_print_meta: n_embd           = 2048
0.00.050.290 I llm_load_print_meta: n_layer          = 24
0.00.050.292 I llm_load_print_meta: n_head           = 16
0.00.050.293 I llm_load_print_meta: n_head_kv        = 16
0.00.050.293 I llm_load_print_meta: n_rot            = 32
0.00.050.293 I llm_load_print_meta: n_swa            = 0
0.00.050.293 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.293 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.294 I llm_load_print_meta: n_gqa            = 1
0.00.050.295 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.295 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.296 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.296 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.297 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.297 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.297 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.298 I llm_load_print_meta: n_ff             = 8192
0.00.050.298 I llm_load_print_meta: n_expert         = 0
0.00.050.301 I llm_load_print_meta: n_expert_used    = 0
0.00.050.301 I llm_load_print_meta: causal attn      = 1
0.00.050.301 I llm_load_print_meta: pooling type     = 0
0.00.050.301 I llm_load_print_meta: rope type        = 2
0.00.050.301 I llm_load_print_meta: rope scaling     = linear
0.00.050.302 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.302 I llm_load_print_meta: freq_scale_train = 1
0.00.050.302 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.303 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.303 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.303 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.303 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.303 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.303 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.315 I llm_load_print_meta: model type       = 1.4B
0.00.050.315 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.316 I llm_load_print_meta: model params     = 1.41 B
0.00.050.316 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.317 I llm_load_print_meta: general.name     = 1.4B
0.00.050.317 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.318 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.318 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.318 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.318 I llm_load_print_meta: LF token         = 128 ''
0.00.050.318 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.319 I llm_load_print_meta: max token length = 1024
0.00.052.234 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.234 I llm_load_tensors: offloading output layer to GPU
0.00.052.234 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.244 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.245 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.146 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.147 I llama_new_context_with_model: n_ctx         = 128
0.00.053.147 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.148 I llama_new_context_with_model: n_batch       = 128
0.00.053.148 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.148 I llama_new_context_with_model: flash_attn    = 0
0.00.053.148 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.149 I llama_new_context_with_model: freq_scale    = 1
0.00.053.149 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.149 I ggml_metal_init: allocating
0.00.053.152 I ggml_metal_init: found device: Apple M4
0.00.053.154 I ggml_metal_init: picking default device: Apple M4
0.00.053.699 I ggml_metal_init: using embedded metal library
0.00.055.623 I ggml_metal_init: GPU name:   Apple M4
0.00.055.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.625 I ggml_metal_init: simdgroup reduction   = true
0.00.055.626 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.626 I ggml_metal_init: has bfloat            = true
0.00.055.626 I ggml_metal_init: use bfloat            = true
0.00.055.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.627 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.908 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.912 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.925 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.838 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.839 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.840 I llama_new_context_with_model: graph nodes  = 967
0.00.065.840 I llama_new_context_with_model: graph splits = 2
0.00.065.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.283 I 
0.00.558.305 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.558.308 I perplexity: tokenizing the input ..
0.00.565.868 I perplexity: tokenization took 7.558 ms
0.00.565.879 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.697.925 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.699.180 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.699.192 I llama_perf_context_print:        load time =     549.54 ms
0.00.699.193 I llama_perf_context_print: prompt eval time =     131.82 ms /   128 tokens (    1.03 ms per token,   971.03 tokens per second)
0.00.699.194 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.699.194 I llama_perf_context_print:       total time =     140.91 ms /   129 tokens
0.00.699.671 I ggml_metal_free: deallocating

real	0m0.713s
user	0m0.077s
sys	0m0.100s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.012.309 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.772 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.778 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.784 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.785 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.788 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.788 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.790 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.790 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.791 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.421 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.424 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.424 I llama_model_loader: - type  f32:  194 tensors
0.00.027.425 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.425 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.425 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.917 I llm_load_vocab: special tokens cache size = 25
0.00.054.081 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.084 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.084 I llm_load_print_meta: arch             = gptneox
0.00.054.084 I llm_load_print_meta: vocab type       = BPE
0.00.054.085 I llm_load_print_meta: n_vocab          = 50304
0.00.054.085 I llm_load_print_meta: n_merges         = 50009
0.00.054.085 I llm_load_print_meta: vocab_only       = 0
0.00.054.085 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.085 I llm_load_print_meta: n_embd           = 2048
0.00.054.086 I llm_load_print_meta: n_layer          = 24
0.00.054.089 I llm_load_print_meta: n_head           = 16
0.00.054.089 I llm_load_print_meta: n_head_kv        = 16
0.00.054.089 I llm_load_print_meta: n_rot            = 32
0.00.054.092 I llm_load_print_meta: n_swa            = 0
0.00.054.092 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.092 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.093 I llm_load_print_meta: n_gqa            = 1
0.00.054.094 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.095 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.095 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.096 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.096 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.096 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.096 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.097 I llm_load_print_meta: n_ff             = 8192
0.00.054.097 I llm_load_print_meta: n_expert         = 0
0.00.054.098 I llm_load_print_meta: n_expert_used    = 0
0.00.054.099 I llm_load_print_meta: causal attn      = 1
0.00.054.099 I llm_load_print_meta: pooling type     = 0
0.00.054.099 I llm_load_print_meta: rope type        = 2
0.00.054.099 I llm_load_print_meta: rope scaling     = linear
0.00.054.100 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.100 I llm_load_print_meta: freq_scale_train = 1
0.00.054.101 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.102 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.102 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.102 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.102 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.102 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.102 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.114 I llm_load_print_meta: model type       = 1.4B
0.00.054.114 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.054.115 I llm_load_print_meta: model params     = 1.41 B
0.00.054.115 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.054.115 I llm_load_print_meta: general.name     = 1.4B
0.00.054.116 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.116 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.116 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.116 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.116 I llm_load_print_meta: LF token         = 128 ''
0.00.054.116 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.117 I llm_load_print_meta: max token length = 1024
0.00.055.743 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.743 I llm_load_tensors: offloading output layer to GPU
0.00.055.743 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.753 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.754 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.595 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.596 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.596 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.596 I llama_new_context_with_model: n_batch       = 2048
0.00.056.596 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.596 I llama_new_context_with_model: flash_attn    = 0
0.00.056.597 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.597 I llama_new_context_with_model: freq_scale    = 1
0.00.056.598 I ggml_metal_init: allocating
0.00.056.604 I ggml_metal_init: found device: Apple M4
0.00.056.607 I ggml_metal_init: picking default device: Apple M4
0.00.057.152 I ggml_metal_init: using embedded metal library
0.00.059.117 I ggml_metal_init: GPU name:   Apple M4
0.00.059.118 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.119 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.119 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.120 I ggml_metal_init: simdgroup reduction   = true
0.00.059.120 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.120 I ggml_metal_init: has bfloat            = true
0.00.059.121 I ggml_metal_init: use bfloat            = true
0.00.059.121 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.122 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.509 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.515 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.534 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.557 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.559 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.559 I llama_new_context_with_model: graph nodes  = 967
0.00.087.559 I llama_new_context_with_model: graph splits = 2
0.00.087.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.199 I main: llama threadpool init, n_threads = 4
0.00.625.237 I 
0.00.625.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.625.256 I 
0.00.625.424 I sampler seed: 1234
0.00.625.428 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.438 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.440 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.405.107 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60169.49 tokens per second)
0.01.405.108 I llama_perf_context_print:        load time =     612.89 ms
0.01.405.108 I llama_perf_context_print: prompt eval time =      36.54 ms /     7 tokens (    5.22 ms per token,   191.56 tokens per second)
0.01.405.109 I llama_perf_context_print:        eval time =     740.13 ms /    63 runs   (   11.75 ms per token,    85.12 tokens per second)
0.01.405.112 I llama_perf_context_print:       total time =     779.91 ms /    70 tokens
0.01.405.289 I ggml_metal_free: deallocating

real	0m1.423s
user	0m0.110s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.479 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.181 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.188 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.191 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.090 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.950 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.950 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.951 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.952 I llama_model_loader: - type  f32:  194 tensors
0.00.022.952 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.952 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.952 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.015 I llm_load_vocab: special tokens cache size = 25
0.00.049.017 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.021 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.021 I llm_load_print_meta: arch             = gptneox
0.00.049.022 I llm_load_print_meta: vocab type       = BPE
0.00.049.022 I llm_load_print_meta: n_vocab          = 50304
0.00.049.022 I llm_load_print_meta: n_merges         = 50009
0.00.049.022 I llm_load_print_meta: vocab_only       = 0
0.00.049.024 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.024 I llm_load_print_meta: n_embd           = 2048
0.00.049.024 I llm_load_print_meta: n_layer          = 24
0.00.049.027 I llm_load_print_meta: n_head           = 16
0.00.049.028 I llm_load_print_meta: n_head_kv        = 16
0.00.049.028 I llm_load_print_meta: n_rot            = 32
0.00.049.028 I llm_load_print_meta: n_swa            = 0
0.00.049.029 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.029 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.029 I llm_load_print_meta: n_gqa            = 1
0.00.049.030 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.033 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.033 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.034 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.034 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.036 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.036 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.037 I llm_load_print_meta: n_ff             = 8192
0.00.049.037 I llm_load_print_meta: n_expert         = 0
0.00.049.037 I llm_load_print_meta: n_expert_used    = 0
0.00.049.037 I llm_load_print_meta: causal attn      = 1
0.00.049.037 I llm_load_print_meta: pooling type     = 0
0.00.049.038 I llm_load_print_meta: rope type        = 2
0.00.049.038 I llm_load_print_meta: rope scaling     = linear
0.00.049.038 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.038 I llm_load_print_meta: freq_scale_train = 1
0.00.049.040 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.040 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.040 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.041 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.041 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.041 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.041 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.047 I llm_load_print_meta: model type       = 1.4B
0.00.049.048 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.048 I llm_load_print_meta: model params     = 1.41 B
0.00.049.048 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.049 I llm_load_print_meta: general.name     = 1.4B
0.00.049.049 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.049 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.049 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.049 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.050 I llm_load_print_meta: LF token         = 128 ''
0.00.049.050 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.050 I llm_load_print_meta: max token length = 1024
0.00.050.864 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.864 I llm_load_tensors: offloading output layer to GPU
0.00.050.864 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.870 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.870 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.910 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.911 I llama_new_context_with_model: n_ctx         = 128
0.00.051.911 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.911 I llama_new_context_with_model: n_batch       = 128
0.00.051.911 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.911 I llama_new_context_with_model: flash_attn    = 0
0.00.051.912 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.912 I llama_new_context_with_model: freq_scale    = 1
0.00.051.912 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.913 I ggml_metal_init: allocating
0.00.051.916 I ggml_metal_init: found device: Apple M4
0.00.051.918 I ggml_metal_init: picking default device: Apple M4
0.00.052.464 I ggml_metal_init: using embedded metal library
0.00.054.414 I ggml_metal_init: GPU name:   Apple M4
0.00.054.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.417 I ggml_metal_init: simdgroup reduction   = true
0.00.054.417 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.417 I ggml_metal_init: has bfloat            = true
0.00.054.417 I ggml_metal_init: use bfloat            = true
0.00.054.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.418 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.455 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.460 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.475 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.386 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.387 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.388 I llama_new_context_with_model: graph nodes  = 967
0.00.064.388 I llama_new_context_with_model: graph splits = 2
0.00.064.400 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.123 I 
0.00.549.142 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.549.145 I perplexity: tokenizing the input ..
0.00.556.270 I perplexity: tokenization took 7.123 ms
0.00.556.281 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.690.753 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.692.109 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.692.130 I llama_perf_context_print:        load time =     540.64 ms
0.00.692.132 I llama_perf_context_print: prompt eval time =     134.24 ms /   128 tokens (    1.05 ms per token,   953.54 tokens per second)
0.00.692.132 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.692.133 I llama_perf_context_print:       total time =     143.01 ms /   129 tokens
0.00.692.648 I ggml_metal_free: deallocating

real	0m0.706s
user	0m0.076s
sys	0m0.100s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.216 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.180 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.181 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.181 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.182 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.183 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.184 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.184 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.185 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.186 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.186 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.187 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.023 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.131 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.925 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.926 I llama_model_loader: - type  f32:  194 tensors
0.00.024.926 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.927 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.120 I llm_load_vocab: special tokens cache size = 25
0.00.051.254 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.256 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.257 I llm_load_print_meta: arch             = gptneox
0.00.051.257 I llm_load_print_meta: vocab type       = BPE
0.00.051.257 I llm_load_print_meta: n_vocab          = 50304
0.00.051.258 I llm_load_print_meta: n_merges         = 50009
0.00.051.258 I llm_load_print_meta: vocab_only       = 0
0.00.051.258 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.258 I llm_load_print_meta: n_embd           = 2048
0.00.051.258 I llm_load_print_meta: n_layer          = 24
0.00.051.261 I llm_load_print_meta: n_head           = 16
0.00.051.262 I llm_load_print_meta: n_head_kv        = 16
0.00.051.262 I llm_load_print_meta: n_rot            = 32
0.00.051.263 I llm_load_print_meta: n_swa            = 0
0.00.051.263 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.263 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.264 I llm_load_print_meta: n_gqa            = 1
0.00.051.264 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.265 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.266 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.266 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.266 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.266 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.267 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.267 I llm_load_print_meta: n_ff             = 8192
0.00.051.267 I llm_load_print_meta: n_expert         = 0
0.00.051.268 I llm_load_print_meta: n_expert_used    = 0
0.00.051.268 I llm_load_print_meta: causal attn      = 1
0.00.051.269 I llm_load_print_meta: pooling type     = 0
0.00.051.269 I llm_load_print_meta: rope type        = 2
0.00.051.269 I llm_load_print_meta: rope scaling     = linear
0.00.051.271 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.272 I llm_load_print_meta: freq_scale_train = 1
0.00.051.272 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.272 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.272 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.272 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.273 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.273 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.273 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.284 I llm_load_print_meta: model type       = 1.4B
0.00.051.285 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.285 I llm_load_print_meta: model params     = 1.41 B
0.00.051.286 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.286 I llm_load_print_meta: general.name     = 1.4B
0.00.051.286 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.286 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.286 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.286 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.287 I llm_load_print_meta: LF token         = 128 ''
0.00.051.287 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.287 I llm_load_print_meta: max token length = 1024
0.00.052.900 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.900 I llm_load_tensors: offloading output layer to GPU
0.00.052.900 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.910 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.911 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.745 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.746 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.746 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.746 I llama_new_context_with_model: n_batch       = 2048
0.00.053.747 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.747 I llama_new_context_with_model: flash_attn    = 0
0.00.053.747 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.748 I llama_new_context_with_model: freq_scale    = 1
0.00.053.748 I ggml_metal_init: allocating
0.00.053.751 I ggml_metal_init: found device: Apple M4
0.00.053.753 I ggml_metal_init: picking default device: Apple M4
0.00.054.302 I ggml_metal_init: using embedded metal library
0.00.056.240 I ggml_metal_init: GPU name:   Apple M4
0.00.056.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.242 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.242 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.242 I ggml_metal_init: simdgroup reduction   = true
0.00.056.243 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.243 I ggml_metal_init: has bfloat            = true
0.00.056.243 I ggml_metal_init: use bfloat            = true
0.00.056.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.327 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.337 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.358 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.301 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.302 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.303 I llama_new_context_with_model: graph nodes  = 967
0.00.085.303 I llama_new_context_with_model: graph splits = 2
0.00.085.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.489 I main: llama threadpool init, n_threads = 4
0.00.717.537 I 
0.00.717.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.717.577 I 
0.00.717.809 I sampler seed: 1234
0.00.717.817 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.859 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.859 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.859 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.558.991 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60787.67 tokens per second)
0.01.558.992 I llama_perf_context_print:        load time =     708.27 ms
0.01.558.993 I llama_perf_context_print: prompt eval time =      38.64 ms /     7 tokens (    5.52 ms per token,   181.15 tokens per second)
0.01.558.993 I llama_perf_context_print:        eval time =     799.59 ms /    63 runs   (   12.69 ms per token,    78.79 tokens per second)
0.01.558.994 I llama_perf_context_print:       total time =     841.51 ms /    70 tokens
0.01.559.158 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.109s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.059 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.860 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.862 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.867 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.868 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.869 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.869 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.869 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.870 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.870 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.870 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.874 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.499 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.500 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.501 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.501 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.502 I llama_model_loader: - type  f32:  194 tensors
0.00.024.502 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.502 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.637 I llm_load_vocab: special tokens cache size = 25
0.00.050.790 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.793 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.793 I llm_load_print_meta: arch             = gptneox
0.00.050.793 I llm_load_print_meta: vocab type       = BPE
0.00.050.793 I llm_load_print_meta: n_vocab          = 50304
0.00.050.794 I llm_load_print_meta: n_merges         = 50009
0.00.050.794 I llm_load_print_meta: vocab_only       = 0
0.00.050.794 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.794 I llm_load_print_meta: n_embd           = 2048
0.00.050.794 I llm_load_print_meta: n_layer          = 24
0.00.050.797 I llm_load_print_meta: n_head           = 16
0.00.050.798 I llm_load_print_meta: n_head_kv        = 16
0.00.050.798 I llm_load_print_meta: n_rot            = 32
0.00.050.798 I llm_load_print_meta: n_swa            = 0
0.00.050.798 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.799 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.799 I llm_load_print_meta: n_gqa            = 1
0.00.050.800 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.801 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.801 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.802 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.802 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.802 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.802 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.804 I llm_load_print_meta: n_ff             = 8192
0.00.050.806 I llm_load_print_meta: n_expert         = 0
0.00.050.806 I llm_load_print_meta: n_expert_used    = 0
0.00.050.806 I llm_load_print_meta: causal attn      = 1
0.00.050.807 I llm_load_print_meta: pooling type     = 0
0.00.050.807 I llm_load_print_meta: rope type        = 2
0.00.050.807 I llm_load_print_meta: rope scaling     = linear
0.00.050.807 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.808 I llm_load_print_meta: freq_scale_train = 1
0.00.050.808 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.808 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.808 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.808 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.809 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.809 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.809 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.821 I llm_load_print_meta: model type       = 1.4B
0.00.050.821 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.821 I llm_load_print_meta: model params     = 1.41 B
0.00.050.822 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.822 I llm_load_print_meta: general.name     = 1.4B
0.00.050.822 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.822 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.822 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.822 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.823 I llm_load_print_meta: LF token         = 128 ''
0.00.050.823 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.823 I llm_load_print_meta: max token length = 1024
0.00.052.795 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.796 I llm_load_tensors: offloading output layer to GPU
0.00.052.796 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.806 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.807 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.727 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.728 I llama_new_context_with_model: n_ctx         = 128
0.00.053.728 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.728 I llama_new_context_with_model: n_batch       = 128
0.00.053.729 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.729 I llama_new_context_with_model: flash_attn    = 0
0.00.053.729 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.729 I llama_new_context_with_model: freq_scale    = 1
0.00.053.730 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.730 I ggml_metal_init: allocating
0.00.053.735 I ggml_metal_init: found device: Apple M4
0.00.053.739 I ggml_metal_init: picking default device: Apple M4
0.00.054.274 I ggml_metal_init: using embedded metal library
0.00.056.245 I ggml_metal_init: GPU name:   Apple M4
0.00.056.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.248 I ggml_metal_init: simdgroup reduction   = true
0.00.056.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.248 I ggml_metal_init: has bfloat            = true
0.00.056.248 I ggml_metal_init: use bfloat            = true
0.00.056.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.331 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.335 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.351 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.279 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.280 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.280 I llama_new_context_with_model: graph nodes  = 967
0.00.066.281 I llama_new_context_with_model: graph splits = 2
0.00.066.293 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.847 I 
0.00.649.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.649.875 I perplexity: tokenizing the input ..
0.00.657.183 I perplexity: tokenization took 7.307 ms
0.00.657.193 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.035 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.799.191 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.799.214 I llama_perf_context_print:        load time =     639.78 ms
0.00.799.215 I llama_perf_context_print: prompt eval time =     140.60 ms /   128 tokens (    1.10 ms per token,   910.41 tokens per second)
0.00.799.216 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.216 I llama_perf_context_print:       total time =     149.37 ms /   129 tokens
0.00.799.732 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.076s
sys	0m0.127s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.012 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.600 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.602 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.603 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.603 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.603 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.604 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.604 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.605 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.605 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.605 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.606 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.606 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.608 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.608 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.608 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.543 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.504 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.504 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.505 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.505 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.506 I llama_model_loader: - type  f32:  194 tensors
0.00.025.506 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.801 I llm_load_vocab: special tokens cache size = 25
0.00.051.800 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.803 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.803 I llm_load_print_meta: arch             = gptneox
0.00.051.803 I llm_load_print_meta: vocab type       = BPE
0.00.051.803 I llm_load_print_meta: n_vocab          = 50304
0.00.051.804 I llm_load_print_meta: n_merges         = 50009
0.00.051.804 I llm_load_print_meta: vocab_only       = 0
0.00.051.804 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.804 I llm_load_print_meta: n_embd           = 2048
0.00.051.804 I llm_load_print_meta: n_layer          = 24
0.00.051.807 I llm_load_print_meta: n_head           = 16
0.00.051.808 I llm_load_print_meta: n_head_kv        = 16
0.00.051.810 I llm_load_print_meta: n_rot            = 32
0.00.051.810 I llm_load_print_meta: n_swa            = 0
0.00.051.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.812 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.812 I llm_load_print_meta: n_gqa            = 1
0.00.051.813 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.814 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.815 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.815 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.815 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.815 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.815 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.816 I llm_load_print_meta: n_ff             = 8192
0.00.051.816 I llm_load_print_meta: n_expert         = 0
0.00.051.817 I llm_load_print_meta: n_expert_used    = 0
0.00.051.817 I llm_load_print_meta: causal attn      = 1
0.00.051.819 I llm_load_print_meta: pooling type     = 0
0.00.051.820 I llm_load_print_meta: rope type        = 2
0.00.051.820 I llm_load_print_meta: rope scaling     = linear
0.00.051.820 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.821 I llm_load_print_meta: freq_scale_train = 1
0.00.051.821 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.821 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.821 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.821 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.821 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.822 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.828 I llm_load_print_meta: model type       = 1.4B
0.00.051.829 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.829 I llm_load_print_meta: model params     = 1.41 B
0.00.051.830 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.831 I llm_load_print_meta: general.name     = 1.4B
0.00.051.831 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.831 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.831 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.832 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.832 I llm_load_print_meta: LF token         = 128 ''
0.00.051.832 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.833 I llm_load_print_meta: max token length = 1024
0.00.053.600 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.601 I llm_load_tensors: offloading output layer to GPU
0.00.053.601 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.606 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.606 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.526 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.527 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.527 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.527 I llama_new_context_with_model: n_batch       = 2048
0.00.054.527 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.527 I llama_new_context_with_model: flash_attn    = 0
0.00.054.528 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.528 I llama_new_context_with_model: freq_scale    = 1
0.00.054.528 I ggml_metal_init: allocating
0.00.054.533 I ggml_metal_init: found device: Apple M4
0.00.054.536 I ggml_metal_init: picking default device: Apple M4
0.00.055.069 I ggml_metal_init: using embedded metal library
0.00.056.955 I ggml_metal_init: GPU name:   Apple M4
0.00.056.957 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.957 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.957 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.958 I ggml_metal_init: simdgroup reduction   = true
0.00.056.959 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.959 I ggml_metal_init: has bfloat            = true
0.00.056.960 I ggml_metal_init: use bfloat            = true
0.00.056.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.636 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.645 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.665 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.626 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.627 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.628 I llama_new_context_with_model: graph nodes  = 967
0.00.085.628 I llama_new_context_with_model: graph splits = 2
0.00.085.650 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.033 I main: llama threadpool init, n_threads = 4
0.00.758.066 I 
0.00.758.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.758.098 I 
0.00.758.343 I sampler seed: 1234
0.00.758.348 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.359 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.359 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.361 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.633.576 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.633.577 I llama_perf_context_print:        load time =     748.02 ms
0.01.633.577 I llama_perf_context_print: prompt eval time =      38.51 ms /     7 tokens (    5.50 ms per token,   181.77 tokens per second)
0.01.633.578 I llama_perf_context_print:        eval time =     833.71 ms /    63 runs   (   13.23 ms per token,    75.57 tokens per second)
0.01.633.578 I llama_perf_context_print:       total time =     875.54 ms /    70 tokens
0.01.633.769 I ggml_metal_free: deallocating

real	0m1.651s
user	0m0.109s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4200 (46c69e0e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.867 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.482 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.483 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.489 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.490 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.490 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.491 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.491 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.492 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.492 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.492 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.493 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.493 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.495 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.495 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.345 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.182 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.184 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.184 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.184 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.184 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.185 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.185 I llama_model_loader: - type  f32:  194 tensors
0.00.023.186 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.115 I llm_load_vocab: special tokens cache size = 25
0.00.049.152 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.155 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.155 I llm_load_print_meta: arch             = gptneox
0.00.049.156 I llm_load_print_meta: vocab type       = BPE
0.00.049.156 I llm_load_print_meta: n_vocab          = 50304
0.00.049.156 I llm_load_print_meta: n_merges         = 50009
0.00.049.156 I llm_load_print_meta: vocab_only       = 0
0.00.049.157 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.157 I llm_load_print_meta: n_embd           = 2048
0.00.049.157 I llm_load_print_meta: n_layer          = 24
0.00.049.159 I llm_load_print_meta: n_head           = 16
0.00.049.160 I llm_load_print_meta: n_head_kv        = 16
0.00.049.160 I llm_load_print_meta: n_rot            = 32
0.00.049.161 I llm_load_print_meta: n_swa            = 0
0.00.049.161 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.161 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.162 I llm_load_print_meta: n_gqa            = 1
0.00.049.162 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.163 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.164 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.164 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.164 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.164 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.164 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.165 I llm_load_print_meta: n_ff             = 8192
0.00.049.165 I llm_load_print_meta: n_expert         = 0
0.00.049.165 I llm_load_print_meta: n_expert_used    = 0
0.00.049.165 I llm_load_print_meta: causal attn      = 1
0.00.049.166 I llm_load_print_meta: pooling type     = 0
0.00.049.166 I llm_load_print_meta: rope type        = 2
0.00.049.166 I llm_load_print_meta: rope scaling     = linear
0.00.049.166 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.167 I llm_load_print_meta: freq_scale_train = 1
0.00.049.167 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.167 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.167 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.167 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.167 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.167 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.168 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.179 I llm_load_print_meta: model type       = 1.4B
0.00.049.179 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.179 I llm_load_print_meta: model params     = 1.41 B
0.00.049.180 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.180 I llm_load_print_meta: general.name     = 1.4B
0.00.049.180 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.180 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.180 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.180 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.181 I llm_load_print_meta: LF token         = 128 ''
0.00.049.181 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.181 I llm_load_print_meta: max token length = 1024
0.00.050.670 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.670 I llm_load_tensors: offloading output layer to GPU
0.00.050.670 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.679 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.680 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.504 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.505 I llama_new_context_with_model: n_ctx         = 128
0.00.051.505 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.505 I llama_new_context_with_model: n_batch       = 128
0.00.051.505 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.506 I llama_new_context_with_model: flash_attn    = 0
0.00.051.506 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.506 I llama_new_context_with_model: freq_scale    = 1
0.00.051.506 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.507 I ggml_metal_init: allocating
0.00.051.510 I ggml_metal_init: found device: Apple M4
0.00.051.512 I ggml_metal_init: picking default device: Apple M4
0.00.052.053 I ggml_metal_init: using embedded metal library
0.00.053.979 I ggml_metal_init: GPU name:   Apple M4
0.00.053.981 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.981 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.982 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.982 I ggml_metal_init: simdgroup reduction   = true
0.00.053.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.982 I ggml_metal_init: has bfloat            = true
0.00.053.982 I ggml_metal_init: use bfloat            = true
0.00.053.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.982 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.984 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.998 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.859 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.860 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.860 I llama_new_context_with_model: graph nodes  = 967
0.00.063.861 I llama_new_context_with_model: graph splits = 2
0.00.063.873 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.946 I 
0.00.435.005 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.435.015 I perplexity: tokenizing the input ..
0.00.442.694 I perplexity: tokenization took 7.677 ms
0.00.442.710 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.582.801 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.583.969 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.583.983 I llama_perf_context_print:        load time =     426.07 ms
0.00.583.985 I llama_perf_context_print: prompt eval time =     139.84 ms /   128 tokens (    1.09 ms per token,   915.30 tokens per second)
0.00.583.986 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.583.986 I llama_perf_context_print:       total time =     149.05 ms /   129 tokens
0.00.584.301 I ggml_metal_free: deallocating

real	0m0.597s
user	0m0.076s
sys	0m0.093s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4200 (46c69e0e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11560a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11560a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11560ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11560b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11560b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11560bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11560c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11560cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11560d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11560d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11560da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11560df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11560eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11560f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11560fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x115610190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1156108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x115610fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1156116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x115611ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1156125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x115612d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x115613420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x115613cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1156143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1156146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x115614cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x115615920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115615e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115616120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1156165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115616880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115617110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115617650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115617910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115617db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115618250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1156186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115618b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115619030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1156194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115619970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x115619e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11561a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11561a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11561ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11561b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11561bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11561c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11561c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11561cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11561d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11561d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11561df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11561e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11561eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11561f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11561f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11561f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x115620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1156203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115620860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x115620d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1156211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x115621640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x115621ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x115621f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x115622420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1156228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x115622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x115623200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1156236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x115623b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115623fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115624480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115624920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115624dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115625260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115625700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x115625ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x115626040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1156264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115626980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115626e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1156272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115627760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x115627c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1156280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115628540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1156289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115628e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115629320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1156297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11562a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11562a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11562aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11561b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11562b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11562b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11562b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11562be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11562c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11562c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11562cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11562d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11562d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11562da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11562ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11562e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11562e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11562ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11562f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11562f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11562fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11562ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1156303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115630870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115630d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1156311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115631650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115631af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115631f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x115632430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1156328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x115632d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x115633210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1156336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x115633b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x115633ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x115634490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x115634930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x115634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x115635270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x115635710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x115635bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x115636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1156364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x115636990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x115636e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1156372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x115637770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x115637c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1156380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x115638550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1156389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115638e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115639330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1156397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115639c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11563a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11563a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11563aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11563afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11563b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11563ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11563bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11563c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11563c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11563ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11563d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11563da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11563e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11563e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11563ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11563f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11563f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11563fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115640370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1156408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115640e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115641360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1156418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115641e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x115642350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1156428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115642df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115643340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115643890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115643de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115644330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115644880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115644dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115645320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115645870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x115645dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x115646310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x115646860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115646db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115647300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x115647850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115647da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1156482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x115648840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115648d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1156492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115649830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115649d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11564a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11564a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11564ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11564b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11564b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11564bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11564c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11564c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11564cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11564d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11564d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11564dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11564e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11564e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11564ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11564f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11564f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11564fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1156507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115650d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115651260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1156517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115651d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115652250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1156527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115652c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1156530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115653580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115653a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115653ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115654360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115654800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115654ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115655140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1156555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115655a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115655f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1156563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115656910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115657030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115657750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115657e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115658590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115658850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115658e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115659470 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.050 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115504ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115504f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1155053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x115505830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x115505ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x115506110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x115506580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1155069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x115506e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x115507360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1155077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x115507e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x115508970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x115509120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x115509930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11550a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11550a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11550ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11550b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11550bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11550c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11550cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11550d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11550da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11550e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11550e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11550e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11550eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11550ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11550f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11550f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11550fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115510200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1155104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115510930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115510da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115511210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x115511680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115511af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115511f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1155123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115512840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x115512cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115513120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x115513590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115513a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115513e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1155142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x115514750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x115514bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x115515030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1155154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x115515910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x115515d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1155161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x115516660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x115516bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1155170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x115517540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1155179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x115517e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115518290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x115518700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x115518b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x115518fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x115519450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1155198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x115519d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11551a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11551a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11551aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11551aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11551b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11551b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11551bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11551c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11551c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11551c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11551ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11551d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11551d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11551db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11551dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11551e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11551e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11551ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11551f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11551f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11551fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11551fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115520340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1155207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115520c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115521090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115521500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115521970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x115521de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115522250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1155226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115522b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115522fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115523410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115523880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115523cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115524160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1155245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115524a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115524eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115525320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115525790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115525c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115526070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1155264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115526950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x115526dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115527230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1155276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115527b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115527f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1155283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115528860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115528cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115529140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1155295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x115529a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x115529e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11552a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11552a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11552abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11552b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11552b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11552b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11552bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11552c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11552c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11552caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11552cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11552d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11552d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11552dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11552e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11552e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11552ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11552ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11552f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11552f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11552fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115530030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1155304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115530910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115530d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1155311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115531660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115531ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115531f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1155323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115532820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115532c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115533100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115533570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1155339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115533e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1155342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115534730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115534ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x115535010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115535480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115536010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1155362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115536590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115536a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115536e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1155372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115537750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x115537bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115538030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1155384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115538910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115538d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1155391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115539660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115539ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115539f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11553a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11553a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11553ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11553b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11553b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11553b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11553be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11553c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11553c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11553cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11553d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11553d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11553d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11553dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11553e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11553e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11553eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11553ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11553f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11553f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11553fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1155400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115540550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1155409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115540e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1155412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115541710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x115541b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115541ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115542460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1155428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115542d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1155431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115543620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x115543a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115543f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115544370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1155447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115544c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1155450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115545530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1155459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115545e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115546280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1155466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115546b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115546fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115547440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1155478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115547d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115548190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115548600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115548a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115548ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115549350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115549e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11554a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11554acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11554b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11554b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11554b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11554bde0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115649400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115649870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x115649ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11564a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11564a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11564aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11564aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11564b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11564b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11564bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11564c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11564c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11564cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11564d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11564de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11564e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11564ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11564f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11564fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1156503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x115650ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1156511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1156518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x115651f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x115652680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x115652af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x115652f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1156533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115653840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115653cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x115654120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115654590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115654a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115654cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115655130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1156555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115655a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x115655e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1156562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115656760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x115656bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115657040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1156574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115657920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x115657d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115658200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115658670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x115658ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x115658f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1156593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11560b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11560bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11560acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11560b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1156098f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11560a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x115617640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x115617ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x115617f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x115618390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x115618800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115618c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1156190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x115619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1156199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x115619e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11561a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11561a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11561ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11561aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11561b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11561b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11561bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11561c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11561c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11561ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11561cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11561d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11561d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11561dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11561e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11561e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11561e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11561ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11561f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11561f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11561fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11561ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115620440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1156208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115620d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115621190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115621600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115621a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115621ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115622350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1156227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115622c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1156230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115623510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115623980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115623df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115624260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1156246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115624b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115624fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115625420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115625d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115626170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1156265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115626a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x115626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115627330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1156277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115627c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115628080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1156284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115628960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x115628dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1156296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115629b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x115629f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11562a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11562a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11562ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11562b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11562b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11562ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11562bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11562c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11562c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11562cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11562d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11562d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11562d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11562ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11562e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11562e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11562eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11562ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11562f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11562f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11562fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115630130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1156305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115630a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115630e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1156312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115631bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115632040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1156324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115632920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115632d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115633200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115633670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115633ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1156343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115634830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115634ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115635110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115635580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1156359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115635e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1156365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115636a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115636ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115637330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1156377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x115637c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115638080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1156384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115638960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115639240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1156396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115639b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115639f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11563a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11563a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11563ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11563b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11563b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11563ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11563bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11563c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11563c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11563cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11563d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11563d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11563d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11563ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11563e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11563e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11563eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11563ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11563f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11563f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11563fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x115640130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1156405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115640a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115640e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1156412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115641760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x115641bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115642040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1156424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115642920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115642d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x115643200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115643670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x115643ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115643f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1156443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115644830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115644ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x115645110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115645580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1156459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115645e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1156462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115646740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115646bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115647020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115647490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115647900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115647d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1156481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115648650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115648ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115615e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1156162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115616730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115616ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11560d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11560dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11560e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11560ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11560ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11560f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11560f770 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.870s
user	0m0.290s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4200 (46c69e0e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b00a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b00a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b00ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b00b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b00b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b00be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b00c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b00c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b00cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b00d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b00d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b00dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b00e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b00f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b00f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b0100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b0107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b010ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b011600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b011dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b0124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b012c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b013330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b013bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b0142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b0145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b014bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b015830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b015d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b016030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b0164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b016790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b017020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b017560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b017820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b017cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b018160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b018600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b018aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b018f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b0193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b019880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b019d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b01a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b01a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b01aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b01b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b01b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b01bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b01c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b01cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b01d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b01d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b01de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b01e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b01eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b01ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b01f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b01f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b020010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b0202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b020770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b020c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b0210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b021550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b0219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b021e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b022330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b0227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b022c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b023110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b0235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b023a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b023ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b024390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b024830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b024cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b025170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b025610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b025ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b025f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b0263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b026890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b026d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b0271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b027b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b027fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b028450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b0288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b028d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b029230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b0296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b029b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b02a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b02a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b02a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b01b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b02afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b02b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b02b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b02bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b02c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b02c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b02cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b02d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b02d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b02d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b02dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b02e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b02e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b02ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b02f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b02f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b02f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14b02fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14b0302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14b030780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14b030c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14b0310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14b031560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14b031a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14b031ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14b032340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14b0327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14b032c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14b033120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14b0335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14b033a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14b033f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14b0343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14b034840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14b034ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14b035180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14b035620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14b035ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14b035f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14b036400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14b0368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14b036d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14b0371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14b037680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14b037b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14b037fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14b038460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14b038900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14b038da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14b039240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14b0396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14b039b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14b03a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14b03a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14b03a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14b03aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14b03b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14b03b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14b03bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14b03c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14b03c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14b03cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14b03d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14b03d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14b03dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14b03e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14b03ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14b03f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14b03f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14b03fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14b040280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14b0407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14b040d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14b041270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14b0417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14b041d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14b042260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14b0427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14b042d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14b043250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14b0437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14b043cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14b044240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14b044790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14b044ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14b045230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14b045780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14b045cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14b046220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14b046770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14b046cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14b047210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14b047760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14b047cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14b048200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14b048750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14b048ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14b0491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14b049740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14b049c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14b04a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14b04a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14b04ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14b04b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14b04b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14b04bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14b04c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14b04c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14b04cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14b04d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14b04d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14b04dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14b04e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14b04e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14b04ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14b04f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14b04f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14b04fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14b050180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14b0506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14b050c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14b051170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14b0516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14b051c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14b052160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14b0526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14b052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14b052ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14b053490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14b053930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14b053dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14b054270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14b054710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14b054bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14b055050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14b0554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14b055990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14b055e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14b0562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14b056820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14b056f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14b057660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14b057d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14b0584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14b058760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14b058d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14b059380 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.083.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149f0aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149f0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149f0b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149f0b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149f0bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149f0c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149f0c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149f0c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149f0cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149f0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149f0d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149f0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149f0e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149f0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149f0f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149f10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149f10770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149f10e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149f115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149f11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149f124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149f12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149f132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149f13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149f14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149f143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149f146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149f14b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149f14f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149f153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149f158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149f15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149f16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149f16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149f169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149f16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149f17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149f17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149f17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149f18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149f18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149f18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149f19170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149f19670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149f19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149f19fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149f1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149f1a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149f1ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149f1b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149f1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149f1ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149f1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149f1c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149f1c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149f1cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149f1d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149f1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149f1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149f1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149f1ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149f1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149f1f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149f1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149f200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149f20560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149f20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149f20ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149f21340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149f217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149f21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149f22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149f225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149f22a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149f22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149f233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149f23840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149f23ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149f24180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149f24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149f24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149f24f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149f25400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149f258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149f25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149f261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149f26680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149f26b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149f26fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149f27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149f27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149f27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149f28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149f286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149f28b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149f29020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149f294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149f29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149f29e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149f2a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149f2a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149f2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149f2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149f2b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149f2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149f2be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149f2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149f2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149f2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149f2d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149f2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149f2da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149f2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149f2e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149f2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149f2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149f2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149f2f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149f2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149f2ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149f303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149f30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149f311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149f31640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149f31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149f31f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149f32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149f328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149f32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149f33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149f336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149f33b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149f33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149f34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149f34920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149f34dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149f35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149f35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149f35ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149f36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149f364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149f36980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149f36e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149f372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149f37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149f37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149f380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149f38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149f389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149f38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149f39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149f39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149f39dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149f3a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149f3a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149f3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149f3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149f3b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149f3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149f3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149f3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149f3d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149f3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149f3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149f3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149f3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149f3ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149f3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149f3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149f3fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149f40180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149f406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149f40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149f41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149f416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149f41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149f42160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149f426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149f42c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149f43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149f436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149f43bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149f44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149f44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149f44be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149f45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149f45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149f45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149f46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149f46670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149f46bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149f47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149f47660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149f47bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149f48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149f48650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149f48ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149f490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149f49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149f49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149f4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149f4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149f4ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149f4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149f4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149f4bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149f4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149f4c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149f4cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149f4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149f4d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149f4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149f4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149f4e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149f4eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149f4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149f4f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149f4fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149f50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149f505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149f50b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149f51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149f51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149f519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149f51e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149f522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149f52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149f52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149f530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149f53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149f53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149f53eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149f54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149f547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149f54c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149f551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149f56020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149f56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149f56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149f57120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149f57730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149f57d40 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149e05510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149e05980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149e05df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149e06260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149e066d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149e06b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149e06fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149e07420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149e07890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149e07d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149e08170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149e08890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149e093b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149e09b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149e0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149e0aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149e0b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149e0b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149e0bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149e0c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149e0ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149e0d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149e0dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149e0e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149e0eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149e0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149e0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149e0f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149e0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149e0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149e10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149e10730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149e10ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149e10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149e112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149e11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149e11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149e12020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149e12490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149e12900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149e12d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149e131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149e13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149e13ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149e13f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149e143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149e14810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149e14c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149e150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149e15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149e159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149e15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149e162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149e16720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149e16b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149e17000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149e17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149e17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149e17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149e18350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149e187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149e18c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149e190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149e19510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149e19980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149e19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149e1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149e1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149e1ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149e1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149e1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149e1b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149e1bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149e1c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149e1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149e1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149e1cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149e1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149e1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149e1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149e1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149e1e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149e1e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149e1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149e1f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149e1f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149e1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149e1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149e20400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149e20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149e20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149e21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149e215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149e21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149e21ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149e22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149e22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149e22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149e23060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149e234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149e23940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149e23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149e24220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149e24690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149e24b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149e24f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149e253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149e25850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149e25cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149e26130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149e265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149e26a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149e26e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149e272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149e27760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149e27bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149e28040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149e284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149e28920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149e28d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149e29200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149e29670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149e29ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149e29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149e2a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149e2a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149e2aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149e2b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149e2b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149e2b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149e2be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149e2c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149e2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149e2cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149e2d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149e2d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149e2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149e2dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149e2e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149e2e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149e2eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149e2ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149e2f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149e2f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149e2fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149e300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149e30560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149e309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149e30e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149e312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149e31720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149e31b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149e32000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149e32470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149e328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149e32d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149e331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149e33630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149e33aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149e33f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149e34380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149e347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149e34c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149e350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149e35540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149e359b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149e35e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149e369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149e36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149e36f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149e373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149e37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149e37c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149e380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149e38560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149e389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149e38e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149e392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149e39720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149e39b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149e3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149e3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149e3a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149e3ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149e3b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149e3b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149e3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149e3bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149e3c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149e3c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149e3cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149e3d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149e3d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149e3d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149e3de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149e3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149e3e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149e3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149e3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149e3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149e3f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149e3fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149e401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149e40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149e40a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149e40ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149e41360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149e417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149e41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149e420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149e42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149e42990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149e42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149e43270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149e436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149e43b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149e43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149e44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149e448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149e44d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149e45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149e455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149e45a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149e45ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149e46340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149e467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149e46c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149e47090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149e47500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149e47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149e47de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149e48250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149e486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149e48b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149e48fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149e49410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149e49880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149e49cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149e4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149e4af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149e4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149e4bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149e4c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149e4c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149e4c780 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.907s
user	0m0.240s
sys	0m0.128s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.53 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.10 sec*proc (2 tests)

Total Test time (real) =   1.10 sec
        1.12 real         0.71 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.27 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.15 user         0.04 sys
```
